# For v0.1 workload: WnD, DIEN
# For v0.2 workload: BERT, ResNet
FROM intel/oneapi-aikit:2022.3.0-devel-ubuntu18.04

SHELL ["/bin/bash", "-c"]
WORKDIR /root/
ENV PATH /opt/intel/oneapi/intelpython/latest/condabin:$PATH
RUN apt-get update -y && apt-get install -y git openssh-server gcc-8 g++-8 \
        openssh-server openjdk-8-jdk sshpass vim \
        && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 7 \
        && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 8 \
        && update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 7 \
        && update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 8

SHELL ["conda", "run", "-n", "tensorflow", "/bin/bash", "-c"]
RUN python -m pip install --no-cache-dir --ignore-installed psutil sklearn \
        tqdm prefetch_generator absl-py pydot dill typing-extensions~=3.7.4 \
        six~=1.15.0 h5py~=3.1.0 grpcio~=1.34.0 pandas future sigopt pytest
RUN source /opt/intel/oneapi/setvars.sh --ccl-configuration=cpu_icc --force \
        && conda activate tensorflow \
        && python -m pip install --no-cache-dir intel-tensorflow==2.10 \
        && HOROVOD_WITHOUT_MPI=1 HOROVOD_CPU_OPERATIONS=CCL \
        HOROVOD_WITHOUT_MXNET=1 HOROVOD_WITHOUT_PYTORCH=1 HOROVOD_WITH_TENSORFLOW=1 \
        python -m pip install --no-cache-dir horovod \
        && python -m pip install --no-cache-dir --no-deps tensorflow-transform==0.24.1 tensorflow-metadata==0.14.0
SHELL ["conda", "run", "-n", "tensorflow", "/bin/bash", "-c"]
RUN python -m pip install "git+https://github.com/mlperf/logging.git@1.0.0"

SHELL ["conda", "run", "-n", "base", "/bin/bash", "-c"]
RUN python -m pip install --no-cache-dir --ignore-installed sigopt pytest
RUN python -m pip install pyrecdp pyarrow notebook

SHELL ["/bin/bash", "-c"]
RUN sed -i 's/#Port 22/Port 12344/g' /etc/ssh/sshd_config
RUN sed -i 's/#   Port 22/    Port 12344/g' /etc/ssh/ssh_config
RUN echo 'PermitRootLogin yes' >> /etc/ssh/sshd_config
# install spark
RUN wget -qO- https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz | tar xvz -C /home/
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV SPARK_HOME /home/spark-3.2.1-bin-hadoop3.2
ENV PYTHONPATH $SPARK_HOME/python/:$PYTHONPATH
ENV PYTHONPATH $SPARK_HOME/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH
ADD spark/spark-defaults.conf /home/spark-3.2.1-bin-hadoop3.2/conf/spark-defaults.conf
ADD spark/start_spark_service.sh /home/start_spark_service.sh
RUN chmod +x /home/start_spark_service.sh
ADD spark/spark-env.sh /home/spark-env.sh
RUN echo "if [ -z \$SPARK_HOME ]; then source /home/spark-env.sh; fi" >> /etc/bash.bashrc
RUN mkdir -p /home/vmagent/app/e2eaiok/spark_data_processing/spark_local_dir
RUN mkdir -p /home/mnt/applicationHistory
RUN conda init bash
RUN echo "source /opt/intel/oneapi/setvars.sh --ccl-configuration=cpu_icc --force" >> /etc/bash.bashrc
RUN echo "root:docker" | chpasswd
ENTRYPOINT [""]
