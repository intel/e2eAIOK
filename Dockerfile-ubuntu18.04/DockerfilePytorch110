# For v0.2 workload: RNNT
FROM intel/oneapi-aikit:2022.2-devel-ubuntu18.04

SHELL ["/bin/bash", "-c"]
WORKDIR /root/
ENV PATH /opt/intel/oneapi/intelpython/latest/condabin:$PATH
RUN apt-get update -y && apt-get install -y libsndfile1-dev openssh-server openjdk-8-jdk sshpass

SHELL ["conda", "run", "-n", "pytorch-1.10.0", "/bin/bash", "-c"]
RUN python -m pip install pyyaml typing cffi cmake ninja numpy Unidecode prefetch_generator \
            onnx tqdm inflect sentencepiece librosa tensorboard sigopt pytest
RUN python -m pip install oneccl_bind_pt==1.10.0 -f https://developer.intel.com/ipex-whl-stable
RUN python -m pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110==1.9.0
RUN python -m pip install git+https://github.com/NVIDIA/dllogger#egg=dllogger
RUN python -m pip install "git+https://github.com/mlperf/logging.git@1.0.0"
RUN git clone https://github.com/HawkAaron/warp-transducer && cd warp-transducer \
    && mkdir build && cd build \
    && cmake .. && make && cd ../pytorch_binding \
    && python setup.py install
RUN python -m pip install torchvision torchsummary easydict opencv-python scikit-image timm boto3 ptflops sentencepiece
RUN python -m pip install torchaudio==0.10.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html

SHELL ["conda", "run", "-n", "base", "/bin/bash", "-c"]
RUN python -m pip install sigopt pytest pyarrow notebook pyrecdp
RUN python -m pip install e2eAIOK --pre
RUN python -m pip install e2eAIOK-sda --pre --no-deps --ignore-installed

SHELL ["/bin/bash", "-c"]
RUN sed -i 's/#Port 22/Port 12345/g' /etc/ssh/sshd_config
RUN sed -i 's/#   Port 22/    Port 12345/g' /etc/ssh/ssh_config
RUN echo 'PermitRootLogin yes' >> /etc/ssh/sshd_config
RUN wget -qO- https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz | tar xvz -C /home/
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV SPARK_HOME /home/spark-3.2.1-bin-hadoop3.2
ENV PYTHONPATH $SPARK_HOME/python/:$PYTHONPATH
ENV PYTHONPATH $SPARK_HOME/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH
ADD spark/spark-defaults.conf /home/spark-3.2.1-bin-hadoop3.2/conf/spark-defaults.conf
ADD spark/start_spark_service.sh /home/start_spark_service.sh
RUN chmod +x /home/start_spark_service.sh
ADD spark/spark-env.sh /home/spark-env.sh
RUN echo "if [ -z \$SPARK_HOME ]; then source /home/spark-env.sh; fi" >> /etc/bash.bashrc
RUN mkdir -p /home/vmagent/app/e2eaiok/spark_data_processing/spark_local_dir
RUN mkdir -p /home/mnt/applicationHistory
RUN conda init bash
RUN echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc
RUN echo "root:docker" | chpasswd
ENTRYPOINT [""]