{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2b8b2b",
   "metadata": {},
   "source": [
    "# WnD Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611394e",
   "metadata": {},
   "source": [
    "Recommendation systems drive engagement on many of the most popular online platforms. As the volume of data available to power these systems grows exponentially, users are increasingly turning from more traditional machine learning methods to highly expressive deep learning models to improve the quality of recommendations. Google's Wide and Deep recommender system is a popular model for recommendation problems for its robustness to signal sparsity.\n",
    "This notebook contains step by step guide on how to optimize WnD model with IntelÂ® End-to-End AI Optimization Kit, and detailed performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a783f5e",
   "metadata": {},
   "source": [
    "# Content\n",
    "* [Model Architecture](#Model-Architecture)\n",
    "* [Optimizations](#Optimizations)\n",
    "* [Performance Overview](#Performance-Overview)\n",
    "* [DEMO](#DEMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df0609",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "<img src=\"./img/wnd.png\" width=\"800\"/>\n",
    "\n",
    "Wide and Deep model was published by Google at 2016. It jointly train wide linear models and deep neural networks, combined the benefits of memorization and generalization for recommender system. It's the first time to introduce neural network to CTR model.\n",
    "\n",
    "The wide component is a generalized linear model. The feature set includes raw input features and transformed features\n",
    "The deep component is a feed-forward neural network. The sparse, high-dimensional categorical features are first converted into an embedding vector and fed into the hidden layers of a neural network in the forward pass\n",
    "The wide component and deep component are combined using a weighted sum of their output log odds as the prediction and fed to logistic loss function for joint training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1f075",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad100f",
   "metadata": {},
   "source": [
    "### Distributed Training\n",
    "\n",
    "Use horovod for distributed training and mpirun to launch training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18b1d8",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "Long idle time per training step for horovod communication, horovod paramter sync consume much time during distributed training, causing poor scaling performance. The overhead mainly caused by large embedding table.\n",
    "\n",
    "<img src=\"./img/wnd_profile.png\" width=\"600\"/><figure>Distributed training profiling</figure>\n",
    "\n",
    "Replace custom layer (contains embedding layer) with TensorFlow dense layer help to reduce embedding parameter size, thus reduce parameter size needed to sync by horovod, fix horovod poor scaling issue. Per step training time reduced from 5.16s to 2.71s, got about 1.9x speedup.\n",
    "\n",
    "<img src=\"./img/wnd_traintime_custom_emd.png\" width=\"600\"/><figure>custom layer</figure>\n",
    "<img src=\"./img/wnd_traintime_tf_emd.png\" width=\"600\"/><figure>TensorFlow build-in layer</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19cd46",
   "metadata": {},
   "source": [
    "### Horovod Optimization With OneCCL\n",
    "\n",
    "Deep part embedding table cost long time hovorod communication, and Allgather is the most time-consuming operation. Enable Intel OneCCL in horovod helps to reduce Allgather time consumption, which delivers 1.2x speedup.\n",
    "\n",
    "<img src=\"./img/wnd_woccl.png\" width=\"600\"/><figure>horovod timeline profiling w/o OneCCL</figure>\n",
    "<img src=\"./img/wnd_wccl.png\" width=\"600\"/><figure>horovod timeline profiling w/ OneCCL</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1aaac4",
   "metadata": {},
   "source": [
    "### Framework Related Optimization\n",
    "\n",
    "set CCL affinity, horovod thread affinity, MPI socket binding, KMP affinity, OMP_NUM_THREADS\n",
    "\n",
    "```bash\n",
    "export CCL_WORKER_COUNT=2 # set CCL thread number\n",
    "export CCL_WORKER_AFFINITY=\"16,17,34,35\" # set CCL thread affinity\n",
    "export HOROVOD_THREAD_AFFINITY=\"53,71\" # set horovod thread affinity\n",
    "export I_MPI_PIN_DOMAIN=socket # set socket binding for MPI\n",
    "export I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=\"16,17,34,35,52,53,70,71\" # exclude CCL threads\n",
    "\n",
    "mpirun -genv OMP_NUM_THREADS=16 -map-by socket -n 2 -ppn 2 -hosts localhost -genv I_MPI_PIN_DOMAIN=socket -genv OMP_PROC_BIND=true -genv KMP_BLOCKTIME=1 -genv KMP_AFFINITY=granularity=fine,compact,1,0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90924745",
   "metadata": {},
   "source": [
    "### Early Stop\n",
    "\n",
    "Training baseline MAP stopped at 0.6553, with optimizations on training process, model converge faster and achieve 0.6553 MAP at 1.5K steps, no need to training to 9K steps. Enable early stop at 0.6553 MAP.\n",
    "\n",
    "<img src=\"./img/wnd_map_GPU.png\"/><figure>baseline metric curv</figure>\n",
    "<img src=\"./img/wnd_early_stop_cpu.png\"/><figure>optimized metric curv</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b7082",
   "metadata": {},
   "source": [
    "### Input Pipeline Optimization\n",
    "\n",
    "Training needs more system resources while input pipeline not, the resources preemption between input pipeline and training caused performance overhead. By reducing system resources allocated for input pipeline to free more resources for training, input pipeline time consuming reduced from 8.2% to 3.2% among entire training time.\n",
    "\n",
    "<img src=\"./img/wnd_input_pipeline_orig.png\" width=\"600\"/><figure>original profiling</figure>\n",
    "<img src=\"./img/wnd_input_pipeline_opt.png\" width=\"600\"/><figure>optimized profiling</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e3f6a",
   "metadata": {},
   "source": [
    "### HPO With SDA (Smart Democratization Advisor)\n",
    "\n",
    "SDA config\n",
    "\n",
    "```\n",
    "Parameters for SDA auto optimization:\n",
    "- dnn_hidden_unit1: [64, 128, 256, 512] #layer width of dnn_hidden_unit1\n",
    "- dnn_hidden_unit2: [64, 128, 256, 512] #layer width of dnn_hidden_unit2\n",
    "- dnn_hidden_unit3: [64, 128, 256, 512] #layer width of dnn_hidden_unit3\n",
    "- deep_learning_rate: 0.0001~0.1 #deep part learning rate\n",
    "- linear_learning_rate: 0.01~1.0 #linear part learning rate\n",
    "- deep_warmup_epochs: 1~8 #deep part warmup epochs\n",
    "- deep_dropout: 0~0.5 #deep part dropout\n",
    "metrics:\n",
    "- name: training_time # training time threshold\n",
    "  objective: minimize\n",
    "  threshold: 1800\n",
    "- name: MAP # training metric threshold\n",
    "  objective: maximize\n",
    "  threshold: 0.6553\n",
    "metric:\n",
    "- name: MAP\n",
    "  threshold: 0.6553\n",
    "```\n",
    "\n",
    "request suggestions from SDA\n",
    "\n",
    "```python\n",
    "suggestion = self.conn.experiments(self.experiment.id).suggestions().create()\n",
    "```\n",
    "\n",
    "<img src=\"./img/wnd_sda.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f0c0c",
   "metadata": {},
   "source": [
    "## Performance Overview\n",
    "\n",
    "<img src=\"./img/wnd_perf.png\" width=\"900\"/>\n",
    "\n",
    "* Intel optimized TensorFlow: apply OpenMP and KMP optimizations (AFFINITY, NUM_THREADS etc.) for CPU\n",
    "* Distributed training: horovod scaling delivered 1.93x speedup from 1 node to 4 nodes, got poor scaling performance\n",
    "* Model optimization: reducing sparse embedding size helped to reduce horovod communication data size, delivered better scaling performance, 4 nodes training delivered 2.7x speed up over 1 node\n",
    "* Lighter model: reducing deep hidden unit from [1024, 1024, 1024, 1024, 1024] to [1024, 512, 256] delivered 1.14x speedup\n",
    "* Early stop: stop training when MAP@12 reached pre-defined value (0.6553) , training took 904 steps delivered 4.14x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9c741",
   "metadata": {},
   "source": [
    "# DEMO\n",
    "* [Environment Setup](#Environment-Setup)\n",
    "* [Data Process](#Data-Process)\n",
    "* [Launch Training](#Launch-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca039366",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "``` bash\n",
    "# Setup ENV\n",
    "git clone https://github.com/intel/e2eAIOK.git\n",
    "cd e2eAIOK\n",
    "git submodule update --init --recursive\n",
    "python3 scripts/start_e2eaiok_docker.py -b tensorflow -w ${host0} ${host1} ${host2} ${host3} --proxy \"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd9f17",
   "metadata": {},
   "source": [
    "## Enter Docker\n",
    "```\n",
    "sshpass -p docker ssh ${host0} -p 12344\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9a8f5",
   "metadata": {},
   "source": [
    "## Workflow Prepare\n",
    "\n",
    "``` bash\n",
    "# prepare model codes\n",
    "cd /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2\n",
    "bash patch_wnd.patch\n",
    "\n",
    "# Download Dataset\n",
    "# download and unzip dataset from https://www.kaggle.com/c/outbrain-click-prediction/data to /home/vmagent/app/dataset/outbrain/orig\n",
    "\n",
    "# source spark env\n",
    "source /home/spark-env.sh\n",
    "\n",
    "# Start services\n",
    "# only if there is no spark service running, may check ${localhost}:8080 to confirm\n",
    "/home/start_spark_service.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47448679",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0215585f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 22:02:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/31 22:02:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "Drop rows with empty \"geo_location\"...\n",
      "Drop rows with empty \"platform\"...\n",
      "valid_set_df time: 38.694966077804565                                           ]\n",
      "train_set_df time: 42.35809636116028                                            1]\n",
      "train/test dataset generation time: 95.60888910293579\n",
      "22/10/31 22:04:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "test_set_enriched_df time: 67.92218327522278                                    0]]0]\n",
      "train_set_enriched_df time: 83.92503476142883                                   \n",
      "WARNING:tensorflow:From /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2/data/outbrain/spark/preproc.py:654: from_feature_spec (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "from_feature_spec is a deprecated, use schema_utils.schema_from_feature_spec\n",
      "2022-10-31 22:09:16.917781\tComputing min and max\n",
      "[Row(min(ad_views)=0, max(ad_views)=144659, min(doc_views)=0, max(doc_views)=556631, min(doc_event_days_since_published)=0.0, max(doc_event_days_since_published)=3650.0, min(doc_ad_days_since_published)=0.0, max(doc_ad_days_since_published)=3648.0)]\n",
      "feature engineering time: 328.1621606349945                                     \n",
      "data convert time: 178.53657913208008                                           \n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2; sh scripts/spark_preproc.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d6539",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c36c2",
   "metadata": {},
   "source": [
    "edit conf/e2eaiok_defaults_wnd_example.conf\n",
    "\n",
    "```\n",
    "### GLOBAL SETTINGS ###\n",
    "observation_budget: 1\n",
    "save_path: /home/vmagent/app/e2eaiok/result/\n",
    "ppn: 2\n",
    "ccl_worker_num: 2\n",
    "global_batch_size: 524288\n",
    "num_epochs: 20\n",
    "cores: 104\n",
    "iface: lo\n",
    "hosts:\n",
    "- localhost\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569bf322",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data format is tfrecords\n",
      "2022-10-31 22:20:33,833 - E2EAIOK.SDA - INFO - ### Ready to submit current task  ###\n",
      "2022-10-31 22:20:33,833 - E2EAIOK.SDA - INFO - Model Advisor created\n",
      "2022-10-31 22:20:33,833 - E2EAIOK.SDA - INFO - model parameter initialized\n",
      "2022-10-31 22:20:33,833 - E2EAIOK.SDA - INFO - start to launch training\n",
      "2022-10-31 22:20:33,833 - sigopt - INFO - training launch command: mpirun -genv OMP_NUM_THREADS=24 -map-by socket -n 2 -ppn 2 -hosts localhost -print-rank-map -genv I_MPI_PIN_DOMAIN=socket -genv OMP_PROC_BIND=true -genv KMP_BLOCKTIME=1 -genv KMP_AFFINITY=granularity=fine,compact,1,0 /opt/intel/oneapi/intelpython/latest/envs/tensorflow/bin/python -u /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2/main.py --results_dir /home/vmagent/app/e2eaiok/result --model_dir /home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c --train_data_pattern '/home/vmagent/app/dataset/outbrain/train/part*' --eval_data_pattern '/home/vmagent/app/dataset/outbrain/valid/part*' --dataset_meta_file /home/vmagent/app/dataset/outbrain/outbrain_meta.yaml --global_batch_size 524288 --eval_batch_size 524288 --num_epochs 20 --metric MAP --metric_threshold 0.6553 --linear_learning_rate 0.8 --deep_learning_rate 0.00048 --deep_warmup_epochs 6 --deep_hidden_units 1024 512 256 --deep_dropout 0.1 \n",
      "(localhost:0,1)\n",
      "\n",
      "rank: 0\n",
      "WARNING:tensorflow:command line arguments: {\"train_data_pattern\": \"/home/vmagent/app/dataset/outbrain/train/part*\", \"eval_data_pattern\": \"/home/vmagent/app/dataset/outbrain/valid/part*\", \"dataset_meta_file\": \"/home/vmagent/app/dataset/outbrain/outbrain_meta.yaml\", \"model_dir\": \"/home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c\", \"results_dir\": \"/home/vmagent/app/e2eaiok/result\", \"global_batch_size\": 524288, \"eval_batch_size\": 524288, \"num_epochs\": 20, \"amp\": false, \"xla\": false, \"linear_learning_rate\": 0.8, \"deep_learning_rate\": 0.00048, \"deep_warmup_epochs\": 6.0, \"metric\": \"MAP\", \"metric_threshold\": 0.6553, \"deep_hidden_units\": [1024, 512, 256], \"deep_dropout\": 0.1, \"evaluate\": false, \"use_checkpoint\": false, \"benchmark\": false, \"benchmark_warmup_steps\": 500, \"benchmark_steps\": 1000}\n",
      "All feature columns: ['doc_event_days_since_published_log_01scaled', 'doc_ad_days_since_published_log_01scaled', 'doc_event_doc_ad_sim_categories', 'doc_event_doc_ad_sim_topics', 'doc_event_doc_ad_sim_entities', 'pop_document_id', 'pop_publisher_id', 'pop_source_id', 'pop_ad_id', 'pop_advertiser_id', 'pop_campain_id', 'doc_views_log_01scaled', 'ad_views_log_01scaled', 'ad_id', 'campaign_id', 'doc_event_id', 'event_platform', 'doc_id', 'ad_advertiser', 'doc_event_source_id', 'doc_event_publisher_id', 'doc_ad_source_id', 'doc_ad_publisher_id', 'event_geo_location', 'event_country', 'event_country_state', 'display_id']\n",
      "2022-10-31 22:20:35.999767: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "rank: 0\n",
      "WARNING:tensorflow:command line arguments: {\"train_data_pattern\": \"/home/vmagent/app/dataset/outbrain/train/part*\", \"eval_data_pattern\": \"/home/vmagent/app/dataset/outbrain/valid/part*\", \"dataset_meta_file\": \"/home/vmagent/app/dataset/outbrain/outbrain_meta.yaml\", \"model_dir\": \"/home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c\", \"results_dir\": \"/home/vmagent/app/e2eaiok/result\", \"global_batch_size\": 524288, \"eval_batch_size\": 524288, \"num_epochs\": 20, \"amp\": false, \"xla\": false, \"linear_learning_rate\": 0.8, \"deep_learning_rate\": 0.00048, \"deep_warmup_epochs\": 6.0, \"metric\": \"MAP\", \"metric_threshold\": 0.6553, \"deep_hidden_units\": [1024, 512, 256], \"deep_dropout\": 0.1, \"evaluate\": false, \"use_checkpoint\": false, \"benchmark\": false, \"benchmark_warmup_steps\": 500, \"benchmark_steps\": 1000}\n",
      "All feature columns: ['doc_event_days_since_published_log_01scaled', 'doc_ad_days_since_published_log_01scaled', 'doc_event_doc_ad_sim_categories', 'doc_event_doc_ad_sim_topics', 'doc_event_doc_ad_sim_entities', 'pop_document_id', 'pop_publisher_id', 'pop_source_id', 'pop_ad_id', 'pop_advertiser_id', 'pop_campain_id', 'doc_views_log_01scaled', 'ad_views_log_01scaled', 'ad_id', 'campaign_id', 'doc_event_id', 'event_platform', 'doc_id', 'ad_advertiser', 'doc_event_source_id', 'doc_event_publisher_id', 'doc_ad_source_id', 'doc_ad_publisher_id', 'event_geo_location', 'event_country', 'event_country_state', 'display_id']\n",
      "2022-10-31 22:20:36.034748: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:deep columns: 26\n",
      "WARNING:tensorflow:wide columns: 26\n",
      "WARNING:tensorflow:wide&deep intersection: 13\n",
      "WARNING:tensorflow:deep columns: 26\n",
      "WARNING:tensorflow:wide columns: 26\n",
      "WARNING:tensorflow:wide&deep intersection: 13\n",
      "INFO:tensorflow:Steps per epoch: 113\n",
      "2022-10-31 22:20:36.887454: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-31 22:20:36.888542: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz\n",
      "INFO:tensorflow:Steps per epoch: 113\n",
      "2022-10-31 22:20:36.942484: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-31 22:20:36.943445: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "OMP: Warning #181: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined\n",
      "OMP: Warning #181: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined\n",
      "INFO:tensorflow:step: 0, {'binary_accuracy': '0.6016', 'auc': '0.5021', 'loss': '0.7530', 'time': '24.2400'}\n",
      "INFO:tensorflow:step: 0, {'binary_accuracy': '0.4571', 'auc': '0.4836', 'loss': '0.9701', 'time': '24.6243'}\n",
      "WARNING:tensorflow:From /opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:From /opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "INFO:tensorflow:step: 4, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.5337937, 'loss_val': 0.50193626, 'map_val': 0.49194891763793175}\n",
      "INFO:tensorflow:step: 4, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.495642, 'loss_val': 0.5045759, 'map_val': 0.4620609457200919}\n",
      "INFO:tensorflow:step: 8, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.5974096, 'loss_val': 0.4947621, 'map_val': 0.5552392365429155}\n",
      "INFO:tensorflow:step: 8, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.58074886, 'loss_val': 0.4968695, 'map_val': 0.5292482514281001}\n",
      "INFO:tensorflow:step: 10, {'binary_accuracy': '0.7384', 'auc': '0.5264', 'loss': '0.6194', 'time': '105.5927'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step: 10, {'binary_accuracy': '0.7393', 'auc': '0.5222', 'loss': '0.6132', 'time': '106.3577'}\n",
      "INFO:tensorflow:step: 12, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6279781, 'loss_val': 0.489207, 'map_val': 0.5816698235747075}\n",
      "INFO:tensorflow:step: 12, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.624491, 'loss_val': 0.4853109, 'map_val': 0.5727072315939128}\n",
      "INFO:tensorflow:step: 16, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6489798, 'loss_val': 0.4800695, 'map_val': 0.5976102011193667}\n",
      "INFO:tensorflow:step: 16, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6449559, 'loss_val': 0.48311973, 'map_val': 0.5906039729331615}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy_val': 0.80656815, 'auc_val': 0.6591209, 'loss_val': 0.47905082, 'map_val': 0.6038914937133115}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.65709734, 'loss_val': 0.48283547, 'map_val': 0.6003330487114618}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy': '0.7852', 'auc': '0.5649', 'loss': '0.5570', 'time': '95.7254'}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy': '0.7938', 'auc': '0.5718', 'loss': '0.5482', 'time': '96.7700'}\n",
      "INFO:tensorflow:step: 24, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.66769165, 'loss_val': 0.47429454, 'map_val': 0.6102883407523839}\n",
      "INFO:tensorflow:step: 24, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6661933, 'loss_val': 0.4815665, 'map_val': 0.6071224360962856}\n",
      "INFO:tensorflow:step: 28, {'binary_accuracy_val': 0.80659485, 'auc_val': 0.67344606, 'loss_val': 0.47240043, 'map_val': 0.6137836892131214}\n",
      "INFO:tensorflow:step: 28, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.673699, 'loss_val': 0.4774559, 'map_val': 0.6129478570255393}\n",
      "INFO:tensorflow:step: 30, {'binary_accuracy': '0.7665', 'auc': '0.6061', 'loss': '0.5614', 'time': '91.7695'}\n",
      "INFO:tensorflow:step: 30, {'binary_accuracy': '0.7707', 'auc': '0.6080', 'loss': '0.5432', 'time': '93.1394'}\n",
      "INFO:tensorflow:step: 32, {'binary_accuracy_val': 0.80659676, 'auc_val': 0.676975, 'loss_val': 0.4715227, 'map_val': 0.6162070424697011}\n",
      "INFO:tensorflow:step: 32, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6771047, 'loss_val': 0.48009443, 'map_val': 0.6153508433055928}\n",
      "INFO:tensorflow:step: 36, {'binary_accuracy_val': 0.80661774, 'auc_val': 0.6795186, 'loss_val': 0.47098798, 'map_val': 0.618357693453835}\n",
      "INFO:tensorflow:step: 36, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6808785, 'loss_val': 0.47933823, 'map_val': 0.6178837830260725}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy_val': 0.80664635, 'auc_val': 0.68140584, 'loss_val': 0.47056755, 'map_val': 0.6197736737475232}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6825851, 'loss_val': 0.48117265, 'map_val': 0.6193905245148676}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy': '0.7899', 'auc': '0.6268', 'loss': '0.5192', 'time': '95.3529'}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy': '0.7919', 'auc': '0.6357', 'loss': '0.5107', 'time': '96.6926'}\n",
      "INFO:tensorflow:step: 44, {'binary_accuracy_val': 0.80677605, 'auc_val': 0.68416536, 'loss_val': 0.46731168, 'map_val': 0.6217401180482837}\n",
      "INFO:tensorflow:step: 44, {'binary_accuracy_val': 0.8065815, 'auc_val': 0.6853437, 'loss_val': 0.4786206, 'map_val': 0.6213630104317197}\n",
      "INFO:tensorflow:step: 48, {'binary_accuracy_val': 0.80677223, 'auc_val': 0.68515795, 'loss_val': 0.46755356, 'map_val': 0.622366845719621}\n",
      "INFO:tensorflow:step: 48, {'binary_accuracy_val': 0.8065796, 'auc_val': 0.68724775, 'loss_val': 0.4778859, 'map_val': 0.6224993351163074}\n",
      "INFO:tensorflow:step: 50, {'binary_accuracy': '0.7866', 'auc': '0.6441', 'loss': '0.5132', 'time': '92.4862'}\n",
      "INFO:tensorflow:step: 50, {'binary_accuracy': '0.7841', 'auc': '0.6515', 'loss': '0.5162', 'time': '92.6802'}\n",
      "INFO:tensorflow:step: 52, {'binary_accuracy_val': 0.8068104, 'auc_val': 0.6866544, 'loss_val': 0.4667109, 'map_val': 0.6232887752051962}\n",
      "INFO:tensorflow:step: 52, {'binary_accuracy_val': 0.8065796, 'auc_val': 0.688055, 'loss_val': 0.4796402, 'map_val': 0.6230204284171725}\n",
      "INFO:tensorflow:step: 56, {'binary_accuracy_val': 0.80680656, 'auc_val': 0.68729615, 'loss_val': 0.4673074, 'map_val': 0.6237436063772431}\n",
      "INFO:tensorflow:step: 56, {'binary_accuracy_val': 0.8065796, 'auc_val': 0.6898591, 'loss_val': 0.47725165, 'map_val': 0.6242360688777816}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy_val': 0.8068962, 'auc_val': 0.688982, 'loss_val': 0.46465346, 'map_val': 0.6245492269271273}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy_val': 0.80659485, 'auc_val': 0.6912186, 'loss_val': 0.4761902, 'map_val': 0.6252171036814032}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy': '0.7923', 'auc': '0.6582', 'loss': '0.4975', 'time': '95.7726'}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy': '0.7994', 'auc': '0.6660', 'loss': '0.4813', 'time': '96.4025'}\n",
      "INFO:tensorflow:step: 64, {'binary_accuracy_val': 0.8068695, 'auc_val': 0.6894559, 'loss_val': 0.465414, 'map_val': 0.6249542538960562}\n",
      "INFO:tensorflow:step: 64, {'binary_accuracy_val': 0.80659485, 'auc_val': 0.6919725, 'loss_val': 0.4766275, 'map_val': 0.6256356460272625}\n",
      "INFO:tensorflow:step: 68, {'binary_accuracy_val': 0.8069992, 'auc_val': 0.6908586, 'loss_val': 0.46274748, 'map_val': 0.6257374766412069}\n",
      "INFO:tensorflow:step: 68, {'binary_accuracy_val': 0.8066063, 'auc_val': 0.6932833, 'loss_val': 0.4750617, 'map_val': 0.6261928129413068}\n",
      "INFO:tensorflow:step: 70, {'binary_accuracy': '0.7965', 'auc': '0.6726', 'loss': '0.4885', 'time': '91.9168'}\n",
      "INFO:tensorflow:step: 70, {'binary_accuracy': '0.7880', 'auc': '0.6645', 'loss': '0.4966', 'time': '93.0298'}\n",
      "INFO:tensorflow:step: 72, {'binary_accuracy_val': 0.80703354, 'auc_val': 0.6917957, 'loss_val': 0.4620873, 'map_val': 0.6263778096876418}\n",
      "INFO:tensorflow:step: 72, {'binary_accuracy_val': 0.8066006, 'auc_val': 0.6943239, 'loss_val': 0.47420055, 'map_val': 0.6267473186842498}\n",
      "INFO:tensorflow:step: 76, {'binary_accuracy_val': 0.8070717, 'auc_val': 0.69263786, 'loss_val': 0.46120656, 'map_val': 0.6268461718884741}\n",
      "INFO:tensorflow:step: 76, {'binary_accuracy_val': 0.80661774, 'auc_val': 0.69524765, 'loss_val': 0.4724981, 'map_val': 0.6274399954473022}\n",
      "INFO:tensorflow:step: 80, {'binary_accuracy_val': 0.80706024, 'auc_val': 0.69316906, 'loss_val': 0.46163183, 'map_val': 0.6270663191442578}\n",
      "2022-10-31 22:33:40.876302: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "INFO:tensorflow:step: 80, {'binary_accuracy_val': 0.8066311, 'auc_val': 0.69613796, 'loss_val': 0.47146982, 'map_val': 0.6278299575056401}\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "2022-10-31 22:33:48.223248: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: /home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c/assets\n",
      "INFO:tensorflow:Final eval result: {'binary_accuracy_val': 0.80706024, 'auc_val': 0.69316906, 'loss_val': 0.46163183, 'map_val': 0.6270663191442578}\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/opt/intel/oneapi/intelpython/latest/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: /home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c/assets\n",
      "INFO:tensorflow:Final eval result: {'binary_accuracy_val': 0.8066311, 'auc_val': 0.69613796, 'loss_val': 0.47146982, 'map_val': 0.6278299575056401}\n",
      "2022-10-31 22:33:58,128 - sigopt - INFO - Training completed based in sigopt suggestion, took 804.2940199375153 secs\n",
      "2022-10-31 22:33:58,128 - E2EAIOK.SDA - INFO - training script completed\n",
      "\n",
      "We found the best model! Here is the model explaination\n",
      "\n",
      "===============================================\n",
      "***    Best Trained Model    ***\n",
      "===============================================\n",
      "  Model Type: wnd\n",
      "  Model Saved Path: /home/vmagent/app/e2eaiok/result/61fab909cb1e8fb00e45984efd42565c\n",
      "  Sigopt Experiment id is None\n",
      "  === Result Metrics ===\n",
      "    MAP: 0.6278299575056401\n",
      "    training_time: 804.2940199375153\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/e2eaiok; python run_e2eaiok.py --data_path /home/vmagent/app/dataset/outbrain/ --model_name wnd --conf conf/e2eaiok_defaults_wnd_example.conf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
