{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ded5a89",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/e2eAIOK/blob/main/demo/builtin/wnd/WND_DEMO.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b8b2b",
   "metadata": {},
   "source": [
    "# WnD Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611394e",
   "metadata": {},
   "source": [
    "Recommendation systems drive engagement on many of the most popular online platforms. As the volume of data available to power these systems grows exponentially, users are increasingly turning from more traditional machine learning methods to highly expressive deep learning models to improve the quality of recommendations. Google's Wide and Deep recommender system is a popular model for recommendation problems for its robustness to signal sparsity.\n",
    "This notebook contains step by step guide on how to optimize WnD model with IntelÂ® End-to-End AI Optimization Kit, and detailed performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a783f5e",
   "metadata": {},
   "source": [
    "# Content\n",
    "* [Overview](#Overview)\n",
    "    * [Model Architecture](#Model-Architecture)\n",
    "    * [Optimizations](#Optimizations)\n",
    "    * [Performance](#Performance)\n",
    "* [Getting Started](#Getting-Started)\n",
    "    * [1. Environment Setup](#1.-Environment-Setup)\n",
    "    * [2. Workflow Prepare](#2.-Workflow-Prepare)\n",
    "    * [3. Data Prepare](#3.-Data-Prepare)\n",
    "    * [4. Train](#4.-Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90cf8f7",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df0609",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "<img src=\"./img/wnd.png\" width=\"800\"/>\n",
    "\n",
    "Wide and Deep model was published by Google at 2016. It jointly train wide linear models and deep neural networks, combined the benefits of memorization and generalization for recommender system. It's the first time to introduce neural network to CTR model.\n",
    "\n",
    "The wide component is a generalized linear model. The feature set includes raw input features and transformed features\n",
    "The deep component is a feed-forward neural network. The sparse, high-dimensional categorical features are first converted into an embedding vector and fed into the hidden layers of a neural network in the forward pass\n",
    "The wide component and deep component are combined using a weighted sum of their output log odds as the prediction and fed to logistic loss function for joint training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1f075",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad100f",
   "metadata": {},
   "source": [
    "### Distributed Training\n",
    "\n",
    "Use horovod for distributed training and mpirun to launch training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18b1d8",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "Long idle time per training step for horovod communication, horovod paramter sync consume much time during distributed training, causing poor scaling performance. The overhead mainly caused by large embedding table.\n",
    "\n",
    "<img src=\"./img/wnd_profile.png\" width=\"600\"/><figure>Distributed training profiling</figure>\n",
    "\n",
    "Replace custom layer (contains embedding layer) with TensorFlow dense layer help to reduce embedding parameter size, thus reduce parameter size needed to sync by horovod, fix horovod poor scaling issue. Per step training time reduced from 5.16s to 2.71s, got about 1.9x speedup.\n",
    "\n",
    "<img src=\"./img/wnd_traintime_custom_emd.png\" width=\"600\"/><figure>custom layer</figure>\n",
    "<img src=\"./img/wnd_traintime_tf_emd.png\" width=\"600\"/><figure>TensorFlow build-in layer</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19cd46",
   "metadata": {},
   "source": [
    "### Horovod Optimization With OneCCL\n",
    "\n",
    "Deep part embedding table cost long time hovorod communication, and Allgather is the most time-consuming operation. Enable Intel OneCCL in horovod helps to reduce Allgather time consumption, which delivers 1.2x speedup.\n",
    "\n",
    "<img src=\"./img/wnd_woccl.png\" width=\"600\"/><figure>horovod timeline profiling w/o OneCCL</figure>\n",
    "<img src=\"./img/wnd_wccl.png\" width=\"600\"/><figure>horovod timeline profiling w/ OneCCL</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1aaac4",
   "metadata": {},
   "source": [
    "### Framework Related Optimization\n",
    "\n",
    "set CCL affinity, horovod thread affinity, MPI socket binding, KMP affinity, OMP_NUM_THREADS\n",
    "\n",
    "```bash\n",
    "export CCL_WORKER_COUNT=2 # set CCL thread number\n",
    "export CCL_WORKER_AFFINITY=\"16,17,34,35\" # set CCL thread affinity\n",
    "export HOROVOD_THREAD_AFFINITY=\"53,71\" # set horovod thread affinity\n",
    "export I_MPI_PIN_DOMAIN=socket # set socket binding for MPI\n",
    "export I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=\"16,17,34,35,52,53,70,71\" # exclude CCL threads\n",
    "\n",
    "mpirun -genv OMP_NUM_THREADS=16 -map-by socket -n 2 -ppn 2 -hosts localhost -genv I_MPI_PIN_DOMAIN=socket -genv OMP_PROC_BIND=true -genv KMP_BLOCKTIME=1 -genv KMP_AFFINITY=granularity=fine,compact,1,0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90924745",
   "metadata": {},
   "source": [
    "### Early Stop\n",
    "\n",
    "Training baseline MAP stopped at 0.6553, with optimizations on training process, model converge faster and achieve 0.6553 MAP at 1.5K steps, no need to training to 9K steps. Enable early stop at 0.6553 MAP.\n",
    "\n",
    "<img src=\"./img/wnd_map_GPU.png\"/><figure>baseline metric curv</figure>\n",
    "<img src=\"./img/wnd_early_stop_cpu.png\"/><figure>optimized metric curv</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b7082",
   "metadata": {},
   "source": [
    "### Input Pipeline Optimization\n",
    "\n",
    "Training needs more system resources while input pipeline not, the resources preemption between input pipeline and training caused performance overhead. By reducing system resources allocated for input pipeline to free more resources for training, input pipeline time consuming reduced from 8.2% to 3.2% among entire training time.\n",
    "\n",
    "<img src=\"./img/wnd_input_pipeline_orig.png\" width=\"600\"/><figure>original profiling</figure>\n",
    "<img src=\"./img/wnd_input_pipeline_opt.png\" width=\"600\"/><figure>optimized profiling</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e3f6a",
   "metadata": {},
   "source": [
    "### HPO With SDA (Smart Democratization Advisor)\n",
    "\n",
    "SDA config\n",
    "\n",
    "```\n",
    "Parameters for SDA auto optimization:\n",
    "- dnn_hidden_unit1: [64, 128, 256, 512] #layer width of dnn_hidden_unit1\n",
    "- dnn_hidden_unit2: [64, 128, 256, 512] #layer width of dnn_hidden_unit2\n",
    "- dnn_hidden_unit3: [64, 128, 256, 512] #layer width of dnn_hidden_unit3\n",
    "- deep_learning_rate: 0.0001~0.1 #deep part learning rate\n",
    "- linear_learning_rate: 0.01~1.0 #linear part learning rate\n",
    "- deep_warmup_epochs: 1~8 #deep part warmup epochs\n",
    "- deep_dropout: 0~0.5 #deep part dropout\n",
    "metrics:\n",
    "- name: training_time # training time threshold\n",
    "  objective: minimize\n",
    "  threshold: 1800\n",
    "- name: MAP # training metric threshold\n",
    "  objective: maximize\n",
    "  threshold: 0.6553\n",
    "metric:\n",
    "- name: MAP\n",
    "  threshold: 0.6553\n",
    "```\n",
    "\n",
    "request suggestions from SDA\n",
    "\n",
    "```python\n",
    "suggestion = self.conn.experiments(self.experiment.id).suggestions().create()\n",
    "```\n",
    "\n",
    "<img src=\"./img/wnd_sda.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f0c0c",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "<img src=\"./img/wnd_perf.png\" width=\"900\"/>\n",
    "\n",
    "* Intel optimized TensorFlow: apply OpenMP and KMP optimizations (AFFINITY, NUM_THREADS etc.) for CPU\n",
    "* Distributed training: horovod scaling delivered 1.93x speedup from 1 node to 4 nodes, got poor scaling performance\n",
    "* Model optimization: reducing sparse embedding size helped to reduce horovod communication data size, delivered better scaling performance, 4 nodes training delivered 2.7x speed up over 1 node\n",
    "* Lighter model: reducing deep hidden unit from [1024, 1024, 1024, 1024, 1024] to [1024, 512, 256] delivered 1.14x speedup\n",
    "* Early stop: stop training when MAP@12 reached pre-defined value (0.6553) , training took 904 steps delivered 4.14x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9c741",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "* [1. Environment Setup](#1.-Environment-Setup)\n",
    "* [2. Workflow Prepare](#2.-Workflow-Prepare)\n",
    "* [3. Data Prepare](#3.-Data-Prepare)\n",
    "* [4. Train](#4.-Train)\n",
    "\n",
    "Notes: in order to run this demo, please follow `Environment Setup`, `Workflow Prepare` and `Data Prepare` section for pre-requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca039366",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Option 1 Setup Environment with Pip\n",
    "pre-work: move e2eAIOK source code to /home/vmagent/app/e2eaiok. Install spark and start spark services for data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install sigopt future pydot dill pyyaml\n",
    "pip install --no-cache-dir intel-tensorflow==2.10\n",
    "HOROVOD_WITHOUT_MPI=1 HOROVOD_CPU_OPERATIONS=CCL \\\n",
    "    HOROVOD_WITHOUT_MXNET=1 HOROVOD_WITHOUT_PYTORCH=1 HOROVOD_WITH_TENSORFLOW=1 \\\n",
    "    pip install --no-cache-dir horovod\n",
    "pip install --no-cache-dir --no-deps tensorflow-transform==0.24.1 tensorflow-metadata==0.14.0\n",
    "pip install \"git+https://github.com/mlperf/logging.git@1.0.0\"\n",
    "pip install e2eAIOK-sda --pre --no-deps --ignore-installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ff1b7",
   "metadata": {},
   "source": [
    "### Option 2 Setup Environment with Docker\n",
    "``` bash\n",
    "# Setup ENV\n",
    "git clone https://github.com/intel/e2eAIOK.git\n",
    "cd e2eAIOK\n",
    "git submodule update --init --recursive\n",
    "python3 scripts/start_e2eaiok_docker.py -b tensorflow -w ${host0} ${host1} ${host2} ${host3} --proxy \"\"\n",
    "# Enter Docker\n",
    "sshpass -p docker ssh ${host0} -p 12344\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9a8f5",
   "metadata": {},
   "source": [
    "## 2. Workflow Prepare\n",
    "\n",
    "``` bash\n",
    "# prepare model codes\n",
    "bash workflow_prepare_wnd.sh\n",
    "\n",
    "# source spark env\n",
    "source /home/spark-env.sh\n",
    "\n",
    "# Start services\n",
    "# only if there is no spark service running, may check ${localhost}:8080 to confirm\n",
    "/home/start_spark_service.sh\n",
    "```\n",
    "\n",
    "a simple example of config file, one can refer to conf/e2eaiok_defaults_wnd_example.conf for whole config file\n",
    "\n",
    "```yaml\n",
    "### GLOBAL SETTINGS ###\n",
    "observation_budget: 1\n",
    "save_path: /home/vmagent/app/e2eaiok/result/\n",
    "ppn: 2\n",
    "ccl_worker_num: 2\n",
    "global_batch_size: 524288\n",
    "num_epochs: 20\n",
    "cores: 72\n",
    "iface: lo\n",
    "hosts:\n",
    "- localhost\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47448679",
   "metadata": {},
   "source": [
    "## 3. Data Prepare\n",
    "\n",
    "```bash\n",
    "# Download Dataset\n",
    "# download and unzip dataset from https://www.kaggle.com/c/outbrain-click-prediction/data to /home/vmagent/app/dataset/outbrain/orig\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0215585f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 22:02:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/31 22:02:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "Drop rows with empty \"geo_location\"...\n",
      "Drop rows with empty \"platform\"...\n",
      "valid_set_df time: 38.694966077804565                                           ]\n",
      "train_set_df time: 42.35809636116028                                            1]\n",
      "train/test dataset generation time: 95.60888910293579\n",
      "22/10/31 22:04:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "test_set_enriched_df time: 67.92218327522278                                    0]]0]\n",
      "train_set_enriched_df time: 83.92503476142883                                   \n",
      "WARNING:tensorflow:From /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2/data/outbrain/spark/preproc.py:654: from_feature_spec (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "from_feature_spec is a deprecated, use schema_utils.schema_from_feature_spec\n",
      "2022-10-31 22:09:16.917781\tComputing min and max\n",
      "[Row(min(ad_views)=0, max(ad_views)=144659, min(doc_views)=0, max(doc_views)=556631, min(doc_event_days_since_published)=0.0, max(doc_event_days_since_published)=3650.0, min(doc_ad_days_since_published)=0.0, max(doc_ad_days_since_published)=3648.0)]\n",
      "feature engineering time: 328.1621606349945                                     \n",
      "data convert time: 178.53657913208008                                           \n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2; sh scripts/spark_preproc.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d6539",
   "metadata": {},
   "source": [
    "## 4. Train\n",
    "\n",
    "Edit config file to control SDA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0189c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 06:47:56,462 - E2EAIOK.SDA - INFO - ### Ready to submit current task  ###\n",
      "Exception ignored in: <function SDA.__del__ at 0x7fb9b4048310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vmagent/app/e2eaiok/e2eAIOK/SDA/SDA.py\", line 79, in __del__\n",
      "    with open(f\"{self.custom_result_path}/latest_hydro_model\", 'w') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'None/latest_hydro_model'\n",
      "2023-03-23 06:47:56,465 - E2EAIOK.SDA - INFO - Model Advisor created\n",
      "2023-03-23 06:47:56,466 - E2EAIOK.SDA - INFO - model parameter initialized\n",
      "2023-03-23 06:47:56,467 - E2EAIOK.SDA - INFO - start to launch training\n",
      "2023-03-23 06:47:56,469 - sigopt - INFO - training launch command: mpirun -genv OMP_NUM_THREADS=24 -map-by socket -n 2 -ppn 2 -hosts localhost -print-rank-map -genv I_MPI_PIN_DOMAIN=socket -genv OMP_PROC_BIND=true -genv KMP_BLOCKTIME=1 -genv KMP_AFFINITY=granularity=fine,compact,1,0 /opt/intel/oneapi/intelpython/latest/bin/python -u /home/vmagent/app/e2eaiok/modelzoo/WnD/TensorFlow2/main.py --results_dir /home/vmagent/app/e2eaiok/result/ --model_dir /home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845 --train_data_pattern '/home/vmagent/app/dataset/outbrain/train/part*' --eval_data_pattern '/home/vmagent/app/dataset/outbrain/valid/part*' --dataset_meta_file /home/vmagent/app/dataset/outbrain/outbrain_meta.yaml --global_batch_size 524288 --eval_batch_size 524288 --num_epochs 20 --metric MAP --metric_threshold 0.6553 --linear_learning_rate 0.8 --deep_learning_rate 0.00048 --deep_warmup_epochs 6 --deep_hidden_units 1024 512 256 --deep_dropout 0.1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data format is tfrecords\n",
      "params: {'ppn': 2, 'cores': 104, 'hosts': ['localhost'], 'ccl_worker_num': 2, 'python_executable': None, 'global_batch_size': 524288, 'num_epochs': 20, 'model_dir': './', 'observation_budget': 1, 'save_path': '/home/vmagent/app/e2eaiok/result/', 'dataset_meta_path': '/home/vmagent/app/dataset/outbrain/outbrain_meta.yaml', 'train_dataset_path': '/home/vmagent/app/dataset/outbrain/train', 'eval_dataset_path': '/home/vmagent/app/dataset/outbrain/valid', 'data_path': '/home/vmagent/app/dataset/outbrain/', 'enable_sigopt': False, 'python_path': '/opt/intel/oneapi/intelpython/latest/bin/python', 'iface': 'enp24s0f0', 'model_parameter': {'project': 'sda', 'experiment': 'wnd', 'parameters': [{'grid': [64, 128, 256, 512, 1024, 2048], 'name': 'dnn_hidden_unit1', 'type': 'int'}, {'grid': [64, 128, 256, 512, 1024, 2048], 'name': 'dnn_hidden_unit2', 'type': 'int'}, {'grid': [64, 128, 256, 512, 1024, 2048], 'name': 'dnn_hidden_unit3', 'type': 'int'}, {'bounds': {'max': 0.1, 'min': 0.0001}, 'name': 'deep_learning_rate', 'transformation': 'log', 'type': 'double'}, {'bounds': {'max': 1.0, 'min': 0.01}, 'name': 'linear_learning_rate', 'transformation': 'log', 'type': 'double'}, {'bounds': {'max': 8, 'min': 1}, 'name': 'deep_warmup_epochs', 'type': 'int'}, {'bounds': {'max': 0.5, 'min': 0}, 'name': 'deep_dropout', 'type': 'double'}], 'metrics': [{'name': 'training_time', 'objective': 'minimize', 'threshold': 1800}, {'name': 'MAP', 'objective': 'maximize', 'threshold': 0.6553}], 'metric': [{'name': 'MAP', 'threshold': 0.6553}]}, 'model_name': 'wnd', 'interative': False, 'num_instances': 1, 'num_cores': 1, 'metric': {'name': 'MAP', 'threshold': 0.6553}}\n",
      "(localhost:0,1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 06:47:56.712457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 06:47:56.713833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 0\n",
      "rank: 0\n",
      "All feature columns: ['doc_event_days_since_published_log_01scaled', 'doc_ad_days_since_published_log_01scaled', 'doc_event_doc_ad_sim_categories', 'doc_event_doc_ad_sim_topics', 'doc_event_doc_ad_sim_entities', 'pop_document_id', 'pop_publisher_id', 'pop_source_id', 'pop_ad_id', 'pop_advertiser_id', 'pop_campain_id', 'doc_views_log_01scaled', 'ad_views_log_01scaled', 'ad_id', 'campaign_id', 'doc_event_id', 'event_platform', 'doc_id', 'ad_advertiser', 'doc_event_source_id', 'doc_event_publisher_id', 'doc_ad_source_id', 'doc_ad_publisher_id', 'event_geo_location', 'event_country', 'event_country_state', 'display_id']\n",
      "All feature columns: ['doc_event_days_since_published_log_01scaled', 'doc_ad_days_since_published_log_01scaled', 'doc_event_doc_ad_sim_categories', 'doc_event_doc_ad_sim_topics', 'doc_event_doc_ad_sim_entities', 'pop_document_id', 'pop_publisher_id', 'pop_source_id', 'pop_ad_id', 'pop_advertiser_id', 'pop_campain_id', 'doc_views_log_01scaled', 'ad_views_log_01scaled', 'ad_id', 'campaign_id', 'doc_event_id', 'event_platform', 'doc_id', 'ad_advertiser', 'doc_event_source_id', 'doc_event_publisher_id', 'doc_ad_source_id', 'doc_ad_publisher_id', 'event_geo_location', 'event_country', 'event_country_state', 'display_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:command line arguments: {\"train_data_pattern\": \"/home/vmagent/app/dataset/outbrain/train/part*\", \"eval_data_pattern\": \"/home/vmagent/app/dataset/outbrain/valid/part*\", \"dataset_meta_file\": \"/home/vmagent/app/dataset/outbrain/outbrain_meta.yaml\", \"model_dir\": \"/home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845\", \"results_dir\": \"/home/vmagent/app/e2eaiok/result/\", \"global_batch_size\": 524288, \"eval_batch_size\": 524288, \"num_epochs\": 20, \"amp\": false, \"xla\": false, \"linear_learning_rate\": 0.8, \"deep_learning_rate\": 0.00048, \"deep_warmup_epochs\": 6.0, \"metric\": \"MAP\", \"metric_threshold\": 0.6553, \"deep_hidden_units\": [1024, 512, 256], \"deep_dropout\": 0.1, \"evaluate\": false, \"use_checkpoint\": false, \"benchmark\": false, \"benchmark_warmup_steps\": 500, \"benchmark_steps\": 1000}\n",
      "WARNING:tensorflow:command line arguments: {\"train_data_pattern\": \"/home/vmagent/app/dataset/outbrain/train/part*\", \"eval_data_pattern\": \"/home/vmagent/app/dataset/outbrain/valid/part*\", \"dataset_meta_file\": \"/home/vmagent/app/dataset/outbrain/outbrain_meta.yaml\", \"model_dir\": \"/home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845\", \"results_dir\": \"/home/vmagent/app/e2eaiok/result/\", \"global_batch_size\": 524288, \"eval_batch_size\": 524288, \"num_epochs\": 20, \"amp\": false, \"xla\": false, \"linear_learning_rate\": 0.8, \"deep_learning_rate\": 0.00048, \"deep_warmup_epochs\": 6.0, \"metric\": \"MAP\", \"metric_threshold\": 0.6553, \"deep_hidden_units\": [1024, 512, 256], \"deep_dropout\": 0.1, \"evaluate\": false, \"use_checkpoint\": false, \"benchmark\": false, \"benchmark_warmup_steps\": 500, \"benchmark_steps\": 1000}\n",
      "2023-03-23 06:47:58.510237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 06:47:58.510395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:deep columns: 26\n",
      "WARNING:tensorflow:wide columns: 26\n",
      "WARNING:tensorflow:wide&deep intersection: 13\n",
      "WARNING:tensorflow:deep columns: 26\n",
      "WARNING:tensorflow:wide columns: 26\n",
      "WARNING:tensorflow:wide&deep intersection: 13\n",
      "INFO:tensorflow:Steps per epoch: 113\n",
      "INFO:tensorflow:Steps per epoch: 113\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "OMP: Warning #182: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined\n",
      "OMP: Warning #182: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined\n",
      "INFO:tensorflow:step: 0, {'binary_accuracy': '0.4316', 'auc': '0.4966', 'loss': '1.0370', 'time': '22.7671'}\n",
      "INFO:tensorflow:step: 0, {'binary_accuracy': '0.5493', 'auc': '0.4601', 'loss': '0.8407', 'time': '22.9273'}\n",
      "INFO:tensorflow:step: 4, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.51582694, 'loss_val': 0.5057239, 'map_val': 0.4789929582872797}\n",
      "INFO:tensorflow:step: 4, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.49622852, 'loss_val': 0.5055178, 'map_val': 0.46047758946746276}\n",
      "INFO:tensorflow:step: 8, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.60037553, 'loss_val': 0.48830375, 'map_val': 0.5590866315100389}\n",
      "INFO:tensorflow:step: 8, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.57561606, 'loss_val': 0.49429637, 'map_val': 0.5363070349866916}\n",
      "INFO:tensorflow:step: 10, {'binary_accuracy': '0.7483', 'auc': '0.5347', 'loss': '0.6034', 'time': '92.9971'}\n",
      "INFO:tensorflow:step: 10, {'binary_accuracy': '0.7407', 'auc': '0.5349', 'loss': '0.6052', 'time': '94.3366'}\n",
      "INFO:tensorflow:step: 12, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.631354, 'loss_val': 0.48803684, 'map_val': 0.5851058799623087}\n",
      "INFO:tensorflow:step: 12, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.61140656, 'loss_val': 0.48605433, 'map_val': 0.5788131440836224}\n",
      "INFO:tensorflow:step: 16, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6550803, 'loss_val': 0.4805795, 'map_val': 0.6027827995332162}\n",
      "INFO:tensorflow:step: 16, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.63060427, 'loss_val': 0.48239917, 'map_val': 0.5965223303096653}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6674335, 'loss_val': 0.4766211, 'map_val': 0.6118459198224614}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy_val': 0.80657005, 'auc_val': 0.6459687, 'loss_val': 0.4760792, 'map_val': 0.608165385725955}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy': '0.7812', 'auc': '0.5709', 'loss': '0.5524', 'time': '84.1588'}\n",
      "INFO:tensorflow:step: 20, {'binary_accuracy': '0.7848', 'auc': '0.5711', 'loss': '0.5578', 'time': '84.8011'}\n",
      "INFO:tensorflow:step: 24, {'binary_accuracy_val': 0.80657196, 'auc_val': 0.6747546, 'loss_val': 0.4741633, 'map_val': 0.6170158759185808}\n",
      "INFO:tensorflow:step: 24, {'binary_accuracy_val': 0.80656815, 'auc_val': 0.6552154, 'loss_val': 0.47408754, 'map_val': 0.6141576870906444}\n",
      "INFO:tensorflow:step: 28, {'binary_accuracy_val': 0.80656624, 'auc_val': 0.68024594, 'loss_val': 0.47094023, 'map_val': 0.6207187878992777}\n",
      "INFO:tensorflow:step: 28, {'binary_accuracy_val': 0.80657387, 'auc_val': 0.66239464, 'loss_val': 0.46987066, 'map_val': 0.6190677797042365}\n",
      "INFO:tensorflow:step: 30, {'binary_accuracy': '0.7663', 'auc': '0.6053', 'loss': '0.5549', 'time': '80.6929'}\n",
      "INFO:tensorflow:step: 30, {'binary_accuracy': '0.7659', 'auc': '0.5978', 'loss': '0.5542', 'time': '82.4123'}\n",
      "INFO:tensorflow:step: 32, {'binary_accuracy_val': 0.80656624, 'auc_val': 0.6821552, 'loss_val': 0.47156826, 'map_val': 0.622716356269747}\n",
      "INFO:tensorflow:step: 32, {'binary_accuracy_val': 0.80659676, 'auc_val': 0.6668354, 'loss_val': 0.4683485, 'map_val': 0.6214276095509272}\n",
      "INFO:tensorflow:step: 36, {'binary_accuracy_val': 0.80656624, 'auc_val': 0.68478185, 'loss_val': 0.46986023, 'map_val': 0.6248661851757609}\n",
      "INFO:tensorflow:step: 36, {'binary_accuracy_val': 0.80661774, 'auc_val': 0.6695579, 'loss_val': 0.4678133, 'map_val': 0.6229767444451461}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy_val': 0.8065834, 'auc_val': 0.68728405, 'loss_val': 0.46690968, 'map_val': 0.626344784628979}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy_val': 0.80672836, 'auc_val': 0.6729757, 'loss_val': 0.46475303, 'map_val': 0.6249665103658183}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy': '0.7881', 'auc': '0.6376', 'loss': '0.5144', 'time': '83.9635'}\n",
      "INFO:tensorflow:step: 40, {'binary_accuracy': '0.7896', 'auc': '0.6349', 'loss': '0.5135', 'time': '84.8017'}\n",
      "INFO:tensorflow:step: 44, {'binary_accuracy_val': 0.8065815, 'auc_val': 0.68771935, 'loss_val': 0.46844375, 'map_val': 0.6268193371385271}\n",
      "INFO:tensorflow:step: 44, {'binary_accuracy_val': 0.80674744, 'auc_val': 0.675209, 'loss_val': 0.4644949, 'map_val': 0.6256405183786289}\n",
      "INFO:tensorflow:step: 48, {'binary_accuracy_val': 0.8065891, 'auc_val': 0.6889895, 'loss_val': 0.46735984, 'map_val': 0.6277734781420207}\n",
      "INFO:tensorflow:step: 48, {'binary_accuracy_val': 0.8068447, 'auc_val': 0.67673886, 'loss_val': 0.46301174, 'map_val': 0.6267493701444478}\n",
      "INFO:tensorflow:step: 50, {'binary_accuracy': '0.7866', 'auc': '0.6568', 'loss': '0.5133', 'time': '81.0037'}\n",
      "INFO:tensorflow:step: 50, {'binary_accuracy': '0.7900', 'auc': '0.6580', 'loss': '0.5073', 'time': '82.0526'}\n",
      "INFO:tensorflow:step: 52, {'binary_accuracy_val': 0.8066082, 'auc_val': 0.6898565, 'loss_val': 0.46700692, 'map_val': 0.6281898921913023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step: 52, {'binary_accuracy_val': 0.8068619, 'auc_val': 0.6773412, 'loss_val': 0.4630782, 'map_val': 0.6269048126310016}\n",
      "INFO:tensorflow:step: 56, {'binary_accuracy_val': 0.80662346, 'auc_val': 0.6912416, 'loss_val': 0.46513453, 'map_val': 0.6289117440123722}\n",
      "INFO:tensorflow:step: 56, {'binary_accuracy_val': 0.80693436, 'auc_val': 0.6789451, 'loss_val': 0.4619302, 'map_val': 0.6276270785147853}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy_val': 0.80662155, 'auc_val': 0.6914401, 'loss_val': 0.46614832, 'map_val': 0.6290294068335345}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy_val': 0.80698013, 'auc_val': 0.6806805, 'loss_val': 0.46096572, 'map_val': 0.6284364907265818}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy': '0.7960', 'auc': '0.6625', 'loss': '0.4891', 'time': '83.8929'}\n",
      "INFO:tensorflow:step: 60, {'binary_accuracy': '0.7930', 'auc': '0.6588', 'loss': '0.4944', 'time': '84.8086'}\n",
      "INFO:tensorflow:step: 64, {'binary_accuracy_val': 0.80664825, 'auc_val': 0.6928923, 'loss_val': 0.46354833, 'map_val': 0.6297094408105818}\n",
      "INFO:tensorflow:step: 64, {'binary_accuracy_val': 0.8070793, 'auc_val': 0.68183917, 'loss_val': 0.45993412, 'map_val': 0.6291536274262849}\n",
      "INFO:tensorflow:step: 68, {'binary_accuracy_val': 0.8066406, 'auc_val': 0.6930742, 'loss_val': 0.46470577, 'map_val': 0.6297997128852147}\n",
      "INFO:tensorflow:step: 68, {'binary_accuracy_val': 0.80710983, 'auc_val': 0.68303263, 'loss_val': 0.4595153, 'map_val': 0.6294004102265865}\n",
      "INFO:tensorflow:step: 70, {'binary_accuracy': '0.7875', 'auc': '0.6619', 'loss': '0.4971', 'time': '80.9408'}\n",
      "INFO:tensorflow:step: 70, {'binary_accuracy': '0.7920', 'auc': '0.6606', 'loss': '0.4933', 'time': '82.1190'}\n",
      "INFO:tensorflow:step: 72, {'binary_accuracy_val': 0.80664253, 'auc_val': 0.6931512, 'loss_val': 0.46540284, 'map_val': 0.6298491218789899}\n",
      "INFO:tensorflow:step: 72, {'binary_accuracy_val': 0.8073158, 'auc_val': 0.6851886, 'loss_val': 0.4580811, 'map_val': 0.6304974331028683}\n",
      "INFO:tensorflow:step: 76, {'binary_accuracy_val': 0.80664635, 'auc_val': 0.6942606, 'loss_val': 0.4636444, 'map_val': 0.6303748050863806}\n",
      "INFO:tensorflow:step: 76, {'binary_accuracy_val': 0.8072815, 'auc_val': 0.6857041, 'loss_val': 0.45818916, 'map_val': 0.6305514750457707}\n",
      "INFO:tensorflow:step: 80, {'binary_accuracy_val': 0.8066654, 'auc_val': 0.695322, 'loss_val': 0.46188027, 'map_val': 0.6310518886887302}\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "INFO:tensorflow:step: 80, {'binary_accuracy_val': 0.8073559, 'auc_val': 0.6876492, 'loss_val': 0.4572031, 'map_val': 0.6309602310691915}\n",
      "INFO:tensorflow:Assets written to: /home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845/assets\n",
      "INFO:tensorflow:Final eval result: {'binary_accuracy_val': 0.8066654, 'auc_val': 0.695322, 'loss_val': 0.46188027, 'map_val': 0.6310518886887302}\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/intel/oneapi/intelpython/latest/lib/python3.9/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['display_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "INFO:tensorflow:Assets written to: /home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845/assets\n",
      "INFO:tensorflow:Final eval result: {'binary_accuracy_val': 0.8073559, 'auc_val': 0.6876492, 'loss_val': 0.4572031, 'map_val': 0.6309602310691915}\n",
      "2023-03-23 06:59:47,407 - sigopt - INFO - Training completed based in sigopt suggestion, took 710.9380166530609 secs\n",
      "2023-03-23 06:59:47,409 - E2EAIOK.SDA - INFO - training script completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/vmagent/app/e2eaiok/result/6c37b08d7939bb0d5de5f1e4c4303884f82b1c47d3d61173d1caf86893e9a845',\n",
       " [{'name': 'MAP', 'value': 0.6309602310691915},\n",
       "  {'name': 'training_time', 'value': 710.9380166530609}])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2eAIOK.SDA.SDA import SDA\n",
    "import yaml\n",
    "\n",
    "# create SDA settings\n",
    "settings = {}\n",
    "settings[\"data_path\"] = \"/home/vmagent/app/dataset/outbrain/\"\n",
    "settings[\"enable_sigopt\"] = False\n",
    "settings[\"python_path\"] = \"/opt/intel/oneapi/intelpython/latest/bin/python\"\n",
    "settings[\"train_path\"] = \"e2eaiok/modelzoo/WnD/TensorFlow2/main.py\"\n",
    "# load WnD settings\n",
    "with open(\"e2eaiok/tests/cicd/conf/e2eaiok_defaults_wnd_example.conf\") as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "settings.update(conf)\n",
    "\n",
    "sda = SDA(model=\"wnd\", settings=settings)\n",
    "sda.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f9fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
