{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e293d57f",
   "metadata": {},
   "source": [
    "# AIDK - Democratized Neural Architecture Search(DENAS)-Computer Vision\n",
    "\n",
    "<font size=3> Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. As one of the core capabilities of AIDK, DENAS is a hardware aware train-free neural architecture search framework to generate compact networks, and thus generate lightweight network architectures that can deliver higher throuhgput. DENAS is a domain-specific compact neural architecture search solution that support CV, NLP and RecSys domain.</font>\n",
    "\n",
    "<font size = 3> DENAS core componements includes: \n",
    "- Reduced Search Space: The architecture is composed of some popular network skeleton like ResNet(bottleneck blocks,residual blocks), recurrent cells based model and transformer based model. Incorporating prior knowledge about the typical properties of architectures well-suited for a task can reduce the size of the search space and simplify the search.\n",
    "   \n",
    "- Hardware Aware Search Algorithm: Add more Intel optimzed network operators and leverage Intel tools to search and trainging as fast as possible on the CPU. We extend LINAS's multi-objective evolutionary algorithm (MOEA) to search better network architecture for the hardware aware multi-objective optimization(train-free score, latency, FLOPs and so on).\n",
    "    \n",
    "- Train-free evaluation: A novel zero-cost metrics combined network trainablity, network expressivity and generalization. Computation of DE-Score only takes a few forward inferences without training, making it extremely fast, lightweight and data-free. \n",
    "\n",
    "</font>\n",
    "\n",
    "<img src=\"./img/denas_prototype.png\" width=\"80%\"> \n",
    "\n",
    "# DENAS Search Space\n",
    "\n",
    "<font size=3>\n",
    "Computer Vision Base Search Space:\n",
    "    \n",
    "- ResNet like skeleton: consists of residual blocks and bottleneck blocks, number of layers, convolutional layer kernel size, number of filters,\n",
    "\n",
    "\n",
    "<img src=\"./img/nas_bench_201.PNG\" width=\"50%\">\n",
    "<center>Overall architecture for NAS-Bench-201</center>\n",
    "    \n",
    "\n",
    "- Autoformer search space(Transformer based):embedding dimension, number of heads, query/key/value dimension, MLP ratio\n",
    ",network depth.\n",
    "    \n",
    "<img src=\"./img/autoformer_details.gif\" width=\"50%\">  \n",
    "<center>Autoformer search space flow</center>\n",
    "</font>\n",
    "\n",
    "<font size=3>Natural Language Processing:\n",
    "- NLP conventional Search Space: All conventional recurrent cells (RNN, LSTM, GRU) as particular instances; Linear dimension size, Blending (element wise), Element wise product and sum; activations: Tanh, Sigmoid, and LeakyReLU\n",
    "    \n",
    "- BERT-large: Embedding dimension, number of heads, query/key/value dimension, Number of transformers, MLP layer size, activations\n",
    "\n",
    "\n",
    "\n",
    "Recommendation System: MLP layer, some recurrent cells(RNN, LSTM, GRU) based neural network and Graph Neural Network\n",
    "</font>\n",
    "\n",
    "# DENAS Search Engine\n",
    "<font size=3>Evolutionary Algorithm\n",
    "\n",
    "- Initialization: Initialization the network architecture with random sample the possible component of each layer\n",
    "- Selection:  Utilize our training-free proxy to rank the architectures in terms of their  evaluation score and  measure the other objective metries(like latency and flops sizeâ€¦) which will combine the LINAS multi objective search method\n",
    "- Crossover and Mutation: Search over the candidate transformer architectures. Focus search on relevant portions of the search space, we enforce an upper bound on the latency of sampled architectures and reject samples outside this bound. \n",
    "\n",
    "</font>\n",
    "\n",
    "<img src=\"./img/EA.PNG\" width=\"50%\">\n",
    "<center>Evolutionary Algorithm</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train-free Score(DE_Score)\n",
    "\n",
    "\n",
    "\n",
    "<font size=4> DE_Score_cnn: </font> \n",
    "\n",
    "<font size=3>Trainability, expressivity, and generalization are three important, distinct, and complementary properties to characterize and understand neural networks. For our prototype DE_Score_base, we combine network expressivity and trainablity as our proxy for performance predictor.</font>\n",
    "<img src=\"./img/DE_Score_base.PNG\" width=\"50%\">\n",
    "<center>DE_Score_cnn algorithm</center>\n",
    "<font size=4> DE_Score_transformer: </font> \n",
    "\n",
    "<font size=3> Recently, Transformer has achieved remarkable success in several natural language processing. The progresses are highly relevant to the architecture design, then it is worthwhile to propose Transformer based Neural Architecture Search to search for better automatically. We will propose an unified effective method to synaptic diversity of MSA(multi-head self-attention) and synaptic saliency of MLP, which are the basic component of transformer.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
