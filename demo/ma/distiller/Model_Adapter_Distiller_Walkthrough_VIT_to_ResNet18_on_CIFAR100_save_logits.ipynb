{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea0224",
   "metadata": {},
   "source": [
    "# Model Adapter Distiller Walkthrough DEMO - Save logits\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and datasets. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. However, during the distillation process, teacher forwarding usually takes a lot of time. We can use logits saving function in distiller to save predictions from teacher in adavance, then lots of time can be saved during student training. \n",
    "\n",
    "This demo mainly introduces the usage of Distiller saving logits function. Take image classification as an example, it shows how to use distiller to save logits from VIT pretrained model, which will be used to guide the learning of ResNet18 in next [demo](./Model_Adapter_Distiller_Walkthrough_VIT_to_ResNet18_CIFAR100_train_with_logits.ipynb).\n",
    "\n",
    "To enable saving logits function, we just need to add two steps in original pipeline:\n",
    "- Wrap train_dataset with DataWrapper\n",
    "- Call prepare_logits() in Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb236d1",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* [Model Adapter Distiller Overview](#Model-Adapter-Distller-Overview)\n",
    "* [1. Environment Setup](#1.-Environment-Setup)\n",
    "* [2. Save Logits with Distiller](#2.-Save-Logits-with-Distiller)\n",
    "    * [2.1 Prepare Data](#2.1-Prepare-Data)\n",
    "    * [2.2 Create Distiller](#2.2-Create-Distiller)\n",
    "    * [2.3 Save Logits](#2.3-Save-Logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a0a46",
   "metadata": {},
   "source": [
    "# Model Adapter Distiller Overview\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "\n",
    "<img src=\"../imgs/distiller.png\" width=\"60%\">\n",
    "<center>Model Adapter Distiller Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d390c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "1.1 prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init –recursive\n",
    "    ```\n",
    "    \n",
    "1.2 build docker image\n",
    "   proxy\n",
    "   ``` bash\n",
    "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
    "   ```\n",
    "   \n",
    "1.3 run docker\n",
    "   ``` bash\n",
    "   sshpass -p docker ssh ${host0} -p 12347\n",
    "   ```\n",
    "   \n",
    "1.4 Run in conda and set up e2eAIOK\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   ```\n",
    "   \n",
    "1.5 Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ce35d",
   "metadata": {},
   "source": [
    "# 2. Save Logits with Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c2e32",
   "metadata": {},
   "source": [
    "Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f878badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28cbf4",
   "metadata": {},
   "source": [
    "## 2.1 Prepare Data\n",
    "### Prepare transformer and dataset\n",
    "For teacher, as pretrained model is trained on large imgage size, scale 32\\*32 to 224\\*224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c016312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGE_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.Resize(224),  # pretrained model is trained on large imgage size, scale 32x32 to 224x224\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69896736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_folder='/home/vmagent/app/data/dataset/cifar' # dataset location\n",
    "train_set = datasets.CIFAR100(root=data_folder, train=True, download=True, transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ccda0",
   "metadata": {},
   "source": [
    "### Warp dataset with DataWrapper\n",
    "Warp train dataset with DataWrapper, which helps to save data augmentation information during the forwarding of teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b46a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.distiller.utils import logits_wrap_dataset\n",
    "logits_path = \"/home/vmagent/app/data/model/demo/distiller/cifar100_kd_vit/logits_demo\" # path to save the logits\n",
    "train_set = logits_wrap_dataset(train_set, logits_path=logits_path, num_classes=100, save_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210a1e2",
   "metadata": {},
   "source": [
    "### Create dataloader\n",
    "\n",
    "Note: We need to save all the data without any sampling, make sure you have disable \"channel_last\" or \"sampler\" in dataloader, which can avoid data lossing in later logits using process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232dee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=128, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc18f19",
   "metadata": {},
   "source": [
    "## 2.2 Create Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ac60a",
   "metadata": {},
   "source": [
    "### Prepare teacher model\n",
    "To use distiller, we need to prepare teacher model to guide the training. Here we select pretrained [vit_base-224-in21k-ft-cifar100 from HuggingFace](https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b99862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "teacher_model = ViTForImageClassification.from_pretrained('edumunozsala/vit_base-224-in21k-ft-cifar100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481901ed",
   "metadata": {},
   "source": [
    "### Create Distiller with KD type\n",
    "### Define Distiller\n",
    "Here we define a distiller using KD algorithm, and it take a teacher model as input. If teacher comes from Hugginface, please clarify \"teacher_type\" with a name starting with \"huggingface\", otherwise no need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d750e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2eAIOK.ModelAdapter.engine_core.distiller import KD\n",
    "distiller= KD(teacher_model,teacher_type=\"huggingface_vit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f1c55",
   "metadata": {},
   "source": [
    "## 2.3 Save Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cedb3",
   "metadata": {},
   "source": [
    "Call prepare_logits() of distiller to save the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15548288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-13 06:48:11 save 0/391\n",
      "2023-02-13 06:49:03 save 10/391\n",
      "2023-02-13 06:49:55 save 20/391\n",
      "2023-02-13 06:50:47 save 30/391\n",
      "2023-02-13 06:51:40 save 40/391\n",
      "2023-02-13 06:52:31 save 50/391\n",
      "2023-02-13 06:53:24 save 60/391\n",
      "2023-02-13 06:54:15 save 70/391\n",
      "2023-02-13 06:55:05 save 80/391\n",
      "2023-02-13 06:55:56 save 90/391\n",
      "2023-02-13 06:56:47 save 100/391\n",
      "2023-02-13 06:57:39 save 110/391\n",
      "2023-02-13 06:58:29 save 120/391\n",
      "2023-02-13 06:59:20 save 130/391\n",
      "2023-02-13 07:00:11 save 140/391\n",
      "2023-02-13 07:01:02 save 150/391\n",
      "2023-02-13 07:01:52 save 160/391\n",
      "2023-02-13 07:02:44 save 170/391\n",
      "2023-02-13 07:03:36 save 180/391\n",
      "2023-02-13 07:04:27 save 190/391\n",
      "2023-02-13 07:05:17 save 200/391\n",
      "2023-02-13 07:06:08 save 210/391\n",
      "2023-02-13 07:06:59 save 220/391\n",
      "2023-02-13 07:07:48 save 230/391\n",
      "2023-02-13 07:08:38 save 240/391\n",
      "2023-02-13 07:09:31 save 250/391\n",
      "2023-02-13 07:10:22 save 260/391\n",
      "2023-02-13 07:11:13 save 270/391\n",
      "2023-02-13 07:12:03 save 280/391\n",
      "2023-02-13 07:12:54 save 290/391\n",
      "2023-02-13 07:13:46 save 300/391\n",
      "2023-02-13 07:14:39 save 310/391\n",
      "2023-02-13 07:15:30 save 320/391\n",
      "2023-02-13 07:16:20 save 330/391\n",
      "2023-02-13 07:17:10 save 340/391\n",
      "2023-02-13 07:18:00 save 350/391\n",
      "2023-02-13 07:18:53 save 360/391\n",
      "2023-02-13 07:19:44 save 370/391\n",
      "2023-02-13 07:20:36 save 380/391\n",
      "2023-02-13 07:21:27 save 390/391\n",
      "Epoch 0 took 2001.7463374137878 seconds\n"
     ]
    }
   ],
   "source": [
    "distiller.prepare_logits(train_loader, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
