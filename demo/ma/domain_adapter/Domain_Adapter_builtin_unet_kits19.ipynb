{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a80bb39",
   "metadata": {},
   "source": [
    "# Domain Adapter Built-in Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f19121",
   "metadata": {},
   "source": [
    "Transfer Learning is a general and convenient framework for transfer knowledge from pretrained model and/or source domain data to target task. Specifically,\n",
    "* **Fine-tuning**: Transfer knowledge from pretrained model with the same/different network structure, which greatly speedups training without accuracy regression.\n",
    "* **Domain adaption**: Transfer knowledge from source domain data without target label.\n",
    "\n",
    "Directly applying pre-trained model into target domain cannot always work due to covariate shift and label shift, while fine-tuning is also not working due to the expensive labeling in some domains. Even if users invest resource in labeling, it will be time-consuming and delays the model deployment.\n",
    "\n",
    "In this demo, we will introduce how to use domain adaptation to transfer knowledge in medical image semantic segmentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6041c3ec",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e31c9d2c",
   "metadata": {},
   "source": [
    "* [Domain Adapter Overview](#Domain-Adapter-Overview)\n",
    "* [Getting Started](#Getting-Started)\n",
    "    * [Environment Setup](#Environment-Setup)\n",
    "    * [Task Description](#Task-Description)\n",
    "    * [Launch Training](#Launch-Training)\n",
    "    * [Launch Inference & Evaluation](#Launch-Inference-&-Evaluation)\n",
    "    * [Visualization](#Visualization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4378f208",
   "metadata": {},
   "source": [
    "# Domain Adapter Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5215cbde",
   "metadata": {},
   "source": [
    "Adapter aims at reusing the transferable knowledge with the help of another labeled dataset with same learning task. That is, achieving better generalization with little labeled target dataset or achieving a competitive performance in label-free target dataset.\n",
    "\n",
    "The following picture show the network strcture of domain adaption, which add a discriminator to users' base network, and try to differentiate the souce domain data and target domain data, hence, it can force the feature extractor to learn a generalized feature representation among domains.\n",
    "\n",
    "![Adapter](../imgs/adapter.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d70f725",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f2ca5b",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f58bf8c3",
   "metadata": {},
   "source": [
    "1. prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init â€“recursive\n",
    "    ```\n",
    "2. pull docker image\n",
    "   ```\n",
    "   docker pull e2eaiok/e2eaiok-pytorch112\n",
    "   ```\n",
    "3. run docker\n",
    "   ``` bash\n",
    "   docker run -it --name model_adapter --shm-size=10g --privileged --network host \\\n",
    "   -v ${dataset_path}:/home/vmagent/app/data  \\\n",
    "   -v `pwd`:/home/vmagent/app/e2eaiok \\\n",
    "   -w /home/vmagent/app/e2eaiok e2eaiok/e2eaiok-pytorch112 /bin/bash \n",
    "   ```\n",
    "4. Run in conda and install dependency\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   cd /home/vmagent/app/e2eaiok/modelzoo/unet\n",
    "   sh patch_unet.sh\n",
    "   ```\n",
    "5. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   pip install jupyter && \\\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56ee020",
   "metadata": {},
   "source": [
    "## Task Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeee25f2",
   "metadata": {},
   "source": [
    "* Our source domain is AMOS dataset(Download AMOS data from [here](https://amos22.grand-challenge.org/Dataset/)), which provides 500 CT and 100 MRI scans with voxel-level annotations of 15 abdominal organs, including the spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, pancreas, right adrenal gland, left adrenal gland, duodenum, bladder, prostate/uterus.\n",
    "* Our target domain is KiTS dataset(Download KiTS data from [here](https://github.com/neheller/kits19)), which provides 300 CT scans with voxel-level annotations of kidney organs and kidney tumor.\n",
    "* Remember to put all your data in right places, now your files should be located at:\n",
    "   - Images at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/imagesTr/```\n",
    "   - Labels/Segmentations at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/labelsTr/```\n",
    "   - Please refer to [here](https://github.com/MIC-DKFZ/nnUNet) to know how to put all your data in your `${dataset_path}` in right format.\n",
    "* Our task is to explore reliable kidney semantic segmentation methodologies with the help of labeled AMOS dataset and unlabeled KiTS dataset, evalutaion metric is kidney dice score in target domain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25dbd171",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a67fdf64",
   "metadata": {},
   "source": [
    "- We take [3D-UNet](https://arxiv.org/abs/1606.06650) as users' base model\n",
    "- We will first pre-train model in AMOS dataset, and use this pre-trained model later for prameter initialization for domain adaptation\n",
    "- Then we apply domain adaptation algorithm to transfer knowledge from AMOS dataset to KiTS dataset\n",
    "    - We use a DANN-like model architecture, the DANN algorithm is illustrated as follows:\n",
    "    ![dann](../imgs/dann.png)\n",
    "- Notice: \n",
    "    - we donot use **any label** from target domain KiTS, we only use label from source domain AMOS for training\n",
    "    - *For demostration, we only train 1 epochs:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea13eb9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "For that I will be using the following source data configuration:\n",
      "num_classes:  15\n",
      "modalities:  {0: 'CT'}\n",
      "use_mask_for_norm OrderedDict([(0, False)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'CT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([140, 264, 264]), 'current_spacing': array([3.22, 1.62, 1.62]), 'original_spacing': array([3.22, 1.62, 1.62]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "\n",
      "I am using source data from this folder:  /home/vmagent/app/dataset/nnUNet_preprocessed/Task508_AMOS_kidney/nnUNetData_plans_v2.1_trgSp_kits19\n",
      "###############################################\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainer_DA_V2.nnUNetTrainer_DA_V2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  1\n",
      "modalities:  {0: 'CT'}\n",
      "use_mask_for_norm OrderedDict([(0, False)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'CT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 192, 192]), 'median_patient_size_in_voxels': array([ 84, 291, 291]), 'current_spacing': array([3.22, 1.62, 1.62]), 'original_spacing': array([3.22, 1.62, 1.62]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /home/vmagent/app/dataset/nnUNet_preprocessed/Task507_KiTS_kidney/nnUNetData_plans_v2.1_trgSp_kits19\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2022-10-14 07:07:07.833856: Using splits from existing split file: /home/vmagent/app/dataset/nnUNet_preprocessed/Task507_KiTS_kidney/splits_final.pkl\n",
      "2022-10-14 07:07:07.834207: The split file contains 5 splits.\n",
      "2022-10-14 07:07:07.834274: Desired fold for training: 1\n",
      "2022-10-14 07:07:07.834334: This split has 4 training and 1 validation cases.\n",
      "loading dataset\n",
      "loading all case properties\n",
      "unpacking dataset\n",
      "done\n",
      "################### Loading pretrained weights from file  /home/vmagent/app/dataset/nnUNet_trained_models/nnUNet/3d_fullres/Task508_AMOS_kidney/nnUNetTrainerV2__nnUNetPlansv2.1_trgSp_kits19/fold_1/model_final_checkpoint.model ###################\n",
      "################### Done ###################\n",
      "2022-10-14 07:07:09.178776: lr index 0: 0.002\n",
      "2022-10-14 07:07:09.179135: lr index 1: 0.01\n",
      "2022-10-14 07:07:09.190930: WARNING!!! You are attempting to run training on a CPU (torch.cuda.is_available() is False). This can be VERY slow!\n",
      "2022-10-14 07:07:12.011414: Unable to plot network architecture:\n",
      "2022-10-14 07:07:12.012101: No module named 'hiddenlayer'\n",
      "2022-10-14 07:07:12.012269: \n",
      "printing the network instead:\n",
      "\n",
      "2022-10-14 07:07:12.012412: Generic_UNet_DA_V2(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(480, 240, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(240, 240, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(120, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(120, 60, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(60, 60, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(60, 30, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(30, 30, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(1, 30, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(30, 30, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(30, 60, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(60, 60, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(60, 120, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(120, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(120, 240, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(240, 240, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(240, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "    (1): ConvTranspose3d(320, 240, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(240, 120, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(120, 60, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(60, 30, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(240, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(120, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(60, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(30, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2022-10-14 07:07:12.019301: \n",
      "\n",
      "2022-10-14 07:07:12.020003: \n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 08:03:19.386149: train loss : 1.0404\n",
      "2022-10-14 08:05:40.979750: validation loss: 0.0087\n",
      "2022-10-14 08:05:40.981636: Average global foreground Dice: [0.0]\n",
      "2022-10-14 08:05:40.981980: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2022-10-14 08:05:41.279021: lr index 0: 0.001982\n",
      "2022-10-14 08:05:41.279208: lr index 1: 0.00991\n",
      "2022-10-14 08:05:41.279391: This epoch took 3509.259070 s\n",
      "\n",
      "2022-10-14 08:05:41.280476: saving checkpoint...\n",
      "2022-10-14 08:05:41.633380: done, saving took 0.35 seconds\n",
      "case_00004 (2, 80, 309, 309)\n",
      "debug: mirroring True mirror_axes (0, 1, 2)\n",
      "step_size: 0.5\n",
      "do mirror: True\n",
      "data shape: (1, 80, 309, 309)\n",
      "patch size: [ 80 160 160]\n",
      "steps (x, y, and z): [[0], [0, 74, 149], [0, 74, 149]]\n",
      "number of tiles: 9\n",
      "computing Gaussian\n",
      "done\n",
      "prediction done\n",
      "force_separate_z: None interpolation order: 1\n",
      "separate z: True lowres axis [0]\n",
      "separate z, order in z is 0 order inplane is 1\n",
      "2022-10-14 08:07:06.740799: finished prediction\n",
      "2022-10-14 08:07:06.742050: evaluation of raw predictions\n",
      "/work/AIDK/AIDK/TransferLearningKit/src/task/medical_segmentation/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
      "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
      "2022-10-14 08:07:07.743454: mean dice score is: 0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "! cd /home/vmagent/app/e2eaiok/modelzoo/unet && sh scripts/run_single_opt.sh 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d9de47",
   "metadata": {},
   "source": [
    "## Launch Inference & Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba19a5d",
   "metadata": {
    "id": "LetWZXletANY"
   },
   "source": [
    "* We use following command for perform inference and evaluation, you can find your predictions in `${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict/`\n",
    "\n",
    "* In our experiement, even without the target label data, adapter achieve 10.67x training speedup, while keep the 93% performance ratio.\n",
    "    * Notice: this result is for illustration only and will depend on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /home/vmagent/app/e2eaiok/modelzoo/unet && sh scripts/run_predict.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d82954e1",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7f11904",
   "metadata": {},
   "source": [
    "- For this we would advise to use [MITK](https://www.mitk.org/wiki/The_Medical_Imaging_Interaction_Toolkit_(MITK)) which already has some great [tutorials](https://www.mitk.org/wiki/Tutorials). \n",
    "    - If you have not already downloaded it, here is the [MITK Download Link](https://www.mitk.org/wiki/Downloads)\n",
    "    \n",
    "- Here is a demostration of visualization result from MITK on KiTS dataset\n",
    "\n",
    "![KiTS_visualization](../imgs/KiTS_visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e903885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (default, Aug 30 2018, 14:32:33) \n[GCC 8.2.1 20180801 (Red Hat 8.2.1-2)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
