{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "294b76e8",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/e2eAIOK/blob/main/demo/ma/domain_adapter/Model_Adapter_Domain_Adapter_Walkthrough_Unet_KITS19.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6b825a6c",
      "metadata": {},
      "source": [
        "# Model Adapter Domain Adapter Walkthrough Unet KITS19"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca37cef7",
      "metadata": {},
      "source": [
        "In this demo, we will introduce how to use Domain Adapter to transfer knowledge in medical image semantic segmentation.\n",
        "Unlike the [built-in demo](./Model_Adapter_Domain_Adapter_builtin_Unet_KITS19.ipynb), we will illustrate how to invoke the Model Adaptor API on your own workflow."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "248788bf",
      "metadata": {},
      "source": [
        "# Content\n",
        "\n",
        "* [Overview](#overview)\n",
        "    * [Model Adapter Domain Adapter Overview](#Model-Adapter-Domain-Adapter-Overview)\n",
        "* [Getting Started](#Getting-Started)\n",
        "    * [1. Environment Setup](#1.-Environment-Setup)\n",
        "    * [2. Data Prepare](#2.-data-prepare)\n",
        "    * [3. Model Prepare](#3.-model-prepare)\n",
        "    * [4. Train](#4.-train)\n",
        "    * [5. Inference](#5.-inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114e7427",
      "metadata": {},
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Adapter Domain Adapter Overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6db049db",
      "metadata": {},
      "source": [
        "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
        "\n",
        "Directly applying pre-trained model into target domain cannot always work due to covariate shift and label shift, while fine-tuning is also not working due to the expensive labeling in some domains. Even if users invest resource in labeling, it will be time-consuming and delays the model deployment.\n",
        "\n",
        "Domain Adapter aims at reusing the transferable knowledge with the help of another labeled dataset with same learning task. That is, achieving better generalization with little labeled target dataset or achieving a competitive performance in label-free target dataset.\n",
        "\n",
        "The following picture show the network strcture of domain adaption, which add a discriminator to users' base network, and try to differentiate the souce domain data and target domain data, hence, it can force the feature extractor to learn a generalized feature representation among domains.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src='../imgs/adapter.png' width='80%' height='80%' title='Adapter Architecture'>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fecf3c9",
      "metadata": {},
      "source": [
        "# Getting Started"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "34ccaca4",
      "metadata": {},
      "source": [
        "- **Note1: this demo cannot run directly on colab, since it require you to download dataset manually, and store all files according to the specified directory hierarchy. Please refer to [2. Data Prepare](#2-data-prepare) for more details.**\n",
        "- **Note2: The performance data from this demo is just based on a sampled dataset for better demonstration, any performance data in the below cell does not stand for the actual performance of this toolkit.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "56c57153",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Option 1) Use Pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64bc60b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install e2eAIOK-ModelAdapter --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d909d08",
      "metadata": {},
      "source": [
        "### (Option 2) Use Docker\n",
        "\n",
        "Step1. prepare code\n",
        "   ``` bash\n",
        "   git clone https://github.com/intel/e2eAIOK.git\n",
        "   cd e2eAIOK\n",
        "   git submodule update --init –recursive\n",
        "   ```\n",
        "    \n",
        "Step2. build docker image\n",
        "   ``` bash\n",
        "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
        "   ```\n",
        "   \n",
        "Step3. run docker and start conda env\n",
        "   ``` bash\n",
        "   sshpass -p docker ssh ${host0} -p 12347\n",
        "   conda activate pytorch-1.12.0\n",
        "   ```\n",
        "  \n",
        "Step4. Start the jupyter notebook and tensorboard service\n",
        "   ``` bash\n",
        "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
        "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
        "   ```\n",
        "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c036b323",
      "metadata": {},
      "source": [
        "## 2. Data Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Download"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3e8e52de",
      "metadata": {},
      "source": [
        "* Our source domain is AMOS dataset(Download AMOS data from [here](https://amos22.grand-challenge.org/Dataset/)), which provides 500 CT and 100 MRI scans with voxel-level annotations of 15 abdominal organs, including the spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, pancreas, right adrenal gland, left adrenal gland, duodenum, bladder, prostate/uterus.\n",
        "* Our target domain is KiTS dataset(Download KiTS data from [here](https://github.com/neheller/kits19)), which provides 300 CT scans with voxel-level annotations of kidney organs and kidney tumor.\n",
        "* Our task is to explore reliable kidney semantic segmentation methodologies with the help of labeled AMOS dataset and unlabeled KiTS dataset, evalutaion metric is kidney dice score in target domain."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2e24033e",
      "metadata": {
        "id": "2e24033e"
      },
      "source": [
        "- Then, setup some enviroment variables\n",
        "    - It tell the program where to read data, and where to write the output model and log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa82c8ed",
      "metadata": {
        "id": "aa82c8ed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['nnUNet_raw_data_base'] = \"/home/vmagent/app/data/nnUNet_raw_data_base\" \n",
        "os.environ['nnUNet_preprocessed'] = \"/home/vmagent/app/data/nnUNet_preprocessed\"\n",
        "os.environ['RESULTS_FOLDER'] = \"/home/vmagent/app/data/nnUNet_trained_models\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "28e90bd5",
      "metadata": {},
      "source": [
        "* After downloading the dataset, remember to put all your data in right places, now your files should be located at:\n",
        "   - Images at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/TaskId_TaskName/imagesTr/```\n",
        "   - Labels/Segmentations at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/TaskId_TaskName/labelsTr/```\n",
        "   - Please refer to [here](https://github.com/MIC-DKFZ/nnUNet) to know how to put all your data in your `${dataset_path}` in right format."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "98b52a0f",
      "metadata": {
        "id": "98b52a0f"
      },
      "source": [
        "- Now the structure should look like (*for simlicy, we only take 5 case of each task for demostration*):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20621b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/home/vmagent/app/dataset/nnUNet_raw_data_base/nnUNet_raw_data\u001b[00m\r\n",
            "├── \u001b[01;34mTask041_KiTS\u001b[00m\r\n",
            "│   ├── dataset.json\r\n",
            "│   ├── \u001b[01;34mimagesTr\u001b[00m\r\n",
            "│   │   ├── \u001b[01;31mcase_00000_0000.nii.gz\u001b[00m\r\n",
            "│   │   ├── \u001b[01;31mcase_00001_0000.nii.gz\u001b[00m\r\n",
            "│   │   ├── \u001b[01;31mcase_00002_0000.nii.gz\u001b[00m\r\n",
            "│   │   ├── \u001b[01;31mcase_00003_0000.nii.gz\u001b[00m\r\n",
            "│   │   └── \u001b[01;31mcase_00004_0000.nii.gz\u001b[00m\r\n",
            "│   └── \u001b[01;34mlabelsTr\u001b[00m\r\n",
            "│       ├── \u001b[01;31mcase_00000.nii.gz\u001b[00m\r\n",
            "│       ├── \u001b[01;31mcase_00001.nii.gz\u001b[00m\r\n",
            "│       ├── \u001b[01;31mcase_00002.nii.gz\u001b[00m\r\n",
            "│       ├── \u001b[01;31mcase_00003.nii.gz\u001b[00m\r\n",
            "│       └── \u001b[01;31mcase_00004.nii.gz\u001b[00m\r\n",
            "└── \u001b[01;34mTask505_AMOS\u001b[00m\r\n",
            "    ├── \u001b[01;34mimagesTr\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0001.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0004.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0005.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0006.nii.gz\u001b[00m\r\n",
            "    │   └── \u001b[01;31mamos_0007.nii.gz\u001b[00m\r\n",
            "    ├── \u001b[01;34mlabelsTr\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0001.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0004.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0005.nii.gz\u001b[00m\r\n",
            "    │   ├── \u001b[01;31mamos_0006.nii.gz\u001b[00m\r\n",
            "    │   └── \u001b[01;31mamos_0007.nii.gz\u001b[00m\r\n",
            "    └── task1_dataset.json\r\n",
            "\r\n",
            "6 directories, 22 files\r\n"
          ]
        }
      ],
      "source": [
        "!tree $nnUNet_raw_data_base/nnUNet_raw_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed565ba3",
      "metadata": {},
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd18fcc",
      "metadata": {},
      "source": [
        "- In this part, we do the following thing:\n",
        "    - Keep the both data in the same axis ordering, for background knowledge, you can refer to [here](https://www.jarvis73.com/2019/06/24/Medical-Imaging-Guide/#13-%E5%9D%90%E6%A0%87%E7%B3%BB%E7%BB%9F)\n",
        "        - Axis ordering: it determines in what direction we see the medical image, it is adjustable, and something like rotation in natural images, we should make the two dataset have same perspective;\n",
        "    - Change the tumor annotation in KiTS to kidney, because we cannot know the tumor from source domain AMOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c35ed11",
      "metadata": {
        "id": "2c35ed11",
        "outputId": "d22010ad-421d-4ef6-b87b-446f75fa7ce5"
      },
      "outputs": [],
      "source": [
        "%cd modelzoo/unet/nnUNet/nnunet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b46833",
      "metadata": {},
      "outputs": [],
      "source": [
        "%% bash\n",
        "python dataset_conversion/amos_convert_label.py\n",
        "python dataset_conversion/kits_convert_label.py basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824fadb4",
      "metadata": {},
      "source": [
        "- Before going any further, verify that the data is present and labels and data matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288f18c4",
      "metadata": {
        "id": "288f18c4",
        "outputId": "e8983523-f67f-4de5-b710-1e0127b6f00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying training set\n",
            "checking case case_00000\n",
            "checking case case_00001\n",
            "checking case case_00002\n",
            "checking case case_00003\n",
            "checking case case_00004\n",
            "Verifying label values\n",
            "Expected label values are [0, 1]\n",
            "Labels OK\n",
            "Dataset OK\n"
          ]
        }
      ],
      "source": [
        "!nnUNet_plan_and_preprocess -t 507 --verify_dataset_integrity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22797018",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying training set\n",
            "checking case amos_0001\n",
            "checking case amos_0004\n",
            "checking case amos_0005\n",
            "checking case amos_0006\n",
            "checking case amos_0007\n",
            "Verifying label values\n",
            "Expected label values are [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Labels OK\n",
            "Dataset OK\n"
          ]
        }
      ],
      "source": [
        "!nnUNet_plan_and_preprocess -t 508 --verify_dataset_integrity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Target Spacing Sample & Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8971113d",
      "metadata": {},
      "source": [
        "- We need to perform same target spacing sample && normalization in both domains, and saves it into the \"nnUNet_preprocessed\" folder.\n",
        "\n",
        "    - Voxel Spacing: it is the distance between voxels, it influence the image size, we can understand it as the resolution in natural images. Every image have different voxel spacing even if they are in the exact one dataset, it is not suitable for convolution operations according to literature, so we usually doing some resampling operations to make the voxel spacing is same in every image of both dataset;\n",
        "\n",
        "    - Intensity: it is the float value of every pixel in each slice of the grey CT images, usually same organs have similar intensity distribution even if they are captured by different scanners. Currently we use the intensity mean and std from the foreground of source domain dataset to perform normalization in both datasets, since foreground of source dataset have more classes, and we need to segmentation target dataset to these classes, so target dataset executed the same normalization.\n",
        "    \n",
        "- So we first process the target domain, get the dataset characteristic, and then apply it to the source domain\n",
        "- Also some rule based parameters will be extracted in this step, such as model architecture, learning rate, batch size..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1935c266",
      "metadata": {
        "collapsed": true,
        "id": "1935c266",
        "outputId": "54ec9b31-6d19-4110-d0cd-807dddd74c45"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "nnUNet_plan_and_preprocess -t 508 -pl2d None -pl3d ExperimentPlanner3D_v21_customTargetSpacing_kits19\n",
        "nnUNet_plan_and_preprocess -t 507 -pl2d None -pl3d ExperimentPlanner3D_v21_customTargetSpacing_kits19 -no_pp\n",
        "python dataset_conversion/kits_convert_label.py intensity\n",
        "nnUNet_plan_and_preprocess -t 507 -pl2d None -pl3d ExperimentPlanner3D_v21_customTargetSpacing_kits19 -no_plan"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "432fc3a0",
      "metadata": {},
      "source": [
        "## 3. Model Prepare"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ced2f9dc",
      "metadata": {},
      "source": [
        "### Demo Code Prepare"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "96e8b678",
      "metadata": {},
      "source": [
        "- First download the workflow preparation script\n",
        "    ``` bash\n",
        "    wget https://raw.githubusercontent.com/intel/e2eAIOK/main/demo/ma/domain_adapter/workflow_prepare_ma_da.sh\n",
        "    ```\n",
        "- Then run this script to prepare the workflow\n",
        "    ```bash\n",
        "    sh workflow_prepare_ma_da.sh\n",
        "    ```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrap model with Model Adapter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0b4844a7",
      "metadata": {},
      "source": [
        "In the demo code, we actually make some changes on users' original model by using the Model Adapter API.\n",
        "\n",
        "The following is an example on conversion on users' `backbone` model, after using Model Adapter API `make_transferrable_with_domain_adaption`, we get a converted model `converted_model`, then replace the original `backbone` model with this `converted_model` in users' training circle."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7455b5d3",
      "metadata": {},
      "source": [
        "```python\n",
        "from e2eAIOK.ModelAdapter.backbone.unet.generic_UNet_DA import Generic_UNet_DA\n",
        "from e2eAIOK.ModelAdapter.engine_core.adapter.adversarial.DA_Loss import CACDomainAdversarialLoss\n",
        "from e2eAIOK.ModelAdapter.engine_core.transferrable_model import make_transferrable_with_domain_adaption\n",
        "\n",
        "backbone = Generic_UNet_DA(\n",
        "    self.threeD, self.num_input_channels, \n",
        "    self.base_num_features, self.num_classes,         \n",
        "    self.conv_per_stage, self.net_num_pool_op_kernel_sizes, \n",
        "    self.net_conv_kernel_sizes\n",
        ")\n",
        "\n",
        "adv_kwargs = {\n",
        "    'input_channels': backbone.encoder_channels,\n",
        "    'threeD': self.threeD,\n",
        "    'pool_op_kernel_sizes': self.net_num_pool_op_kernel_sizes,\n",
        "    'loss_weight': self.loss_weights[2:]\n",
        "}\n",
        "cac_domain_adv = CACDomainAdversarialLoss(**adv_kwargs)\n",
        "\n",
        "converted_model = make_transferrable_with_domain_adaption(\n",
        "    backbone, None, cac_domain_adv, \n",
        "    False, self.source_loss_weight, 1.0)\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3db4cbfb",
      "metadata": {},
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-train Target Domain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33198d4",
      "metadata": {},
      "source": [
        "- We will first pre-train model in AMOS dataset, and use this pre-trained model later for prameter initialization for domain adaptation\n",
        "- We use [3D-UNet](https://arxiv.org/abs/1606.06650) to train the model\n",
        "- *For demostration, we only train 1 epochs:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c17945",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "nnUNet_train 3d_fullres nnUNetTrainerV2 508 1 --epochs 1 -p nnUNetPlansv2.1_trgSp_kits19 --disable_postprocessing_on_folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Domain Adaption from AMOS to KiTS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "949e0fb1",
      "metadata": {},
      "source": [
        "- We use a DANN-like model architecture, the DANN algorithm is illustrated as follows:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src='../imgs/dann.png' width='80%' height='80%' title='DANN Architecture'>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2ea85b",
      "metadata": {
        "id": "cd2ea85b"
      },
      "source": [
        "- Now we use Model Adapter API to transfer knowledge from AMOS dataset to KiTS dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ff69ba5f",
      "metadata": {
        "id": "ff69ba5f"
      },
      "source": [
        "- After using `make_transferrable_with_domain_adaption`, we got an adapted model, we use this model for further training. We use the following command to start training, we omit the training process since it will take very long time(hundreds of hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea13eb9",
      "metadata": {
        "collapsed": true,
        "id": "6ea13eb9",
        "outputId": "d85b6177-7b23-4e39-c3f5-bacb833c8533"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "nnUNet_train_da 3d_fullres nnUNetTrainer_DA_V2 508 507 1 \\\n",
        "    -p nnUNetPlansv2.1_trgSp_kits19 \\\n",
        "    -sp nnUNetPlansv2.1_trgSp_kits19 \\\n",
        "    --epochs 1 --loss_weights 1 0 1 0 0 \\\n",
        "    -pretrained_weights /home/vmagent/app/dataset/nnUNet_trained_models/nnUNet/3d_fullres/Task508_AMOS_kidney/nnUNetTrainerV2__nnUNetPlansv2.1_trgSp_kits19/fold_1/model_final_checkpoint.model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9b820e",
      "metadata": {},
      "source": [
        "- Notice: \n",
        "    - we donot use **any label** from target domain KiTS, we only use label from source domain AMOS for training\n",
        "    - *For demostration, we only train 1 epochs:*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "43cce6b0",
      "metadata": {},
      "source": [
        "## 5. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference on KiTS Dataset with Adapted Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2b4d13",
      "metadata": {},
      "source": [
        "- Now we use the adapted model trained in last section to perferm inference on KiTS dataset\n",
        "\n",
        "- We use following command for perform inference and evaluation, you can find your predictions in `${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02dc02ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "!time nnUNet_predict \\\n",
        "    -i ${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/testTr/ \\\n",
        "    -o ${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict/ \\\n",
        "    -f 1 \\\n",
        "    -t 507 -m 3d_fullres -p nnUNetPlansv2.1_trgSp_kits19 \\\n",
        "    --disable_tta \\\n",
        "    -tr nnUNetTrainer_DA_V2 \\\n",
        "    --overwrite_existing \\\n",
        "    --disable_mixed_precision "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Prediction on KiTS Using the Given Label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d40972",
      "metadata": {},
      "source": [
        "- Note while evaluating: \n",
        "    - The label is not used in training, it is only used in this evaluation step\n",
        "    - In practical, if you donnot have any label, you can just skip this step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "37d5479d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The final dice score is 0.89\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "nnUNet_evaluate_folder \\\n",
        "    -ref ${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/labelsTr \\\n",
        "    -pred ${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict \\\n",
        "    -l 1 \\\n",
        "    --common"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of Data and Segmentations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c096371a",
      "metadata": {},
      "source": [
        "- Download files from server:\n",
        "   - Images from: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/imagesTr/```\n",
        "   - Segmentations from: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/labelsTr/```\n",
        "   - predictions from: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict/```\n",
        "- After downloading these files you can visualize them with any volumetric visualization program.\n",
        "For this we would advise to use [MITK](https://www.mitk.org/wiki/The_Medical_Imaging_Interaction_Toolkit_(MITK)) which already has some great [tutorials](https://www.mitk.org/wiki/Tutorials). \n",
        "    - If you have not already downloaded it, here is the [MITK Download Link](https://www.mitk.org/wiki/Downloads) \n",
        "- Here is a demostration of visualization result from MITK on KiTS dataset\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src='../imgs/KiTS_visualization.png' width='80%' height='80%' title='KiTS_visualization'>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154aa11b",
      "metadata": {
        "id": "154aa11b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e903885",
      "metadata": {
        "id": "8e903885"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
