{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5a9460",
   "metadata": {},
   "source": [
    "# AIOK Model Adapter Finetuner DEMO\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "This demo mainly introduces the usage of Finetuner. Take image classification as an example, it shows how to integrate finetuner with ResNet50 on CIFAR100 dataset. This is a build-in usage, you can find customized detailed demo at [here](./Model_Adapter_Finetuner_customized_resnet50_CIFAR100.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c721aa",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* [Model Adapter Finetuner Overview](#Model-Adapter-Finetuner-Overview)\n",
    "* [Getting Started](#Getting-Started)\n",
    "    * [Environment Setup](#Environment-Setup)\n",
    "    * [Launch Training on baseline](#Launch-Training-on-baseline)\n",
    "    * [Launch Training with Finetuner](#Launch-Training-with-Finetuner)\n",
    "    * [Performance](#Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87340cc8",
   "metadata": {},
   "source": [
    "## Model Adapter Finetunner Overview\n",
    "Finetuner is based on pretraining and finetuning technology, it can transfer knowledge from pretrained model to target model with same network structure. \n",
    "\n",
    "Pretrained models usually are generated by pretraining process, which is training specific model  on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace. Finetunner retrieves the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode. With finetunner, we can greatly improve training speed, and usually achieves better performance.\n",
    "\n",
    "<img src=\"../imgs/finetuner.png\" width=\"50%\">\n",
    "<center>Model Adapter Finetuner Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb1345",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fe6b4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "1. prepare code\n",
    "    ``` bash\n",
    "    git clone https://github.com/intel/e2eAIOK.git\n",
    "    cd e2eAIOK\n",
    "    git submodule update --init â€“recursive\n",
    "    ```\n",
    "2. build docker image\n",
    "   ```\n",
    "   cd Dockerfile-ubuntu18.04 && docker build -t e2eaiok-pytorch112 . -f DockerfilePytorch112 && cd .. && yes \n",
    "   ```\n",
    "3. run docker\n",
    "   ``` bash\n",
    "   docker run -it --name model_adapter --shm-size=10g --privileged --network host \\\n",
    "   -v ${dataset_path}:/home/vmagent/app/data  \\\n",
    "   -v `pwd`:/home/vmagent/app/e2eaiok \\\n",
    "   -w /home/vmagent/app/e2eaiok e2eaiok-pytorch112 /bin/bash \n",
    "   ```\n",
    "4. Run in conda and set up e2eAIOK\n",
    "   ```bash\n",
    "   conda activate pytorch-1.12.0\n",
    "   python setup.py sdist && pip install dist/e2eAIOK-*.*.*.tar.gz\n",
    "   ```\n",
    "5. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c8801",
   "metadata": {},
   "source": [
    "## Launch training on baseline\n",
    "First we train a vanilla ResNet50 on CIFAR100.\n",
    "\n",
    "### Configuration\n",
    "Create a configuration for ResNet50 with CIFAR100.\n",
    "```yaml\n",
    "# basic experiment setting\n",
    "experiment:\n",
    "  project: \"baseline\"\n",
    "  tag: \"cifar100_res50\"\n",
    "output_dir: \"/home/vmagent/app/data/model\"\n",
    "\n",
    "### dataset and model setting\n",
    "data_set: \"cifar100\"\n",
    "data_path:  \"/home/vmagent/app/data/dataset/cifar\"\n",
    "num_workers: 4\n",
    "\n",
    "model_type: \"resnet50\"\n",
    "\n",
    "## training setting\n",
    "train_epochs: 1\n",
    "optimizer: \"SGD\"\n",
    "learning_rate: 0.00753\n",
    "weight_decay: 0.00115\n",
    "momentum: 0.9\n",
    "\n",
    "lr_scheduler: \"CosineAnnealingLR\"\n",
    "lr_scheduler_config:\n",
    "    T_max: 200\n",
    "\n",
    "early_stop: \"EarlyStopping\"\n",
    "early_stop_config:\n",
    "    tolerance_epoch: 15\n",
    "\n",
    "```\n",
    "You can also find this configuration at [here](../../../conf/ma/demo/baseline/cifar100_res50.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce65be0",
   "metadata": {},
   "source": [
    "### Launch training\n",
    "**Training resnet50 on CIFAR100 from scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e294640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "configurations:\n",
      "{'train_batch_size': 128, 'start_epoch': 0, 'initial_pretrain': '', 'kd': {'temperature': 4}, 'drop_last': False, 'optimizer': 'SGD', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'enable_ipex': False, 'log_interval_step': 10, 'train_epochs': 1, 'metric_threshold': 100.0, 'profiler': False, 'warmup_scheduler_epoch': 0, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'eval_metric': 'accuracy', 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_resnet50_cifar100', 'weight_decay': 0.00115, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/baseline/cifar100_res50/profile/profile_resnet50_cifar100_1675653984'}, 'finetuner': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'pretrained_num_classes': 10, 'finetuned_lr': 0.01, 'frozen': False}, 'dist_backend': 'gloo', 'lr_scheduler': 'CosineAnnealingLR', 'output_dir': '/home/vmagent/app/data/model', 'device': 'cpu', 'model_save_interval': 40, 'learning_rate': 0.00753, 'pretrain': '', 'warmup_scheduler': '', 'early_stop': 'EarlyStopping', 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'input_size': 32, 'test_transform': 'default', 'pin_mem': False, 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'eval_batch_size': 128, 'eval_epochs': 1, 'eval_step': 10, 'momentum': 0.9, 'experiment': {'project': 'baseline', 'tag': 'cifar100_res50', 'strategy': ''}, 'data_set': 'cifar100', 'criterion': 'CrossEntropyLoss', 'seed': 0, 'model_type': 'resnet50', 'num_workers': 4, 'train_transform': 'default'}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "Epoch [0] learning rate: [0.00753]\n",
      "[2023-02-06 03:26:27] rank(0) epoch(0) step (0/391) Train: loss = 6.5908;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:35] rank(0) epoch(0) step (10/391) Train: loss = 5.6120;\taccuracy = 2.3438\n",
      "[2023-02-06 03:26:37] rank(0) epoch(0) step (20/391) Train: loss = 6.1267;\taccuracy = 0.0000\n",
      "[2023-02-06 03:26:39] rank(0) epoch(0) step (30/391) Train: loss = 5.4950;\taccuracy = 3.9062\n",
      "[2023-02-06 03:26:41] rank(0) epoch(0) step (40/391) Train: loss = 5.1365;\taccuracy = 2.3438\n",
      "[2023-02-06 03:26:43] rank(0) epoch(0) step (50/391) Train: loss = 5.8884;\taccuracy = 4.6875\n",
      "[2023-02-06 03:26:45] rank(0) epoch(0) step (60/391) Train: loss = 5.4579;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:47] rank(0) epoch(0) step (70/391) Train: loss = 5.1344;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:49] rank(0) epoch(0) step (80/391) Train: loss = 5.7166;\taccuracy = 3.1250\n",
      "[2023-02-06 03:26:51] rank(0) epoch(0) step (90/391) Train: loss = 5.5491;\taccuracy = 1.5625\n",
      "[2023-02-06 03:26:53] rank(0) epoch(0) step (100/391) Train: loss = 5.1875;\taccuracy = 2.3438\n",
      "[2023-02-06 03:27:00] rank(0) epoch(0) step (110/391) Train: loss = 5.4646;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:02] rank(0) epoch(0) step (120/391) Train: loss = 5.4944;\taccuracy = 3.1250\n",
      "[2023-02-06 03:27:04] rank(0) epoch(0) step (130/391) Train: loss = 6.2241;\taccuracy = 1.5625\n",
      "[2023-02-06 03:27:06] rank(0) epoch(0) step (140/391) Train: loss = 5.2008;\taccuracy = 0.0000\n",
      "[2023-02-06 03:27:08] rank(0) epoch(0) step (150/391) Train: loss = 5.4579;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:10] rank(0) epoch(0) step (160/391) Train: loss = 7.0275;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:12] rank(0) epoch(0) step (170/391) Train: loss = 4.3376;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:15] rank(0) epoch(0) step (180/391) Train: loss = 4.4311;\taccuracy = 7.0312\n",
      "[2023-02-06 03:27:17] rank(0) epoch(0) step (190/391) Train: loss = 5.0786;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:19] rank(0) epoch(0) step (200/391) Train: loss = 5.6823;\taccuracy = 4.6875\n",
      "[2023-02-06 03:27:28] rank(0) epoch(0) step (210/391) Train: loss = 5.6876;\taccuracy = 3.9062\n",
      "[2023-02-06 03:27:31] rank(0) epoch(0) step (220/391) Train: loss = 6.4414;\taccuracy = 3.9062\n",
      "[2023-02-06 03:27:34] rank(0) epoch(0) step (230/391) Train: loss = 5.6525;\taccuracy = 3.1250\n",
      "[2023-02-06 03:27:37] rank(0) epoch(0) step (240/391) Train: loss = 5.0057;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:40] rank(0) epoch(0) step (250/391) Train: loss = 6.0380;\taccuracy = 9.3750\n",
      "[2023-02-06 03:27:43] rank(0) epoch(0) step (260/391) Train: loss = 4.6368;\taccuracy = 6.2500\n",
      "[2023-02-06 03:27:46] rank(0) epoch(0) step (270/391) Train: loss = 4.4934;\taccuracy = 10.9375\n",
      "[2023-02-06 03:27:49] rank(0) epoch(0) step (280/391) Train: loss = 4.6952;\taccuracy = 13.2812\n",
      "[2023-02-06 03:27:52] rank(0) epoch(0) step (290/391) Train: loss = 4.4364;\taccuracy = 4.6875\n",
      "[2023-02-06 03:27:54] rank(0) epoch(0) step (300/391) Train: loss = 5.3730;\taccuracy = 5.4688\n",
      "[2023-02-06 03:28:03] rank(0) epoch(0) step (310/391) Train: loss = 6.9364;\taccuracy = 3.1250\n",
      "[2023-02-06 03:28:06] rank(0) epoch(0) step (320/391) Train: loss = 4.9181;\taccuracy = 3.1250\n",
      "[2023-02-06 03:28:08] rank(0) epoch(0) step (330/391) Train: loss = 5.3563;\taccuracy = 5.4688\n",
      "[2023-02-06 03:28:10] rank(0) epoch(0) step (340/391) Train: loss = 4.2426;\taccuracy = 6.2500\n",
      "[2023-02-06 03:28:12] rank(0) epoch(0) step (350/391) Train: loss = 4.8778;\taccuracy = 7.8125\n",
      "[2023-02-06 03:28:15] rank(0) epoch(0) step (360/391) Train: loss = 4.4332;\taccuracy = 7.8125\n",
      "[2023-02-06 03:28:17] rank(0) epoch(0) step (370/391) Train: loss = 4.9217;\taccuracy = 6.2500\n",
      "[2023-02-06 03:28:21] rank(0) epoch(0) step (380/391) Train: loss = 5.1312;\taccuracy = 4.6875\n",
      "[2023-02-06 03:28:23] rank(0) epoch(0) step (390/391) Train: loss = 5.2084;\taccuracy = 7.5000\n",
      "2023-02-06 03:28:30 0/391\n",
      "2023-02-06 03:28:31 10/391\n",
      "2023-02-06 03:28:31 20/391\n",
      "2023-02-06 03:28:32 30/391\n",
      "2023-02-06 03:28:32 40/391\n",
      "2023-02-06 03:28:33 50/391\n",
      "2023-02-06 03:28:34 60/391\n",
      "2023-02-06 03:28:34 70/391\n",
      "[2023-02-06 03:28:35] rank(0) epoch(0) Validation: accuracy = 7.7700;\tloss = 4.0517\n",
      "Best Epoch: 0, accuracy: 7.769999980926514\n",
      "Epoch 0 took 135.11660027503967 seconds\n",
      "Total seconds:135.1177\n",
      "Totally take 137.19904828071594 seconds\n"
     ]
    }
   ],
   "source": [
    "! python /home/vmagent/app/e2eaiok/e2eAIOK/ModelAdapter/main.py --cfg /home/vmagent/app/e2eaiok/conf/ma/demo/baseline/cifar100_res50.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551a8b5",
   "metadata": {},
   "source": [
    "## Launch Training with Finetuner\n",
    "Then we train ResNet50 on CIFAR100 with Finetuner to show the performance imrpovement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b8fae",
   "metadata": {},
   "source": [
    "### Prepare pretrained model \n",
    "Download pretrained ResNet50 model on ImageNet21k from [here](https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth) and put it at `${dataset_path}/pretrained/resnet50_miil_21k.pth`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5b7b7",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Create a configuration for Finetuner with ResNet50 with CIFAR100\n",
    "\n",
    "```yaml\n",
    "# basic experiment setting\n",
    "experiment:\n",
    "  project: \"finetuner\"\n",
    "  tag: \"cifar100_res50_PretrainI21k\"\n",
    "  strategy: \"OnlyFinetuneStrategy\"\n",
    "output_dir: \"/home/vmagent/app/data/model\"\n",
    "\n",
    "### dataset and model setting\n",
    "data_set: \"cifar100\"\n",
    "data_path:  \"/home/vmagent/app/data/dataset/cifar\"\n",
    "num_workers: 4\n",
    "input_size: 112\n",
    "\n",
    "model_type: \"resnet50\"\n",
    "\n",
    "## finetuner setting\n",
    "finetuner:\n",
    "    type: \"Basic\"\n",
    "    pretrain: '/home/vmagent/app/data/pretrained/resnet50_miil_21k.pth'\n",
    "    pretrained_num_classes: 11221\n",
    "    finetuned_lr: 0.00445\n",
    "    frozen: False\n",
    "\n",
    "## training setting\n",
    "train_epochs: 1\n",
    "optimizer: \"SGD\"\n",
    "learning_rate: 0.00753\n",
    "weight_decay: 0.00115\n",
    "momentum: 0.9\n",
    "\n",
    "lr_scheduler: \"CosineAnnealingLR\"\n",
    "lr_scheduler_config:\n",
    "    T_max: 200\n",
    "\n",
    "early_stop: \"EarlyStopping\"\n",
    "early_stop_config:\n",
    "    tolerance_epoch: 15\n",
    "```\n",
    "You can also find this configuration at [here](../../../conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6506202",
   "metadata": {},
   "source": [
    "### Launch Training with Finetuner\n",
    "**Training resnet50 on CIFAR100 with Finetuner:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fac25c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "See abnormal behavior in dataloader when enable IPEX in PyTorch 1.12, set enable_ipex to False!\n",
      "configurations:\n",
      "{'lr_scheduler': 'CosineAnnealingLR', 'pretrain': '', 'eval_epochs': 1, 'criterion': 'CrossEntropyLoss', 'data_set': 'cifar100', 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'output_dir': '/home/vmagent/app/data/model', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'eval_batch_size': 128, 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'enable_ipex': False, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'metric_threshold': 100.0, 'start_epoch': 0, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'train_batch_size': 128, 'experiment': {'strategy': 'OnlyFinetuneStrategy', 'project': 'finetuner', 'tag': 'cifar100_res50_PretrainI21k'}, 'optimizer': 'SGD', 'learning_rate': 0.00753, 'model_type': 'resnet50', 'seed': 0, 'device': 'cpu', 'eval_step': 10, 'num_workers': 4, 'momentum': 0.9, 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_PretrainI21k_resnet50_OnlyFinetuneStrategy_cifar100', 'warmup_scheduler': '', 'profiler': False, 'finetuner': {'frozen': False, 'pretrained_num_classes': 11221, 'pretrain': '/home/vmagent/app/data/pretrained/resnet50_miil_21k.pth', 'finetuned_lr': 0.00445, 'initial_pretrain': '', 'type': 'Basic'}, 'input_size': 112, 'initial_pretrain': '', 'log_interval_step': 10, 'test_transform': 'default', 'model_save_interval': 40, 'warmup_scheduler_epoch': 0, 'drop_last': False, 'pin_mem': False, 'weight_decay': 0.00115, 'kd': {'temperature': 4}, 'early_stop': 'EarlyStopping', 'train_transform': 'default', 'dist_backend': 'gloo', 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/profile_resnet50_OnlyFinetuneStrategy_cifar100_1675655019'}, 'train_epochs': 1, 'eval_metric': 'accuracy'}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "could not load layer: fc.weight; mismatch shape: target [torch.Size([100, 2048])] != pretrained [torch.Size([11221, 2048])]\n",
      "could not load layer: fc.bias; mismatch shape: target [torch.Size([100])] != pretrained [torch.Size([11221])]\n",
      "Epoch [0] learning rate: [0.00445, 0.00753]\n",
      "[2023-02-06 03:43:45] rank(0) epoch(0) step (0/391) Train: total_loss = 4.7609;\tbackbone_loss = 4.7609;\taccuracy = 0.0000\n",
      "[2023-02-06 03:44:16] rank(0) epoch(0) step (10/391) Train: total_loss = 4.6885;\tbackbone_loss = 4.6885;\taccuracy = 1.5625\n",
      "[2023-02-06 03:45:03] rank(0) epoch(0) step (20/391) Train: total_loss = 4.4324;\tbackbone_loss = 4.4324;\taccuracy = 4.6875\n",
      "[2023-02-06 03:45:44] rank(0) epoch(0) step (30/391) Train: total_loss = 4.0681;\tbackbone_loss = 4.0681;\taccuracy = 17.1875\n",
      "[2023-02-06 03:46:35] rank(0) epoch(0) step (40/391) Train: total_loss = 3.8115;\tbackbone_loss = 3.8115;\taccuracy = 21.0938\n",
      "[2023-02-06 03:47:16] rank(0) epoch(0) step (50/391) Train: total_loss = 3.3029;\tbackbone_loss = 3.3029;\taccuracy = 34.3750\n",
      "[2023-02-06 03:47:42] rank(0) epoch(0) step (60/391) Train: total_loss = 2.8760;\tbackbone_loss = 2.8760;\taccuracy = 37.5000\n",
      "[2023-02-06 03:48:07] rank(0) epoch(0) step (70/391) Train: total_loss = 2.2407;\tbackbone_loss = 2.2407;\taccuracy = 55.4688\n",
      "[2023-02-06 03:48:32] rank(0) epoch(0) step (80/391) Train: total_loss = 1.9484;\tbackbone_loss = 1.9484;\taccuracy = 53.1250\n",
      "[2023-02-06 03:48:59] rank(0) epoch(0) step (90/391) Train: total_loss = 1.7266;\tbackbone_loss = 1.7266;\taccuracy = 58.5938\n",
      "[2023-02-06 03:49:18] rank(0) epoch(0) step (100/391) Train: total_loss = 1.4433;\tbackbone_loss = 1.4433;\taccuracy = 60.1562\n",
      "[2023-02-06 03:49:43] rank(0) epoch(0) step (110/391) Train: total_loss = 1.4062;\tbackbone_loss = 1.4062;\taccuracy = 60.1562\n",
      "[2023-02-06 03:50:03] rank(0) epoch(0) step (120/391) Train: total_loss = 1.2962;\tbackbone_loss = 1.2962;\taccuracy = 67.9688\n",
      "[2023-02-06 03:50:24] rank(0) epoch(0) step (130/391) Train: total_loss = 1.2078;\tbackbone_loss = 1.2078;\taccuracy = 65.6250\n",
      "[2023-02-06 03:50:43] rank(0) epoch(0) step (140/391) Train: total_loss = 1.2846;\tbackbone_loss = 1.2846;\taccuracy = 64.0625\n",
      "[2023-02-06 03:51:03] rank(0) epoch(0) step (150/391) Train: total_loss = 0.9217;\tbackbone_loss = 0.9217;\taccuracy = 75.0000\n",
      "[2023-02-06 03:51:24] rank(0) epoch(0) step (160/391) Train: total_loss = 0.9014;\tbackbone_loss = 0.9014;\taccuracy = 75.7812\n",
      "[2023-02-06 03:51:43] rank(0) epoch(0) step (170/391) Train: total_loss = 1.1210;\tbackbone_loss = 1.1210;\taccuracy = 68.7500\n",
      "[2023-02-06 03:52:03] rank(0) epoch(0) step (180/391) Train: total_loss = 0.8800;\tbackbone_loss = 0.8800;\taccuracy = 73.4375\n",
      "[2023-02-06 03:52:23] rank(0) epoch(0) step (190/391) Train: total_loss = 0.8269;\tbackbone_loss = 0.8269;\taccuracy = 75.7812\n",
      "[2023-02-06 03:52:42] rank(0) epoch(0) step (200/391) Train: total_loss = 1.0181;\tbackbone_loss = 1.0181;\taccuracy = 75.7812\n",
      "[2023-02-06 03:53:08] rank(0) epoch(0) step (210/391) Train: total_loss = 0.7856;\tbackbone_loss = 0.7856;\taccuracy = 76.5625\n",
      "[2023-02-06 03:53:28] rank(0) epoch(0) step (220/391) Train: total_loss = 0.9602;\tbackbone_loss = 0.9602;\taccuracy = 70.3125\n",
      "[2023-02-06 03:53:49] rank(0) epoch(0) step (230/391) Train: total_loss = 0.7803;\tbackbone_loss = 0.7803;\taccuracy = 79.6875\n",
      "[2023-02-06 03:54:09] rank(0) epoch(0) step (240/391) Train: total_loss = 0.7919;\tbackbone_loss = 0.7919;\taccuracy = 74.2188\n",
      "[2023-02-06 03:54:28] rank(0) epoch(0) step (250/391) Train: total_loss = 0.8529;\tbackbone_loss = 0.8529;\taccuracy = 74.2188\n",
      "[2023-02-06 03:54:47] rank(0) epoch(0) step (260/391) Train: total_loss = 0.6466;\tbackbone_loss = 0.6466;\taccuracy = 85.1562\n",
      "[2023-02-06 03:55:07] rank(0) epoch(0) step (270/391) Train: total_loss = 0.7036;\tbackbone_loss = 0.7036;\taccuracy = 74.2188\n",
      "[2023-02-06 03:55:27] rank(0) epoch(0) step (280/391) Train: total_loss = 0.9187;\tbackbone_loss = 0.9187;\taccuracy = 73.4375\n",
      "[2023-02-06 03:55:47] rank(0) epoch(0) step (290/391) Train: total_loss = 0.7531;\tbackbone_loss = 0.7531;\taccuracy = 78.1250\n",
      "[2023-02-06 03:56:07] rank(0) epoch(0) step (300/391) Train: total_loss = 0.7439;\tbackbone_loss = 0.7439;\taccuracy = 79.6875\n",
      "[2023-02-06 03:56:33] rank(0) epoch(0) step (310/391) Train: total_loss = 0.5691;\tbackbone_loss = 0.5691;\taccuracy = 85.1562\n",
      "[2023-02-06 03:56:53] rank(0) epoch(0) step (320/391) Train: total_loss = 0.7912;\tbackbone_loss = 0.7912;\taccuracy = 79.6875\n",
      "[2023-02-06 03:57:14] rank(0) epoch(0) step (330/391) Train: total_loss = 0.6261;\tbackbone_loss = 0.6261;\taccuracy = 81.2500\n",
      "[2023-02-06 03:57:32] rank(0) epoch(0) step (340/391) Train: total_loss = 0.6606;\tbackbone_loss = 0.6606;\taccuracy = 79.6875\n",
      "[2023-02-06 03:57:51] rank(0) epoch(0) step (350/391) Train: total_loss = 0.6889;\tbackbone_loss = 0.6889;\taccuracy = 78.1250\n",
      "[2023-02-06 03:58:10] rank(0) epoch(0) step (360/391) Train: total_loss = 0.7860;\tbackbone_loss = 0.7860;\taccuracy = 80.4688\n",
      "[2023-02-06 03:58:30] rank(0) epoch(0) step (370/391) Train: total_loss = 0.4082;\tbackbone_loss = 0.4082;\taccuracy = 87.5000\n",
      "[2023-02-06 03:58:49] rank(0) epoch(0) step (380/391) Train: total_loss = 0.5800;\tbackbone_loss = 0.5800;\taccuracy = 84.3750\n",
      "[2023-02-06 03:59:09] rank(0) epoch(0) step (390/391) Train: total_loss = 0.7530;\tbackbone_loss = 0.7530;\taccuracy = 77.5000\n",
      "2023-02-06 03:59:17 0/391\n",
      "2023-02-06 03:59:24 10/391\n",
      "2023-02-06 03:59:31 20/391\n",
      "2023-02-06 03:59:39 30/391\n",
      "2023-02-06 03:59:47 40/391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-06 03:59:54 50/391\n",
      "2023-02-06 04:00:01 60/391\n",
      "2023-02-06 04:00:08 70/391\n",
      "[2023-02-06 04:00:14] rank(0) epoch(0) Validation: accuracy = 80.6200;\tloss = 0.6625\n",
      "Best Epoch: 0, accuracy: 80.62000274658203\n",
      "Epoch 0 took 998.8511202335358 seconds\n",
      "Total seconds:998.85387\n",
      "Totally take 1001.8232228755951 seconds\n"
     ]
    }
   ],
   "source": [
    "! python /home/vmagent/app/e2eaiok/e2eAIOK/ModelAdapter/main.py --cfg /home/vmagent/app/e2eaiok/conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f0017",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "We can see the result after training 1 epoch: finetuning achieves **80.62%** validation accuracy, while training from scratch achieves only **7.77%**.\n",
    "\n",
    "<img src=\"../imgs/finetuner_result.png\" width=\"80%\">\n",
    "<center>Model Adapter Finetuner Performance</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
