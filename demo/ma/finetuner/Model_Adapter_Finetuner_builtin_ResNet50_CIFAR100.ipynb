{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouyu5/e2eAIOK/blob/da-demo/demo/ma/finetuner/Model_Adapter_Finetuner_builtin_ResNet50_CIFAR100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5a9460",
      "metadata": {
        "id": "7f5a9460"
      },
      "source": [
        "# Model Adapter Finetuner Builtin DEMO\n",
        "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
        "\n",
        "This demo mainly introduces the usage of Finetuner. Take image classification as an example, it shows how to integrate finetuner with ResNet50 on CIFAR100 dataset. This is a build-in usage, you can find customized detailed demo at [here](./Model_Adapter_Finetuner_Walkthrough_ResNet50_CIFAR100.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c721aa",
      "metadata": {
        "id": "09c721aa"
      },
      "source": [
        "# Content\n",
        "\n",
        "* [Model Adapter Finetuner Overview](#Model-Adapter-Finetuner-Overview)\n",
        "* [Getting Started](#Getting-Started)\n",
        "    * [1. Environment Setup](#1.-Environment-Setup)\n",
        "    * [2. Launch training on baseline](#2.-Launch-training-on-baseline)\n",
        "    * [3. Launch training with Finetuner](#3.-Launch-training-with-Finetuner)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87340cc8",
      "metadata": {
        "id": "87340cc8"
      },
      "source": [
        "## Model Adapter Fine-tuner Overview\n",
        "Finetuner is based on pretraining and finetuning technology, it can transfer knowledge from pretrained model to target model with same network structure. \n",
        "\n",
        "Pretrained models usually are generated by pretraining process, which is training specific model  on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace. Finetunner retrieves the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode. With finetunner, we can greatly improve training speed, and usually achieves better performance.\n",
        "\n",
        "<img src=\"https://github.com/zhouyu5/e2eAIOK/blob/da-demo/demo/ma/imgs/finetuner.png?raw=1\" width=\"50%\">\n",
        "<center>Model Adapter Finetuner Structure</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56eb1345",
      "metadata": {
        "id": "56eb1345"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f16cc7",
      "metadata": {
        "id": "53f16cc7"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e622621",
      "metadata": {
        "id": "4e622621"
      },
      "source": [
        "### (Option 1) Use Pip install - recommend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09e6799",
      "metadata": {
        "id": "f09e6799",
        "outputId": "21f78a2e-02bf-451e-d600-5100308006a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting e2eAIOK-ModelAdapter\n",
            "  Downloading e2eAIOK_ModelAdapter-1.0.1b2023031702-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 198 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.13.1 in /home/xianyang/.local/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.13.1)\n",
            "Requirement already satisfied: timm in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.6.12)\n",
            "Requirement already satisfied: transformers in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (4.8.2)\n",
            "Requirement already satisfied: intel-extension-for-pytorch==1.12.100 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (1.12.100)\n",
            "Requirement already satisfied: nnunet in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (1.7.0)\n",
            "Requirement already satisfied: easydict in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (1.10)\n",
            "Requirement already satisfied: thop in /home/xianyang/.local/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: tllib==0.4 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.4)\n",
            "Requirement already satisfied: torchsummary in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (1.5.1)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.12.1)\n",
            "Requirement already satisfied: tensorboard in /home/xianyang/.local/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (2.10.0)\n",
            "Requirement already satisfied: scikit-learn in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from e2eAIOK-ModelAdapter) (0.24.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from torchvision==0.13.1->e2eAIOK-ModelAdapter) (9.1.0)\n",
            "Requirement already satisfied: typing-extensions in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from torchvision==0.13.1->e2eAIOK-ModelAdapter) (3.10.0.0)\n",
            "Requirement already satisfied: requests in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from torchvision==0.13.1->e2eAIOK-ModelAdapter) (2.28.2)\n",
            "Requirement already satisfied: numpy in /home/xianyang/.local/lib/python3.8/site-packages (from torchvision==0.13.1->e2eAIOK-ModelAdapter) (1.22.4)\n",
            "Requirement already satisfied: torch==1.12.1 in /home/xianyang/.local/lib/python3.8/site-packages (from torchvision==0.13.1->e2eAIOK-ModelAdapter) (1.12.1)\n",
            "Requirement already satisfied: huggingface-hub in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from timm->e2eAIOK-ModelAdapter) (0.0.12)\n",
            "Requirement already satisfied: pyyaml in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from timm->e2eAIOK-ModelAdapter) (5.4.1)\n",
            "Requirement already satisfied: filelock in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (3.0.12)\n",
            "Requirement already satisfied: packaging in /home/xianyang/.local/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (23.0)\n",
            "Requirement already satisfied: sacremoses in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (2021.7.6)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from transformers->e2eAIOK-ModelAdapter) (4.51.0)\n",
            "Requirement already satisfied: psutil in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from intel-extension-for-pytorch==1.12.100->e2eAIOK-ModelAdapter) (5.8.0)\n",
            "Requirement already satisfied: tifffile in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (2022.10.10)\n",
            "Requirement already satisfied: medpy in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (0.4.0)\n",
            "Requirement already satisfied: dicom2nifti in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (2.4.7)\n",
            "Requirement already satisfied: scipy in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (1.6.3)\n",
            "Requirement already satisfied: batchgenerators>=0.23 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (0.24)\n",
            "Requirement already satisfied: SimpleITK in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (2.2.1)\n",
            "Requirement already satisfied: pandas in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (1.4.2)\n",
            "Requirement already satisfied: nibabel in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (5.0.0)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (0.19.3)\n",
            "Requirement already satisfied: sklearn in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from nnunet->e2eAIOK-ModelAdapter) (0.0.post1)\n",
            "Requirement already satisfied: prettytable in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tllib==0.4->e2eAIOK-ModelAdapter) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tllib==0.4->e2eAIOK-ModelAdapter) (3.5.1)\n",
            "Requirement already satisfied: opencv-python in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tllib==0.4->e2eAIOK-ModelAdapter) (4.7.0.68)\n",
            "Requirement already satisfied: numba in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tllib==0.4->e2eAIOK-ModelAdapter) (0.55.2)\n",
            "Requirement already satisfied: webcolors in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tllib==0.4->e2eAIOK-ModelAdapter) (1.12)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (3.19.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (2.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (50.3.1.post20201107)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (2.11.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (1.47.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/xianyang/.local/lib/python3.8/site-packages (from tensorboard->e2eAIOK-ModelAdapter) (0.4.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from scikit-learn->e2eAIOK-ModelAdapter) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from scikit-learn->e2eAIOK-ModelAdapter) (2.1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.13.1->e2eAIOK-ModelAdapter) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.13.1->e2eAIOK-ModelAdapter) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.13.1->e2eAIOK-ModelAdapter) (2020.6.20)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.13.1->e2eAIOK-ModelAdapter) (1.26.14)\n",
            "Requirement already satisfied: six in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from sacremoses->transformers->e2eAIOK-ModelAdapter) (1.15.0)\n",
            "Requirement already satisfied: click in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from sacremoses->transformers->e2eAIOK-ModelAdapter) (8.0.1)\n",
            "Requirement already satisfied: python-gdcm in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from dicom2nifti->nnunet->e2eAIOK-ModelAdapter) (3.0.20)\n",
            "Requirement already satisfied: pydicom>=2.2.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from dicom2nifti->nnunet->e2eAIOK-ModelAdapter) (2.3.1)\n",
            "Requirement already satisfied: unittest2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from batchgenerators>=0.23->nnunet->e2eAIOK-ModelAdapter) (1.1.0)\n",
            "Requirement already satisfied: future in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from batchgenerators>=0.23->nnunet->e2eAIOK-ModelAdapter) (0.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from pandas->nnunet->e2eAIOK-ModelAdapter) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from pandas->nnunet->e2eAIOK-ModelAdapter) (2021.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from scikit-image>=0.14->nnunet->e2eAIOK-ModelAdapter) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from scikit-image>=0.14->nnunet->e2eAIOK-ModelAdapter) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from scikit-image>=0.14->nnunet->e2eAIOK-ModelAdapter) (2.24.0)\n",
            "Requirement already satisfied: wcwidth in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from prettytable->tllib==0.4->e2eAIOK-ModelAdapter) (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from matplotlib->tllib==0.4->e2eAIOK-ModelAdapter) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from matplotlib->tllib==0.4->e2eAIOK-ModelAdapter) (2.4.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from matplotlib->tllib==0.4->e2eAIOK-ModelAdapter) (4.32.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from matplotlib->tllib==0.4->e2eAIOK-ModelAdapter) (1.4.2)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from numba->tllib==0.4->e2eAIOK-ModelAdapter) (0.38.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->e2eAIOK-ModelAdapter) (4.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/xianyang/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->e2eAIOK-ModelAdapter) (2.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->e2eAIOK-ModelAdapter) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->e2eAIOK-ModelAdapter) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->e2eAIOK-ModelAdapter) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->e2eAIOK-ModelAdapter) (1.3.1)\n",
            "Requirement already satisfied: argparse in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from unittest2->batchgenerators>=0.23->nnunet->e2eAIOK-ModelAdapter) (1.4.0)\n",
            "Requirement already satisfied: traceback2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from unittest2->batchgenerators>=0.23->nnunet->e2eAIOK-ModelAdapter) (1.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->e2eAIOK-ModelAdapter) (3.7.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard->e2eAIOK-ModelAdapter) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->e2eAIOK-ModelAdapter) (3.2.0)\n",
            "Requirement already satisfied: linecache2 in /home/xianyang/sw/miniconda3/lib/python3.8/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet->e2eAIOK-ModelAdapter) (1.0.0)\n",
            "Installing collected packages: e2eAIOK-ModelAdapter\n",
            "Successfully installed e2eAIOK-ModelAdapter-1.0.1b2023031702\n"
          ]
        }
      ],
      "source": [
        "!pip install e2eAIOK-ModelAdapter --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e2d8e2f",
      "metadata": {
        "id": "1e2d8e2f"
      },
      "source": [
        "### (Option 2) Use Docker \n",
        "\n",
        "Step1. prepare code\n",
        "   ``` bash\n",
        "   git clone https://github.com/intel/e2eAIOK.git\n",
        "   cd e2eAIOK\n",
        "   git submodule update --init –recursive\n",
        "   ```\n",
        "    \n",
        "Step2. build docker image\n",
        "   ``` bash\n",
        "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
        "   ```\n",
        "   \n",
        "Step3. run docker and start conda env\n",
        "   ``` bash\n",
        "   sshpass -p docker ssh ${host0} -p 12347\n",
        "   conda activate pytorch-1.12.0\n",
        "   ```\n",
        "  \n",
        "Step4. Start the jupyter notebook and tensorboard service\n",
        "   ``` bash\n",
        "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
        "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
        "   ```\n",
        "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5c8801",
      "metadata": {
        "id": "6c5c8801"
      },
      "source": [
        "## 2. Launch training on baseline\n",
        "First we train a vanilla ResNet50 on CIFAR100 as baseline.\n",
        "\n",
        "### 2.1 Configuration\n",
        "Create a configuration for ResNet50 with CIFAR100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b75762d",
      "metadata": {
        "id": "5b75762d",
        "outputId": "c2184b13-7785-4df4-f850-796a1ae69e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-19 22:25:15--  https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/baseline/cifar100_res50.yaml\n",
            "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.55\n",
            "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.55|:913... connected.\n",
            "Proxy request sent, awaiting response... 200 OK\n",
            "Length: 512 [text/plain]\n",
            "Saving to: ‘cifar100_res50.yaml’\n",
            "\n",
            "100%[======================================>] 512         --.-K/s   in 0s      \n",
            "\n",
            "2023-03-19 22:25:17 (15.2 MB/s) - ‘cifar100_res50.yaml’ saved [512/512]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/baseline/cifar100_res50.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84a44f41",
      "metadata": {
        "id": "84a44f41",
        "outputId": "a6eaa934-a0f0-4401-a22d-3d2f9e8ddebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experiment:\r\n",
            "  project: \"demo\"\r\n",
            "  tag: \"cifar100_res50\"\r\n",
            "  \r\n",
            "output_dir: \"./data\"\r\n",
            "train_epochs: 1\r\n",
            "\r\n",
            "### dataset\r\n",
            "data_set: \"cifar100\"\r\n",
            "data_path:  \"./data\"\r\n",
            "num_workers: 4\r\n",
            "\r\n",
            "### model\r\n",
            "model_type: \"resnet50\"\r\n",
            "\r\n",
            "## optimizer\r\n",
            "optimizer: \"SGD\"\r\n",
            "learning_rate: 0.00753\r\n",
            "weight_decay: 0.00115\r\n",
            "momentum: 0.9\r\n",
            "\r\n",
            "### scheduler\r\n",
            "lr_scheduler: \"CosineAnnealingLR\"\r\n",
            "lr_scheduler_config:\r\n",
            "    T_max: 200\r\n",
            "\r\n",
            "### early stop\r\n",
            "early_stop: \"EarlyStopping\"\r\n",
            "early_stop_config:\r\n",
            "    tolerance_epoch: 15\r\n"
          ]
        }
      ],
      "source": [
        "!cat cifar100_res50.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce65be0",
      "metadata": {
        "id": "dce65be0"
      },
      "source": [
        "### 2.2 Launch training\n",
        "**Training resnet50 on CIFAR100 from scratch:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e294640",
      "metadata": {
        "id": "3e294640",
        "outputId": "a16efe13-bad7-447d-9f0b-c9d1ae944e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "configurations:\n",
            "{'train_batch_size': 128, 'start_epoch': 0, 'initial_pretrain': '', 'kd': {'temperature': 4}, 'drop_last': False, 'optimizer': 'SGD', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'enable_ipex': False, 'log_interval_step': 10, 'train_epochs': 1, 'metric_threshold': 100.0, 'profiler': False, 'warmup_scheduler_epoch': 0, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'eval_metric': 'accuracy', 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_resnet50_cifar100', 'weight_decay': 0.00115, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/baseline/cifar100_res50/profile/profile_resnet50_cifar100_1675653984'}, 'finetuner': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'pretrained_num_classes': 10, 'finetuned_lr': 0.01, 'frozen': False}, 'dist_backend': 'gloo', 'lr_scheduler': 'CosineAnnealingLR', 'output_dir': '/home/vmagent/app/data/model', 'device': 'cpu', 'model_save_interval': 40, 'learning_rate': 0.00753, 'pretrain': '', 'warmup_scheduler': '', 'early_stop': 'EarlyStopping', 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'input_size': 32, 'test_transform': 'default', 'pin_mem': False, 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'eval_batch_size': 128, 'eval_epochs': 1, 'eval_step': 10, 'momentum': 0.9, 'experiment': {'project': 'baseline', 'tag': 'cifar100_res50', 'strategy': ''}, 'data_set': 'cifar100', 'criterion': 'CrossEntropyLoss', 'seed': 0, 'model_type': 'resnet50', 'num_workers': 4, 'train_transform': 'default'}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model params:  23712932\n",
            "Epoch [0] learning rate: [0.00753]\n",
            "[2023-02-06 03:26:27] rank(0) epoch(0) step (0/391) Train: loss = 6.5908;\taccuracy = 0.7812\n",
            "[2023-02-06 03:26:35] rank(0) epoch(0) step (10/391) Train: loss = 5.6120;\taccuracy = 2.3438\n",
            "[2023-02-06 03:26:37] rank(0) epoch(0) step (20/391) Train: loss = 6.1267;\taccuracy = 0.0000\n",
            "[2023-02-06 03:26:39] rank(0) epoch(0) step (30/391) Train: loss = 5.4950;\taccuracy = 3.9062\n",
            "[2023-02-06 03:26:41] rank(0) epoch(0) step (40/391) Train: loss = 5.1365;\taccuracy = 2.3438\n",
            "[2023-02-06 03:26:43] rank(0) epoch(0) step (50/391) Train: loss = 5.8884;\taccuracy = 4.6875\n",
            "[2023-02-06 03:26:45] rank(0) epoch(0) step (60/391) Train: loss = 5.4579;\taccuracy = 0.7812\n",
            "[2023-02-06 03:26:47] rank(0) epoch(0) step (70/391) Train: loss = 5.1344;\taccuracy = 0.7812\n",
            "[2023-02-06 03:26:49] rank(0) epoch(0) step (80/391) Train: loss = 5.7166;\taccuracy = 3.1250\n",
            "[2023-02-06 03:26:51] rank(0) epoch(0) step (90/391) Train: loss = 5.5491;\taccuracy = 1.5625\n",
            "[2023-02-06 03:26:53] rank(0) epoch(0) step (100/391) Train: loss = 5.1875;\taccuracy = 2.3438\n",
            "[2023-02-06 03:27:00] rank(0) epoch(0) step (110/391) Train: loss = 5.4646;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:02] rank(0) epoch(0) step (120/391) Train: loss = 5.4944;\taccuracy = 3.1250\n",
            "[2023-02-06 03:27:04] rank(0) epoch(0) step (130/391) Train: loss = 6.2241;\taccuracy = 1.5625\n",
            "[2023-02-06 03:27:06] rank(0) epoch(0) step (140/391) Train: loss = 5.2008;\taccuracy = 0.0000\n",
            "[2023-02-06 03:27:08] rank(0) epoch(0) step (150/391) Train: loss = 5.4579;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:10] rank(0) epoch(0) step (160/391) Train: loss = 7.0275;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:12] rank(0) epoch(0) step (170/391) Train: loss = 4.3376;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:15] rank(0) epoch(0) step (180/391) Train: loss = 4.4311;\taccuracy = 7.0312\n",
            "[2023-02-06 03:27:17] rank(0) epoch(0) step (190/391) Train: loss = 5.0786;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:19] rank(0) epoch(0) step (200/391) Train: loss = 5.6823;\taccuracy = 4.6875\n",
            "[2023-02-06 03:27:28] rank(0) epoch(0) step (210/391) Train: loss = 5.6876;\taccuracy = 3.9062\n",
            "[2023-02-06 03:27:31] rank(0) epoch(0) step (220/391) Train: loss = 6.4414;\taccuracy = 3.9062\n",
            "[2023-02-06 03:27:34] rank(0) epoch(0) step (230/391) Train: loss = 5.6525;\taccuracy = 3.1250\n",
            "[2023-02-06 03:27:37] rank(0) epoch(0) step (240/391) Train: loss = 5.0057;\taccuracy = 5.4688\n",
            "[2023-02-06 03:27:40] rank(0) epoch(0) step (250/391) Train: loss = 6.0380;\taccuracy = 9.3750\n",
            "[2023-02-06 03:27:43] rank(0) epoch(0) step (260/391) Train: loss = 4.6368;\taccuracy = 6.2500\n",
            "[2023-02-06 03:27:46] rank(0) epoch(0) step (270/391) Train: loss = 4.4934;\taccuracy = 10.9375\n",
            "[2023-02-06 03:27:49] rank(0) epoch(0) step (280/391) Train: loss = 4.6952;\taccuracy = 13.2812\n",
            "[2023-02-06 03:27:52] rank(0) epoch(0) step (290/391) Train: loss = 4.4364;\taccuracy = 4.6875\n",
            "[2023-02-06 03:27:54] rank(0) epoch(0) step (300/391) Train: loss = 5.3730;\taccuracy = 5.4688\n",
            "[2023-02-06 03:28:03] rank(0) epoch(0) step (310/391) Train: loss = 6.9364;\taccuracy = 3.1250\n",
            "[2023-02-06 03:28:06] rank(0) epoch(0) step (320/391) Train: loss = 4.9181;\taccuracy = 3.1250\n",
            "[2023-02-06 03:28:08] rank(0) epoch(0) step (330/391) Train: loss = 5.3563;\taccuracy = 5.4688\n",
            "[2023-02-06 03:28:10] rank(0) epoch(0) step (340/391) Train: loss = 4.2426;\taccuracy = 6.2500\n",
            "[2023-02-06 03:28:12] rank(0) epoch(0) step (350/391) Train: loss = 4.8778;\taccuracy = 7.8125\n",
            "[2023-02-06 03:28:15] rank(0) epoch(0) step (360/391) Train: loss = 4.4332;\taccuracy = 7.8125\n",
            "[2023-02-06 03:28:17] rank(0) epoch(0) step (370/391) Train: loss = 4.9217;\taccuracy = 6.2500\n",
            "[2023-02-06 03:28:21] rank(0) epoch(0) step (380/391) Train: loss = 5.1312;\taccuracy = 4.6875\n",
            "[2023-02-06 03:28:23] rank(0) epoch(0) step (390/391) Train: loss = 5.2084;\taccuracy = 7.5000\n",
            "2023-02-06 03:28:30 0/391\n",
            "2023-02-06 03:28:31 10/391\n",
            "2023-02-06 03:28:31 20/391\n",
            "2023-02-06 03:28:32 30/391\n",
            "2023-02-06 03:28:32 40/391\n",
            "2023-02-06 03:28:33 50/391\n",
            "2023-02-06 03:28:34 60/391\n",
            "2023-02-06 03:28:34 70/391\n",
            "[2023-02-06 03:28:35] rank(0) epoch(0) Validation: accuracy = 7.7700;\tloss = 4.0517\n",
            "Best Epoch: 0, accuracy: 7.769999980926514\n",
            "Epoch 0 took 135.11660027503967 seconds\n",
            "Total seconds:135.1177\n",
            "Totally take 137.19904828071594 seconds\n"
          ]
        }
      ],
      "source": [
        "! python -u /usr/local/lib/python3.9/dist-packages/e2eAIOK/ModelAdapter/main.py --conf cifar100_res50.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8551a8b5",
      "metadata": {
        "id": "8551a8b5"
      },
      "source": [
        "## 3. Launch training with Finetuner\n",
        "Then we train ResNet50 on CIFAR100 with Finetuner to show the performance imrpovement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42b8fae",
      "metadata": {
        "id": "e42b8fae"
      },
      "source": [
        "### 3.1 Prepare pretrained model \n",
        "Download pretrained ResNet50 model on ImageNet21k and put it in \"data\" folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a823b2c",
      "metadata": {
        "id": "6a823b2c",
        "outputId": "67af58fd-37f5-4f7b-cb64-b1b3d15a7314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-19 22:35:31--  https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth\n",
            "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
            "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
            "Proxy request sent, awaiting response... 200 OK\n",
            "Length: 186531247 (178M) [application/octet-stream]\n",
            "Saving to: ‘resnet50_miil_21k.pth’\n",
            "\n",
            "100%[======================================>] 186,531,247 16.3MB/s   in 13s    \n",
            "\n",
            "2023-03-19 22:35:45 (13.9 MB/s) - ‘resnet50_miil_21k.pth’ saved [186531247/186531247]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth && mkdir data && mv resnet50_miil_21k.pth data/ "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec5b7b7",
      "metadata": {
        "id": "4ec5b7b7"
      },
      "source": [
        "### 3.2 Configuration\n",
        "\n",
        "Create a configuration for Finetuner with ResNet50 with CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62596edc",
      "metadata": {
        "id": "62596edc",
        "outputId": "372ef710-dab6-4a0c-8315-fc47f0ca32db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-19 22:47:45--  https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml\n",
            "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
            "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
            "Proxy request sent, awaiting response... 200 OK\n",
            "Length: 788 [text/plain]\n",
            "Saving to: ‘cifar100_res50PretrainI21k.yaml’\n",
            "\n",
            "100%[======================================>] 788         --.-K/s   in 0s      \n",
            "\n",
            "2023-03-19 22:47:46 (22.8 MB/s) - ‘cifar100_res50PretrainI21k.yaml’ saved [788/788]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d434a30",
      "metadata": {
        "id": "1d434a30",
        "outputId": "98dd2ed5-b2e7-4d13-a6fb-5b44913c20d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experiment:\r\n",
            "  project: \"finetuner\"\r\n",
            "  tag: \"cifar100_res50_PretrainI21k\"\r\n",
            "  strategy: \"OnlyFinetuneStrategy\"\r\n",
            "\r\n",
            "output_dir: \".data/\"\r\n",
            "train_epochs: 1\r\n",
            "enable_ipex: True\r\n",
            "\r\n",
            "### dataset\r\n",
            "data_set: \"cifar100\"\r\n",
            "data_path:  \".data/\"\r\n",
            "num_workers: 4\r\n",
            "input_size: 112\r\n",
            "\r\n",
            "### model\r\n",
            "model_type: \"resnet50\"\r\n",
            "\r\n",
            "## finetuner\r\n",
            "finetuner:\r\n",
            "    type: \"Basic\"\r\n",
            "    pretrain: '.data/resnet50_miil_21k.pth'\r\n",
            "    pretrained_num_classes: 11221\r\n",
            "    finetuned_lr: 0.00445\r\n",
            "    frozen: False\r\n",
            "\r\n",
            "## optimizer\r\n",
            "optimizer: \"SGD\"\r\n",
            "learning_rate: 0.00753\r\n",
            "weight_decay: 0.00115\r\n",
            "momentum: 0.9\r\n",
            "\r\n",
            "### scheduler\r\n",
            "lr_scheduler: \"CosineAnnealingLR\"\r\n",
            "lr_scheduler_config:\r\n",
            "    T_max: 200\r\n",
            "\r\n",
            "### early stop\r\n",
            "early_stop: \"EarlyStopping\"\r\n",
            "early_stop_config:\r\n",
            "    tolerance_epoch: 5"
          ]
        }
      ],
      "source": [
        "! cat cifar100_res50PretrainI21k.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6506202",
      "metadata": {
        "id": "f6506202"
      },
      "source": [
        "### 3.3 Launch Training with Finetuner\n",
        "**Training resnet50 on CIFAR100 with Finetuner:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fac25c5",
      "metadata": {
        "id": "9fac25c5",
        "outputId": "8c133549-3a1e-438f-f0a0-17adfaa3be2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "See abnormal behavior in dataloader when enable IPEX in PyTorch 1.12, set enable_ipex to False!\n",
            "configurations:\n",
            "{'lr_scheduler': 'CosineAnnealingLR', 'pretrain': '', 'eval_epochs': 1, 'criterion': 'CrossEntropyLoss', 'data_set': 'cifar100', 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'output_dir': '/home/vmagent/app/data/model', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'eval_batch_size': 128, 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'enable_ipex': False, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'metric_threshold': 100.0, 'start_epoch': 0, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'train_batch_size': 128, 'experiment': {'strategy': 'OnlyFinetuneStrategy', 'project': 'finetuner', 'tag': 'cifar100_res50_PretrainI21k'}, 'optimizer': 'SGD', 'learning_rate': 0.00753, 'model_type': 'resnet50', 'seed': 0, 'device': 'cpu', 'eval_step': 10, 'num_workers': 4, 'momentum': 0.9, 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_PretrainI21k_resnet50_OnlyFinetuneStrategy_cifar100', 'warmup_scheduler': '', 'profiler': False, 'finetuner': {'frozen': False, 'pretrained_num_classes': 11221, 'pretrain': '/home/vmagent/app/data/pretrained/resnet50_miil_21k.pth', 'finetuned_lr': 0.00445, 'initial_pretrain': '', 'type': 'Basic'}, 'input_size': 112, 'initial_pretrain': '', 'log_interval_step': 10, 'test_transform': 'default', 'model_save_interval': 40, 'warmup_scheduler_epoch': 0, 'drop_last': False, 'pin_mem': False, 'weight_decay': 0.00115, 'kd': {'temperature': 4}, 'early_stop': 'EarlyStopping', 'train_transform': 'default', 'dist_backend': 'gloo', 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/profile_resnet50_OnlyFinetuneStrategy_cifar100_1675655019'}, 'train_epochs': 1, 'eval_metric': 'accuracy'}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model params:  23712932\n",
            "could not load layer: fc.weight; mismatch shape: target [torch.Size([100, 2048])] != pretrained [torch.Size([11221, 2048])]\n",
            "could not load layer: fc.bias; mismatch shape: target [torch.Size([100])] != pretrained [torch.Size([11221])]\n",
            "Epoch [0] learning rate: [0.00445, 0.00753]\n",
            "[2023-02-06 03:43:45] rank(0) epoch(0) step (0/391) Train: total_loss = 4.7609;\tbackbone_loss = 4.7609;\taccuracy = 0.0000\n",
            "[2023-02-06 03:44:16] rank(0) epoch(0) step (10/391) Train: total_loss = 4.6885;\tbackbone_loss = 4.6885;\taccuracy = 1.5625\n",
            "[2023-02-06 03:45:03] rank(0) epoch(0) step (20/391) Train: total_loss = 4.4324;\tbackbone_loss = 4.4324;\taccuracy = 4.6875\n",
            "[2023-02-06 03:45:44] rank(0) epoch(0) step (30/391) Train: total_loss = 4.0681;\tbackbone_loss = 4.0681;\taccuracy = 17.1875\n",
            "[2023-02-06 03:46:35] rank(0) epoch(0) step (40/391) Train: total_loss = 3.8115;\tbackbone_loss = 3.8115;\taccuracy = 21.0938\n",
            "[2023-02-06 03:47:16] rank(0) epoch(0) step (50/391) Train: total_loss = 3.3029;\tbackbone_loss = 3.3029;\taccuracy = 34.3750\n",
            "[2023-02-06 03:47:42] rank(0) epoch(0) step (60/391) Train: total_loss = 2.8760;\tbackbone_loss = 2.8760;\taccuracy = 37.5000\n",
            "[2023-02-06 03:48:07] rank(0) epoch(0) step (70/391) Train: total_loss = 2.2407;\tbackbone_loss = 2.2407;\taccuracy = 55.4688\n",
            "[2023-02-06 03:48:32] rank(0) epoch(0) step (80/391) Train: total_loss = 1.9484;\tbackbone_loss = 1.9484;\taccuracy = 53.1250\n",
            "[2023-02-06 03:48:59] rank(0) epoch(0) step (90/391) Train: total_loss = 1.7266;\tbackbone_loss = 1.7266;\taccuracy = 58.5938\n",
            "[2023-02-06 03:49:18] rank(0) epoch(0) step (100/391) Train: total_loss = 1.4433;\tbackbone_loss = 1.4433;\taccuracy = 60.1562\n",
            "[2023-02-06 03:49:43] rank(0) epoch(0) step (110/391) Train: total_loss = 1.4062;\tbackbone_loss = 1.4062;\taccuracy = 60.1562\n",
            "[2023-02-06 03:50:03] rank(0) epoch(0) step (120/391) Train: total_loss = 1.2962;\tbackbone_loss = 1.2962;\taccuracy = 67.9688\n",
            "[2023-02-06 03:50:24] rank(0) epoch(0) step (130/391) Train: total_loss = 1.2078;\tbackbone_loss = 1.2078;\taccuracy = 65.6250\n",
            "[2023-02-06 03:50:43] rank(0) epoch(0) step (140/391) Train: total_loss = 1.2846;\tbackbone_loss = 1.2846;\taccuracy = 64.0625\n",
            "[2023-02-06 03:51:03] rank(0) epoch(0) step (150/391) Train: total_loss = 0.9217;\tbackbone_loss = 0.9217;\taccuracy = 75.0000\n",
            "[2023-02-06 03:51:24] rank(0) epoch(0) step (160/391) Train: total_loss = 0.9014;\tbackbone_loss = 0.9014;\taccuracy = 75.7812\n",
            "[2023-02-06 03:51:43] rank(0) epoch(0) step (170/391) Train: total_loss = 1.1210;\tbackbone_loss = 1.1210;\taccuracy = 68.7500\n",
            "[2023-02-06 03:52:03] rank(0) epoch(0) step (180/391) Train: total_loss = 0.8800;\tbackbone_loss = 0.8800;\taccuracy = 73.4375\n",
            "[2023-02-06 03:52:23] rank(0) epoch(0) step (190/391) Train: total_loss = 0.8269;\tbackbone_loss = 0.8269;\taccuracy = 75.7812\n",
            "[2023-02-06 03:52:42] rank(0) epoch(0) step (200/391) Train: total_loss = 1.0181;\tbackbone_loss = 1.0181;\taccuracy = 75.7812\n",
            "[2023-02-06 03:53:08] rank(0) epoch(0) step (210/391) Train: total_loss = 0.7856;\tbackbone_loss = 0.7856;\taccuracy = 76.5625\n",
            "[2023-02-06 03:53:28] rank(0) epoch(0) step (220/391) Train: total_loss = 0.9602;\tbackbone_loss = 0.9602;\taccuracy = 70.3125\n",
            "[2023-02-06 03:53:49] rank(0) epoch(0) step (230/391) Train: total_loss = 0.7803;\tbackbone_loss = 0.7803;\taccuracy = 79.6875\n",
            "[2023-02-06 03:54:09] rank(0) epoch(0) step (240/391) Train: total_loss = 0.7919;\tbackbone_loss = 0.7919;\taccuracy = 74.2188\n",
            "[2023-02-06 03:54:28] rank(0) epoch(0) step (250/391) Train: total_loss = 0.8529;\tbackbone_loss = 0.8529;\taccuracy = 74.2188\n",
            "[2023-02-06 03:54:47] rank(0) epoch(0) step (260/391) Train: total_loss = 0.6466;\tbackbone_loss = 0.6466;\taccuracy = 85.1562\n",
            "[2023-02-06 03:55:07] rank(0) epoch(0) step (270/391) Train: total_loss = 0.7036;\tbackbone_loss = 0.7036;\taccuracy = 74.2188\n",
            "[2023-02-06 03:55:27] rank(0) epoch(0) step (280/391) Train: total_loss = 0.9187;\tbackbone_loss = 0.9187;\taccuracy = 73.4375\n",
            "[2023-02-06 03:55:47] rank(0) epoch(0) step (290/391) Train: total_loss = 0.7531;\tbackbone_loss = 0.7531;\taccuracy = 78.1250\n",
            "[2023-02-06 03:56:07] rank(0) epoch(0) step (300/391) Train: total_loss = 0.7439;\tbackbone_loss = 0.7439;\taccuracy = 79.6875\n",
            "[2023-02-06 03:56:33] rank(0) epoch(0) step (310/391) Train: total_loss = 0.5691;\tbackbone_loss = 0.5691;\taccuracy = 85.1562\n",
            "[2023-02-06 03:56:53] rank(0) epoch(0) step (320/391) Train: total_loss = 0.7912;\tbackbone_loss = 0.7912;\taccuracy = 79.6875\n",
            "[2023-02-06 03:57:14] rank(0) epoch(0) step (330/391) Train: total_loss = 0.6261;\tbackbone_loss = 0.6261;\taccuracy = 81.2500\n",
            "[2023-02-06 03:57:32] rank(0) epoch(0) step (340/391) Train: total_loss = 0.6606;\tbackbone_loss = 0.6606;\taccuracy = 79.6875\n",
            "[2023-02-06 03:57:51] rank(0) epoch(0) step (350/391) Train: total_loss = 0.6889;\tbackbone_loss = 0.6889;\taccuracy = 78.1250\n",
            "[2023-02-06 03:58:10] rank(0) epoch(0) step (360/391) Train: total_loss = 0.7860;\tbackbone_loss = 0.7860;\taccuracy = 80.4688\n",
            "[2023-02-06 03:58:30] rank(0) epoch(0) step (370/391) Train: total_loss = 0.4082;\tbackbone_loss = 0.4082;\taccuracy = 87.5000\n",
            "[2023-02-06 03:58:49] rank(0) epoch(0) step (380/391) Train: total_loss = 0.5800;\tbackbone_loss = 0.5800;\taccuracy = 84.3750\n",
            "[2023-02-06 03:59:09] rank(0) epoch(0) step (390/391) Train: total_loss = 0.7530;\tbackbone_loss = 0.7530;\taccuracy = 77.5000\n",
            "2023-02-06 03:59:17 0/391\n",
            "2023-02-06 03:59:24 10/391\n",
            "2023-02-06 03:59:31 20/391\n",
            "2023-02-06 03:59:39 30/391\n",
            "2023-02-06 03:59:47 40/391\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-06 03:59:54 50/391\n",
            "2023-02-06 04:00:01 60/391\n",
            "2023-02-06 04:00:08 70/391\n",
            "[2023-02-06 04:00:14] rank(0) epoch(0) Validation: accuracy = 80.6200;\tloss = 0.6625\n",
            "Best Epoch: 0, accuracy: 80.62000274658203\n",
            "Epoch 0 took 998.8511202335358 seconds\n",
            "Total seconds:998.85387\n",
            "Totally take 1001.8232228755951 seconds\n"
          ]
        }
      ],
      "source": [
        "! python -u /usr/local/lib/python3.9/dist-packages/e2eAIOK/ModelAdapter/main.py --cfg cifar100_res50PretrainI21k.yaml"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}