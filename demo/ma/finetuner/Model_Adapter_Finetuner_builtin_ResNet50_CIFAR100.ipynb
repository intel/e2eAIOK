{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dfb46b",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/e2eAIOK/blob/main/demo/ma/finetuner/Model_Adapter_Finetuner_builtin_ResNet50_CIFAR100.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a9460",
   "metadata": {
    "id": "7f5a9460"
   },
   "source": [
    "# Model Adapter Finetuner Builtin DEMO\n",
    "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
    "\n",
    "This demo mainly introduces the usage of Finetuner. Take image classification as an example, it shows how to integrate finetuner with ResNet50 on CIFAR100 dataset. This is a build-in usage, you can find customized detailed demo at [here](./Model_Adapter_Finetuner_Walkthrough_ResNet50_CIFAR100.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c721aa",
   "metadata": {
    "id": "09c721aa"
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Overview](#Overview)\n",
    "    * [Model Adapter Finetuner Overview](#Model-Adapter-Finetuner-Overview)\n",
    "* [Getting Started](#Getting-Started)\n",
    "    * [1. Environment Setup](#1.-Environment-Setup)\n",
    "    * [2. Launch training on baseline](#2.-Launch-training-on-baseline)\n",
    "    * [3. Launch training with Finetuner](#3.-Launch-training-with-Finetuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87340cc8",
   "metadata": {
    "id": "87340cc8"
   },
   "source": [
    "# Overview\n",
    "\n",
    "## Model Adapter Finetuner Overview\n",
    "Finetuner is based on pretraining and finetuning technology, it can transfer knowledge from pretrained model to target model with same network structure. \n",
    "\n",
    "Pretrained models usually are generated by pretraining process, which is training specific model  on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace. Finetunner retrieves the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode. With finetunner, we can greatly improve training speed, and usually achieves better performance.\n",
    "\n",
    "<img src=\"../imgs/finetuner.png\" width=\"50%\">\n",
    "<center>Model Adapter Finetuner Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb1345",
   "metadata": {
    "id": "56eb1345"
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f16cc7",
   "metadata": {
    "id": "53f16cc7"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e622621",
   "metadata": {
    "id": "4e622621"
   },
   "source": [
    "### (Option 1) Use Pip install\n",
    "We can directly install ModelAdapter module from Intel® End-to-End AI Optimization Kit with following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e6799",
   "metadata": {
    "id": "f09e6799",
    "outputId": "21f78a2e-02bf-451e-d600-5100308006a6"
   },
   "outputs": [],
   "source": [
    "!pip install e2eAIOK-ModelAdapter --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d8e2f",
   "metadata": {
    "id": "1e2d8e2f"
   },
   "source": [
    "### (Option 2) Use Docker \n",
    "\n",
    "We can also use Docker, which contains a complete environment.\n",
    "\n",
    "Step1. prepare code\n",
    "   ``` bash\n",
    "   git clone https://github.com/intel/e2eAIOK.git\n",
    "   cd e2eAIOK\n",
    "   git submodule update --init –recursive\n",
    "   ```\n",
    "    \n",
    "Step2. build docker image\n",
    "   ``` bash\n",
    "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
    "   ```\n",
    "   \n",
    "Step3. run docker and start conda env\n",
    "   ``` bash\n",
    "   sshpass -p docker ssh ${host0} -p 12347\n",
    "   conda activate pytorch-1.12.0\n",
    "   ```\n",
    "  \n",
    "Step4. Start the jupyter notebook and tensorboard service\n",
    "   ``` bash\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
    "   ```\n",
    "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c8801",
   "metadata": {
    "id": "6c5c8801"
   },
   "source": [
    "## 2. Launch training on baseline\n",
    "First we train a vanilla ResNet50 on CIFAR100 as baseline for comparison.\n",
    "\n",
    "### 2.1 Configuration\n",
    "Let's download a configuration for ResNet50 with CIFAR100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75762d",
   "metadata": {
    "id": "5b75762d",
    "outputId": "c2184b13-7785-4df4-f850-796a1ae69e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-19 22:25:15--  https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/baseline/cifar100_res50.yaml\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.55\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.55|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 512 [text/plain]\n",
      "Saving to: ‘cifar100_res50.yaml’\n",
      "\n",
      "100%[======================================>] 512         --.-K/s   in 0s      \n",
      "\n",
      "2023-03-19 22:25:17 (15.2 MB/s) - ‘cifar100_res50.yaml’ saved [512/512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/baseline/cifar100_res50.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74789ad",
   "metadata": {},
   "source": [
    "Have a detailed look into the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a44f41",
   "metadata": {
    "id": "84a44f41",
    "outputId": "a6eaa934-a0f0-4401-a22d-3d2f9e8ddebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment:\r\n",
      "  project: \"demo\"\r\n",
      "  tag: \"cifar100_res50\"\r\n",
      "  \r\n",
      "output_dir: \"./data\"\r\n",
      "train_epochs: 1\r\n",
      "\r\n",
      "### dataset\r\n",
      "data_set: \"cifar100\"\r\n",
      "data_path:  \"./data\"\r\n",
      "num_workers: 4\r\n",
      "\r\n",
      "### model\r\n",
      "model_type: \"resnet50\"\r\n",
      "\r\n",
      "## optimizer\r\n",
      "optimizer: \"SGD\"\r\n",
      "learning_rate: 0.00753\r\n",
      "weight_decay: 0.00115\r\n",
      "momentum: 0.9\r\n",
      "\r\n",
      "### scheduler\r\n",
      "lr_scheduler: \"CosineAnnealingLR\"\r\n",
      "lr_scheduler_config:\r\n",
      "    T_max: 200\r\n",
      "\r\n",
      "### early stop\r\n",
      "early_stop: \"EarlyStopping\"\r\n",
      "early_stop_config:\r\n",
      "    tolerance_epoch: 15\r\n"
     ]
    }
   ],
   "source": [
    "!cat cifar100_res50.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce65be0",
   "metadata": {
    "id": "dce65be0"
   },
   "source": [
    "### 2.2 Launch training\n",
    "**Training resnet50 on CIFAR100 from scratch:**\n",
    "\n",
    "We can directly train the model with only one-line command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e294640",
   "metadata": {
    "id": "3e294640",
    "outputId": "a16efe13-bad7-447d-9f0b-c9d1ae944e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "configurations:\n",
      "{'train_batch_size': 128, 'start_epoch': 0, 'initial_pretrain': '', 'kd': {'temperature': 4}, 'drop_last': False, 'optimizer': 'SGD', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'enable_ipex': False, 'log_interval_step': 10, 'train_epochs': 1, 'metric_threshold': 100.0, 'profiler': False, 'warmup_scheduler_epoch': 0, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'eval_metric': 'accuracy', 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_resnet50_cifar100', 'weight_decay': 0.00115, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/baseline/cifar100_res50/profile/profile_resnet50_cifar100_1675653984'}, 'finetuner': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'pretrained_num_classes': 10, 'finetuned_lr': 0.01, 'frozen': False}, 'dist_backend': 'gloo', 'lr_scheduler': 'CosineAnnealingLR', 'output_dir': '/home/vmagent/app/data/model', 'device': 'cpu', 'model_save_interval': 40, 'learning_rate': 0.00753, 'pretrain': '', 'warmup_scheduler': '', 'early_stop': 'EarlyStopping', 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'input_size': 32, 'test_transform': 'default', 'pin_mem': False, 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'eval_batch_size': 128, 'eval_epochs': 1, 'eval_step': 10, 'momentum': 0.9, 'experiment': {'project': 'baseline', 'tag': 'cifar100_res50', 'strategy': ''}, 'data_set': 'cifar100', 'criterion': 'CrossEntropyLoss', 'seed': 0, 'model_type': 'resnet50', 'num_workers': 4, 'train_transform': 'default'}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "Epoch [0] learning rate: [0.00753]\n",
      "[2023-02-06 03:26:27] rank(0) epoch(0) step (0/391) Train: loss = 6.5908;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:35] rank(0) epoch(0) step (10/391) Train: loss = 5.6120;\taccuracy = 2.3438\n",
      "[2023-02-06 03:26:37] rank(0) epoch(0) step (20/391) Train: loss = 6.1267;\taccuracy = 0.0000\n",
      "[2023-02-06 03:26:39] rank(0) epoch(0) step (30/391) Train: loss = 5.4950;\taccuracy = 3.9062\n",
      "[2023-02-06 03:26:41] rank(0) epoch(0) step (40/391) Train: loss = 5.1365;\taccuracy = 2.3438\n",
      "[2023-02-06 03:26:43] rank(0) epoch(0) step (50/391) Train: loss = 5.8884;\taccuracy = 4.6875\n",
      "[2023-02-06 03:26:45] rank(0) epoch(0) step (60/391) Train: loss = 5.4579;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:47] rank(0) epoch(0) step (70/391) Train: loss = 5.1344;\taccuracy = 0.7812\n",
      "[2023-02-06 03:26:49] rank(0) epoch(0) step (80/391) Train: loss = 5.7166;\taccuracy = 3.1250\n",
      "[2023-02-06 03:26:51] rank(0) epoch(0) step (90/391) Train: loss = 5.5491;\taccuracy = 1.5625\n",
      "[2023-02-06 03:26:53] rank(0) epoch(0) step (100/391) Train: loss = 5.1875;\taccuracy = 2.3438\n",
      "[2023-02-06 03:27:00] rank(0) epoch(0) step (110/391) Train: loss = 5.4646;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:02] rank(0) epoch(0) step (120/391) Train: loss = 5.4944;\taccuracy = 3.1250\n",
      "[2023-02-06 03:27:04] rank(0) epoch(0) step (130/391) Train: loss = 6.2241;\taccuracy = 1.5625\n",
      "[2023-02-06 03:27:06] rank(0) epoch(0) step (140/391) Train: loss = 5.2008;\taccuracy = 0.0000\n",
      "[2023-02-06 03:27:08] rank(0) epoch(0) step (150/391) Train: loss = 5.4579;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:10] rank(0) epoch(0) step (160/391) Train: loss = 7.0275;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:12] rank(0) epoch(0) step (170/391) Train: loss = 4.3376;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:15] rank(0) epoch(0) step (180/391) Train: loss = 4.4311;\taccuracy = 7.0312\n",
      "[2023-02-06 03:27:17] rank(0) epoch(0) step (190/391) Train: loss = 5.0786;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:19] rank(0) epoch(0) step (200/391) Train: loss = 5.6823;\taccuracy = 4.6875\n",
      "[2023-02-06 03:27:28] rank(0) epoch(0) step (210/391) Train: loss = 5.6876;\taccuracy = 3.9062\n",
      "[2023-02-06 03:27:31] rank(0) epoch(0) step (220/391) Train: loss = 6.4414;\taccuracy = 3.9062\n",
      "[2023-02-06 03:27:34] rank(0) epoch(0) step (230/391) Train: loss = 5.6525;\taccuracy = 3.1250\n",
      "[2023-02-06 03:27:37] rank(0) epoch(0) step (240/391) Train: loss = 5.0057;\taccuracy = 5.4688\n",
      "[2023-02-06 03:27:40] rank(0) epoch(0) step (250/391) Train: loss = 6.0380;\taccuracy = 9.3750\n",
      "[2023-02-06 03:27:43] rank(0) epoch(0) step (260/391) Train: loss = 4.6368;\taccuracy = 6.2500\n",
      "[2023-02-06 03:27:46] rank(0) epoch(0) step (270/391) Train: loss = 4.4934;\taccuracy = 10.9375\n",
      "[2023-02-06 03:27:49] rank(0) epoch(0) step (280/391) Train: loss = 4.6952;\taccuracy = 13.2812\n",
      "[2023-02-06 03:27:52] rank(0) epoch(0) step (290/391) Train: loss = 4.4364;\taccuracy = 4.6875\n",
      "[2023-02-06 03:27:54] rank(0) epoch(0) step (300/391) Train: loss = 5.3730;\taccuracy = 5.4688\n",
      "[2023-02-06 03:28:03] rank(0) epoch(0) step (310/391) Train: loss = 6.9364;\taccuracy = 3.1250\n",
      "[2023-02-06 03:28:06] rank(0) epoch(0) step (320/391) Train: loss = 4.9181;\taccuracy = 3.1250\n",
      "[2023-02-06 03:28:08] rank(0) epoch(0) step (330/391) Train: loss = 5.3563;\taccuracy = 5.4688\n",
      "[2023-02-06 03:28:10] rank(0) epoch(0) step (340/391) Train: loss = 4.2426;\taccuracy = 6.2500\n",
      "[2023-02-06 03:28:12] rank(0) epoch(0) step (350/391) Train: loss = 4.8778;\taccuracy = 7.8125\n",
      "[2023-02-06 03:28:15] rank(0) epoch(0) step (360/391) Train: loss = 4.4332;\taccuracy = 7.8125\n",
      "[2023-02-06 03:28:17] rank(0) epoch(0) step (370/391) Train: loss = 4.9217;\taccuracy = 6.2500\n",
      "[2023-02-06 03:28:21] rank(0) epoch(0) step (380/391) Train: loss = 5.1312;\taccuracy = 4.6875\n",
      "[2023-02-06 03:28:23] rank(0) epoch(0) step (390/391) Train: loss = 5.2084;\taccuracy = 7.5000\n",
      "2023-02-06 03:28:30 0/391\n",
      "2023-02-06 03:28:31 10/391\n",
      "2023-02-06 03:28:31 20/391\n",
      "2023-02-06 03:28:32 30/391\n",
      "2023-02-06 03:28:32 40/391\n",
      "2023-02-06 03:28:33 50/391\n",
      "2023-02-06 03:28:34 60/391\n",
      "2023-02-06 03:28:34 70/391\n",
      "[2023-02-06 03:28:35] rank(0) epoch(0) Validation: accuracy = 7.7700;\tloss = 4.0517\n",
      "Best Epoch: 0, accuracy: 7.769999980926514\n",
      "Epoch 0 took 135.11660027503967 seconds\n",
      "Total seconds:135.1177\n",
      "Totally take 137.19904828071594 seconds\n"
     ]
    }
   ],
   "source": [
    "! python -u /usr/local/lib/python3.9/dist-packages/e2eAIOK/ModelAdapter/main.py --cfg cifar100_res50.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551a8b5",
   "metadata": {
    "id": "8551a8b5"
   },
   "source": [
    "## 3. Launch training with Finetuner\n",
    "Then we train ResNet50 on CIFAR100 with Finetuner to show the performance imrpovement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b8fae",
   "metadata": {
    "id": "e42b8fae"
   },
   "source": [
    "### 3.1 Prepare pretrained model \n",
    "Download pretrained ResNet50 model on ImageNet21k and put it in \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a823b2c",
   "metadata": {
    "id": "6a823b2c",
    "outputId": "67af58fd-37f5-4f7b-cb64-b1b3d15a7314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-27 09:57:01--  https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.55\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.55|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 186531247 (178M) [application/octet-stream]\n",
      "Saving to: ‘resnet50_miil_21k.pth’\n",
      "\n",
      "100%[======================================>] 186,531,247 5.44MB/s   in 31s    \n",
      "\n",
      "2023-03-27 09:57:34 (5.67 MB/s) - ‘resnet50_miil_21k.pth’ saved [186531247/186531247]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/resnet50_miil_21k.pth && mkdir data && mv resnet50_miil_21k.pth data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5b7b7",
   "metadata": {
    "id": "4ec5b7b7"
   },
   "source": [
    "### 3.2 Configuration\n",
    "\n",
    "Now we download a configuration for Finetuner with ResNet50 with CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62596edc",
   "metadata": {
    "id": "62596edc",
    "outputId": "372ef710-dab6-4a0c-8315-fc47f0ca32db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-19 22:47:45--  https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 788 [text/plain]\n",
      "Saving to: ‘cifar100_res50PretrainI21k.yaml’\n",
      "\n",
      "100%[======================================>] 788         --.-K/s   in 0s      \n",
      "\n",
      "2023-03-19 22:47:46 (22.8 MB/s) - ‘cifar100_res50PretrainI21k.yaml’ saved [788/788]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/intel/e2eAIOK/main/conf/ma/demo/finetuner/cifar100_res50PretrainI21k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cafd64",
   "metadata": {},
   "source": [
    "Have a detailed look into the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d434a30",
   "metadata": {
    "id": "1d434a30",
    "outputId": "98dd2ed5-b2e7-4d13-a6fb-5b44913c20d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment:\r\n",
      "  project: \"finetuner\"\r\n",
      "  tag: \"cifar100_res50_PretrainI21k\"\r\n",
      "  strategy: \"OnlyFinetuneStrategy\"\r\n",
      "\r\n",
      "output_dir: \".data/\"\r\n",
      "train_epochs: 1\r\n",
      "enable_ipex: True\r\n",
      "\r\n",
      "### dataset\r\n",
      "data_set: \"cifar100\"\r\n",
      "data_path:  \".data/\"\r\n",
      "num_workers: 4\r\n",
      "input_size: 112\r\n",
      "\r\n",
      "### model\r\n",
      "model_type: \"resnet50\"\r\n",
      "\r\n",
      "## finetuner\r\n",
      "finetuner:\r\n",
      "    type: \"Basic\"\r\n",
      "    pretrain: '.data/resnet50_miil_21k.pth'\r\n",
      "    pretrained_num_classes: 11221\r\n",
      "    finetuned_lr: 0.00445\r\n",
      "    frozen: False\r\n",
      "\r\n",
      "## optimizer\r\n",
      "optimizer: \"SGD\"\r\n",
      "learning_rate: 0.00753\r\n",
      "weight_decay: 0.00115\r\n",
      "momentum: 0.9\r\n",
      "\r\n",
      "### scheduler\r\n",
      "lr_scheduler: \"CosineAnnealingLR\"\r\n",
      "lr_scheduler_config:\r\n",
      "    T_max: 200\r\n",
      "\r\n",
      "### early stop\r\n",
      "early_stop: \"EarlyStopping\"\r\n",
      "early_stop_config:\r\n",
      "    tolerance_epoch: 5"
     ]
    }
   ],
   "source": [
    "! cat cifar100_res50PretrainI21k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6506202",
   "metadata": {
    "id": "f6506202"
   },
   "source": [
    "### 3.3 Launch Training with Finetuner\n",
    "**Training resnet50 on CIFAR100 with Finetuner:**\n",
    "\n",
    "Only need to change the configuration file, we can directly train the model with Fine-tuner in only one-line command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac25c5",
   "metadata": {
    "id": "9fac25c5",
    "outputId": "8c133549-3a1e-438f-f0a0-17adfaa3be2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "See abnormal behavior in dataloader when enable IPEX in PyTorch 1.12, set enable_ipex to False!\n",
      "configurations:\n",
      "{'lr_scheduler': 'CosineAnnealingLR', 'pretrain': '', 'eval_epochs': 1, 'criterion': 'CrossEntropyLoss', 'data_set': 'cifar100', 'early_stop_config': {'tolerance_epoch': 15, 'delta': 0.0001, 'is_max': True}, 'dkd': {'alpha': 1.0, 'beta': 8.0, 'temperature': 4.0, 'warmup': 20}, 'output_dir': '/home/vmagent/app/data/model', 'data_path': '/home/vmagent/app/data/dataset/cifar', 'loss_weight': {'backbone': 1.0, 'distiller': 0.0, 'adapter': 0.0}, 'eval_batch_size': 128, 'lr_scheduler_config': {'decay_stages': [], 'decay_patience': 10, 'T_max': 200, 'decay_rate': 0.1}, 'enable_ipex': False, 'distiller': {'type': '', 'teacher': {'type': '', 'initial_pretrain': '', 'pretrain': '', 'frozen': True}, 'save_logits': False, 'use_saved_logits': False, 'check_logits': False, 'logits_path': '', 'logits_topk': 0, 'save_logits_start_epoch': 0}, 'metric_threshold': 100.0, 'start_epoch': 0, 'adapter': {'type': '', 'feature_size': 1, 'feature_layer_name': 'x'}, 'train_batch_size': 128, 'experiment': {'strategy': 'OnlyFinetuneStrategy', 'project': 'finetuner', 'tag': 'cifar100_res50_PretrainI21k'}, 'optimizer': 'SGD', 'learning_rate': 0.00753, 'model_type': 'resnet50', 'seed': 0, 'device': 'cpu', 'eval_step': 10, 'num_workers': 4, 'momentum': 0.9, 'tensorboard_dir': '/home/vmagent/app/data/tensorboard/cifar100_res50_PretrainI21k_resnet50_OnlyFinetuneStrategy_cifar100', 'warmup_scheduler': '', 'profiler': False, 'finetuner': {'frozen': False, 'pretrained_num_classes': 11221, 'pretrain': '/home/vmagent/app/data/pretrained/resnet50_miil_21k.pth', 'finetuned_lr': 0.00445, 'initial_pretrain': '', 'type': 'Basic'}, 'input_size': 112, 'initial_pretrain': '', 'log_interval_step': 10, 'test_transform': 'default', 'model_save_interval': 40, 'warmup_scheduler_epoch': 0, 'drop_last': False, 'pin_mem': False, 'weight_decay': 0.00115, 'kd': {'temperature': 4}, 'early_stop': 'EarlyStopping', 'train_transform': 'default', 'dist_backend': 'gloo', 'profiler_config': {'skip_first': 1, 'wait': 1, 'warmup': 1, 'active': 2, 'repeat': 1, 'trace_file': '/home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/profile_resnet50_OnlyFinetuneStrategy_cifar100_1675655019'}, 'train_epochs': 1, 'eval_metric': 'accuracy'}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "could not load layer: fc.weight; mismatch shape: target [torch.Size([100, 2048])] != pretrained [torch.Size([11221, 2048])]\n",
      "could not load layer: fc.bias; mismatch shape: target [torch.Size([100])] != pretrained [torch.Size([11221])]\n",
      "Epoch [0] learning rate: [0.00445, 0.00753]\n",
      "[2023-02-06 03:43:45] rank(0) epoch(0) step (0/391) Train: total_loss = 4.7609;\tbackbone_loss = 4.7609;\taccuracy = 0.0000\n",
      "[2023-02-06 03:44:16] rank(0) epoch(0) step (10/391) Train: total_loss = 4.6885;\tbackbone_loss = 4.6885;\taccuracy = 1.5625\n",
      "[2023-02-06 03:45:03] rank(0) epoch(0) step (20/391) Train: total_loss = 4.4324;\tbackbone_loss = 4.4324;\taccuracy = 4.6875\n",
      "[2023-02-06 03:45:44] rank(0) epoch(0) step (30/391) Train: total_loss = 4.0681;\tbackbone_loss = 4.0681;\taccuracy = 17.1875\n",
      "[2023-02-06 03:46:35] rank(0) epoch(0) step (40/391) Train: total_loss = 3.8115;\tbackbone_loss = 3.8115;\taccuracy = 21.0938\n",
      "[2023-02-06 03:47:16] rank(0) epoch(0) step (50/391) Train: total_loss = 3.3029;\tbackbone_loss = 3.3029;\taccuracy = 34.3750\n",
      "[2023-02-06 03:47:42] rank(0) epoch(0) step (60/391) Train: total_loss = 2.8760;\tbackbone_loss = 2.8760;\taccuracy = 37.5000\n",
      "[2023-02-06 03:48:07] rank(0) epoch(0) step (70/391) Train: total_loss = 2.2407;\tbackbone_loss = 2.2407;\taccuracy = 55.4688\n",
      "[2023-02-06 03:48:32] rank(0) epoch(0) step (80/391) Train: total_loss = 1.9484;\tbackbone_loss = 1.9484;\taccuracy = 53.1250\n",
      "[2023-02-06 03:48:59] rank(0) epoch(0) step (90/391) Train: total_loss = 1.7266;\tbackbone_loss = 1.7266;\taccuracy = 58.5938\n",
      "[2023-02-06 03:49:18] rank(0) epoch(0) step (100/391) Train: total_loss = 1.4433;\tbackbone_loss = 1.4433;\taccuracy = 60.1562\n",
      "[2023-02-06 03:49:43] rank(0) epoch(0) step (110/391) Train: total_loss = 1.4062;\tbackbone_loss = 1.4062;\taccuracy = 60.1562\n",
      "[2023-02-06 03:50:03] rank(0) epoch(0) step (120/391) Train: total_loss = 1.2962;\tbackbone_loss = 1.2962;\taccuracy = 67.9688\n",
      "[2023-02-06 03:50:24] rank(0) epoch(0) step (130/391) Train: total_loss = 1.2078;\tbackbone_loss = 1.2078;\taccuracy = 65.6250\n",
      "[2023-02-06 03:50:43] rank(0) epoch(0) step (140/391) Train: total_loss = 1.2846;\tbackbone_loss = 1.2846;\taccuracy = 64.0625\n",
      "[2023-02-06 03:51:03] rank(0) epoch(0) step (150/391) Train: total_loss = 0.9217;\tbackbone_loss = 0.9217;\taccuracy = 75.0000\n",
      "[2023-02-06 03:51:24] rank(0) epoch(0) step (160/391) Train: total_loss = 0.9014;\tbackbone_loss = 0.9014;\taccuracy = 75.7812\n",
      "[2023-02-06 03:51:43] rank(0) epoch(0) step (170/391) Train: total_loss = 1.1210;\tbackbone_loss = 1.1210;\taccuracy = 68.7500\n",
      "[2023-02-06 03:52:03] rank(0) epoch(0) step (180/391) Train: total_loss = 0.8800;\tbackbone_loss = 0.8800;\taccuracy = 73.4375\n",
      "[2023-02-06 03:52:23] rank(0) epoch(0) step (190/391) Train: total_loss = 0.8269;\tbackbone_loss = 0.8269;\taccuracy = 75.7812\n",
      "[2023-02-06 03:52:42] rank(0) epoch(0) step (200/391) Train: total_loss = 1.0181;\tbackbone_loss = 1.0181;\taccuracy = 75.7812\n",
      "[2023-02-06 03:53:08] rank(0) epoch(0) step (210/391) Train: total_loss = 0.7856;\tbackbone_loss = 0.7856;\taccuracy = 76.5625\n",
      "[2023-02-06 03:53:28] rank(0) epoch(0) step (220/391) Train: total_loss = 0.9602;\tbackbone_loss = 0.9602;\taccuracy = 70.3125\n",
      "[2023-02-06 03:53:49] rank(0) epoch(0) step (230/391) Train: total_loss = 0.7803;\tbackbone_loss = 0.7803;\taccuracy = 79.6875\n",
      "[2023-02-06 03:54:09] rank(0) epoch(0) step (240/391) Train: total_loss = 0.7919;\tbackbone_loss = 0.7919;\taccuracy = 74.2188\n",
      "[2023-02-06 03:54:28] rank(0) epoch(0) step (250/391) Train: total_loss = 0.8529;\tbackbone_loss = 0.8529;\taccuracy = 74.2188\n",
      "[2023-02-06 03:54:47] rank(0) epoch(0) step (260/391) Train: total_loss = 0.6466;\tbackbone_loss = 0.6466;\taccuracy = 85.1562\n",
      "[2023-02-06 03:55:07] rank(0) epoch(0) step (270/391) Train: total_loss = 0.7036;\tbackbone_loss = 0.7036;\taccuracy = 74.2188\n",
      "[2023-02-06 03:55:27] rank(0) epoch(0) step (280/391) Train: total_loss = 0.9187;\tbackbone_loss = 0.9187;\taccuracy = 73.4375\n",
      "[2023-02-06 03:55:47] rank(0) epoch(0) step (290/391) Train: total_loss = 0.7531;\tbackbone_loss = 0.7531;\taccuracy = 78.1250\n",
      "[2023-02-06 03:56:07] rank(0) epoch(0) step (300/391) Train: total_loss = 0.7439;\tbackbone_loss = 0.7439;\taccuracy = 79.6875\n",
      "[2023-02-06 03:56:33] rank(0) epoch(0) step (310/391) Train: total_loss = 0.5691;\tbackbone_loss = 0.5691;\taccuracy = 85.1562\n",
      "[2023-02-06 03:56:53] rank(0) epoch(0) step (320/391) Train: total_loss = 0.7912;\tbackbone_loss = 0.7912;\taccuracy = 79.6875\n",
      "[2023-02-06 03:57:14] rank(0) epoch(0) step (330/391) Train: total_loss = 0.6261;\tbackbone_loss = 0.6261;\taccuracy = 81.2500\n",
      "[2023-02-06 03:57:32] rank(0) epoch(0) step (340/391) Train: total_loss = 0.6606;\tbackbone_loss = 0.6606;\taccuracy = 79.6875\n",
      "[2023-02-06 03:57:51] rank(0) epoch(0) step (350/391) Train: total_loss = 0.6889;\tbackbone_loss = 0.6889;\taccuracy = 78.1250\n",
      "[2023-02-06 03:58:10] rank(0) epoch(0) step (360/391) Train: total_loss = 0.7860;\tbackbone_loss = 0.7860;\taccuracy = 80.4688\n",
      "[2023-02-06 03:58:30] rank(0) epoch(0) step (370/391) Train: total_loss = 0.4082;\tbackbone_loss = 0.4082;\taccuracy = 87.5000\n",
      "[2023-02-06 03:58:49] rank(0) epoch(0) step (380/391) Train: total_loss = 0.5800;\tbackbone_loss = 0.5800;\taccuracy = 84.3750\n",
      "[2023-02-06 03:59:09] rank(0) epoch(0) step (390/391) Train: total_loss = 0.7530;\tbackbone_loss = 0.7530;\taccuracy = 77.5000\n",
      "2023-02-06 03:59:17 0/391\n",
      "2023-02-06 03:59:24 10/391\n",
      "2023-02-06 03:59:31 20/391\n",
      "2023-02-06 03:59:39 30/391\n",
      "2023-02-06 03:59:47 40/391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-06 03:59:54 50/391\n",
      "2023-02-06 04:00:01 60/391\n",
      "2023-02-06 04:00:08 70/391\n",
      "[2023-02-06 04:00:14] rank(0) epoch(0) Validation: accuracy = 80.6200;\tloss = 0.6625\n",
      "Best Epoch: 0, accuracy: 80.62000274658203\n",
      "Epoch 0 took 998.8511202335358 seconds\n",
      "Total seconds:998.85387\n",
      "Totally take 1001.8232228755951 seconds\n"
     ]
    }
   ],
   "source": [
    "! python -u /usr/local/lib/python3.9/dist-packages/e2eAIOK/ModelAdapter/main.py --cfg cifar100_res50PretrainI21k.yaml"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
