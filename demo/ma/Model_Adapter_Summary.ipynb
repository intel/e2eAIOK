{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cee0bb6",
   "metadata": {},
   "source": [
    "# Intel® End-to-End AI Optimization Kit Model Adapter DEMO\n",
    "Model Adapter extends Intel® End-to-End AI Optimization Kit optimized models with knowledge transfer technology. It is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and datasets. Model Adapter mainly contains three components served for different cases: Finetuner for pretraining & fine-tuning, Distiller for knowledge distillation and Domain Adapter for domain adaption. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c721aa",
   "metadata": {},
   "source": [
    "# Content\n",
    "* [Model Adapter Structure](#Model-Adapter-Structure)\n",
    "    * [Overview](#Overview)\n",
    "    * [Finetuner](#Finetuner)\n",
    "    * [Distiller](#Distiller)\n",
    "    * [Domain Adapter](#Domain-Adapter)\n",
    "* [Try Model Adapter](#Try-Model-Adapter)\n",
    "    * [Finetuner on Image Classification](#Finetuner-on-Image-Classification)\n",
    "    * [Distiller on Image Classification](#Distiller-on-Image-Classification)\n",
    "    * [Domain Adapter on Medical Segmentation](#Domain-Adapter-on-Medical-Segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac2a67",
   "metadata": {},
   "source": [
    "# Model Adapter Structure\n",
    "## Overview\n",
    "\n",
    "Model Adapter is a general and convenient framework for transfer knowledge from pretrained model and/or source domain data to target task. Its objectives are:\n",
    "* Transfer knowledge from pretrained model with the same/different network structure, which greatly speedups training without accuracy regression.\n",
    "* Transfer knowledge from source domain data without target label.\n",
    "\n",
    "There are three modules in Model Adapter: Finetuner for pretraining & fine-tuning, Distiller for knowledge distillation and Model Adapter for domain adaption.  What’s more, Model Adaptor makes additional efforts on CPU optimization of training and inference, both on single-node and distributed modes. \n",
    "\n",
    "### Struture\n",
    "The overview strucuture of model adapter is showed as below:\n",
    "<img src=\"./imgs/overview.png\" width=\"60%\">\n",
    "\n",
    "### Transferrable Model\n",
    "\n",
    "In Model Adapter, all the three components share a unified API and can be easily integrated with existing pipeline with few codes modification.\n",
    "\n",
    "We use a transferrable model as a container, which contains a backbone (the original model), a finetunner, an adapter, a distiller, and is used to enhance backbone with transfer learning ability.\n",
    "\n",
    "We can use to make a model to be transferrable:\n",
    "\n",
    "```\n",
    "transferrable_model = make_transferrable (model, loss, finetunner, distiller, adapter,...)\n",
    "```\n",
    "Then we can use transferrable_model to train like original model with the help of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87340cc8",
   "metadata": {},
   "source": [
    "##  Finetunner\n",
    "Finetuner is based on pretraining and finetuning technology, it can transfer knowledge from pretrained model to target model with same network structure. Pretrained models usually are generated by pretraining process, which is training specific model  on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace. Finetunner retrieves the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode. With finetunner, we can greatly improve training speed, and usually achieves better performance.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"50%\" src=\"./imgs/finetuner.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462e304",
   "metadata": {},
   "source": [
    "## Distiller\n",
    "Distiller is based on knowledge distillation technology, it can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure. Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge. With distiller, we can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "\n",
    "<img src=\"./imgs/distiller.png\" width=\"60%\">\n",
    "<center>Model Adapter Distiller Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215cbde",
   "metadata": {},
   "source": [
    "### Domain Adapter\n",
    "Domain Adapter is based on domain transfer technology, it can transfer knowledge from source domain(cheap labels) to target domain (few label or label-free).  Direct applying pre-trained model into target domain always cannot work due to covariate shift and label shift,  while labeling could be expensive in some domains and delays the model deployment time, which make fine-tuning not working. Adapter aims at reusing the transferable knowledge with the help of another labeled dataset with same learning task. That is, achieving better generalization with little labeled target dataset or achieving a competitive performance in label-free target dataset.\n",
    "\n",
    "<img src=\"./imgs/adapter.png\" width=\"70%\">\n",
    "<center>Model Adapter Domain Adapter Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efd94c",
   "metadata": {},
   "source": [
    "# Try Model Adapter\n",
    "\n",
    "### Built-in Demos\n",
    "- [Finetuner on Image Classification](./finetuner/Model_Adapter_Finetuner_builtin_ResNet50_CIFAR100.ipynb)\n",
    "- [Distiller on Image Classification](./distiller/Model_Adapter_Distiller_builtin_ResNet18_CIFAR100.ipynb)\n",
    "- [Domain Adapter on Medical Segmentation](./domain_adapter/Model_Adapter_Domain_Adapter_builtin_Unet_KITS19.ipynb)\n",
    "\n",
    "### API usage for Customized usage\n",
    "- [Finetuner on Image Classification](./finetuner/Model_Adapter_Finetuner_customized_ResNet50_CIFAR100.ipynb)\n",
    "- [Distiller on Image Classification](./distiller/Model_Adapter_Distiller_customized_ResNet18_CIFAR100.ipynb)\n",
    "    - Optimized with logits saving\n",
    "        - [Save logits](./distiller/Model_Adapter_Distiller_customized_VIT_on_CIFAR100_save_logits.ipynb)\n",
    "        - [Train with saved logits](./distiller/Model_Adapter_Distiller_customized_ResNet18_CIFAR100_train_with_logits.ipynb)\n",
    "- [Domain Adapter on Medical Segmentation](./domain_adapter/Model_Adapter_Domain_Adapter_customized_Unet_KITS19.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
