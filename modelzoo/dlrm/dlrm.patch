diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 3563e00..cc013a1 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -5,7 +5,7 @@ possible.
 ## Pull Requests
 We actively welcome your pull requests.
 
-1. Fork the repo and create your branch from `main`.
+1. Fork the repo and create your branch from `master`.
 2. If you've added code that should be tested, add tests.
 3. If you've changed APIs, update the documentation.
 4. Ensure the test suite passes.
diff --git a/README.md b/README.md
deleted file mode 100644
index c6b5b99..0000000
--- a/README.md
+++ /dev/null
@@ -1,389 +0,0 @@
-Deep Learning Recommendation Model for Personalization and Recommendation Systems:
-=================================================================================
-*Copyright (c) Facebook, Inc. and its affiliates.*
-
-Description:
-------------
-An implementation of a deep learning recommendation model (DLRM)
-The model input consists of dense and sparse features. The former is a vector
-of floating point values. The latter is a list of sparse indices into
-embedding tables, which consist of vectors of floating point values.
-The selected vectors are passed to mlp networks denoted by triangles,
-in some cases the vectors are interacted through operators (Ops).
-```
-output:
-                    probability of a click
-model:                        |
-                             /\
-                            /__\
-                              |
-      _____________________> Op  <___________________
-    /                         |                      \
-   /\                        /\                      /\
-  /__\                      /__\           ...      /__\
-   |                          |                       |
-   |                         Op                      Op
-   |                    ____/__\_____           ____/__\____
-   |                   |_Emb_|____|__|    ...  |_Emb_|__|___|
-input:
-[ dense features ]     [sparse indices] , ..., [sparse indices]
-```
- More precise definition of model layers:
- 1) fully connected layers of an mlp
-
-    z = f(y)
-
-    y = Wx + b
-
- 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])
-
-    z = Op(e1,...,ek)
-
-    obtain vectors e1=E[:,p1], ..., ek=E[:,pk]
-
- 3) Operator Op can be one of the following
-
-    Sum(e1,...,ek) = e1 + ... + ek
-
-    Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]
-
-    Cat(e1,...,ek) = [e1', ..., ek']'
-
-    where ' denotes transpose operation
-
-Cite [Work](https://arxiv.org/abs/1906.00091):
-```
-@article{DLRM19,
-  author    = {Maxim Naumov and Dheevatsa Mudigere and Hao{-}Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole{-}Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
-  title     = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
-  journal   = {CoRR},
-  volume    = {abs/1906.00091},
-  year      = {2019},
-  url       = {https://arxiv.org/abs/1906.00091},
-}
-```
-
-Related Work:
-
-On the [system architecture implications](https://arxiv.org/abs/1906.03109), with DLRM as one of the benchmarks,
-```
-@article{ArchImpl19,
-  author    = {Udit Gupta and Xiaodong Wang and Maxim Naumov and Carole{-}Jean Wu and Brandon Reagen and David Brooks and Bradford Cottel and Kim M. Hazelwood and Bill Jia and Hsien{-}Hsin S. Lee and Andrey Malevich and Dheevatsa Mudigere and Mikhail Smelyanskiy and Liang Xiong and Xuan Zhang},
-  title     = {The Architectural Implications of Facebook's DNN-based Personalized Recommendation},
-  journal   = {CoRR},
-  volume    = {abs/1906.03109},
-  year      = {2019},
-  url       = {https://arxiv.org/abs/1906.03109},
-}
-```
-
-On the [embedding compression techniques (for number of vectors)](https://arxiv.org/abs/1909.02107), with DLRM as one of the benchmarks,
-```
-@article{QuoRemTrick19,
-  author    = {Hao{-}Jun Michael Shi and Dheevatsa Mudigere and Maxim Naumov and Jiyan Yang},
-  title     = {Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems},
-  journal   = {CoRR},
-  volume    = {abs/1909.02107},
-  year      = {2019},
-  url       = {https://arxiv.org/abs/1909.02107},
-}
-```
-
-On the [embedding compression techniques (for dimension of vectors)](https://arxiv.org/abs/1909.11810), with DLRM as one of the benchmarks,
-```
-@article{MixDimTrick19,
-  author    = {Antonio Ginart and Maxim Naumov and Dheevatsa Mudigere and Jiyan Yang and James Zou},
-  title     = {Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems},
-  journal   = {CoRR},
-  volume    = {abs/1909.11810},
-  year      = {2019},
-  url       = {https://arxiv.org/abs/1909.11810},
-}
-```
-
-Implementation
---------------
-**DLRM PyTorch**. Implementation of DLRM in PyTorch framework:
-
-       dlrm_s_pytorch.py
-
-**DLRM Caffe2**. Implementation of DLRM in Caffe2 framework:
-
-       dlrm_s_caffe2.py
-
-**DLRM Data**. Implementation of DLRM data generation and loading:
-
-       dlrm_data_pytorch.py, dlrm_data_caffe2.py, data_utils.py
-
-**DLRM Tests**. Implementation of DLRM tests in ./test
-
-       dlrm_s_test.sh
-
-**DLRM Benchmarks**. Implementation of DLRM benchmarks in ./bench
-
-       dlrm_s_criteo_kaggle.sh, dlrm_s_criteo_terabyte.sh, dlrm_s_benchmark.sh
-
-Related Work:
-
-On the [Glow framework](https://github.com/pytorch/glow) implementation
-```
-https://github.com/pytorch/glow/blob/master/tests/unittests/RecommendationSystemTest.cpp
-```
-On the [FlexFlow framework](https://github.com/flexflow/FlexFlow) distributed implementation with Legion backend
-```
-https://github.com/flexflow/FlexFlow/blob/master/examples/cpp/DLRM/dlrm.cc
-```
-
-How to run dlrm code?
---------------------
-1) A sample run of the code, with a tiny model is shown below
-```
-$ python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6
-time/loss/accuracy (if enabled):
-Finished training it 1/3 of epoch 0, -1.00 ms/it, loss 0.451893, accuracy 0.000%
-Finished training it 2/3 of epoch 0, -1.00 ms/it, loss 0.402002, accuracy 0.000%
-Finished training it 3/3 of epoch 0, -1.00 ms/it, loss 0.275460, accuracy 0.000%
-```
-2) A sample run of the code, with a tiny model in debug mode
-```
-$ python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6 --debug-mode
-model arch:
-mlp top arch 3 layers, with input to output dimensions:
-[8 4 2 1]
-# of interactions
-8
-mlp bot arch 2 layers, with input to output dimensions:
-[4 3 2]
-# of features (sparse and dense)
-4
-dense feature size
-4
-sparse feature size
-2
-# of embeddings (= # of sparse features) 3, with dimensions 2x:
-[4 3 2]
-data (inputs and targets):
-mini-batch: 0
-[[0.69647 0.28614 0.22685 0.55131]
- [0.71947 0.42311 0.98076 0.68483]]
-[[[1], [0, 1]], [[0], [1]], [[1], [0]]]
-[[0.55679]
- [0.15896]]
-mini-batch: 1
-[[0.36179 0.22826 0.29371 0.63098]
- [0.0921  0.4337  0.43086 0.49369]]
-[[[1], [0, 2, 3]], [[1], [1, 2]], [[1], [1]]]
-[[0.15307]
- [0.69553]]
-mini-batch: 2
-[[0.60306 0.54507 0.34276 0.30412]
- [0.41702 0.6813  0.87546 0.51042]]
-[[[2], [0, 1, 2]], [[1], [2]], [[1], [1]]]
-[[0.31877]
- [0.69197]]
-initial parameters (weights and bias):
-[[ 0.05438 -0.11105]
- [ 0.42513  0.34167]
- [-0.1426  -0.45641]
- [-0.19523 -0.10181]]
-[[ 0.23667  0.57199]
- [-0.16638  0.30316]
- [ 0.10759  0.22136]]
-[[-0.49338 -0.14301]
- [-0.36649 -0.22139]]
-[[0.51313 0.66662 0.10591 0.13089]
- [0.32198 0.66156 0.84651 0.55326]
- [0.85445 0.38484 0.31679 0.35426]]
-[0.17108 0.82911 0.33867]
-[[0.55237 0.57855 0.52153]
- [0.00269 0.98835 0.90534]]
-[0.20764 0.29249]
-[[0.52001 0.90191 0.98363 0.25754 0.56436 0.80697 0.39437 0.73107]
- [0.16107 0.6007  0.86586 0.98352 0.07937 0.42835 0.20454 0.45064]
- [0.54776 0.09333 0.29686 0.92758 0.569   0.45741 0.75353 0.74186]
- [0.04858 0.7087  0.83924 0.16594 0.781   0.28654 0.30647 0.66526]]
-[0.11139 0.66487 0.88786 0.69631]
-[[0.44033 0.43821 0.7651  0.56564]
- [0.0849  0.58267 0.81484 0.33707]]
-[0.92758 0.75072]
-[[0.57406 0.75164]]
-[0.07915]
-DLRM_Net(
-  (emb_l): ModuleList(
-    (0): EmbeddingBag(4, 2, mode=sum)
-    (1): EmbeddingBag(3, 2, mode=sum)
-    (2): EmbeddingBag(2, 2, mode=sum)
-  )
-  (bot_l): Sequential(
-    (0): Linear(in_features=4, out_features=3, bias=True)
-    (1): ReLU()
-    (2): Linear(in_features=3, out_features=2, bias=True)
-    (3): ReLU()
-  )
-  (top_l): Sequential(
-    (0): Linear(in_features=8, out_features=4, bias=True)
-    (1): ReLU()
-    (2): Linear(in_features=4, out_features=2, bias=True)
-    (3): ReLU()
-    (4): Linear(in_features=2, out_features=1, bias=True)
-    (5): Sigmoid()
-  )
-)
-time/loss/accuracy (if enabled):
-Finished training it 1/3 of epoch 0, -1.00 ms/it, loss 0.451893, accuracy 0.000%
-Finished training it 2/3 of epoch 0, -1.00 ms/it, loss 0.402002, accuracy 0.000%
-Finished training it 3/3 of epoch 0, -1.00 ms/it, loss 0.275460, accuracy 0.000%
-updated parameters (weights and bias):
-[[ 0.0543  -0.1112 ]
- [ 0.42513  0.34167]
- [-0.14283 -0.45679]
- [-0.19532 -0.10197]]
-[[ 0.23667  0.57199]
- [-0.1666   0.30285]
- [ 0.10751  0.22124]]
-[[-0.49338 -0.14301]
- [-0.36664 -0.22164]]
-[[0.51313 0.66663 0.10591 0.1309 ]
- [0.32196 0.66154 0.84649 0.55324]
- [0.85444 0.38482 0.31677 0.35425]]
-[0.17109 0.82907 0.33863]
-[[0.55238 0.57857 0.52154]
- [0.00265 0.98825 0.90528]]
-[0.20764 0.29244]
-[[0.51996 0.90184 0.98368 0.25752 0.56436 0.807   0.39437 0.73107]
- [0.16096 0.60055 0.86596 0.98348 0.07938 0.42842 0.20453 0.45064]
- [0.5476  0.0931  0.29701 0.92752 0.56902 0.45752 0.75351 0.74187]
- [0.04849 0.70857 0.83933 0.1659  0.78101 0.2866  0.30646 0.66526]]
-[0.11137 0.66482 0.88778 0.69627]
-[[0.44029 0.43816 0.76502 0.56561]
- [0.08485 0.5826  0.81474 0.33702]]
-[0.92754 0.75067]
-[[0.57379 0.7514 ]]
-[0.07908]
-```
-
-Testing
--------
-Testing scripts to confirm functional correctness of the code
-```
-./test/dlrm_s_test.sh
-Running commands ...
-python dlrm_s_pytorch.py
-python dlrm_s_caffe2.py
-Checking results ...
-diff test1 (no numeric values in the output = SUCCESS)
-diff test2 (no numeric values in the output = SUCCESS)
-diff test3 (no numeric values in the output = SUCCESS)
-diff test4 (no numeric values in the output = SUCCESS)
-```
-
-*NOTE: Testing scripts accept extra arguments which will be passed along to the model, such as --use-gpu*
-
-Benchmarking
-------------
-1) Performance benchmarking
-    ```
-    ./bench/dlrm_s_benchmark.sh
-    ```
-
-2) The code supports interface with the [Criteo Kaggle Display Advertising Challenge Dataset](https://ailab.criteo.com/ressources/).
-   - Please do the following to prepare the dataset for use with DLRM code:
-     - First, specify the raw data file (train.txt) as downloaded with --raw-data-file=<path/train.txt>
-     - This is then pre-processed (categorize, concat across days...) to allow using with dlrm code
-     - The processed data is stored as *.npz file in <root_dir>/input/*.npz
-     - The processed file (*.npz) can be used for subsequent runs with --processed-data-file=<path/*.npz>
-   - The model can be trained using the following script
-     ```
-     ./bench/dlrm_s_criteo_kaggle.sh [--test-freq=1024]
-     ```
-
-<img src="./kaggle_dac_loss_accuracy_plots.png" width="900" height="320">
-
-3) The code supports interface with the [Criteo Terabyte Dataset](https://labs.criteo.com/2013/12/download-terabyte-click-logs/).
-   - Please do the following to prepare the dataset for use with DLRM code:
-     - First, download the raw data files day_0.gz, ...,day_23.gz and unzip them
-     - Specify the location of the unzipped text files day_0, ...,day_23, using --raw-data-file=<path/day> (the day number will be appended automatically)
-     - These are then pre-processed (categorize, concat across days...) to allow using with dlrm code
-     - The processed data is stored as *.npz file in <root_dir>/input/*.npz
-     - The processed file (*.npz) can be used for subsequent runs with --processed-data-file=<path/*.npz>
-   - The model can be trained using the following script
-    ```
-      ./bench/dlrm_s_criteo_terabyte.sh ["--test-freq=10240 --memory-map --data-sub-sample-rate=0.875"]
-    ```
-    - Corresponding pre-trained model is available under [CC-BY-NC license](https://creativecommons.org/licenses/by-nc/2.0/) and can be downloaded here
-    [dlrm_emb64_subsample0.875_maxindrange10M_pretrained.pt](https://dlrm.s3-us-west-1.amazonaws.com/models/tb0875_10M.pt)
-
-<img src="./terabyte_0875_loss_accuracy_plots.png" width="900" height="320">
-
-*NOTE: Benchmarking scripts accept extra arguments which will be passed along to the model, such as --num-batches=100 to limit the number of data samples*
-
-4) The code supports interface with [MLPerf benchmark](https://mlperf.org).
-   - Please refer to the following training parameters
-   ```
-     --mlperf-logging that keeps track of multiple metrics, including area under the curve (AUC)
-
-     --mlperf-acc-threshold that allows early stopping based on accuracy metric
-
-     --mlperf-auc-threshold that allows early stopping based on AUC metric
-
-     --mlperf-bin-loader that enables preprocessing of data into a single binary file
-
-     --mlperf-bin-shuffle that controls whether a random shuffle of mini-batches is performed
-   ```
-   - The MLPerf training model is completely specified and can be trained using the following script
-   ```
-     ./bench/run_and_time.sh [--use-gpu]
-   ```
-   - Corresponding pre-trained model is available under [CC-BY-NC license](https://creativecommons.org/licenses/by-nc/2.0/) and can be downloaded here
-     [dlrm_emb128_subsample0.0_maxindrange40M_pretrained.pt](https://dlrm.s3-us-west-1.amazonaws.com/models/tb00_40M.pt)
-
-5) The code now supports synchronous distributed training, we support gloo/nccl/mpi backend, we provide launching mode for [pytorch distributed launcher](https://pytorch.org/docs/stable/distributed.html#launch-utility) and Mpirun. For MPI, users need to write their own MPI launching scripts for configuring the running hosts. For example, using pytorch distributed launcher, we can have the following command as launching scripts:
-```
-# for single node 8 gpus and nccl as backend on randomly generated dataset:
-python -m torch.distributed.launch --nproc_per_node=8 dlrm_s_pytorch.py --arch-embedding-size="80000-80000-80000-80000-80000-80000-80000-80000" --arch-sparse-feature-size=64 --arch-mlp-bot="128-128-128-128" --arch-mlp-top="512-512-512-256-1" --max-ind-range=40000000
---data-generation=random --loss-function=bce --round-targets=True --learning-rate=1.0 --mini-batch-size=2048 --print-freq=2 --print-time --test-freq=2 --test-mini-batch-size=2048 --memory-map --use-gpu --num-batches=100 --dist-backend=nccl
-
-# for multiple nodes, user can add the related argument according to the launcher manual like:
---nnodes=2 --node_rank=0 --master_addr="192.168.1.1" --master_port=1234
-```
-
-
-Model checkpoint saving/loading
--------------------------------
-During training, the model can be saved using --save-model=<path/model.pt>
-
-The model is saved if there is an improvement in test accuracy (which is checked at --test-freq intervals).
-
-A previously saved model can be loaded using --load-model=<path/model.pt>
-
-Once loaded the model can be used to continue training, with the saved model being a checkpoint.
-Alternatively, the saved model can be used to evaluate only on the test data-set by specifying --inference-only option.
-
-
-Version
--------
-0.1 : Initial release of the DLRM code
-
-1.0 : DLRM with distributed training, cpu support for row-wise adagrad optimizer
-
-Requirements
-------------
-pytorch-nightly (*11/10/20*)
-
-scikit-learn
-
-numpy
-
-onnx (*optional*)
-
-pydot (*optional*)
-
-torchviz (*optional*)
-
-mpi (*optional for distributed backend*)
-
-
-License
--------
-This source code is licensed under the MIT license found in the
-LICENSE file in the root directory of this source tree.
diff --git a/analysis_model.py b/analysis_model.py
new file mode 100644
index 0000000..2fa6d91
--- /dev/null
+++ b/analysis_model.py
@@ -0,0 +1,187 @@
+from __future__ import absolute_import, division, print_function, unicode_literals
+
+import os, sys
+import matplotlib
+matplotlib.use("Agg")
+import matplotlib.pyplot
+import pandas as pd
+
+from model_compression.hyparameters import hyparams
+# For model compression
+import distiller
+from distiller.utils import *
+
+# miscellaneous
+import builtins
+import functools
+# import bisect
+# import shutil
+import time
+import json
+import math
+# data generation
+import dlrm_data_pytorch as dp
+
+# numpy
+import numpy as np
+
+# onnx
+# The onnx import causes deprecation warnings every time workers
+# are spawned during testing. So, we filter out those warnings.
+import warnings
+with warnings.catch_warnings():
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+import onnx
+
+# pytorch
+import torch
+import torch.nn as nn
+from torch.nn.parallel.parallel_apply import parallel_apply
+from torch.nn.parallel.replicate import replicate
+from torch.nn.parallel.scatter_gather import gather, scatter
+
+# For distributed run
+import extend_distributed as ext_dist
+
+try:
+    import intel_pytorch_extension as ipex
+    from intel_pytorch_extension import core
+except:
+    pass
+from lamb_bin import Lamb, log_lamb_rs
+
+# quotient-remainder trick
+from tricks.qr_embedding_bag import QREmbeddingBag
+# mixed-dimension trick
+from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
+
+import sklearn.metrics
+import mlperf_logger
+
+# from torchviz import make_dot
+# import torch.nn.functional as Functional
+# from torch.nn.parameter import Parameter
+
+from torch.optim.lr_scheduler import _LRScheduler
+
+exc = getattr(builtins, "IOError", "FileNotFoundError")
+
+from dlrm_s_pytorch_lamb_sparselamb_test import DLRM_Net
+
+def load_model(model_path, args):
+    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
+
+    if (args.data_generation == "dataset"):
+        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
+        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+        nbatches_test = len(test_ld)
+
+        ln_emb = train_data.counts
+        # enforce maximum limit on number of vectors per embedding
+        if args.max_ind_range > 0:
+            ln_emb = np.array(list(map(
+                lambda x: x if x < args.max_ind_range else args.max_ind_range,
+                ln_emb
+            )))
+        m_den = train_data.m_den
+        ln_bot[0] = m_den
+
+    else:
+        # input and target at random
+        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
+        m_den = ln_bot[0]
+        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+
+    ### parse command line arguments ###
+    m_spa = args.arch_sparse_feature_size
+    num_fea = ln_emb.size + 1  # num sparse + num dense features
+    m_den_out = ln_bot[ln_bot.size - 1]
+    if args.arch_interaction_op == "dot":
+        # approach 1: all
+        # num_int = num_fea * num_fea + m_den_out
+        # approach 2: unique
+        if args.arch_interaction_itself:
+            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
+        else:
+            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
+    elif args.arch_interaction_op == "cat":
+        num_int = num_fea * m_den_out
+    else:
+        sys.exit(
+            "ERROR: --arch-interaction-op="
+            + args.arch_interaction_op
+            + " is not supported"
+        )
+    arch_mlp_top_adjusted = str(num_int) + "-" + args.arch_mlp_top
+    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep="-")
+    ndevices = -1
+
+    dlrm = DLRM_Net(
+        m_spa = args.arch_sparse_feature_size,
+        ln_emb = ln_emb,
+        ln_bot = ln_bot,
+        ln_top = ln_top,
+        arch_interaction_op = args.arch_interaction_op,
+        arch_interaction_itself = args.arch_interaction_itself,
+        sigmoid_bot = -1,
+        sigmoid_top = ln_top.size - 2,
+        sync_dense_params = args.sync_dense_params,
+        loss_threshold = args.loss_threshold,
+        ndevices = ndevices,
+        qr_flag = args.qr_flag,
+        qr_operation = args.qr_operation,
+        qr_collisions = args.qr_collisions,
+        qr_threshold = args.qr_threshold,
+        md_flag = args.md_flag,
+        md_threshold = args.md_threshold,
+        sparse_dense_boundary = args.sparse_dense_boundary,
+        bf16 = args.bf16,
+        use_ipex = args.use_ipex
+    )
+
+    model_dict = torch.load(os.path.join(model_path,"dlrm_s_pytorch_"+str(ext_dist.dist.get_rank())+".pkl"))
+    dlrm.load_state_dict({k.replace('module.',''):v for k,v in model_dict["state_dict"].items()})
+
+    return dlrm
+    
+
+def view_elementwise_sparsity(model):
+    origin_model_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')),"model_compression/model/compress/AGP_Structure/test2/")
+    df_sparsity = distiller.weights_sparsity_summary(model=model, param_dims=[1,2,4,5])
+    df_sparsity.to_csv(os.path.join(origin_model_dir,"dlrm_s_pytorch_new_"+str(ext_dist.dist.get_rank())+".csv"))
+    print(df_sparsity[['Name', 'Shape', 'NNZ (dense)', 'NNZ (sparse)']])
+    return df_sparsity
+
+def view_layer_wise_sparsity(df_sparsity):
+    origin_model_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')),"model_compression/model/compress/AGP_Structure/test2/")  
+    matplotlib.rcParams.update({'font.size': 22})
+    spec_df_sparsity = df_sparsity[~df_sparsity["Name"].str.contains("emb")]
+    spec_df_sparsity.to_csv(os.path.join(origin_model_dir,"dlrm_s_pytorch_spec_"+str(ext_dist.dist.get_rank())+".csv"))
+    spec_df_sparsity_new = spec_df_sparsity[['NNZ (dense)', 'NNZ (sparse)']]
+    ax = spec_df_sparsity_new.iloc[0:-1].plot(kind='bar', figsize=[30,30], title="Weights footprint: Sparse vs. Dense\n(element-wise)")
+    ax.set_xticklabels(spec_df_sparsity.Name[:-1], rotation=90)
+    ax.figure.savefig(os.path.join(os.path.dirname(os.path.abspath('__file__')),"model_compression/model/compress/AGP_Structure/test2/layer_wise_sparsity_"+str(ext_dist.dist.get_rank())+".png"))
+
+def remove_layers(model):
+    #layers_to_remove = [param_name for param_name, param in model.named_parameters() if distiller.density(param) == 0]
+    layers_density = [(param_name , distiller.density(param)) for param_name, param in model.named_parameters()]
+    
+    print(layers_density)
+
+def main(args):
+    origin_model_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')),"model_compression/model/compress/AGP_Structure/test2/")
+    origin_dlrm = load_model(origin_model_dir, args).type(torch.float32)
+    #for name, params in origin_dlrm.state_dict().items():
+    #        print('{}:{}:{}'.format(name, params.size(), params.dtype))
+    #df_sparsity = view_elementwise_sparsity(origin_dlrm)
+    #view_layer_wise_sparsity(df_sparsity)
+    remove_layers(origin_dlrm)
+
+
+if __name__ == "__main__":
+    parser = hyparams.parser
+    args = parser.parse_args()
+    ext_dist.init_distributed(backend=args.dist_backend)
+    main(args)
+    
diff --git a/cython/cython_compile.py b/cython/cython_compile.py
deleted file mode 100644
index ffacf08..0000000
--- a/cython/cython_compile.py
+++ /dev/null
@@ -1,26 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-#
-# Description: compile .so from python code
-
-from __future__ import absolute_import, division, print_function, unicode_literals
-
-from setuptools import setup
-from Cython.Build import cythonize
-from distutils.extension import Extension
-
-ext_modules = [
-    Extension(
-        "data_utils_cython",
-        ["data_utils_cython.pyx"],
-        extra_compile_args=['-O3'],
-        extra_link_args=['-O3'],
-    )
-]
-
-setup(
-    name='data_utils_cython',
-    ext_modules=cythonize(ext_modules)
-)
diff --git a/cython/cython_criteo.py b/cython/cython_criteo.py
deleted file mode 100644
index 46a0b7d..0000000
--- a/cython/cython_criteo.py
+++ /dev/null
@@ -1,55 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-#
-# Description: run dataset pre-processing in standalone mode
-# WARNING: These steps are required to work with Cython
-# 1. Instal Cython
-# > sudo yum install Cython
-# 2. Please copy data_utils.py into data_utils_cython.pyx
-# 3. Compile the data_utils_cython.pyx to generate .so
-# (it's important to keep extension .pyx rather than .py
-#  to ensure the C/C++ .so no .py is loaded at import time)
-# > python cython_compile.py build_ext --inplace
-# This should create data_utils_cython.so, which can be loaded below with "import"
-# 4. Run standalone datatset preprocessing to generate .npz files
-# a. Kaggle
-# > python cython_criteo.py --data-set=kaggle --raw-data-file=./input/train.txt
-#   --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz
-# b. Terabyte
-# > python cython_criteo.py --max-ind-range=10000000 [--memory-map] --data-set=terabyte
-#   --raw-data-file=./input/day --processed-data-file=./input/terabyte_processed.npz
-
-from __future__ import absolute_import, division, print_function, unicode_literals
-
-import data_utils_cython as duc
-
-if __name__ == "__main__":
-    ### import packages ###
-    import argparse
-
-    ### parse arguments ###
-    parser = argparse.ArgumentParser(
-        description="Preprocess Criteo dataset"
-    )
-    # model related parameters
-    parser.add_argument("--max-ind-range", type=int, default=-1)
-    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
-    parser.add_argument("--data-randomize", type=str, default="total")  # or day or none
-    parser.add_argument("--memory-map", action="store_true", default=False)
-    parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
-    parser.add_argument("--raw-data-file", type=str, default="")
-    parser.add_argument("--processed-data-file", type=str, default="")
-    args = parser.parse_args()
-
-    duc.loadDataset(
-        args.data_set,
-        args.max_ind_range,
-        args.data_sub_sample_rate,
-        args.data_randomize,
-        "train",
-        args.raw_data_file,
-        args.processed_data_file,
-        args.memory_map
-    )
diff --git a/data_loader_terabyte.py b/data_loader_terabyte.py
index cf0db71..c1f8d50 100644
--- a/data_loader_terabyte.py
+++ b/data_loader_terabyte.py
@@ -14,6 +14,7 @@ import time
 import math
 from tqdm import tqdm
 import argparse
+import extend_distributed as ext_dist
 
 
 class DataLoader:
@@ -175,12 +176,10 @@ def _batch_generator(
 def _test():
     generator = _batch_generator(
         data_filename='day',
-        data_directory='./input',
+        data_directory='/input',
         days=range(23),
         split="train",
-        batch_size=2048,
-        drop_last=True,
-        max_ind_range=-1
+        batch_size=2048
     )
     t1 = time.time()
     for x_int, lS_o, x_cat, y in generator:
@@ -208,11 +207,30 @@ class CriteoBinDataset(Dataset):
 
         self.batch_size = batch_size
         self.max_ind_range = max_ind_range
-        self.bytes_per_entry = (bytes_per_feature * self.tot_fea * batch_size)
+        self.bytes_per_batch = (bytes_per_feature * self.tot_fea * batch_size)
+
+        data_file_size = os.path.getsize(data_file)
+        self.num_batches = math.ceil(data_file_size / self.bytes_per_batch)
+
+        bytes_per_sample = bytes_per_feature * self.tot_fea
+        self.num_samples = data_file_size // bytes_per_sample
+
+        if ext_dist.my_size > 1:
+            self.bytes_per_rank = self.bytes_per_batch // ext_dist.my_size
+        else:
+            self.bytes_per_rank = self.bytes_per_batch
+
+        if ext_dist.my_size > 1 and self.num_batches * self.bytes_per_batch > data_file_size:
+            last_batch = (data_file_size % self.bytes_per_batch) // bytes_per_sample
+            self.bytes_last_batch = last_batch // ext_dist.my_size * bytes_per_sample
+        else:
+            self.bytes_last_batch = self.bytes_per_rank
 
-        self.num_entries = math.ceil(os.path.getsize(data_file) / self.bytes_per_entry)
+        if self.bytes_last_batch == 0:
+            self.num_batches = self.num_batches - 1
+            self.bytes_last_batch = self.bytes_per_rank
 
-        print('data file:', data_file, 'number of batches:', self.num_entries)
+        print('data file:', data_file, 'number of batches:', self.num_batches)
         self.file = open(data_file, 'rb')
 
         with np.load(counts_file) as data:
@@ -222,11 +240,13 @@ class CriteoBinDataset(Dataset):
         self.m_den = 13
 
     def __len__(self):
-        return self.num_entries
+        return self.num_batches
 
     def __getitem__(self, idx):
-        self.file.seek(idx * self.bytes_per_entry, 0)
-        raw_data = self.file.read(self.bytes_per_entry)
+        my_rank = ext_dist.dist.get_rank() if ext_dist.my_size > 1 else 0
+        rank_size = self.bytes_last_batch if idx == (self.num_batches - 1) else self.bytes_per_rank 
+        self.file.seek(idx * self.bytes_per_batch + rank_size * my_rank, 0)
+        raw_data = self.file.read(rank_size)
         array = np.frombuffer(raw_data, dtype=np.int32)
         tensor = torch.from_numpy(array).view((-1, self.tot_fea))
 
@@ -236,9 +256,6 @@ class CriteoBinDataset(Dataset):
                                    max_ind_range=self.max_ind_range,
                                    flag_input_torch_tensor=True)
 
-    def __del__(self):
-        self.file.close()
-
 
 def numpy_to_binary(input_files, output_file_path, split='train'):
     """Convert the data to a binary format to be read with CriteoBinDataset."""
@@ -307,7 +324,7 @@ def _test_bin():
                         required=True)
     args = parser.parse_args()
 
-    _preprocess(args)
+    # _preprocess(args)
 
     binary_data_file = os.path.join(args.output_directory,
                                     '{}_data.bin'.format(args.split))
@@ -316,8 +333,7 @@ def _test_bin():
     dataset_binary = CriteoBinDataset(data_file=binary_data_file,
                                             counts_file=counts_file,
                                             batch_size=2048,)
-    from dlrm_data_pytorch import CriteoDataset 
-    from dlrm_data_pytorch import collate_wrapper_criteo_offset as collate_wrapper_criteo
+    from dlrm_data_pytorch import CriteoDataset, collate_wrapper_criteo
 
     binary_loader = torch.utils.data.DataLoader(
         dataset_binary,
@@ -365,4 +381,4 @@ def _test_bin():
 
 if __name__ == '__main__':
     _test()
-    _test_bin()
+    _test_bin
diff --git a/data_utils.py b/data_utils.py
index bf76dff..daeeb15 100644
--- a/data_utils.py
+++ b/data_utils.py
@@ -40,7 +40,6 @@ from __future__ import absolute_import, division, print_function, unicode_litera
 import sys
 # import os
 from os import path
-from multiprocessing import Process, Manager
 # import io
 # from io import StringIO
 # import collections as coll
@@ -109,7 +108,7 @@ def convertUStringToDistinctIntsUnique(mat, mat_uni, counts):
     return out, mat_uni, counts
 
 
-def processCriteoAdData(d_path, d_file, npzfile, i, convertDicts, pre_comp_counts):
+def processCriteoAdData(d_path, d_file, npzfile, split, convertDicts, pre_comp_counts):
     # Process Kaggle Display Advertising Challenge or Terabyte Dataset
     # by converting unicode strings in X_cat to integers and
     # converting negative integer values in X_int.
@@ -118,48 +117,49 @@ def processCriteoAdData(d_path, d_file, npzfile, i, convertDicts, pre_comp_count
     #
     # Inputs:
     #   d_path (str): path for {kaggle|terabyte}_day_i.npz files
-    #   i (int): splits in the dataset (typically 0 to 7 or 0 to 24)
+    #   split (int): total number of splits in the dataset (typically 7 or 24)
 
     # process data if not all files exist
-    filename_i = npzfile + "_{0}_processed.npz".format(i)
+    for i in range(split):
+        filename_i = npzfile + "_{0}_processed.npz".format(i)
 
-    if path.exists(filename_i):
-        print("Using existing " + filename_i, end="\n")
-    else:
-        print("Not existing " + filename_i)
-        with np.load(npzfile + "_{0}.npz".format(i)) as data:
-            # categorical features
-            '''
-            # Approach 1a: using empty dictionaries
-            X_cat, convertDicts, counts = convertUStringToDistinctIntsDict(
-                data["X_cat"], convertDicts, counts
-            )
-            '''
-            '''
-            # Approach 1b: using empty np.unique
-            X_cat, convertDicts, counts = convertUStringToDistinctIntsUnique(
-                data["X_cat"], convertDicts, counts
+        if path.exists(filename_i):
+            print("Using existing " + filename_i, end="\r")
+        else:
+            with np.load(npzfile + "_{0}.npz".format(i)) as data:
+                # categorical features
+                '''
+                # Approach 1a: using empty dictionaries
+                X_cat, convertDicts, counts = convertUStringToDistinctIntsDict(
+                    data["X_cat"], convertDicts, counts
+                )
+                '''
+                '''
+                # Approach 1b: using empty np.unique
+                X_cat, convertDicts, counts = convertUStringToDistinctIntsUnique(
+                    data["X_cat"], convertDicts, counts
+                )
+                '''
+                # Approach 2a: using pre-computed dictionaries
+                X_cat_t = np.zeros(data["X_cat_t"].shape)
+                for j in range(26):
+                    for k, x in enumerate(data["X_cat_t"][j, :]):
+                        X_cat_t[j, k] = convertDicts[j][x]
+                # continuous features
+                X_int = data["X_int"]
+                X_int[X_int < 0] = 0
+                # targets
+                y = data["y"]
+
+            np.savez_compressed(
+                filename_i,
+                # X_cat = X_cat,
+                X_cat=np.transpose(X_cat_t),  # transpose of the data
+                X_int=X_int,
+                y=y,
             )
-            '''
-            # Approach 2a: using pre-computed dictionaries
-            X_cat_t = np.zeros(data["X_cat_t"].shape)
-            for j in range(26):
-                for k, x in enumerate(data["X_cat_t"][j, :]):
-                    X_cat_t[j, k] = convertDicts[j][x]
-            # continuous features
-            X_int = data["X_int"]
-            X_int[X_int < 0] = 0
-            # targets
-            y = data["y"]
-
-        np.savez_compressed(
-            filename_i,
-            # X_cat = X_cat,
-            X_cat=np.transpose(X_cat_t),  # transpose of the data
-            X_int=X_int,
-            y=y,
-        )
-        print("Processed " + filename_i, end="\n")
+            print("Processed " + filename_i, end="\r")
+    print("")
     # sanity check (applicable only if counts have been pre-computed & are re-computed)
     # for j in range(26):
     #    if pre_comp_counts[j] != counts[j]:
@@ -882,8 +882,7 @@ def getCriteoAdData(
         data_split='train',
         randomize='total',
         criteo_kaggle=True,
-        memory_map=False,
-        dataset_multiprocessing=False,
+        memory_map=False
 ):
     # Passes through entire dataset and defines dictionaries for categorical
     # features and determines the number of total categories.
@@ -969,13 +968,7 @@ def getCriteoAdData(
             npzfile,
             split,
             num_data_in_split,
-            dataset_multiprocessing,
-            convertDictsDay=None,
-            resultDay=None
     ):
-        if dataset_multiprocessing:
-            convertDicts_day = [{} for _ in range(26)]
-
         with open(str(datfile)) as f:
             y = np.zeros(num_data_in_split, dtype="i4")  # 4 byte int
             X_int = np.zeros((num_data_in_split, 13), dtype="i4")  # 4 byte int
@@ -986,7 +979,6 @@ def getCriteoAdData(
                 rand_u = np.random.uniform(low=0.0, high=1.0, size=num_data_in_split)
 
             i = 0
-            percent = 0
             for k, line in enumerate(f):
                 # process a line (data point)
                 line = line.split('\t')
@@ -1012,41 +1004,22 @@ def getCriteoAdData(
                         list(map(lambda x: int(x, 16), line[14:])),
                         dtype=np.int32
                     )
-
                 # count uniques
-                if dataset_multiprocessing:
-                    for j in range(26):
-                        convertDicts_day[j][X_cat[i][j]] = 1
-                    # debug prints
-                    if float(i)/num_data_in_split*100 > percent+1:
-                        percent = int(float(i)/num_data_in_split*100)
-                        print(
-                            "Load %d/%d (%d%%) Split: %d  Label True: %d  Stored: %d"
-                            % (
-                                i,
-                                num_data_in_split,
-                                percent,
-                                split,
-                                target,
-                                y[i],
-                            ),
-                            end="\n",
-                        )
-                else:
-                    for j in range(26):
-                        convertDicts[j][X_cat[i][j]] = 1
-                    # debug prints
-                    print(
-                        "Load %d/%d  Split: %d  Label True: %d  Stored: %d"
-                        % (
-                            i,
-                            num_data_in_split,
-                            split,
-                            target,
-                            y[i],
-                        ),
-                        end="\r",
-                    )
+                for j in range(26):
+                    convertDicts[j][X_cat[i][j]] = 1
+
+                # debug prints
+                print(
+                    "Load %d/%d  Split: %d  Label True: %d  Stored: %d"
+                    % (
+                        i,
+                        num_data_in_split,
+                        split,
+                        target,
+                        y[i],
+                    ),
+                    end="\r",
+                )
                 i += 1
 
             # store num_data_in_split samples or extras at the end of file
@@ -1068,13 +1041,7 @@ def getCriteoAdData(
                     y=y[0:i],
                 )
                 print("\nSaved " + npzfile + "_{0}.npz!".format(split))
-
-        if dataset_multiprocessing:
-            resultDay[split] = i
-            convertDictsDay[split] = convertDicts_day
-            return
-        else:
-            return i
+        return i
 
     # create all splits (reuse existing files if possible)
     recreate_flag = False
@@ -1083,6 +1050,7 @@ def getCriteoAdData(
     # np.random.seed(123)
     # in this case there is a single split in each day
     for i in range(days):
+        datfile_i = npzfile + "_{0}".format(i)  # + ".gz"
         npzfile_i = npzfile + "_{0}.npz".format(i)
         npzfile_p = npzfile + "_{0}_processed.npz".format(i)
         if path.exists(npzfile_i):
@@ -1091,42 +1059,12 @@ def getCriteoAdData(
             print("Skip existing " + npzfile_p)
         else:
             recreate_flag = True
-
-    if recreate_flag:
-        if dataset_multiprocessing:
-            resultDay = Manager().dict()
-            convertDictsDay = Manager().dict()
-            processes = [Process(target=process_one_file,
-                                 name="process_one_file:%i" % i,
-                                 args=(npzfile + "_{0}".format(i),
-                                       npzfile,
-                                       i,
-                                       total_per_file[i],
-                                       dataset_multiprocessing,
-                                       convertDictsDay,
-                                       resultDay,
-                                       )
-                                 ) for i in range(0, days)]
-            for process in processes:
-                process.start()
-            for process in processes:
-                process.join()
-            for day in range(days):
-                total_per_file[day] = resultDay[day]
-                print("Constructing convertDicts Split: {}".format(day))
-                convertDicts_tmp = convertDictsDay[day]
-                for i in range(26):
-                    for j in convertDicts_tmp[i]:
-                        convertDicts[i][j] = 1
-        else:
-            for i in range(days):
-                total_per_file[i] = process_one_file(
-                    npzfile + "_{0}".format(i),
-                    npzfile,
-                    i,
-                    total_per_file[i],
-                    dataset_multiprocessing,
-                )
+            total_per_file[i] = process_one_file(
+                datfile_i,
+                npzfile,
+                i,
+                total_per_file[i],
+            )
 
     # report and save total into a file
     total_count = np.sum(total_per_file)
@@ -1165,26 +1103,7 @@ def getCriteoAdData(
             counts = data["counts"]
 
     # process all splits
-    if dataset_multiprocessing:
-        processes = [Process(target=processCriteoAdData,
-                           name="processCriteoAdData:%i" % i,
-                           args=(d_path,
-                                 d_file,
-                                 npzfile,
-                                 i,
-                                 convertDicts,
-                                 counts,
-                                 )
-                           ) for i in range(0, days)]
-        for process in processes:
-            process.start()
-        for process in processes:
-            process.join()
-
-    else:
-        for i in range(days):
-            processCriteoAdData(d_path, d_file, npzfile, i, convertDicts, counts)
-
+    processCriteoAdData(d_path, d_file, npzfile, days, convertDicts, counts)
     o_file = concatCriteoAdData(
         d_path,
         d_file,
@@ -1226,7 +1145,7 @@ def loadDataset(
     lstr = raw_path.split("/")
     d_path = "/".join(lstr[0:-1]) + "/"
     d_file = lstr[-1].split(".")[0] if dataset == "kaggle" else lstr[-1]
-    npzfile = (d_file + "_day") if dataset == "kaggle" else d_file
+    npzfile = d_path + ((d_file + "_day") if dataset == "kaggle" else d_file)
     # trafile = d_path + ((d_file + "_fea") if dataset == "kaggle" else "fea")
 
     # check if pre-processed data is available
diff --git a/dlrm_data_caffe2.py b/dlrm_data_caffe2.py
index 0bda2ac..12d8d9f 100644
--- a/dlrm_data_caffe2.py
+++ b/dlrm_data_caffe2.py
@@ -15,25 +15,19 @@
 #    ii) Criteo Terabyte Dataset
 #    https://labs.criteo.com/2013/12/download-terabyte-click-logs
 
-
 from __future__ import absolute_import, division, print_function, unicode_literals
 
-import bisect
-import collections
-
 # others
 # from os import path
 import sys
+import bisect
+import collections
 
 import data_utils
 
 # numpy
 import numpy as np
-
-# pytorch
-import torch
 from numpy import random as ra
-from torch.utils.data import Dataset
 
 
 # Kaggle Display Advertising Challenge Dataset
@@ -43,187 +37,18 @@ from torch.utils.data import Dataset
 #            'day': randomizes each day's data (only works if split = True)
 #            'total': randomizes total dataset
 # split (bool) : to split into train, test, validation data-sets
-
-
-class CriteoDatasetWMemoryMap(Dataset):
-    def __init__(
-        self,
+def read_dataset(
         dataset,
         max_ind_range,
         sub_sample_rate,
+        mini_batch_size,
+        num_batches,
         randomize,
         split="train",
-        raw_path="",
-        pro_data="",
-    ):
-        # dataset
-        # tar_fea = 1   # single target
-        den_fea = 13  # 13 dense  features
-        # spa_fea = 26  # 26 sparse features
-        # tad_fea = tar_fea + den_fea
-        # tot_fea = tad_fea + spa_fea
-        if dataset == "kaggle":
-            days = 7
-        elif dataset == "terabyte":
-            days = 24
-        else:
-            raise (ValueError("Data set option is not supported"))
-        self.max_ind_range = max_ind_range
-
-        # split the datafile into path and filename
-        lstr = raw_path.split("/")
-        self.d_path = "/".join(lstr[0:-1]) + "/"
-        self.d_file = lstr[-1].split(".")[0] if dataset == "kaggle" else lstr[-1]
-        self.npzfile = self.d_path + (
-            (self.d_file + "_day") if dataset == "kaggle" else self.d_file
-        )
-        self.trafile = self.d_path + (
-            (self.d_file + "_fea") if dataset == "kaggle" else "fea"
-        )
-
-        # get a number of samples per day
-        total_file = self.d_path + self.d_file + "_day_count.npz"
-        with np.load(total_file) as data:
-            total_per_file = data["total_per_file"]
-        # compute offsets per file
-        self.offset_per_file = np.array([0] + list(total_per_file))
-        for i in range(days):
-            self.offset_per_file[i + 1] += self.offset_per_file[i]
-        # print(self.offset_per_file)
-
-        # setup data
-        self.split = split
-        if split == "none" or split == "train":
-            self.day = 0
-            self.max_day_range = days if split == "none" else days - 1
-        elif split == "test" or split == "val":
-            self.day = days - 1
-            num_samples = self.offset_per_file[days] - self.offset_per_file[days - 1]
-            self.test_size = int(np.ceil(num_samples / 2.0))
-            self.val_size = num_samples - self.test_size
-        else:
-            sys.exit("ERROR: dataset split is neither none, nor train or test.")
-
-        # load unique counts
-        with np.load(self.d_path + self.d_file + "_fea_count.npz") as data:
-            self.counts = data["counts"]
-        self.m_den = den_fea  # X_int.shape[1]
-        self.n_emb = len(self.counts)
-        print("Sparse features= %d, Dense features= %d" % (self.n_emb, self.m_den))
-
-        # Load the test data
-        # Only a single day is used for testing
-        if self.split == "test" or self.split == "val":
-            # only a single day is used for testing
-            fi = self.npzfile + "_{0}_reordered.npz".format(self.day)
-            with np.load(fi) as data:
-                self.X_int = data["X_int"]  # continuous  feature
-                self.X_cat = data["X_cat"]  # categorical feature
-                self.y = data["y"]  # target
-
-    def __getitem__(self, index):
-
-        if isinstance(index, slice):
-            return [
-                self[idx]
-                for idx in range(
-                    index.start or 0, index.stop or len(self), index.step or 1
-                )
-            ]
-        if self.split == "none" or self.split == "train":
-            # check if need to swicth to next day and load data
-            if index == self.offset_per_file[self.day]:
-                # print("day_boundary switch", index)
-                self.day_boundary = self.offset_per_file[self.day]
-                fi = self.npzfile + "_{0}_reordered.npz".format(self.day)
-                # print('Loading file: ', fi)
-                with np.load(fi) as data:
-                    self.X_int = data["X_int"]  # continuous  feature
-                    self.X_cat = data["X_cat"]  # categorical feature
-                    self.y = data["y"]  # target
-                self.day = (self.day + 1) % self.max_day_range
-
-            i = index - self.day_boundary
-        elif self.split == "test" or self.split == "val":
-            # only a single day is used for testing
-            i = index + (0 if self.split == "test" else self.test_size)
-        else:
-            sys.exit("ERROR: dataset split is neither none, nor train or test.")
-
-        if self.max_ind_range > 0:
-            return self.X_int[i], self.X_cat[i] % self.max_ind_range, self.y[i]
-        else:
-            return self.X_int[i], self.X_cat[i], self.y[i]
-
-    def _default_preprocess(self, X_int, X_cat, y):
-        X_int = torch.log(torch.tensor(X_int, dtype=torch.float) + 1)
-        if self.max_ind_range > 0:
-            X_cat = torch.tensor(X_cat % self.max_ind_range, dtype=torch.long)
-        else:
-            X_cat = torch.tensor(X_cat, dtype=torch.long)
-        y = torch.tensor(y.astype(np.float32))
-
-        return X_int, X_cat, y
-
-    def __len__(self):
-        if self.split == "none":
-            return self.offset_per_file[-1]
-        elif self.split == "train":
-            return self.offset_per_file[-2]
-        elif self.split == "test":
-            return self.test_size
-        elif self.split == "val":
-            return self.val_size
-        else:
-            sys.exit("ERROR: dataset split is neither none, nor train nor test.")
-
-
-def collate_wrapper_criteo(list_of_tuples):
-    # where each tuple is (X_int, X_cat, y)
-    transposed_data = list(zip(*list_of_tuples))
-    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
-    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)
-    T = torch.tensor(transposed_data[2], dtype=torch.float32).view(-1, 1)
-
-    batchSize = X_cat.shape[0]
-    featureCnt = X_cat.shape[1]
-
-    lS_i = [X_cat[:, i] for i in range(featureCnt)]
-    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
-
-    return X_int, torch.stack(lS_o), torch.stack(lS_i), T
-
-
-# Conversion from offset to length
-def offset_to_length_convertor(lS_o, lS_i):
-    def diff(tensor):
-        return tensor[1:] - tensor[:-1]
-
-    return torch.stack(
-        [
-            diff(torch.cat((S_o, torch.tensor(lS_i[ind].shape))).int())
-            for ind, S_o in enumerate(lS_o)
-        ]
-    )
-
-
-def unpack_batch(b, data_gen, data_set):
-    return b[0], b[1], b[2], b[3], torch.ones(b[3].size())
-
-
-def read_dataset(
-    dataset,
-    max_ind_range,
-    sub_sample_rate,
-    mini_batch_size,
-    num_batches,
-    randomize,
-    split="train",
-    raw_data="",
-    processed_data="",
-    memory_map=False,
-    inference_only=False,
-    test_mini_batch_size=1,
+        raw_data="",
+        processed_data="",
+        memory_map=False,
+        inference_only=False,
 ):
     # split the datafile into path and filename
     lstr = raw_data.split("/")
@@ -236,14 +61,8 @@ def read_dataset(
     print("Loading %s dataset..." % dataset)
     nbatches = 0
     file, days = data_utils.loadDataset(
-        dataset,
-        max_ind_range,
-        sub_sample_rate,
-        randomize,
-        split,
-        raw_data,
-        processed_data,
-        memory_map,
+        dataset, max_ind_range, sub_sample_rate, randomize,
+        split, raw_data, processed_data, memory_map
     )
 
     if memory_map:
@@ -251,48 +70,7 @@ def read_dataset(
         # e.g. day_<number>_reordered.npz, what remains is simply to read and feed
         # the data from each file, going in the order of days file-by-file, to the
         # model during training.
-        train_data = CriteoDatasetWMemoryMap(
-            dataset,
-            max_ind_range,
-            sub_sample_rate,
-            randomize,
-            "train",
-            raw_data,
-            processed_data,
-        )
-
-        test_data = CriteoDatasetWMemoryMap(
-            dataset,
-            max_ind_range,
-            sub_sample_rate,
-            randomize,
-            "test",
-            raw_data,
-            processed_data,
-        )
-
-        train_loader = torch.utils.data.DataLoader(
-            train_data,
-            batch_size=mini_batch_size,
-            shuffle=False,
-            num_workers=0,
-            collate_fn=collate_wrapper_criteo,
-            pin_memory=False,
-            drop_last=False,  # True
-        )
-
-        test_loader = torch.utils.data.DataLoader(
-            test_data,
-            batch_size=test_mini_batch_size,
-            shuffle=False,
-            num_workers=0,
-            collate_fn=collate_wrapper_criteo,
-            pin_memory=False,
-            drop_last=False,  # True
-        )
-
-        return train_data, train_loader, test_data, test_loader
-
+        sys.exit("ERROR: --memory-map option is not supported for Caffe2 version.")
     else:
         # load and preprocess data
         with np.load(file) as data:
@@ -307,18 +85,9 @@ def read_dataset(
             total_per_file = data["total_per_file"]
 
         # transform
-        (
-            X_cat_train,
-            X_int_train,
-            y_train,
-            X_cat_val,
-            X_int_val,
-            y_val,
-            X_cat_test,
-            X_int_test,
-            y_test,
-        ) = data_utils.transformCriteoAdData(
-            X_cat, X_int, y, days, split, randomize, total_per_file
+        (X_cat_train, X_int_train, y_train, X_cat_val, X_int_val, y_val,
+         X_cat_test, X_int_test, y_test) = data_utils.transformCriteoAdData(
+             X_cat, X_int, y, days, split, randomize, total_per_file
         )
         ln_emb = counts
         m_den = X_int_train.shape[1]
@@ -355,10 +124,14 @@ def read_dataset(
                 n = min(mini_batch_size, data_size - (j * mini_batch_size))
                 # dense feature
                 idx_start = j * mini_batch_size
-                lX.append((X_int[idx_start : (idx_start + n)]).astype(np.float32))
+                lX.append(
+                    (X_int[idx_start : (idx_start + n)]).astype(np.float32)
+                )
                 # Targets - outputs
                 lT.append(
-                    (y[idx_start : idx_start + n]).reshape(-1, 1).astype(np.int32)
+                    (y[idx_start : idx_start + n])
+                    .reshape(-1, 1)
+                    .astype(np.int32)
                 )
                 # sparse feature (sparse indices)
                 lS_emb_indices = []
@@ -370,7 +143,8 @@ def read_dataset(
                         # num of sparse indices to be used per embedding, e.g. for
                         # store lengths and indices
                         lS_batch_indices += (
-                            (X_cat[idx_start + _b][size].reshape(-1)).astype(np.int32)
+                            (X_cat[idx_start + _b][size].reshape(-1))
+                            .astype(np.int32)
                         ).tolist()
                     lS_emb_indices.append(lS_batch_indices)
                 lS_indices.append(lS_emb_indices)
@@ -391,7 +165,7 @@ def read_dataset(
         (nbatches_t, lX_t, lS_lengths_t, lS_indices_t, lT_t) = assemble_samples(
             X_cat_test, X_int_test, y_test, max_ind_range, "Testing data"
         )
-    # end if memory_map
+    #end if memory_map
 
     return (
         nbatches,
@@ -441,7 +215,11 @@ def generate_random_data(
         # generate a batch of dense and sparse features
         if data_generation == "random":
             (Xt, lS_emb_lengths, lS_emb_indices) = generate_uniform_input_batch(
-                m_den, ln_emb, n, num_indices_per_lookup, num_indices_per_lookup_fixed
+                m_den,
+                ln_emb,
+                n,
+                num_indices_per_lookup,
+                num_indices_per_lookup_fixed
             )
         elif data_generation == "synthetic":
             (Xt, lS_emb_lengths, lS_emb_indices) = generate_synthetic_input_batch(
@@ -451,7 +229,7 @@ def generate_random_data(
                 num_indices_per_lookup,
                 num_indices_per_lookup_fixed,
                 trace_file,
-                enable_padding,
+                enable_padding
             )
         else:
             sys.exit(
diff --git a/dlrm_data_pytorch.py b/dlrm_data_pytorch.py
index 852c577..15dac1d 100644
--- a/dlrm_data_pytorch.py
+++ b/dlrm_data_pytorch.py
@@ -20,7 +20,6 @@ from __future__ import absolute_import, division, print_function, unicode_litera
 
 # others
 from os import path
-import sys
 import bisect
 import collections
 
@@ -29,7 +28,6 @@ import data_utils
 # numpy
 import numpy as np
 from numpy import random as ra
-from collections import deque
 
 
 # pytorch
@@ -39,6 +37,13 @@ from torch.utils.data import Dataset, RandomSampler
 import data_loader_terabyte
 import mlperf_logger
 
+from prefetch_generator import BackgroundGenerator
+
+class DataLoaderX(torch.utils.data.DataLoader):
+
+    def __iter__(self):
+        return BackgroundGenerator(super().__iter__())
+
 
 # Kaggle Display Advertising Challenge Dataset
 # dataset (str): name of dataset (Kaggle or Terabyte)
@@ -58,8 +63,7 @@ class CriteoDataset(Dataset):
             split="train",
             raw_path="",
             pro_data="",
-            memory_map=False,
-            dataset_multiprocessing=False,
+            memory_map=False
     ):
         # dataset
         # tar_fea = 1   # single target
@@ -116,8 +120,7 @@ class CriteoDataset(Dataset):
                 split,
                 randomize,
                 dataset == "kaggle",
-                memory_map,
-                dataset_multiprocessing,
+                memory_map
             )
 
         # get a number of samples per day
@@ -325,7 +328,7 @@ class CriteoDataset(Dataset):
             return len(self.y)
 
 
-def collate_wrapper_criteo_offset(list_of_tuples):
+def collate_wrapper_criteo(list_of_tuples):
     # where each tuple is (X_int, X_cat, y)
     transposed_data = list(zip(*list_of_tuples))
     X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
@@ -350,8 +353,7 @@ def ensure_dataset_preprocessed(args, d_path):
         "train",
         args.raw_data_file,
         args.processed_data_file,
-        args.memory_map,
-        args.dataset_multiprocessing
+        args.memory_map
     )
 
     _ = CriteoDataset(
@@ -362,8 +364,7 @@ def ensure_dataset_preprocessed(args, d_path):
         "test",
         args.raw_data_file,
         args.processed_data_file,
-        args.memory_map,
-        args.dataset_multiprocessing
+        args.memory_map
     )
 
     for split in ['train', 'val', 'test']:
@@ -383,56 +384,15 @@ def ensure_dataset_preprocessed(args, d_path):
                                              split=split)
 
 
-# Conversion from offset to length
-def offset_to_length_converter(lS_o, lS_i):
-    def diff(tensor):
-        return tensor[1:] - tensor[:-1]
-
-    return torch.stack(
-        [
-            diff(torch.cat((S_o, torch.tensor(lS_i[ind].shape))).int())
-            for ind, S_o in enumerate(lS_o)
-        ]
-    )
-
-
-def collate_wrapper_criteo_length(list_of_tuples):
-    # where each tuple is (X_int, X_cat, y)
-    transposed_data = list(zip(*list_of_tuples))
-    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
-    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)
-    T = torch.tensor(transposed_data[2], dtype=torch.float32).view(-1, 1)
+def make_criteo_data_and_loaders(args):
 
-    batchSize = X_cat.shape[0]
-    featureCnt = X_cat.shape[1]
-
-    lS_i = torch.stack([X_cat[:, i] for i in range(featureCnt)])
-    lS_o = torch.stack(
-        [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
-    )
-
-    lS_l = offset_to_length_converter(lS_o, lS_i)
-
-    return X_int, lS_l, lS_i, T
-
-
-def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
     if args.mlperf_logging and args.memory_map and args.data_set == "terabyte":
-        # more efficient for larger batches
-        data_directory = path.dirname(args.raw_data_file)
+
 
         if args.mlperf_bin_loader:
-            lstr = args.processed_data_file.split("/")
-            d_path = "/".join(lstr[0:-1]) + "/" + lstr[-1].split(".")[0]
-            train_file = d_path + "_train.bin"
-            test_file = d_path + "_test.bin"
-            # val_file = d_path + "_val.bin"
-            counts_file = args.raw_data_file + '_fea_count.npz'
-
-            if any(not path.exists(p) for p in [train_file,
-                                                test_file,
-                                                counts_file]):
-                ensure_dataset_preprocessed(args, d_path)
+            train_file = args.train_data_path
+            test_file = args.eval_data_path
+            counts_file = args.day_feature_count
 
             train_data = data_loader_terabyte.CriteoBinDataset(
                 data_file=train_file,
@@ -444,7 +404,7 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
             mlperf_logger.log_event(key=mlperf_logger.constants.TRAIN_SAMPLES,
                                     value=train_data.num_samples)
 
-            train_loader = torch.utils.data.DataLoader(
+            train_loader = DataLoaderX(
                 train_data,
                 batch_size=None,
                 batch_sampler=None,
@@ -466,7 +426,7 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
             mlperf_logger.log_event(key=mlperf_logger.constants.EVAL_SAMPLES,
                                     value=test_data.num_samples)
 
-            test_loader = torch.utils.data.DataLoader(
+            test_loader = DataLoaderX(
                 test_data,
                 batch_size=None,
                 batch_sampler=None,
@@ -487,8 +447,7 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
                 "train",
                 args.raw_data_file,
                 args.processed_data_file,
-                args.memory_map,
-                args.dataset_multiprocessing
+                args.memory_map
             )
 
             test_data = CriteoDataset(
@@ -499,8 +458,7 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
                 "test",
                 args.raw_data_file,
                 args.processed_data_file,
-                args.memory_map,
-                args.dataset_multiprocessing
+                args.memory_map
             )
 
             train_loader = data_loader_terabyte.DataLoader(
@@ -529,8 +487,7 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
             "train",
             args.raw_data_file,
             args.processed_data_file,
-            args.memory_map,
-            args.dataset_multiprocessing,
+            args.memory_map
         )
 
         test_data = CriteoDataset(
@@ -541,14 +498,9 @@ def make_criteo_data_and_loaders(args, offset_to_length_converter=False):
             "test",
             args.raw_data_file,
             args.processed_data_file,
-            args.memory_map,
-            args.dataset_multiprocessing,
+            args.memory_map
         )
 
-        collate_wrapper_criteo = collate_wrapper_criteo_offset
-        if offset_to_length_converter:
-            collate_wrapper_criteo = collate_wrapper_criteo_length
-
         train_loader = torch.utils.data.DataLoader(
             train_data,
             batch_size=args.mini_batch_size,
@@ -590,11 +542,6 @@ class RandomDataset(Dataset):
             trace_file="",
             enable_padding=False,
             reset_seed_on_access=False,
-            rand_data_dist="uniform",
-            rand_data_min=1,
-            rand_data_max=1,
-            rand_data_mu=-1,
-            rand_data_sigma=1,
             rand_seed=0
     ):
         # compute batch size
@@ -619,11 +566,6 @@ class RandomDataset(Dataset):
         self.enable_padding = enable_padding
         self.reset_seed_on_access = reset_seed_on_access
         self.rand_seed = rand_seed
-        self.rand_data_dist = rand_data_dist
-        self.rand_data_min = rand_data_min
-        self.rand_data_max = rand_data_max
-        self.rand_data_mu = rand_data_mu
-        self.rand_data_sigma = rand_data_sigma
 
     def reset_numpy_seed(self, numpy_rand_seed):
         np.random.seed(numpy_rand_seed)
@@ -648,17 +590,12 @@ class RandomDataset(Dataset):
 
         # generate a batch of dense and sparse features
         if self.data_generation == "random":
-            (X, lS_o, lS_i) = generate_dist_input_batch(
+            (X, lS_o, lS_i) = generate_uniform_input_batch(
                 self.m_den,
                 self.ln_emb,
                 n,
                 self.num_indices_per_lookup,
-                self.num_indices_per_lookup_fixed,
-                rand_data_dist=self.rand_data_dist,
-                rand_data_min=self.rand_data_min,
-                rand_data_max=self.rand_data_max,
-                rand_data_mu=self.rand_data_mu,
-                rand_data_sigma=self.rand_data_sigma,
+                self.num_indices_per_lookup_fixed
             )
         elif self.data_generation == "synthetic":
             (X, lS_o, lS_i) = generate_synthetic_input_batch(
@@ -686,7 +623,7 @@ class RandomDataset(Dataset):
         return self.num_batches
 
 
-def collate_wrapper_random_offset(list_of_tuples):
+def collate_wrapper_random(list_of_tuples):
     # where each tuple is (X, lS_o, lS_i, T)
     (X, lS_o, lS_i, T) = list_of_tuples[0]
     return (X,
@@ -695,18 +632,7 @@ def collate_wrapper_random_offset(list_of_tuples):
             T)
 
 
-def collate_wrapper_random_length(list_of_tuples):
-    # where each tuple is (X, lS_o, lS_i, T)
-    (X, lS_o, lS_i, T) = list_of_tuples[0]
-    return (X,
-            offset_to_length_converter(torch.stack(lS_o), lS_i),
-            lS_i,
-            T)
-
-
-def make_random_data_and_loader(args, ln_emb, m_den,
-    offset_to_length_converter=False,
-):
+def make_random_data_and_loader(args, ln_emb, m_den):
 
     train_data = RandomDataset(
         m_den,
@@ -722,40 +648,8 @@ def make_random_data_and_loader(args, ln_emb, m_den,
         args.data_trace_file,
         args.data_trace_enable_padding,
         reset_seed_on_access=True,
-        rand_data_dist=args.rand_data_dist,
-        rand_data_min=args.rand_data_min,
-        rand_data_max=args.rand_data_max,
-        rand_data_mu=args.rand_data_mu,
-        rand_data_sigma=args.rand_data_sigma,
         rand_seed=args.numpy_rand_seed
     )  # WARNING: generates a batch of lookups at once
-
-    test_data = RandomDataset(
-        m_den,
-        ln_emb,
-        args.data_size,
-        args.num_batches,
-        args.mini_batch_size,
-        args.num_indices_per_lookup,
-        args.num_indices_per_lookup_fixed,
-        1,  # num_targets
-        args.round_targets,
-        args.data_generation,
-        args.data_trace_file,
-        args.data_trace_enable_padding,
-        reset_seed_on_access=True,
-        rand_data_dist=args.rand_data_dist,
-        rand_data_min=args.rand_data_min,
-        rand_data_max=args.rand_data_max,
-        rand_data_mu=args.rand_data_mu,
-        rand_data_sigma=args.rand_data_sigma,
-        rand_seed=args.numpy_rand_seed
-    )
-
-    collate_wrapper_random = collate_wrapper_random_offset
-    if offset_to_length_converter:
-        collate_wrapper_random = collate_wrapper_random_length
-
     train_loader = torch.utils.data.DataLoader(
         train_data,
         batch_size=1,
@@ -765,17 +659,7 @@ def make_random_data_and_loader(args, ln_emb, m_den,
         pin_memory=False,
         drop_last=False,  # True
     )
-
-    test_loader = torch.utils.data.DataLoader(
-        test_data,
-        batch_size=1,
-        shuffle=False,
-        num_workers=args.num_workers,
-        collate_fn=collate_wrapper_random,
-        pin_memory=False,
-        drop_last=False,  # True
-    )
-    return train_data, train_loader, test_data, test_loader
+    return train_data, train_loader
 
 
 def generate_random_data(
@@ -791,7 +675,6 @@ def generate_random_data(
     data_generation="random",
     trace_file="",
     enable_padding=False,
-    length=False, # length for caffe2 version (except dlrm_s_caffe2)
 ):
     nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))
     if num_batches != 0:
@@ -815,8 +698,7 @@ def generate_random_data(
                 ln_emb,
                 n,
                 num_indices_per_lookup,
-                num_indices_per_lookup_fixed,
-                length,
+                num_indices_per_lookup_fixed
             )
         elif data_generation == "synthetic":
             (Xt, lS_emb_offsets, lS_emb_indices) = generate_synthetic_input_batch(
@@ -862,7 +744,6 @@ def generate_uniform_input_batch(
     n,
     num_indices_per_lookup,
     num_indices_per_lookup_fixed,
-    length,
 ):
     # dense feature
     Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))
@@ -890,71 +771,6 @@ def generate_uniform_input_batch(
             r = ra.random(sparse_group_size)
             sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
             # reset sparse_group_size in case some index duplicates were removed
-            sparse_group_size = np.int32(sparse_group.size)
-            # store lengths and indices
-            if length: # for caffe2 version
-                lS_batch_offsets += [sparse_group_size]
-            else:
-                lS_batch_offsets += [offset]
-            lS_batch_indices += sparse_group.tolist()
-            # update offset for next iteration
-            offset += sparse_group_size
-        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
-        lS_emb_indices.append(torch.tensor(lS_batch_indices))
-
-    return (Xt, lS_emb_offsets, lS_emb_indices)
-
-
-# random data from uniform or gaussian ditribution (input data)
-def generate_dist_input_batch(
-    m_den,
-    ln_emb,
-    n,
-    num_indices_per_lookup,
-    num_indices_per_lookup_fixed,
-    rand_data_dist,
-    rand_data_min,
-    rand_data_max,
-    rand_data_mu,
-    rand_data_sigma,
-):
-    # dense feature
-    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))
-
-    # sparse feature (sparse indices)
-    lS_emb_offsets = []
-    lS_emb_indices = []
-    # for each embedding generate a list of n lookups,
-    # where each lookup is composed of multiple sparse indices
-    for size in ln_emb:
-        lS_batch_offsets = []
-        lS_batch_indices = []
-        offset = 0
-        for _ in range(n):
-            # num of sparse indices to be used per embedding (between
-            if num_indices_per_lookup_fixed:
-                sparse_group_size = np.int64(num_indices_per_lookup)
-            else:
-                # random between [1,num_indices_per_lookup])
-                r = ra.random(1)
-                sparse_group_size = np.int64(
-                    np.round(max([1.0], r * min(size, num_indices_per_lookup)))
-                )
-            # sparse indices to be used per embedding
-            if rand_data_dist == "gaussian":
-                if rand_data_mu == -1:
-                    rand_data_mu = (rand_data_max + rand_data_min) / 2.0
-                r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
-                sparse_group = np.clip(r, rand_data_min, rand_data_max)
-                sparse_group = np.unique(sparse_group).astype(np.int64)
-            elif rand_data_dist == "uniform":
-                r = ra.random(sparse_group_size)
-                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
-            else:
-                raise(rand_data_dist, "distribution is not supported. \
-                     please select uniform or gaussian")
-
-            # reset sparse_group_size in case some index duplicates were removed
             sparse_group_size = np.int64(sparse_group.size)
             # store lengths and indices
             lS_batch_offsets += [offset]
@@ -1069,22 +885,21 @@ def trace_generate_lru(
     max_sd = list_sd[-1]
     l = len(line_accesses)
     i = 0
-    ztrace = deque()
+    ztrace = []
     for _ in range(out_trace_len):
         sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)
         mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0
 
         # generate memory reference
         if sd == 0:  # new reference #
-            line_ref = line_accesses[0]
-            del line_accesses[0]
+            line_ref = line_accesses.pop(0)
             line_accesses.append(line_ref)
             mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)
             i += 1
         else:  # existing reference #
             line_ref = line_accesses[l - sd]
             mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)
-            del line_accesses[l - sd]
+            line_accesses.pop(l - sd)
             line_accesses.append(line_ref)
         # save generated memory reference
         ztrace.append(mem_ref)
@@ -1120,9 +935,9 @@ def trace_profile(trace, enable_padding=False):
     # number of elements in the array (assuming 1D)
     # n = trace.size
 
-    rstack = deque()  # S
-    stack_distances = deque()  # SDS
-    line_accesses = deque()  # L
+    rstack = []  # S
+    stack_distances = []  # SDS
+    line_accesses = []  # L
     for x in trace:
         r = np.uint64(x / cache_line_size)
         l = len(rstack)
@@ -1134,17 +949,17 @@ def trace_profile(trace, enable_padding=False):
             #          consecutive accesses (e.g. r, r) as 0 rather than 1.
             sd = l - i  # - 1
             # push r to the end of stack_distances
-            stack_distances.appendleft(sd)
+            stack_distances.insert(0, sd)
             # remove r from its position and insert to the top of stack
-            del rstack[i]  # rstack.remove(r)
-            rstack.append(r)
+            rstack.pop(i)  # rstack.remove(r)
+            rstack.insert(l - 1, r)
         except ValueError:  # not found #
             sd = 0  # -1
             # push r to the end of stack_distances/line_accesses
-            stack_distances.appendleft(sd)
-            line_accesses.appendleft(r)
+            stack_distances.insert(0, sd)
+            line_accesses.insert(0, r)
             # push r to the top of stack
-            rstack.append(r)
+            rstack.insert(l, r)
 
     if enable_padding:
         # WARNING: notice that as the ratio between the number of samples (l)
@@ -1176,7 +991,7 @@ def read_trace_from_file(file_path):
                 trace = list(map(lambda x: np.uint64(x), line.split(", ")))
             return trace
     except Exception:
-        print(f"ERROR: trace file '{file_path}' is not available.")
+        print("ERROR: no input trace file has been provided")
 
 
 def write_trace_to_file(file_path, trace):
@@ -1186,7 +1001,7 @@ def write_trace_to_file(file_path, trace):
                 np.array(trace).astype(np.uint64).tofile(f)
         else:
             with open(file_path, "w+") as f:
-                s = str(list(trace))
+                s = str(trace)
                 f.write(s[1 : len(s) - 1])
     except Exception:
         print("ERROR: no output trace file has been provided")
@@ -1197,7 +1012,7 @@ def read_dist_from_file(file_path):
         with open(file_path, "r") as f:
             lines = f.read().splitlines()
     except Exception:
-        print("{file_path} Wrong file or file path")
+        print("Wrong file or file path")
     # read unique accesses
     unique_accesses = [int(el) for el in lines[0].split(", ")]
     # read cumulative distribution (elements are passed as two separate lists)
@@ -1211,19 +1026,20 @@ def write_dist_to_file(file_path, unique_accesses, list_sd, cumm_sd):
     try:
         with open(file_path, "w") as f:
             # unique_acesses
-            s = str(list(unique_accesses))
+            s = str(unique_accesses)
             f.write(s[1 : len(s) - 1] + "\n")
             # list_sd
             s = str(list_sd)
             f.write(s[1 : len(s) - 1] + "\n")
             # cumm_sd
-            s = str(list(cumm_sd))
+            s = str(cumm_sd)
             f.write(s[1 : len(s) - 1] + "\n")
     except Exception:
         print("Wrong file or file path")
 
 
 if __name__ == "__main__":
+    import sys
     import operator
     import argparse
 
@@ -1269,7 +1085,7 @@ if __name__ == "__main__":
     dist_sd = list(
         map(lambda tuple_x_k: tuple_x_k[1] / float(l), dc)
     )  # k = tuple_x_k[1]
-    cumm_sd = deque()  # np.cumsum(dc).tolist() #prefixsum
+    cumm_sd = []  # np.cumsum(dc).tolist() #prefixsum
     for i, (_, k) in enumerate(dc):
         if i == 0:
             cumm_sd.append(k / float(l))
@@ -1280,7 +1096,7 @@ if __name__ == "__main__":
     ### write stack_distance and line_accesses to a file ###
     write_dist_to_file(args.dist_file, line_accesses, list_sd, cumm_sd)
 
-    ### generate corresponding synthetic ###
+    ### generate correspondinf synthetic ###
     # line_accesses, list_sd, cumm_sd = read_dist_from_file(args.dist_file)
     synthetic_trace = trace_generate_lru(
         line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding
diff --git a/dlrm_s_caffe2.py b/dlrm_s_caffe2.py
index 8e3ed74..47b27d6 100644
--- a/dlrm_s_caffe2.py
+++ b/dlrm_s_caffe2.py
@@ -61,7 +61,7 @@ import time
 import copy
 
 # data generation
-import dlrm_data_pytorch as dp
+import dlrm_data_caffe2 as dc
 
 # numpy
 import numpy as np
@@ -73,13 +73,8 @@ import sklearn.metrics
 import warnings
 with warnings.catch_warnings():
     warnings.filterwarnings("ignore", category=DeprecationWarning)
-    try:
-        import onnx
-        import caffe2.python.onnx.frontend
-    except ImportError as error:
-        print('Unable to import onnx or caffe2.python.onnx.frontend ', error)
-
-# from caffe2.python import data_parallel_model
+import onnx
+import caffe2.python.onnx.frontend
 
 # caffe2
 from caffe2.proto import caffe2_pb2
@@ -259,20 +254,6 @@ class DLRM_Net(object):
             #     tag_fc_b,
             #     shape=[m]
             # )
-
-            # initialize the MLP's momentum for the Adagrad optimizer
-            if self.emb_optimizer in ["adagrad", "rwsadagrad"]:
-                # momentum of the weights
-                self.FeedBlobWrapper(
-                    "momentum_mlp_{}_{}".format(tag_layer, 2 * i - 1),
-                    np.full((m, n), 0, dtype=np.float32)
-                )
-                # momentum of the biases
-                self.FeedBlobWrapper(
-                    "momentum_mlp_{}_{}".format(tag_layer, 2 * i),
-                    np.full((m), 0, dtype=np.float32)
-                )
-
             # save the blob shapes for latter (only needed if onnx is requested)
             if self.save_onnx:
                 self.onnx_tsd[tag_fc_w] = (onnx.TensorProto.FLOAT, W.shape)
@@ -311,7 +292,6 @@ class DLRM_Net(object):
         (tag_layer, tag_in, tag_out) = tag
         emb_l = []
         weights_l = []
-        vw_l = []
         for i in range(0, ln.size):
             n = ln[i]
 
@@ -341,61 +321,18 @@ class DLRM_Net(object):
             # with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):
             #     W = model.param_init_net.XavierFill([], tbl_s, shape=[n, m])
             # save the blob shapes for latter (only needed if onnx is requested)
-
-            # initialize the embedding's momentum for the Adagrad optimizer
-            if self.emb_optimizer == "adagrad":
-                self.FeedBlobWrapper("momentum_emb_{}".format(i),
-                    np.full((n, m), 0), add_prefix=False, device_id=d)
-            elif self.emb_optimizer == "rwsadagrad":
-                self.FeedBlobWrapper("momentum_emb_{}".format(i),
-                    np.full((n), 0), add_prefix=False, device_id=d)
-
             if self.save_onnx:
                 self.onnx_tsd[tbl_s] = (onnx.TensorProto.FLOAT, W.shape)
 
             # create operator
-            if self.weighted_pooling is not None:
-                vw_s = on_device + tag_layer + ":::" + "sls" + str(i) + "_v"
-                psw_s = on_device + tag_layer + ":::" + "sls" + str(i) + "_s"
-                VW = np.ones(n).astype(np.float32)
-                self.FeedBlobWrapper(vw_s, VW, False, device_id=d)
-                if self.weighted_pooling == "learned":
-                    vw_l.append(vw_s)
-                    grad_on_weights = True
-                else:
-                    grad_on_weights = False
-                if self.save_onnx:
-                    self.onnx_tsd[vw_s] = (onnx.TensorProto.FLOAT, VW.shape)
-                if self.ndevices <= 1:
-                    PSW = model.net.Gather([vw_s, ind_s], [psw_s])
-                    EE = model.net.SparseLengthsWeightedSum(
-                        [tbl_s, PSW, ind_s, len_s], [sum_s],
-                        grad_on_weights=grad_on_weights
-                    )
-                else:
-                    with core.DeviceScope(
-                        core.DeviceOption(workspace.GpuDeviceType, d)
-                    ):
-                        PSW = model.net.Gather([vw_s, ind_s], [psw_s])
-                        EE = model.net.SparseLengthsWeightedSum(
-                            [tbl_s, PSW, ind_s, len_s], [sum_s],
-                            grad_on_weights=grad_on_weights
-                        )
+            if self.ndevices <= 1:
+                EE = model.net.SparseLengthsSum([tbl_s, ind_s, len_s], [sum_s])
             else:
-                if self.ndevices <= 1:
-                    EE = model.net.SparseLengthsSum(
-                        [tbl_s, ind_s, len_s], [sum_s]
-                    )
-                else:
-                    with core.DeviceScope(
-                        core.DeviceOption(workspace.GpuDeviceType, d)
-                    ):
-                        EE = model.net.SparseLengthsSum(
-                            [tbl_s, ind_s, len_s], [sum_s]
-                        )
+                with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):
+                    EE = model.net.SparseLengthsSum([tbl_s, ind_s, len_s], [sum_s])
             emb_l.append(EE)
 
-        return emb_l, weights_l, vw_l
+        return emb_l, weights_l
 
     def create_interactions(self, x, ly, model, tag):
         (tag_dense_in, tag_sparse_in, tag_int_out) = tag
@@ -438,9 +375,8 @@ class DLRM_Net(object):
     def create_sequential_forward_ops(self):
         # embeddings
         tag = (self.temb, self.tsin, self.tsout)
-        self.emb_l, self.emb_w, self.emb_vw = self.create_emb(
-            self.m_spa, self.ln_emb, self.model, tag
-        )
+        self.emb_l, self.emb_w = self.create_emb(self.m_spa, self.ln_emb,
+                                                 self.model, tag)
         # bottom mlp
         tag = (self.tbot, self.tdin, self.tdout)
         self.bot_l, self.bot_w = self.create_mlp(self.ln_bot, self.sigmoid_bot,
@@ -464,9 +400,8 @@ class DLRM_Net(object):
     def create_parallel_forward_ops(self):
         # distribute embeddings (model parallelism)
         tag = (self.temb, self.tsin, self.tsout)
-        self.emb_l, self.emb_w, self.emb_vw = self.create_emb(
-            self.m_spa, self.ln_emb, self.model, tag
-        )
+        self.emb_l, self.emb_w = self.create_emb(self.m_spa, self.ln_emb,
+                                                 self.model, tag)
         # replicate mlp (data parallelism)
         tag = (self.tbot, self.tdin, self.tdout)
         self.bot_l, self.bot_w = self.create_mlp(self.ln_bot, self.sigmoid_bot,
@@ -546,8 +481,6 @@ class DLRM_Net(object):
         ndevices=-1,
         forward_ops=True,
         enable_prof=False,
-        weighted_pooling=None,
-        emb_optimizer="sgd"
     ):
         super(DLRM_Net, self).__init__()
 
@@ -582,11 +515,6 @@ class DLRM_Net(object):
         self.sigmoid_top = sigmoid_top
         self.save_onnx = save_onnx
         self.ndevices = ndevices
-        self.emb_optimizer = emb_optimizer
-        if weighted_pooling is not None and weighted_pooling != "fixed":
-            self.weighted_pooling = "learned"
-        else:
-            self.weighted_pooling = weighted_pooling
         # onnx types and shapes dictionary
         if self.save_onnx:
             self.onnx_tsd = {}
@@ -825,205 +753,10 @@ class DLRM_Net(object):
             if self.ndevices > 1:
                 with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):
                     self.model.ScatterWeightedSum([w, _tag_one, w_grad.indices,
-                                                w_grad.values, _tag_lr], w)
+                                                   w_grad.values, _tag_lr], w)
             else:
                 self.model.ScatterWeightedSum([w, _tag_one, w_grad.indices,
-                                            w_grad.values, _tag_lr], w)
-
-        # update per sample weights
-        if self.weighted_pooling == "learned":
-            for i, w in enumerate(self.emb_vw):
-                # select device
-                if self.ndevices > 1:
-                    d = i % self.ndevices
-                # create tags
-                on_device = "" if self.ndevices <= 1 else "gpu_" + str(d) + "/"
-                _tag_one = on_device + tag_one
-                _tag_lr = on_device + tag_lr
-                # pickup gradient
-                w_grad = self.gradientMap[w]
-                # update weights
-                if self.ndevices > 1:
-                    with core.DeviceScope(
-                        core.DeviceOption(workspace.GpuDeviceType, d)
-                    ):
-                        self.model.ScatterWeightedSum(
-                            [w, _tag_one, w_grad.indices,
-                            w_grad.values, _tag_lr], w
-                        )
-                else:
-                    self.model.ScatterWeightedSum(
-                        [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w
-                    )
-
-    def adagrad_optimizer(self, learning_rate,
-                        T=None, _gradientMap=None, sync_dense_params=True,
-                        epsilon=1e-10, decay_=0.0, weight_decay_=0.0):
-        # create one, it and lr tags (or use them if already present)
-        if T is not None:
-            (tag_one, tag_it, tag_lr) = T
-        else:
-            (tag_one, tag_it, tag_lr) = ("const_one", "optim_it", "optim_lr")
-
-            # approach 1: feed values directly
-            # self.FeedBlobWrapper(tag_one, np.ones(1).astype(np.float32))
-            # self.FeedBlobWrapper(tag_it, np.zeros(1).astype(np.int64))
-            # it = self.AddLayerWrapper(self.model.Iter, tag_it, tag_it)
-            # lr = self.AddLayerWrapper(self.model.LearningRate, tag_it, tag_lr,
-            #                           base_lr=-1 * learning_rate, policy="fixed")
-            # approach 2: use brew
-            self.AddLayerWrapper(self.model.param_init_net.ConstantFill,
-                                 [], tag_one, shape=[1], value=1.0)
-            self.AddLayerWrapper(brew.iter, self.model, tag_it)
-            self.AddLayerWrapper(self.model.LearningRate, tag_it, tag_lr,
-                                 base_lr=-1 * learning_rate, policy="fixed")
-            # save the blob shapes for latter (only needed if onnx is requested)
-            if self.save_onnx:
-                self.onnx_tsd[tag_one] = (onnx.TensorProto.FLOAT, (1,))
-                self.onnx_tsd[tag_it] = (onnx.TensorProto.INT64, (1,))
-
-        # create gradient maps (or use them if already present)
-        if _gradientMap is not None:
-            self.gradientMap = _gradientMap
-        else:
-            if self.loss.__class__ == list:
-                self.gradientMap = self.model.AddGradientOperators(self.loss)
-            else:
-                self.gradientMap = self.model.AddGradientOperators([self.loss])
-
-        # update weights
-        # approach 1: builtin function
-        # optimizer.build_sgd(self.model, base_learning_rate=learning_rate)
-        # approach 2: custom code
-        # top MLP weight and bias
-        for i, w in enumerate(self.top_w):
-            # allreduce across devices if needed
-            if sync_dense_params and self.ndevices > 1:
-                grad_blobs = [
-                    self.gradientMap["gpu_{}/".format(d) + w]
-                    for d in range(self.ndevices)
-                ]
-                self.model.NCCLAllreduce(grad_blobs, grad_blobs)
-            # update weights
-            self.model.Adagrad(
-                [
-                    w,
-                    "momentum_mlp_top_{}".format(i + 1),
-                    self.gradientMap[w],
-                    tag_lr
-                ],
-                [w, "momentum_mlp_top_{}".format(i + 1)],
-                epsilon=epsilon,
-                decay_=decay_,
-                weight_decay_=weight_decay_
-            )
-
-        # bottom MLP weight and bias
-        for i, w in enumerate(self.bot_w):
-            # allreduce across devices if needed
-            if sync_dense_params and self.ndevices > 1:
-                grad_blobs = [
-                    self.gradientMap["gpu_{}/".format(d) + w]
-                    for d in range(self.ndevices)
-                ]
-                self.model.NCCLAllreduce(grad_blobs, grad_blobs)
-            # update weights
-            self.model.Adagrad(
-                [
-                    w,
-                    "momentum_mlp_bot_{}".format(i + 1),
-                    self.gradientMap[w],
-                    tag_lr
-                ],
-                [w, "momentum_mlp_bot_{}".format(i + 1)],
-                epsilon=epsilon,
-                decay_=decay_,
-                weight_decay_=weight_decay_
-            )
-
-        # update embeddings
-        for i, w in enumerate(self.emb_w):
-            # select device
-            if self.ndevices > 1:
-                d = i % self.ndevices
-            # create tags
-            on_device = "" if self.ndevices <= 1 else "gpu_" + str(d) + "/"
-            _tag_one = on_device + tag_one
-            _tag_lr = on_device + tag_lr
-            # pickup gradient
-            w_grad = self.gradientMap[w]
-            # update weights
-            def add_optimizer():
-                self.model.Unique(
-                    w_grad.indices,
-                    ["unique_w_grad_indices", "remapping_w_grad_indices"]
-                )
-                self.model.UnsortedSegmentSum(
-                    [w_grad.values, "remapping_w_grad_indices"],
-                    "unique_w_grad_values"
-                )
-
-                if self.emb_optimizer == "adagrad":
-                    self.model.SparseAdagrad(
-                        [
-                            w,
-                            "momentum_emb_{}".format(i),
-                            "unique_w_grad_indices",
-                            "unique_w_grad_values",
-                            _tag_lr
-                        ],
-                        [w, "momentum_emb_{}".format(i)],
-                        epsilon=epsilon,
-                        decay_=decay_,
-                        weight_decay_=weight_decay_
-                    )
-
-                elif self.emb_optimizer == "rwsadagrad":
-                    self.model.RowWiseSparseAdagrad(
-                        [
-                            w,
-                            "momentum_emb_{}".format(i),
-                            "unique_w_grad_indices",
-                            "unique_w_grad_values",
-                            _tag_lr
-                        ],
-                        [w, "momentum_emb_{}".format(i)],
-                        epsilon=epsilon,
-                        decay_=decay_,
-                        weight_decay_=weight_decay_
-                    )
-
-            if self.ndevices > 1:
-                with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):
-                    add_optimizer()
-            else:
-                add_optimizer()
-
-        # update per sample weights
-        if self.weighted_pooling == "learned":
-            for i, w in enumerate(self.emb_vw):
-                # select device
-                if self.ndevices > 1:
-                    d = i % self.ndevices
-                # create tags
-                on_device = "" if self.ndevices <= 1 else "gpu_" + str(d) + "/"
-                _tag_one = on_device + tag_one
-                _tag_lr = on_device + tag_lr
-                # pickup gradient
-                w_grad = self.gradientMap[w]
-                # update weights
-                if self.ndevices > 1:
-                    with core.DeviceScope(
-                        core.DeviceOption(workspace.GpuDeviceType, d)
-                    ):
-                        self.model.ScatterWeightedSum(
-                            [w, _tag_one, w_grad.indices,
-                            w_grad.values, _tag_lr], w
-                        )
-                else:
-                    self.model.ScatterWeightedSum(
-                        [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w
-                    )
+                                               w_grad.values, _tag_lr], w)
 
     def print_all(self):
         # approach 1: all
@@ -1040,10 +773,6 @@ class DLRM_Net(object):
         for _, l in enumerate(self.emb_w):
             # print(l)
             print(self.FetchBlobWrapper(l, False))
-        if self.weighted_pooling == "learned":
-            for _, l in enumerate(self.emb_vw):
-                # print(l)
-                print(self.FetchBlobWrapper(l, False))
         for _, l in enumerate(self.bot_w):
             # print(l)
             if self.ndevices > 1:
@@ -1136,7 +865,6 @@ def calculate_metrics(targets, scores):
     # print(" ms")
     return validation_results
 
-
 if __name__ == "__main__":
     ### import packages ###
     import sys
@@ -1158,16 +886,10 @@ if __name__ == "__main__":
     parser.add_argument("--loss-function", type=str, default="mse")   # or bce
     parser.add_argument("--loss-threshold", type=float, default=0.0)  # 1.0e-7
     parser.add_argument("--round-targets", type=bool, default=False)
-    parser.add_argument("--weighted-pooling", type=str, default=None)
     # data
     parser.add_argument("--data-size", type=int, default=1)
     parser.add_argument("--num-batches", type=int, default=0)
     parser.add_argument("--data-generation", type=str, default="random")  # or synthetic or dataset
-    parser.add_argument("--rand-data-dist", type=str, default="uniform")  # uniform or gaussian
-    parser.add_argument("--rand-data-min", type=float, default=0)
-    parser.add_argument("--rand-data-max", type=float, default=1)
-    parser.add_argument("--rand-data-mu", type=float, default=-1)
-    parser.add_argument("--rand-data-sigma", type=float, default=1)
     parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
     parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
     parser.add_argument("--raw-data-file", type=str, default="")
@@ -1178,7 +900,6 @@ if __name__ == "__main__":
     parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
     parser.add_argument("--num-indices-per-lookup", type=int, default=10)
     parser.add_argument("--num-indices-per-lookup-fixed", type=bool, default=False)
-    parser.add_argument("--num-workers", type=int, default=0)
     parser.add_argument("--memory-map", action="store_true", default=False)
     # training
     parser.add_argument("--mini-batch-size", type=int, default=1)
@@ -1188,17 +909,6 @@ if __name__ == "__main__":
     parser.add_argument("--numpy-rand-seed", type=int, default=123)
     parser.add_argument("--sync-dense-params", type=bool, default=True)
     parser.add_argument("--caffe2-net-type", type=str, default="")
-    parser.add_argument("--optimizer", type=str, default="sgd",
-        help="""This is the optimizer for embedding tables.""")
-    parser.add_argument(
-        "--dataset-multiprocessing",
-        action="store_true",
-        default=False,
-        help="The Kaggle dataset can be multiprocessed in an environment \
-                        with more than 7 CPU cores and more than 20 GB of memory. \n \
-                        The Terabyte dataset can be multiprocessed in an environment \
-                        with more than 24 CPU cores and at least 1 TB of memory.",
-    )
     # inference
     parser.add_argument("--inference-only", action="store_true", default=False)
     # onnx (or protobuf with shapes)
@@ -1209,8 +919,6 @@ if __name__ == "__main__":
     # debugging and profiling
     parser.add_argument("--print-freq", type=int, default=1)
     parser.add_argument("--test-freq", type=int, default=-1)
-    parser.add_argument("--test-mini-batch-size", type=int, default=-1)
-    parser.add_argument("--test-num-workers", type=int, default=-1)
     parser.add_argument("--print-time", action="store_true", default=False)
     parser.add_argument("--debug-mode", action="store_true", default=False)
     parser.add_argument("--enable-profiling", action="store_true", default=False)
@@ -1223,23 +931,9 @@ if __name__ == "__main__":
     parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
     args = parser.parse_args()
 
-    if args.dataset_multiprocessing:
-        assert float(sys.version[:3]) > 3.7, "The dataset_multiprocessing " + \
-        "flag is susceptible to a bug in Python 3.7 and under. " + \
-        "https://github.com/facebookresearch/dlrm/issues/172"
-
     ### some basic setup ###
-    # WARNING: to obtain exactly the same initialization for
-    # the weights we need to start from the same random seed.
     np.random.seed(args.numpy_rand_seed)
-
     np.set_printoptions(precision=args.print_precision)
-    if (args.test_mini_batch_size < 0):
-        # if the parameter is not set, use the training batch size
-        args.test_mini_batch_size = args.mini_batch_size
-    if (args.test_num_workers < 0):
-        # if the parameter is not set, use the same parameter for training
-        args.test_num_workers = args.num_workers
 
     use_gpu = args.use_gpu
     if use_gpu:
@@ -1253,29 +947,14 @@ if __name__ == "__main__":
     ### prepare training data ###
     ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
     if args.data_generation == "dataset":
-        if args.num_workers > 0 or args.test_num_workers > 0:
-            print("WARNING: non default --num-workers or --test-num-workers options"
-                    + " are not supported and will be ignored")
-        if args.mini_batch_size != args.test_mini_batch_size:
-            print("WARNING: non default ----test-mini-batch-size option"
-                    + " is not supported and will be ignored")
-
         # input and target from dataset
-
-        train_data, train_ld, test_data, test_ld = \
-            dp.make_criteo_data_and_loaders(
-                args,
-                offset_to_length_converter=True,
-            )
-
-        nbatches = args.num_batches if args.num_batches > 0 \
-            else len(train_ld)
-
-        nbatches_test = len(test_ld)
-
-        ln_emb = train_data.counts
-        m_den = train_data.m_den
-
+        (nbatches, lX, lS_l, lS_i, lT,
+         nbatches_test, lX_test, lS_l_test, lS_i_test, lT_test,
+         ln_emb, m_den) = dc.read_dataset(
+             args.data_set, args.max_ind_range, args.data_sub_sample_rate,
+             args.mini_batch_size, args.num_batches, args.data_randomize, "train",
+             args.raw_data_file, args.processed_data_file, args.memory_map
+        )
         # enforce maximum limit on number of vectors per embedding
         if args.max_ind_range > 0:
             ln_emb = np.array(list(map(
@@ -1283,28 +962,19 @@ if __name__ == "__main__":
                 ln_emb
             )))
         ln_bot[0] = m_den
-
     else:
-        if args.num_workers > 0 or args.test_num_workers > 0:
-            print("WARNING: non default --num-workers or --test-num-workers options"
-                  + " are not supported and will be ignored")
-        if args.mini_batch_size != args.test_mini_batch_size:
-            print("WARNING: non default ----test-mini-batch-size option"
-                  + " is not supported and will be ignored")
-
         # input and target at random
         ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
         m_den = ln_bot[0]
-        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(args, ln_emb, m_den, \
-            offset_to_length_converter=True,
+        (nbatches, lX, lS_l, lS_i, lT) = dc.generate_random_data(
+            m_den, ln_emb, args.data_size, args.num_batches, args.mini_batch_size,
+            args.num_indices_per_lookup, args.num_indices_per_lookup_fixed,
+            1, args.round_targets, args.data_generation, args.data_trace_file,
+            args.data_trace_enable_padding
         )
-        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
-        nbatches_test = len(test_ld)
-        # table_feature_map = {idx : idx for idx in range(len(ln_emb))}
 
     ### parse command line arguments ###
     m_spa = args.arch_sparse_feature_size
-    ln_emb = np.asarray(ln_emb)
     num_fea = ln_emb.size + 1  # num sparse + num dense features
     m_den_out = ln_bot[ln_bot.size - 1]
     if args.arch_interaction_op == "dot":
@@ -1356,13 +1026,12 @@ if __name__ == "__main__":
         print(ln_emb)
 
         print("data (inputs and targets):")
-        for j, inputBatch in enumerate(train_ld):
-            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
+        for j in range(0, nbatches):
             print("mini-batch: %d" % j)
-            print(lX_j)
-            print(lS_l_j)
-            print(lS_i_j)
-            print(lT_j)
+            print(lX[j])
+            print(lS_l[j])
+            print(lS_i[j])
+            print(lT[j].astype(np.float32))
 
     ### construct the neural network specified above ###
     # WARNING: to obtain exactly the same initialization for
@@ -1385,8 +1054,6 @@ if __name__ == "__main__":
             ndevices=ndevices,
             # forward_ops = flag_forward_ops
             enable_prof=args.enable_profiling,
-            weighted_pooling=args.weighted_pooling,
-            emb_optimizer=args.optimizer
         )
     # load nccl if using multiple devices
     if args.sync_dense_params and ndevices > 1:
@@ -1424,22 +1091,11 @@ if __name__ == "__main__":
             dlrm.test_net = core.Net(copy.deepcopy(dlrm.model.net.Proto()))
 
             # specify the optimizer algorithm
-            if args.optimizer == "sgd":
-                dlrm.sgd_optimizer(
-                    args.learning_rate, sync_dense_params=args.sync_dense_params
-                )
-            elif args.optimizer in ["adagrad", "rwsadagrad"]:
-                dlrm.adagrad_optimizer(
-                    args.learning_rate, sync_dense_params=args.sync_dense_params
-                )
-            else:
-                sys.exit("""ERROR: Select an optimizer for
-                                embedding tables : 'sgd', 'adagrad',
-                                or 'rwsadagrad' """)
-
+            dlrm.sgd_optimizer(
+                args.learning_rate, sync_dense_params=args.sync_dense_params
+            )
     # init/create
-    X, lS_l, lS_i, T = next(iter(train_ld)) # does not affect the enumerate(train_ld) in the main loop
-    dlrm.create(X, lS_l, lS_i, T.int())
+    dlrm.create(lX[0], lS_l[0], lS_i[0], lT[0])
 
     ### main loop ###
     best_gA_test = 0
@@ -1454,20 +1110,25 @@ if __name__ == "__main__":
     print("time/loss/accuracy (if enabled):")
     while k < args.nepochs:
         j = 0
-        for j, inputBatch in enumerate(train_ld):
+        while j < nbatches:
+            '''
+            # debug prints
+            print("input and targets")
+            print(lX[j])
+            print(lS_l[j])
+            print(lS_i[j])
+            print(lT[j].astype(np.float32))
+            '''
             # forward and backward pass, where the latter runs only
             # when gradients and loss have been added to the net
             time1 = time.time()
-            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
-            lT_j = lT_j.int() if args.loss_function == "bce" else lT_j
-            dlrm.run(lX_j, lS_l_j, lS_i_j, lT_j)
-
+            dlrm.run(lX[j], lS_l[j], lS_i[j], lT[j])  # args.enable_profiling
             time2 = time.time()
             total_time += time2 - time1
 
             # compte loss and accuracy
             Z = dlrm.get_output()  # numpy array
-            T = lT_j.numpy()
+            T = lT[j]              # numpy array
             '''
             # debug prints
             print("output and loss")
@@ -1485,7 +1146,7 @@ if __name__ == "__main__":
             should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)
             should_test = (
                 (args.test_freq > 0)
-                and (args.data_generation in ["dataset", "random"])
+                and (args.data_generation == "dataset")
                 and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
             )
             if should_print or should_test:
@@ -1503,7 +1164,7 @@ if __name__ == "__main__":
                     "Finished {} it {}/{} of epoch {}, {:.2f} ms/it,".format(
                         str_run_type, j + 1, nbatches, k, gT
                     )
-                    + " loss {:.6f}".format(gL)
+                    + " loss {:.6f}, accuracy {:3.3f} %".format(gL, gA * 100)
                 )
                 total_iter = 0
                 total_samp = 0
@@ -1525,19 +1186,15 @@ if __name__ == "__main__":
                         scores = []
                         targets = []
 
-                    for i, testBatch in enumerate(test_ld):
+                    for i in range(nbatches_test):
                         # early exit if nbatches was set by the user and was exceeded
                         if nbatches > 0 and i >= nbatches:
                             break
 
                         # forward pass
-
-                        lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i = testBatch
-                        lT_test_i = lT_test_i.int() if args.loss_function == "bce" else lT_test_i
-                        dlrm.run(lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i, test_net=True)
-
+                        dlrm.run(lX_test[i], lS_l_test[i], lS_i_test[i], lT_test[i], test_net=True)
                         Z_test = dlrm.get_output()
-                        T_test = lT_test_i.numpy()
+                        T_test = lT_test[i]
 
                         if args.mlperf_logging:
                             scores.append(Z_test)
@@ -1615,7 +1272,8 @@ if __name__ == "__main__":
                               + " reached, stop training")
                         break
 
-            j += 1  # nbatches
+
+            j += 1 # nbatches
         k += 1  # nepochs
 
     # test prints
@@ -1632,7 +1290,7 @@ if __name__ == "__main__":
         # print(value_info)
 
         # WARNING: Why Caffe2 to ONNX net transformation currently does not work?
-        # 1. ONNX does not support SparseLengthsSum operator directly. A workaround
+        # ONNX does not support SparseLengthsSum operator directly. A workaround
         # could be for the Caffe2 ONNX frontend to indirectly map this operator to
         # Gather and ReducedSum ONNX operators, following the PyTorch approach.
         c2f = caffe2.python.onnx.frontend.Caffe2Frontend()
diff --git a/dlrm_s_pytorch.py b/dlrm_s_pytorch.py
index ec3394b..149ace0 100644
--- a/dlrm_s_pytorch.py
+++ b/dlrm_s_pytorch.py
@@ -51,107 +51,64 @@
 # Misha Smelyanskiy, "Deep Learning Recommendation Model for Personalization and
 # Recommendation Systems", CoRR, arXiv:1906.00091, 2019
 
+
 from __future__ import absolute_import, division, print_function, unicode_literals
 
-import argparse
+
+
 
 # miscellaneous
 import builtins
-import datetime
-import json
-import sys
+import functools
+# import bisect
+# import shutil
 import time
-
-# onnx
-# The onnx import causes deprecation warnings every time workers
-# are spawned during testing. So, we filter out those warnings.
-import warnings
-
+import json
 # data generation
 import dlrm_data_pytorch as dp
 
-# For distributed run
-import extend_distributed as ext_dist
-import mlperf_logger
-
 # numpy
 import numpy as np
-import sklearn.metrics
+
+# onnx
+# The onnx import causes deprecation warnings every time workers
+# are spawned during testing. So, we filter out those warnings.
+import warnings
+with warnings.catch_warnings():
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+import onnx
 
 # pytorch
 import torch
 import torch.nn as nn
-from torch._ops import ops
-from torch.autograd.profiler import record_function
 from torch.nn.parallel.parallel_apply import parallel_apply
 from torch.nn.parallel.replicate import replicate
 from torch.nn.parallel.scatter_gather import gather, scatter
-from torch.nn.parameter import Parameter
-from torch.optim.lr_scheduler import _LRScheduler
-import optim.rwsadagrad as RowWiseSparseAdagrad
-from torch.utils.tensorboard import SummaryWriter
 
-# mixed-dimension trick
-from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
+# For distributed run
+import extend_distributed as ext_dist
+
+try:
+    import intel_pytorch_extension as ipex
+    from intel_pytorch_extension import core
+except:
+    pass
+from lamb_bin import Lamb, log_lamb_rs
 
 # quotient-remainder trick
 from tricks.qr_embedding_bag import QREmbeddingBag
+# mixed-dimension trick
+from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
 
-with warnings.catch_warnings():
-    warnings.filterwarnings("ignore", category=DeprecationWarning)
-    try:
-        import onnx
-    except ImportError as error:
-        print("Unable to import onnx. ", error)
+import sklearn.metrics
+import mlperf_logger
 
 # from torchviz import make_dot
 # import torch.nn.functional as Functional
 # from torch.nn.parameter import Parameter
 
-exc = getattr(builtins, "IOError", "FileNotFoundError")
-
-
-def time_wrap(use_gpu):
-    if use_gpu:
-        torch.cuda.synchronize()
-    return time.time()
-
-
-def dlrm_wrap(X, lS_o, lS_i, use_gpu, device, ndevices=1):
-    with record_function("DLRM forward"):
-        if use_gpu:  # .cuda()
-            # lS_i can be either a list of tensors or a stacked tensor.
-            # Handle each case below:
-            if ndevices == 1:
-                lS_i = (
-                    [S_i.to(device) for S_i in lS_i]
-                    if isinstance(lS_i, list)
-                    else lS_i.to(device)
-                )
-                lS_o = (
-                    [S_o.to(device) for S_o in lS_o]
-                    if isinstance(lS_o, list)
-                    else lS_o.to(device)
-                )
-        return dlrm(X.to(device), lS_o, lS_i)
-
-
-def loss_fn_wrap(Z, T, use_gpu, device):
-    with record_function("DLRM loss compute"):
-        if args.loss_function == "mse" or args.loss_function == "bce":
-            return dlrm.loss_fn(Z, T.to(device))
-        elif args.loss_function == "wbce":
-            loss_ws_ = dlrm.loss_ws[T.data.view(-1).long()].view_as(T).to(device)
-            loss_fn_ = dlrm.loss_fn(Z, T.to(device))
-            loss_sc_ = loss_ws_ * loss_fn_
-            return loss_sc_.mean()
-
-
-# The following function is a wrapper to avoid checking this multiple times in th
-# loop below.
-def unpack_batch(b):
-    # Experiment with unweighted samples
-    return b[0], b[1], b[2], b[3], torch.ones(b[3].size()), None
+from torch.optim.lr_scheduler import _LRScheduler
+import os
 
 
 class LRPolicyScheduler(_LRScheduler):
@@ -164,7 +121,11 @@ class LRPolicyScheduler(_LRScheduler):
         if self.decay_start_step < self.num_warmup_steps:
             sys.exit("Learning rate warmup must finish before the decay starts")
 
-        super(LRPolicyScheduler, self).__init__(optimizer)
+        if isinstance(optimizer, tuple):
+            for opt in optimizer:
+                super(LRPolicyScheduler, self).__init__(opt)
+        else:
+            super(LRPolicyScheduler, self).__init__(optimizer)
 
     def get_lr(self):
         step_count = self._step_count
@@ -191,6 +152,23 @@ class LRPolicyScheduler(_LRScheduler):
         return lr
 
 
+class Cast(nn.Module):
+     __constants__ = ['to_dtype']
+ 
+     def __init__(self, to_dtype):
+         super(Cast, self).__init__()
+         self.to_dtype = to_dtype
+ 
+     def forward(self, input):
+         if input.is_mkldnn:
+             return input.to_dense(self.to_dtype)
+         else:
+             return input.to(self.to_dtype)
+ 
+     def extra_repr(self):
+         return 'to(%s)' % self.to_dtype
+
+ 
 ### define dlrm in PyTorch ###
 class DLRM_Net(nn.Module):
     def create_mlp(self, ln, sigmoid_layer):
@@ -201,7 +179,10 @@ class DLRM_Net(nn.Module):
             m = ln[i + 1]
 
             # construct fully connected operator
-            LL = nn.Linear(int(n), int(m), bias=True)
+            if self.use_ipex and self.bf16:
+                LL = ipex.IpexMLPLinear(int(n), int(m), bias=True, output_stays_blocked=(i < ln.size - 2), default_blocking=32)
+            else:
+                LL = nn.Linear(int(n), int(m), bias=True)
 
             # initialize the weights
             # with torch.no_grad():
@@ -220,39 +201,49 @@ class DLRM_Net(nn.Module):
             # approach 3
             # LL.weight = Parameter(torch.tensor(W),requires_grad=True)
             # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)
+
+            if self.bf16 and ipex.is_available():
+                LL.to(torch.bfloat16)
+            # prepack weight for IPEX Linear
+            if hasattr(LL, 'reset_weight_shape'):
+                LL.reset_weight_shape(block_for_dtype=torch.bfloat16)
+
             layers.append(LL)
 
             # construct sigmoid or relu operator
             if i == sigmoid_layer:
+                if self.bf16:
+                    layers.append(Cast(torch.float32))
                 layers.append(nn.Sigmoid())
             else:
-                layers.append(nn.ReLU())
+                if self.use_ipex and self.bf16:
+                    LL.set_activation_type('relu')
+                else:
+                    layers.append(nn.ReLU())
 
         # approach 1: use ModuleList
         # return layers
         # approach 2: use Sequential container to wrap all layers
         return torch.nn.Sequential(*layers)
 
-    def create_emb(self, m, ln, weighted_pooling=None):
+    def create_emb(self, m, ln, local_ln_emb_sparse=None, ln_emb_dense=None):
         emb_l = nn.ModuleList()
-        v_W_l = []
-        for i in range(0, ln.size):
-            if ext_dist.my_size > 1:
-                if i not in self.local_emb_indices:
-                    continue
+        # save the numpy random state
+        np_rand_state = np.random.get_state()
+        emb_dense = nn.ModuleList()
+        emb_sparse = nn.ModuleList()
+        embs = range(len(ln))
+        if local_ln_emb_sparse or ln_emb_dense:
+            embs = local_ln_emb_sparse + ln_emb_dense
+        for i in embs:
+            # Use per table random seed for Embedding initialization
+            np.random.seed(self.l_emb_seeds[i])
             n = ln[i]
-
             # construct embedding operator
             if self.qr_flag and n > self.qr_threshold:
-                EE = QREmbeddingBag(
-                    n,
-                    m,
-                    self.qr_collisions,
-                    operation=self.qr_operation,
-                    mode="sum",
-                    sparse=True,
-                )
-            elif self.md_flag and n > self.md_threshold:
+                EE = QREmbeddingBag(n, m, self.qr_collisions,
+                    operation=self.qr_operation, mode="sum", sparse=True)
+            elif self.md_flag:
                 base = max(m)
                 _m = m[i] if n > self.md_threshold else base
                 EE = PrEmbeddingBag(n, _m, base)
@@ -261,25 +252,44 @@ class DLRM_Net(nn.Module):
                     low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)
                 ).astype(np.float32)
                 EE.embs.weight.data = torch.tensor(W, requires_grad=True)
+
             else:
-                EE = nn.EmbeddingBag(n, m, mode="sum", sparse=True)
                 # initialize embeddings
                 # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))
                 W = np.random.uniform(
                     low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
                 ).astype(np.float32)
                 # approach 1
-                EE.weight.data = torch.tensor(W, requires_grad=True)
+                if n >= self.sparse_dense_boundary:
+                    #n = 39979771
+                    m_sparse = int(m/4)
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m_sparse)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m_sparse, mode="sum", sparse=True, _weight=torch.tensor(W, requires_grad=True))
+                else:
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m, mode="sum", sparse=False, _weight=torch.tensor(W, requires_grad=True))
                 # approach 2
                 # EE.weight.data.copy_(torch.tensor(W))
                 # approach 3
                 # EE.weight = Parameter(torch.tensor(W),requires_grad=True)
-            if weighted_pooling is None:
-                v_W_l.append(None)
-            else:
-                v_W_l.append(torch.ones(n, dtype=torch.float32))
+                if self.bf16 and ipex.is_available():
+                    EE.to(torch.bfloat16)
+               
+            if ext_dist.my_size > 1:
+                if n >= self.sparse_dense_boundary:
+                    emb_sparse.append(EE)
+                else:
+                    emb_dense.append(EE)
+
             emb_l.append(EE)
-        return emb_l, v_W_l
+
+        # Restore the numpy random state
+        np.random.set_state(np_rand_state)
+        return emb_l, emb_dense, emb_sparse
 
     def __init__(
         self,
@@ -300,8 +310,9 @@ class DLRM_Net(nn.Module):
         qr_threshold=200,
         md_flag=False,
         md_threshold=200,
-        weighted_pooling=None,
-        loss_function="bce"
+        bf16=False,
+        use_ipex=False,
+        sparse_dense_boundary = 2048
     ):
         super(DLRM_Net, self).__init__()
 
@@ -322,11 +333,9 @@ class DLRM_Net(nn.Module):
             self.arch_interaction_itself = arch_interaction_itself
             self.sync_dense_params = sync_dense_params
             self.loss_threshold = loss_threshold
-            self.loss_function=loss_function
-            if weighted_pooling is not None and weighted_pooling != "fixed":
-                self.weighted_pooling = "learned"
-            else:
-                self.weighted_pooling = weighted_pooling
+            self.bf16 = bf16
+            self.use_ipex = use_ipex
+            self.sparse_dense_boundary = sparse_dense_boundary
             # create variables for QR embedding if applicable
             self.qr_flag = qr_flag
             if self.qr_flag:
@@ -338,62 +347,45 @@ class DLRM_Net(nn.Module):
             if self.md_flag:
                 self.md_threshold = md_threshold
 
-            # If running distributed, get local slice of embedding tables
+            # generate np seeds for Emb table initialization
+            self.l_emb_seeds = np.random.randint(low=0, high=100000, size=len(ln_emb))
+
+            #If running distributed, get local slice of embedding tables
             if ext_dist.my_size > 1:
                 n_emb = len(ln_emb)
-                if n_emb < ext_dist.my_size:
-                    sys.exit(
-                        "only (%d) sparse features for (%d) devices, table partitions will fail"
-                        % (n_emb, ext_dist.my_size)
-                    )
                 self.n_global_emb = n_emb
-                self.n_local_emb, self.n_emb_per_rank = ext_dist.get_split_lengths(
-                    n_emb
-                )
-                self.local_emb_slice = ext_dist.get_my_slice(n_emb)
-                self.local_emb_indices = list(range(n_emb))[self.local_emb_slice]
-
+                self.rank = ext_dist.dist.get_rank()
+                self.ln_emb_dense = [i for i in range(n_emb) if ln_emb[i] < self.sparse_dense_boundary]
+                self.ln_emb_sparse = [i for i in range(n_emb) if ln_emb[i] >= self.sparse_dense_boundary]
+                n_emb_sparse = len(self.ln_emb_sparse)
+                self.n_local_emb_sparse, self.n_sparse_emb_per_rank = ext_dist.get_split_lengths(n_emb_sparse)
+                self.local_ln_emb_sparse_slice = ext_dist.get_my_slice(n_emb_sparse)
+                self.local_ln_emb_sparse = self.ln_emb_sparse[self.local_ln_emb_sparse_slice]
             # create operators
             if ndevices <= 1:
-                self.emb_l, w_list = self.create_emb(m_spa, ln_emb, weighted_pooling)
-                if self.weighted_pooling == "learned":
-                    self.v_W_l = nn.ParameterList()
-                    for w in w_list:
-                        self.v_W_l.append(Parameter(w))
+                if ext_dist.my_size > 1:
+                    _, self.emb_dense, self.emb_sparse = self.create_emb(m_spa, ln_emb, self.local_ln_emb_sparse, self.ln_emb_dense)
                 else:
-                    self.v_W_l = w_list
+                    self.emb_l, _, _ = self.create_emb(m_spa, ln_emb)
+
             self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)
             self.top_l = self.create_mlp(ln_top, sigmoid_top)
 
-            # quantization
-            self.quantize_emb = False
-            self.emb_l_q = []
-            self.quantize_bits = 32
-
-            # specify the loss function
-            if self.loss_function == "mse":
-                self.loss_fn = torch.nn.MSELoss(reduction="mean")
-            elif self.loss_function == "bce":
-                self.loss_fn = torch.nn.BCELoss(reduction="mean")
-            elif self.loss_function == "wbce":
-                self.loss_ws = torch.tensor(
-                    np.fromstring(args.loss_weights, dtype=float, sep="-")
-                )
-                self.loss_fn = torch.nn.BCELoss(reduction="none")
-            else:
-                sys.exit(
-                    "ERROR: --loss-function=" + self.loss_function + " is not supported"
-                )
-
     def apply_mlp(self, x, layers):
         # approach 1: use ModuleList
         # for layer in layers:
         #     x = layer(x)
         # return x
         # approach 2: use Sequential container to wrap all layers
-        return layers(x)
+        need_padding = self.use_ipex and self.bf16 and x.size(0) % 2 == 1
+        if need_padding:
+            x = torch.nn.functional.pad(input=x, pad=(0,0,0,1), mode='constant', value=0)
+            ret = layers(x)
+            return(ret[:-1,:])
+        else:
+            return layers(x)
 
-    def apply_emb(self, lS_o, lS_i, emb_l, v_W_l):
+    def apply_emb(self, lS_o, lS_i, emb_l):
         # WARNING: notice that we are processing the batch at once. We implicitly
         # assume that the data is laid out such that:
         # 1. each embedding is indexed with a group of sparse indices,
@@ -409,90 +401,41 @@ class DLRM_Net(nn.Module):
             # We are using EmbeddingBag, which implicitly uses sum operator.
             # The embeddings are represented as tall matrices, with sum
             # happening vertically across 0 axis, resulting in a row vector
-            # E = emb_l[k]
+            E = emb_l[k]
+            V = E(sparse_index_group_batch, sparse_offset_group_batch)
 
-            if v_W_l[k] is not None:
-                per_sample_weights = v_W_l[k].gather(0, sparse_index_group_batch)
-            else:
-                per_sample_weights = None
-
-            if self.quantize_emb:
-                s1 = self.emb_l_q[k].element_size() * self.emb_l_q[k].nelement()
-                s2 = self.emb_l_q[k].element_size() * self.emb_l_q[k].nelement()
-                print("quantized emb sizes:", s1, s2)
-
-                if self.quantize_bits == 4:
-                    QV = ops.quantized.embedding_bag_4bit_rowwise_offsets(
-                        self.emb_l_q[k],
-                        sparse_index_group_batch,
-                        sparse_offset_group_batch,
-                        per_sample_weights=per_sample_weights,
-                    )
-                elif self.quantize_bits == 8:
-                    QV = ops.quantized.embedding_bag_byte_rowwise_offsets(
-                        self.emb_l_q[k],
-                        sparse_index_group_batch,
-                        sparse_offset_group_batch,
-                        per_sample_weights=per_sample_weights,
-                    )
-
-                ly.append(QV)
-            else:
-                E = emb_l[k]
-                V = E(
-                    sparse_index_group_batch,
-                    sparse_offset_group_batch,
-                    per_sample_weights=per_sample_weights,
-                )
-
-                ly.append(V)
+            ly.append(V)
 
         # print(ly)
         return ly
-
-    #  using quantizing functions from caffe2/aten/src/ATen/native/quantized/cpu
-    def quantize_embedding(self, bits):
-
-        n = len(self.emb_l)
-        self.emb_l_q = [None] * n
-        for k in range(n):
-            if bits == 4:
-                self.emb_l_q[k] = ops.quantized.embedding_bag_4bit_prepack(
-                    self.emb_l[k].weight
-                )
-            elif bits == 8:
-                self.emb_l_q[k] = ops.quantized.embedding_bag_byte_prepack(
-                    self.emb_l[k].weight
-                )
-            else:
-                return
-        self.emb_l = None
-        self.quantize_emb = True
-        self.quantize_bits = bits
-
+#if self.bf16:
     def interact_features(self, x, ly):
-
+        x = x.to(ly[0].dtype)
         if self.arch_interaction_op == "dot":
-            # concatenate dense and sparse features
-            (batch_size, d) = x.shape
-            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
-            # perform a dot product
-            Z = torch.bmm(T, torch.transpose(T, 1, 2))
-            # append dense feature with the interactions (into a row vector)
-            # approach 1: all
-            # Zflat = Z.view((batch_size, -1))
-            # approach 2: unique
-            _, ni, nj = Z.shape
-            # approach 1: tril_indices
-            # offset = 0 if self.arch_interaction_itself else -1
-            # li, lj = torch.tril_indices(ni, nj, offset=offset)
-            # approach 2: custom
-            offset = 1 if self.arch_interaction_itself else 0
-            li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
-            lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
-            Zflat = Z[:, li, lj]
-            # concatenate dense features and interactions
-            R = torch.cat([x] + [Zflat], dim=1)
+            if self.bf16:
+                T = [x] + ly
+                R = ipex.interaction(*T)
+            else:
+                # concatenate dense and sparse features
+                (batch_size, d) = x.shape
+                T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
+                # perform a dot product
+                Z = torch.bmm(T, torch.transpose(T, 1, 2))
+                # append dense feature with the interactions (into a row vector)
+                # approach 1: all
+                # Zflat = Z.view((batch_size, -1))
+                # approach 2: unique
+                _, ni, nj = Z.shape
+                # approach 1: tril_indices
+                # offset = 0 if self.arch_interaction_itself else -1
+                # li, lj = torch.tril_indices(ni, nj, offset=offset)
+                # approach 2: custom
+                offset = 1 if self.arch_interaction_itself else 0
+                li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
+                lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
+                Zflat = Z[:, li, lj]
+                # concatenate dense features and interactions
+                R = torch.cat([x] + [Zflat], dim=1)
         elif self.arch_interaction_op == "cat":
             # concatenation features (into a row vector)
             R = torch.cat([x] + ly, dim=1)
@@ -506,75 +449,15 @@ class DLRM_Net(nn.Module):
         return R
 
     def forward(self, dense_x, lS_o, lS_i):
+        if self.bf16:
+            dense_x = dense_x.bfloat16()
         if ext_dist.my_size > 1:
-            # multi-node multi-device run
             return self.distributed_forward(dense_x, lS_o, lS_i)
         elif self.ndevices <= 1:
-            # single device run
             return self.sequential_forward(dense_x, lS_o, lS_i)
         else:
-            # single-node multi-device run
             return self.parallel_forward(dense_x, lS_o, lS_i)
 
-    def distributed_forward(self, dense_x, lS_o, lS_i):
-        batch_size = dense_x.size()[0]
-        # WARNING: # of ranks must be <= batch size in distributed_forward call
-        if batch_size < ext_dist.my_size:
-            sys.exit(
-                "ERROR: batch_size (%d) must be larger than number of ranks (%d)"
-                % (batch_size, ext_dist.my_size)
-            )
-        if batch_size % ext_dist.my_size != 0:
-            sys.exit(
-                "ERROR: batch_size %d can not split across %d ranks evenly"
-                % (batch_size, ext_dist.my_size)
-            )
-
-        dense_x = dense_x[ext_dist.get_my_slice(batch_size)]
-        lS_o = lS_o[self.local_emb_slice]
-        lS_i = lS_i[self.local_emb_slice]
-
-        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):
-            sys.exit(
-                "ERROR: corrupted model input detected in distributed_forward call"
-            )
-
-        # embeddings
-        with record_function("DLRM embedding forward"):
-            ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
-
-        # WARNING: Note that at this point we have the result of the embedding lookup
-        # for the entire batch on each rank. We would like to obtain partial results
-        # corresponding to all embedding lookups, but part of the batch on each rank.
-        # Therefore, matching the distribution of output of bottom mlp, so that both
-        # could be used for subsequent interactions on each device.
-        if len(self.emb_l) != len(ly):
-            sys.exit("ERROR: corrupted intermediate result in distributed_forward call")
-
-        a2a_req = ext_dist.alltoall(ly, self.n_emb_per_rank)
-
-        with record_function("DLRM bottom nlp forward"):
-            x = self.apply_mlp(dense_x, self.bot_l)
-
-        ly = a2a_req.wait()
-        ly = list(ly)
-
-        # interactions
-        with record_function("DLRM interaction forward"):
-            z = self.interact_features(x, ly)
-
-        # top mlp
-        with record_function("DLRM top nlp forward"):
-            p = self.apply_mlp(z, self.top_l)
-
-        # clamp output if needed
-        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
-            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))
-        else:
-            z = p
-
-        return z
-
     def sequential_forward(self, dense_x, lS_o, lS_i):
         # process dense features (using bottom mlp), resulting in a row vector
         x = self.apply_mlp(dense_x, self.bot_l)
@@ -583,7 +466,7 @@ class DLRM_Net(nn.Module):
         # print(x.detach().cpu().numpy())
 
         # process sparse features(using embeddings), resulting in a list of row vectors
-        ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
         # for y in ly:
         #     print(y.detach().cpu().numpy())
 
@@ -602,6 +485,52 @@ class DLRM_Net(nn.Module):
 
         return z
 
+    def distributed_forward(self, dense_x, lS_o, lS_i):
+        batch_size = dense_x.size()[0]
+        # WARNING: # of ranks must be <= batch size in distributed_forward call
+        if batch_size < ext_dist.my_size:
+            sys.exit("ERROR: batch_size (%d) must be larger than number of ranks (%d)" % (batch_size, ext_dist.my_size))
+
+        lS_o_dense = [lS_o[i]  for i in self.ln_emb_dense]
+        lS_i_dense = [lS_i[i] for i in self.ln_emb_dense]
+        lS_o_sparse = [lS_o[i] for i in self.ln_emb_sparse]  # partition sparse table in one group
+        lS_i_sparse = [lS_i[i] for i in self.ln_emb_sparse]
+
+        lS_i_sparse = ext_dist.shuffle_data(lS_i_sparse)
+        g_i_sparse = [lS_i_sparse[:, i * batch_size:(i + 1) * batch_size].reshape(-1) for i in range(len(self.local_ln_emb_sparse))]
+        offset = torch.arange(batch_size * ext_dist.my_size).to(device)
+        g_o_sparse = [offset for i in range(self.n_local_emb_sparse)]
+
+        if (len(self.local_ln_emb_sparse) != len(g_o_sparse)) or (len(self.local_ln_emb_sparse) != len(g_i_sparse)):
+           sys.exit("ERROR 0 : corrupted model input detected in distributed_forward call")
+        # sparse embeddings
+        ly_sparse = self.apply_emb(g_o_sparse, g_i_sparse, self.emb_sparse)
+        a2a_req = ext_dist.alltoall(ly_sparse, self.n_sparse_emb_per_rank)
+        # bottom mlp
+        x = self.apply_mlp(dense_x, self.bot_l)
+        # dense embeddings
+        ly_dense = self.apply_emb(lS_o_dense, lS_i_dense, self.emb_dense)
+        ly_sparse = a2a_req.wait()
+        ly_sparse2 = []
+        for i in range(len(ly_sparse)):
+            ly_sparse2.append(ly_sparse[i].repeat(1,4))
+        del ly_sparse
+        #ly_sparse ""= torch.cat(ly_sparse,1)
+        ly = ly_dense + list(ly_sparse2)
+        # interactions
+        z = self.interact_features(x, ly)
+        # top mlp
+        p = self.apply_mlp(z, self.top_l)
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z = torch.clamp(
+                p, min=self.loss_threshold, max=(1.0 - self.loss_threshold)
+            )
+        else:
+            z = p
+
+        return z
+ 
     def parallel_forward(self, dense_x, lS_o, lS_i):
         ### prepare model (overwrite) ###
         # WARNING: # of devices must be >= batch size in parallel_forward call
@@ -622,21 +551,11 @@ class DLRM_Net(nn.Module):
         if self.parallel_model_is_not_prepared:
             # distribute embeddings (model parallelism)
             t_list = []
-            w_list = []
             for k, emb in enumerate(self.emb_l):
                 d = torch.device("cuda:" + str(k % ndevices))
+                emb.to(d)
                 t_list.append(emb.to(d))
-                if self.weighted_pooling == "learned":
-                    w_list.append(Parameter(self.v_W_l[k].to(d)))
-                elif self.weighted_pooling == "fixed":
-                    w_list.append(self.v_W_l[k].to(d))
-                else:
-                    w_list.append(None)
             self.emb_l = nn.ModuleList(t_list)
-            if self.weighted_pooling == "learned":
-                self.v_W_l = nn.ParameterList(w_list)
-            else:
-                self.v_W_l = w_list
             self.parallel_model_is_not_prepared = False
 
         ### prepare input (overwrite) ###
@@ -668,7 +587,7 @@ class DLRM_Net(nn.Module):
         # print(x)
 
         # embeddings
-        ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
         # debug prints
         # print(ly)
 
@@ -721,193 +640,30 @@ class DLRM_Net(nn.Module):
         return z0
 
 
-def dash_separated_ints(value):
-    vals = value.split("-")
-    for val in vals:
-        try:
-            int(val)
-        except ValueError:
-            raise argparse.ArgumentTypeError(
-                "%s is not a valid dash separated list of ints" % value
-            )
-
-    return value
-
-
-def dash_separated_floats(value):
-    vals = value.split("-")
-    for val in vals:
-        try:
-            float(val)
-        except ValueError:
-            raise argparse.ArgumentTypeError(
-                "%s is not a valid dash separated list of floats" % value
-            )
-
-    return value
-
-
-def inference(
-    args,
-    dlrm,
-    best_acc_test,
-    best_auc_test,
-    test_ld,
-    device,
-    use_gpu,
-    log_iter=-1,
-):
-    test_accu = 0
-    test_samp = 0
-
-    if args.mlperf_logging:
-        scores = []
-        targets = []
-
-    for i, testBatch in enumerate(test_ld):
-        # early exit if nbatches was set by the user and was exceeded
-        if nbatches > 0 and i >= nbatches:
-            break
-
-        X_test, lS_o_test, lS_i_test, T_test, W_test, CBPP_test = unpack_batch(
-            testBatch
-        )
-
-        # Skip the batch if batch size not multiple of total ranks
-        if ext_dist.my_size > 1 and X_test.size(0) % ext_dist.my_size != 0:
-            print("Warning: Skiping the batch %d with size %d" % (i, X_test.size(0)))
-            continue
-
-        # forward pass
-        Z_test = dlrm_wrap(
-            X_test,
-            lS_o_test,
-            lS_i_test,
-            use_gpu,
-            device,
-            ndevices=ndevices,
-        )
-        ### gather the distributed results on each rank ###
-        # For some reason it requires explicit sync before all_gather call if
-        # tensor is on GPU memory
-        if Z_test.is_cuda:
-            torch.cuda.synchronize()
-        (_, batch_split_lengths) = ext_dist.get_split_lengths(X_test.size(0))
-        if ext_dist.my_size > 1:
-            Z_test = ext_dist.all_gather(Z_test, batch_split_lengths)
-
-        if args.mlperf_logging:
-            S_test = Z_test.detach().cpu().numpy()  # numpy array
-            T_test = T_test.detach().cpu().numpy()  # numpy array
-            scores.append(S_test)
-            targets.append(T_test)
-        else:
-            with record_function("DLRM accuracy compute"):
-                # compute loss and accuracy
-                S_test = Z_test.detach().cpu().numpy()  # numpy array
-                T_test = T_test.detach().cpu().numpy()  # numpy array
+if __name__ == "__main__":
+    # the reference implementation doesn't clear the cache currently
+    # but the submissions are required to do that
+    mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)
 
-                mbs_test = T_test.shape[0]  # = mini_batch_size except last
-                A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))
+    mlperf_logger.log_start(key=mlperf_logger.constants.INIT_START, log_all_ranks=True)
 
-                test_accu += A_test
-                test_samp += mbs_test
+    ### import packages ###
+    import sys
+    import os
+    import argparse
 
-    if args.mlperf_logging:
-        with record_function("DLRM mlperf sklearn metrics compute"):
-            scores = np.concatenate(scores, axis=0)
-            targets = np.concatenate(targets, axis=0)
-
-            metrics = {
-                "recall": lambda y_true, y_score: sklearn.metrics.recall_score(
-                    y_true=y_true, y_pred=np.round(y_score)
-                ),
-                "precision": lambda y_true, y_score: sklearn.metrics.precision_score(
-                    y_true=y_true, y_pred=np.round(y_score)
-                ),
-                "f1": lambda y_true, y_score: sklearn.metrics.f1_score(
-                    y_true=y_true, y_pred=np.round(y_score)
-                ),
-                "ap": sklearn.metrics.average_precision_score,
-                "roc_auc": sklearn.metrics.roc_auc_score,
-                "accuracy": lambda y_true, y_score: sklearn.metrics.accuracy_score(
-                    y_true=y_true, y_pred=np.round(y_score)
-                ),
-            }
-
-        validation_results = {}
-        for metric_name, metric_function in metrics.items():
-            validation_results[metric_name] = metric_function(targets, scores)
-            writer.add_scalar(
-                "mlperf-metrics-test/" + metric_name,
-                validation_results[metric_name],
-                log_iter,
-            )
-        acc_test = validation_results["accuracy"]
-    else:
-        acc_test = test_accu / test_samp
-        writer.add_scalar("Test/Acc", acc_test, log_iter)
-
-    model_metrics_dict = {
-        "nepochs": args.nepochs,
-        "nbatches": nbatches,
-        "nbatches_test": nbatches_test,
-        "state_dict": dlrm.state_dict(),
-        "test_acc": acc_test,
-    }
-
-    if args.mlperf_logging:
-        is_best = validation_results["roc_auc"] > best_auc_test
-        if is_best:
-            best_auc_test = validation_results["roc_auc"]
-            model_metrics_dict["test_auc"] = best_auc_test
-        print(
-            "recall {:.4f}, precision {:.4f},".format(
-                validation_results["recall"],
-                validation_results["precision"],
-            )
-            + " f1 {:.4f}, ap {:.4f},".format(
-                validation_results["f1"], validation_results["ap"]
-            )
-            + " auc {:.4f}, best auc {:.4f},".format(
-                validation_results["roc_auc"], best_auc_test
-            )
-            + " accuracy {:3.3f} %, best accuracy {:3.3f} %".format(
-                validation_results["accuracy"] * 100, best_acc_test * 100
-            ),
-            flush=True,
-        )
-    else:
-        is_best = acc_test > best_acc_test
-        if is_best:
-            best_acc_test = acc_test
-        print(
-            " accuracy {:3.3f} %, best {:3.3f} %".format(
-                acc_test * 100, best_acc_test * 100
-            ),
-            flush=True,
-        )
-    return model_metrics_dict, is_best
-
-
-def run():
     ### parse arguments ###
     parser = argparse.ArgumentParser(
         description="Train Deep Learning Recommendation Model (DLRM)"
     )
     # model related parameters
     parser.add_argument("--arch-sparse-feature-size", type=int, default=2)
-    parser.add_argument(
-        "--arch-embedding-size", type=dash_separated_ints, default="4-3-2"
-    )
+    parser.add_argument("--arch-embedding-size", type=str, default="4-3-2")
     # j will be replaced with the table number
-    parser.add_argument("--arch-mlp-bot", type=dash_separated_ints, default="4-3-2")
-    parser.add_argument("--arch-mlp-top", type=dash_separated_ints, default="4-2-1")
-    parser.add_argument(
-        "--arch-interaction-op", type=str, choices=["dot", "cat"], default="dot"
-    )
+    parser.add_argument("--arch-mlp-bot", type=str, default="4-3-2")
+    parser.add_argument("--arch-mlp-top", type=str, default="4-2-1")
+    parser.add_argument("--arch-interaction-op", type=str, default="dot")
     parser.add_argument("--arch-interaction-itself", action="store_true", default=False)
-    parser.add_argument("--weighted-pooling", type=str, default=None)
     # embedding table options
     parser.add_argument("--md-flag", action="store_true", default=False)
     parser.add_argument("--md-threshold", type=int, default=200)
@@ -920,9 +676,7 @@ def run():
     # activations and loss
     parser.add_argument("--activation-function", type=str, default="relu")
     parser.add_argument("--loss-function", type=str, default="mse")  # or bce or wbce
-    parser.add_argument(
-        "--loss-weights", type=dash_separated_floats, default="1.0-1.0"
-    )  # for wbce
+    parser.add_argument("--loss-weights", type=str, default="1.0-1.0")  # for wbce
     parser.add_argument("--loss-threshold", type=float, default=0.0)  # 1.0e-7
     parser.add_argument("--round-targets", type=bool, default=False)
     # data
@@ -931,13 +685,6 @@ def run():
     parser.add_argument(
         "--data-generation", type=str, default="random"
     )  # synthetic or dataset
-    parser.add_argument(
-        "--rand-data-dist", type=str, default="uniform"
-    )  # uniform or gaussian
-    parser.add_argument("--rand-data-min", type=float, default=0)
-    parser.add_argument("--rand-data-max", type=float, default=1)
-    parser.add_argument("--rand-data-mu", type=float, default=-1)
-    parser.add_argument("--rand-data-sigma", type=float, default=1)
     parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
     parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
     parser.add_argument("--raw-data-file", type=str, default="")
@@ -957,27 +704,13 @@ def run():
     parser.add_argument("--print-precision", type=int, default=5)
     parser.add_argument("--numpy-rand-seed", type=int, default=123)
     parser.add_argument("--sync-dense-params", type=bool, default=True)
-    parser.add_argument("--optimizer", type=str, default="sgd")
-    parser.add_argument(
-        "--dataset-multiprocessing",
-        action="store_true",
-        default=False,
-        help="The Kaggle dataset can be multiprocessed in an environment \
-                        with more than 7 CPU cores and more than 20 GB of memory. \n \
-                        The Terabyte dataset can be multiprocessed in an environment \
-                        with more than 24 CPU cores and at least 1 TB of memory.",
-    )
     # inference
     parser.add_argument("--inference-only", action="store_true", default=False)
-    # quantize
-    parser.add_argument("--quantize-mlp-with-bit", type=int, default=32)
-    parser.add_argument("--quantize-emb-with-bit", type=int, default=32)
     # onnx
     parser.add_argument("--save-onnx", action="store_true", default=False)
     # gpu
     parser.add_argument("--use-gpu", action="store_true", default=False)
-    # distributed
-    parser.add_argument("--local_rank", type=int, default=-1)
+    # distributed run
     parser.add_argument("--dist-backend", type=str, default="")
     # debugging and profiling
     parser.add_argument("--print-freq", type=int, default=1)
@@ -985,12 +718,13 @@ def run():
     parser.add_argument("--test-mini-batch-size", type=int, default=-1)
     parser.add_argument("--test-num-workers", type=int, default=-1)
     parser.add_argument("--print-time", action="store_true", default=False)
-    parser.add_argument("--print-wall-time", action="store_true", default=False)
     parser.add_argument("--debug-mode", action="store_true", default=False)
     parser.add_argument("--enable-profiling", action="store_true", default=False)
     parser.add_argument("--plot-compute-graph", action="store_true", default=False)
-    parser.add_argument("--tensor-board-filename", type=str, default="run_kaggle_pt")
+    parser.add_argument("--profiling-start-iter", type=int, default=50)
+    parser.add_argument("--profiling-num-iters", type=int, default=100)
     # store/load model
+    parser.add_argument("--out-dir", type=str, default=".")
     parser.add_argument("--save-model", type=str, default="")
     parser.add_argument("--load-model", type=str, default="")
     # mlperf logging (disables other output and stops early)
@@ -999,50 +733,30 @@ def run():
     parser.add_argument("--mlperf-acc-threshold", type=float, default=0.0)
     # stop at target AUC Terabyte (no subsampling) 0.8025
     parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
-    parser.add_argument("--mlperf-bin-loader", action="store_true", default=False)
-    parser.add_argument("--mlperf-bin-shuffle", action="store_true", default=False)
-    # mlperf gradient accumulation iterations
-    parser.add_argument("--mlperf-grad-accum-iter", type=int, default=1)
+    parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
+    parser.add_argument("--mlperf-bin-shuffle", action='store_true', default=False)
     # LR policy
     parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
     parser.add_argument("--lr-decay-start-step", type=int, default=0)
     parser.add_argument("--lr-num-decay-steps", type=int, default=0)
-
-    global args
-    global nbatches
-    global nbatches_test
-    global writer
+    # embedding table is sparse table only if sparse_dense_boundary >= 2048
+    parser.add_argument("--sparse-dense-boundary", type=int, default=2048)
+    # bf16 option
+    parser.add_argument("--bf16", action='store_true', default=False)
+    # ipex option
+    parser.add_argument("--use-ipex", action="store_true", default=False)
+    # lamb
+    parser.add_argument("--optimizer", type=int, default=0, help='optimizer:[0:sgd, 1:lamb/sgd, 2:adagrad, 3:sparseadam]')
+    parser.add_argument("--lamblr", type=float, default=0.01, help='lr for lamb')
+    parser.add_argument("--train-data-path", type=str, default="./data/train.bin")
+    parser.add_argument("--eval-data-path", type=str, default="./data/valid.bin")
+    parser.add_argument("--day-feature-count", type=str, default="./data/day_fea_count.npz")
     args = parser.parse_args()
 
-    if args.dataset_multiprocessing:
-        assert float(sys.version[:3]) > 3.7, "The dataset_multiprocessing " + \
-        "flag is susceptible to a bug in Python 3.7 and under. " + \
-        "https://github.com/facebookresearch/dlrm/issues/172"
+    ext_dist.init_distributed(backend=args.dist_backend)
 
     if args.mlperf_logging:
-        mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)
-        mlperf_logger.log_start(
-            key=mlperf_logger.constants.INIT_START, log_all_ranks=True
-        )
-
-    if args.weighted_pooling is not None:
-        if args.qr_flag:
-            sys.exit("ERROR: quotient remainder with weighted pooling is not supported")
-        if args.md_flag:
-            sys.exit("ERROR: mixed dimensions with weighted pooling is not supported")
-    if args.quantize_emb_with_bit in [4, 8]:
-        if args.qr_flag:
-            sys.exit(
-                "ERROR: 4 and 8-bit quantization with quotient remainder is not supported"
-            )
-        if args.md_flag:
-            sys.exit(
-                "ERROR: 4 and 8-bit quantization with mixed dimensions is not supported"
-            )
-        if args.use_gpu:
-            sys.exit(
-                "ERROR: 4 and 8-bit quantization on GPU is not supported"
-            )
+        print('command line args: ', json.dumps(vars(args)))
 
     ### some basic setup ###
     np.random.seed(args.numpy_rand_seed)
@@ -1050,28 +764,35 @@ def run():
     torch.set_printoptions(precision=args.print_precision)
     torch.manual_seed(args.numpy_rand_seed)
 
-    if args.test_mini_batch_size < 0:
+    if (args.test_mini_batch_size < 0):
         # if the parameter is not set, use the training batch size
         args.test_mini_batch_size = args.mini_batch_size
-    if args.test_num_workers < 0:
+    if (args.test_num_workers < 0):
         # if the parameter is not set, use the same parameter for training
         args.test_num_workers = args.num_workers
+    if (args.mini_batch_size % ext_dist.my_size !=0 or args.test_mini_batch_size % ext_dist.my_size != 0):
+        print("Either test minibatch (%d) or train minibatch (%d) does not split across %d ranks" % (args.test_mini_batch_size, args.mini_batch_size, ext_dist.my_size))
+        sys.exit(1)
 
     use_gpu = args.use_gpu and torch.cuda.is_available()
-
-    if not args.debug_mode:
-        ext_dist.init_distributed(local_rank=args.local_rank, use_gpu=use_gpu, backend=args.dist_backend)
-
+    use_ipex = args.use_ipex
     if use_gpu:
         torch.cuda.manual_seed_all(args.numpy_rand_seed)
         torch.backends.cudnn.deterministic = True
         if ext_dist.my_size > 1:
+            ngpus = torch.cuda.device_count()  # 1
+            if ext_dist.my_local_size > torch.cuda.device_count():
+                print("Not sufficient GPUs available... local_size = %d, ngpus = %d" % (ext_dist.my_local_size, ngpus))
+                sys.exit(1)
             ngpus = 1
             device = torch.device("cuda", ext_dist.my_local_rank)
         else:
-            ngpus = torch.cuda.device_count()
             device = torch.device("cuda", 0)
+            ngpus = torch.cuda.device_count()  # 1
         print("Using {} GPU(s)...".format(ngpus))
+    elif use_ipex:
+        device = torch.device("dpcpp")
+        print("Using IPEX...")
     else:
         device = torch.device("cpu")
         print("Using CPU...")
@@ -1080,51 +801,38 @@ def run():
     ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
     # input data
 
-    if args.mlperf_logging:
-        mlperf_logger.barrier()
-        mlperf_logger.log_end(key=mlperf_logger.constants.INIT_STOP)
-        mlperf_logger.barrier()
-        mlperf_logger.log_start(key=mlperf_logger.constants.RUN_START)
-        mlperf_logger.barrier()
-
-    if args.data_generation == "dataset":
-        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
-        table_feature_map = {idx: idx for idx in range(len(train_data.counts))}
+    mlperf_logger.barrier()
+    mlperf_logger.log_end(key=mlperf_logger.constants.INIT_STOP)
+    mlperf_logger.barrier()
+    mlperf_logger.log_start(key=mlperf_logger.constants.RUN_START)
+    mlperf_logger.barrier()
+
+    if (args.data_generation == "dataset"):
+        train_data, train_ld, test_data, test_ld = \
+            dp.make_criteo_data_and_loaders(args)
         nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
         nbatches_test = len(test_ld)
 
         ln_emb = train_data.counts
         # enforce maximum limit on number of vectors per embedding
         if args.max_ind_range > 0:
-            ln_emb = np.array(
-                list(
-                    map(
-                        lambda x: x if x < args.max_ind_range else args.max_ind_range,
-                        ln_emb,
-                    )
-                )
-            )
-        else:
-            ln_emb = np.array(ln_emb)
+            ln_emb = np.array(list(map(
+                lambda x: x if x < args.max_ind_range else args.max_ind_range,
+                ln_emb
+            )))
         m_den = train_data.m_den
         ln_bot[0] = m_den
+
     else:
         # input and target at random
         ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
         m_den = ln_bot[0]
-        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
         nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
-        nbatches_test = len(test_ld)
-
-    args.ln_emb = ln_emb.tolist()
-    if args.mlperf_logging:
-        print("command line args: ", json.dumps(vars(args)))
 
     ### parse command line arguments ###
     m_spa = args.arch_sparse_feature_size
-    ln_emb = np.asarray(ln_emb)
     num_fea = ln_emb.size + 1  # num sparse + num dense features
-
     m_den_out = ln_bot[ln_bot.size - 1]
     if args.arch_interaction_op == "dot":
         # approach 1: all
@@ -1191,7 +899,7 @@ def run():
             torch.tensor(ln_emb),
             args.md_temperature,  # alpha
             d0=m_spa,
-            round_dim=args.md_round_dims,
+            round_dim=args.md_round_dims
         ).tolist()
 
     # test prints (model arch)
@@ -1227,37 +935,32 @@ def run():
         print(ln_emb)
 
         print("data (inputs and targets):")
-        for j, inputBatch in enumerate(train_ld):
-            X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)
-
-            torch.set_printoptions(precision=4)
+        for j, (X, lS_o, lS_i, T) in enumerate(train_ld):
             # early exit if nbatches was set by the user and has been exceeded
             if nbatches > 0 and j >= nbatches:
                 break
+
             print("mini-batch: %d" % j)
-            print(X.detach().cpu())
+            print(X.detach().cpu().numpy())
             # transform offsets to lengths when printing
             print(
-                torch.IntTensor(
-                    [
-                        np.diff(
-                            S_o.detach().cpu().tolist() + list(lS_i[i].shape)
-                        ).tolist()
-                        for i, S_o in enumerate(lS_o)
-                    ]
-                )
+                [
+                    np.diff(
+                        S_o.detach().cpu().tolist() + list(lS_i[i].shape)
+                    ).tolist()
+                    for i, S_o in enumerate(lS_o)
+                ]
             )
-            print([S_i.detach().cpu() for S_i in lS_i])
-            print(T.detach().cpu())
+            print([S_i.detach().cpu().tolist() for S_i in lS_i])
+            print(T.detach().cpu().numpy())
 
-    global ndevices
     ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
 
     ### construct the neural network specified above ###
     # WARNING: to obtain exactly the same initialization for
     # the weights we need to start from the same random seed.
     # np.random.seed(args.numpy_rand_seed)
-    global dlrm
+    print('Creating the model...')
     dlrm = DLRM_Net(
         m_spa,
         ln_emb,
@@ -1276,10 +979,12 @@ def run():
         qr_threshold=args.qr_threshold,
         md_flag=args.md_flag,
         md_threshold=args.md_threshold,
-        weighted_pooling=args.weighted_pooling,
-        loss_function=args.loss_function
+        sparse_dense_boundary=args.sparse_dense_boundary,
+        bf16 = args.bf16,
+        use_ipex = args.use_ipex
     )
-
+    
+    print('Model created!')
     # test prints
     if args.debug_mode:
         print("initial parameters (weights and bias):")
@@ -1287,21 +992,18 @@ def run():
             print(param.detach().cpu().numpy())
         # print(dlrm)
 
+    if args.use_ipex:
+       dlrm = dlrm.to(device)
+       print(dlrm, device, args.use_ipex)
+
     if use_gpu:
         # Custom Model-Data Parallel
         # the mlps are replicated and use data parallelism, while
         # the embeddings are distributed and use model parallelism
         dlrm = dlrm.to(device)  # .cuda()
         if dlrm.ndevices > 1:
-            dlrm.emb_l, dlrm.v_W_l = dlrm.create_emb(
-                m_spa, ln_emb, args.weighted_pooling
-            )
-        else:
-            if dlrm.weighted_pooling == "fixed":
-                for k, w in enumerate(dlrm.v_W_l):
-                    dlrm.v_W_l[k] = w.cuda()
+            dlrm.emb_l = dlrm.create_emb(m_spa, ln_emb)
 
-    # distribute data parallel mlps
     if ext_dist.my_size > 1:
         if use_gpu:
             device_ids = [ext_dist.my_local_rank]
@@ -1310,66 +1012,121 @@ def run():
         else:
             dlrm.bot_l = ext_dist.DDP(dlrm.bot_l)
             dlrm.top_l = ext_dist.DDP(dlrm.top_l)
+            for i in range(len(dlrm.emb_dense)):
+                dlrm.emb_dense[i] = ext_dist.DDP(dlrm.emb_dense[i])
+
+    # specify the loss function
+    if args.loss_function == "mse":
+        loss_fn = torch.nn.MSELoss(reduction="mean")
+    elif args.loss_function == "bce":
+        loss_fn = torch.nn.BCELoss(reduction="mean")
+    elif args.loss_function == "wbce":
+        loss_ws = torch.tensor(np.fromstring(args.loss_weights, dtype=float, sep="-"))
+        loss_fn = torch.nn.BCELoss(reduction="none")
+    else:
+        sys.exit("ERROR: --loss-function=" + args.loss_function + " is not supported")
 
     if not args.inference_only:
-        if use_gpu and args.optimizer in ["rwsadagrad", "adagrad"]:
-            sys.exit("GPU version of Adagrad is not supported by PyTorch.")
         # specify the optimizer algorithm
-        opts = {
-            "sgd": torch.optim.SGD,
-            "rwsadagrad": RowWiseSparseAdagrad.RWSAdagrad,
-            "adagrad": torch.optim.Adagrad,
-        }
-
-        parameters = (
-            dlrm.parameters()
-            if ext_dist.my_size == 1
-            else [
-                {
-                    "params": [p for emb in dlrm.emb_l for p in emb.parameters()],
-                    "lr": args.learning_rate,
-                },
-                # TODO check this lr setup
-                # bottom mlp has no data parallelism
-                # need to check how do we deal with top mlp
-                {
-                    "params": dlrm.bot_l.parameters(),
-                    "lr": args.learning_rate,
-                },
-                {
-                    "params": dlrm.top_l.parameters(),
-                    "lr": args.learning_rate,
-                },
-            ]
-        )
-        optimizer = opts[args.optimizer](parameters, lr=args.learning_rate)
-        lr_scheduler = LRPolicyScheduler(
-            optimizer,
-            args.lr_num_warmup_steps,
-            args.lr_decay_start_step,
-            args.lr_num_decay_steps,
-        )
+        optimizer_list = ([torch.optim.SGD, ([Lamb, False], torch.optim.SGD),
+                           torch.optim.Adagrad, ([torch.optim.Adam, None], torch.optim.SparseAdam)],
+                          [ipex.SplitSGD, ([Lamb, True], ipex.SplitSGD)])
+        optimizers = optimizer_list[args.bf16 and ipex.is_available()][args.optimizer]
+        print('Chosen optimizer(s): %s' % str(optimizers))
+
+        if ext_dist.my_size == 1:
+            if len(optimizers) == 1:
+                optimizer = optimizers(dlrm.parameters(), lr=args.learning_rate)
+            else:
+                optimizer_dense = optimizers[0][0]([
+                    {"params": dlrm.bot_l.parameters(), "lr": args.learning_rate},
+                    {"params": dlrm.top_l.parameters(), "lr": args.learning_rate}
+                ], lr=args.learning_rate)
+                if optimizers[0][1] is not None:
+                    optimizer_dense.set_bf16(optimizers[0][1])
+                optimizer_sparse = optimizers[1]([
+                    {"params": [p for emb in dlrm.emb_l for p in emb.parameters()], "lr": args.learning_rate},
+                ], lr=args.learning_rate)
+                optimizer = (optimizer_dense, optimizer_sparse)
+        else:
+            if len(optimizers) == 1:
+                optimizer = optimizers([
+                    {"params": [p for emb in dlrm.emb_sparse for p in emb.parameters()],
+                     "lr": args.learning_rate / ext_dist.my_size},
+                    {"params": [p for emb in dlrm.emb_dense for p in emb.parameters()], "lr": args.learning_rate},
+                    {"params": dlrm.bot_l.parameters(), "lr": args.learning_rate},
+                    {"params": dlrm.top_l.parameters(), "lr": args.learning_rate}
+                ], lr=args.learning_rate)
+            else:
+                optimizer_dense = optimizers[0][0]([
+                    {"params": [p for emb in dlrm.emb_dense for p in emb.parameters()], "lr": args.lamblr},
+                    {"params": dlrm.bot_l.parameters(), "lr": args.lamblr},
+                    {"params": dlrm.top_l.parameters(), "lr": args.lamblr}
+                ], lr=args.lamblr, bf16=args.bf16)
+                optimizer_sparse = optimizers[1]([
+                    {"params": [p for emb in dlrm.emb_sparse for p in emb.parameters()],
+                     "lr": args.learning_rate / ext_dist.my_size},
+                ], lr=args.learning_rate)
+                optimizer = (optimizer_dense, optimizer_sparse)
+        lr_scheduler = LRPolicyScheduler(optimizer, args.lr_num_warmup_steps, args.lr_decay_start_step,
+                                      args.lr_num_decay_steps)
 
     ### main loop ###
+    def time_wrap(use_gpu):
+        if use_gpu:
+            torch.cuda.synchronize()
+        return time.time()
+
+    def dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device):
+        if use_gpu or use_ipex:  # .cuda()
+            # lS_i can be either a list of tensors or a stacked tensor.
+            # Handle each case below:
+            lS_i = [S_i.to(device) for S_i in lS_i] if isinstance(lS_i, list) \
+                else lS_i.to(device)
+            lS_o = [S_o.to(device) for S_o in lS_o] if isinstance(lS_o, list) \
+                else lS_o.to(device)
+            return dlrm(
+                X.to(device),
+                lS_o,
+                lS_i
+            )
+        else:
+            return dlrm(X, lS_o, lS_i)
+
+    def loss_fn_wrap(Z, T, use_gpu, use_ipex, device):
+        if args.loss_function == "mse" or args.loss_function == "bce":
+            if use_gpu or use_ipex:
+                return loss_fn(Z, T.to(device))
+            else:
+                return loss_fn(Z, T)
+        elif args.loss_function == "wbce":
+            if use_gpu:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T).to(device)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            else:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            loss_sc_ = loss_ws_ * loss_fn_
+            # debug prints
+            # print(loss_ws_)
+            # print(loss_fn_)
+            return loss_sc_.mean()
 
     # training or inference
-    best_acc_test = 0
+    best_gA_test = 0
     best_auc_test = 0
     skip_upto_epoch = 0
     skip_upto_batch = 0
     total_time = 0
     total_loss = 0
+    total_accu = 0
     total_iter = 0
     total_samp = 0
+    k = 0
 
-    if args.mlperf_logging:
-        mlperf_logger.mlperf_submission_log("dlrm")
-        mlperf_logger.log_event(
-            key=mlperf_logger.constants.SEED, value=args.numpy_rand_seed
-        )
-        mlperf_logger.log_event(
-            key=mlperf_logger.constants.GLOBAL_BATCH_SIZE, value=args.mini_batch_size
-        )
+    mlperf_logger.mlperf_submission_log('dlrm')
+    mlperf_logger.log_event(key=mlperf_logger.constants.SEED, value=args.numpy_rand_seed)
+    mlperf_logger.log_event(key=mlperf_logger.constants.GLOBAL_BATCH_SIZE, value=args.mini_batch_size)
 
     # Load model is specified
     if not (args.load_model == ""):
@@ -1385,27 +1142,29 @@ def run():
                 # note that the call to .to(device) has already happened
                 ld_model = torch.load(
                     args.load_model,
-                    map_location=torch.device("cuda")
+                    map_location=torch.device('cuda')
                     # map_location=lambda storage, loc: storage.cuda(0)
                 )
         else:
             # when targeting inference on CPU
-            ld_model = torch.load(args.load_model, map_location=torch.device("cpu"))
+            ld_model = torch.load(args.load_model, map_location=torch.device('cpu'))
         dlrm.load_state_dict(ld_model["state_dict"])
         ld_j = ld_model["iter"]
         ld_k = ld_model["epoch"]
         ld_nepochs = ld_model["nepochs"]
         ld_nbatches = ld_model["nbatches"]
         ld_nbatches_test = ld_model["nbatches_test"]
-        ld_train_loss = ld_model["train_loss"]
+        ld_gA = ld_model["train_acc"]
+        ld_gL = ld_model["train_loss"]
         ld_total_loss = ld_model["total_loss"]
-        if args.mlperf_logging:
-            ld_gAUC_test = ld_model["test_auc"]
-        ld_acc_test = ld_model["test_acc"]
+        ld_total_accu = ld_model["total_accu"]
+        ld_gA_test = ld_model["test_acc"]
+        ld_gL_test = ld_model["test_loss"]
         if not args.inference_only:
             optimizer.load_state_dict(ld_model["opt_state_dict"])
-            best_acc_test = ld_acc_test
+            best_gA_test = ld_gA_test
             total_loss = ld_total_loss
+            total_accu = ld_total_accu
             skip_upto_epoch = ld_k  # epochs
             skip_upto_batch = ld_j  # batches
         else:
@@ -1418,460 +1177,395 @@ def run():
             )
         )
         print(
-            "Training state: loss = {:.6f}".format(
-                ld_train_loss,
+            "Training state: loss = {:.6f}, accuracy = {:3.3f} %".format(
+                ld_gL, ld_gA * 100
             )
         )
-        if args.mlperf_logging:
-            print(
-                "Testing state: accuracy = {:3.3f} %, auc = {:.3f}".format(
-                    ld_acc_test * 100, ld_gAUC_test
-                )
-            )
-        else:
-            print("Testing state: accuracy = {:3.3f} %".format(ld_acc_test * 100))
-
-    if args.inference_only:
-        # Currently only dynamic quantization with INT8 and FP16 weights are
-        # supported for MLPs and INT4 and INT8 weights for EmbeddingBag
-        # post-training quantization during the inference.
-        # By default we don't do the quantization: quantize_{mlp,emb}_with_bit == 32 (FP32)
-        assert args.quantize_mlp_with_bit in [
-            8,
-            16,
-            32,
-        ], "only support 8/16/32-bit but got {}".format(args.quantize_mlp_with_bit)
-        assert args.quantize_emb_with_bit in [
-            4,
-            8,
-            32,
-        ], "only support 4/8/32-bit but got {}".format(args.quantize_emb_with_bit)
-        if args.quantize_mlp_with_bit != 32:
-            if args.quantize_mlp_with_bit in [8]:
-                quantize_dtype = torch.qint8
-            else:
-                quantize_dtype = torch.float16
-            dlrm = torch.quantization.quantize_dynamic(
-                dlrm, {torch.nn.Linear}, quantize_dtype
+        print(
+            "Testing state: loss = {:.6f}, accuracy = {:3.3f} %".format(
+                ld_gL_test, ld_gA_test * 100
             )
-        if args.quantize_emb_with_bit != 32:
-            dlrm.quantize_embedding(args.quantize_emb_with_bit)
-            # print(dlrm)
-
-    print("time/loss/accuracy (if enabled):")
-
-    if args.mlperf_logging:
-        # LR is logged twice for now because of a compliance checker bug
-        mlperf_logger.log_event(
-            key=mlperf_logger.constants.OPT_BASE_LR, value=args.learning_rate
         )
-        mlperf_logger.log_event(
-            key=mlperf_logger.constants.OPT_LR_WARMUP_STEPS,
-            value=args.lr_num_warmup_steps,
-        )
-
-        # use logging keys from the official HP table and not from the logging library
-        mlperf_logger.log_event(
-            key="sgd_opt_base_learning_rate", value=args.learning_rate
-        )
-        mlperf_logger.log_event(
-            key="lr_decay_start_steps", value=args.lr_decay_start_step
-        )
-        mlperf_logger.log_event(
-            key="sgd_opt_learning_rate_decay_steps", value=args.lr_num_decay_steps
-        )
-        mlperf_logger.log_event(key="sgd_opt_learning_rate_decay_poly_power", value=2)
-
-    tb_file = "./" + args.tensor_board_filename
-    writer = SummaryWriter(tb_file)
 
     ext_dist.barrier()
-    with torch.autograd.profiler.profile(
-        args.enable_profiling, use_cuda=use_gpu, record_shapes=True
-    ) as prof:
-        if not args.inference_only:
-            k = 0
-            total_time_begin = 0
-            while k < args.nepochs:
-                if args.mlperf_logging:
-                    mlperf_logger.barrier()
-                    mlperf_logger.log_start(
-                        key=mlperf_logger.constants.BLOCK_START,
-                        metadata={
-                            mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1),
-                            mlperf_logger.constants.EPOCH_COUNT: 1,
-                        },
-                    )
-                    mlperf_logger.barrier()
-                    mlperf_logger.log_start(
-                        key=mlperf_logger.constants.EPOCH_START,
-                        metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},
-                    )
+    print("time/loss/accuracy (if enabled):")
 
-                if k < skip_upto_epoch:
+    # LR is logged twice for now because of a compliance checker bug
+    mlperf_logger.log_event(key=mlperf_logger.constants.OPT_BASE_LR, value=args.learning_rate)
+    mlperf_logger.log_event(key=mlperf_logger.constants.OPT_LR_WARMUP_STEPS,
+                            value=args.lr_num_warmup_steps)
+
+    # use logging keys from the official HP table and not from the logging library
+    mlperf_logger.log_event(key='sgd_opt_base_learning_rate', value=args.learning_rate)
+    mlperf_logger.log_event(key='lr_decay_start_steps', value=args.lr_decay_start_step)
+    mlperf_logger.log_event(key='sgd_opt_learning_rate_decay_steps', value=args.lr_num_decay_steps)
+    mlperf_logger.log_event(key='sgd_opt_learning_rate_decay_poly_power', value=2)
+
+    # record_shapes=True
+    # if hasattr(torch.autograd.profiler.profile, "resume"):
+    #     prof_support_suspend_resume = True
+    #     prof_arg_dict = {"start_suspended": True}
+    # else:
+    #     prof_support_suspend_resume = False
+    #     prof_arg_dict = { }
+
+    # prof_start_iter = args.profiling_start_iter
+    # prof_end_iter = prof_start_iter + args.profiling_num_iters
+    train_start = time.time()
+    # with torch.autograd.profiler.profile(args.enable_profiling, use_gpu, record_shapes=record_shapes, **prof_arg_dict) as prof:
+    with torch.autograd.profiler.profile(args.enable_profiling, use_gpu) as prof:
+        while k < args.nepochs:
+            mlperf_logger.barrier()
+            mlperf_logger.log_start(key=mlperf_logger.constants.BLOCK_START,
+                                    metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1),
+                                              mlperf_logger.constants.EPOCH_COUNT: 1})
+            mlperf_logger.barrier()
+            mlperf_logger.log_start(key=mlperf_logger.constants.EPOCH_START,
+                                    metadata={mlperf_logger.constants.EPOCH_NUM: k + 1})
+
+            if k < skip_upto_epoch:
+                continue
+
+            accum_time_begin = time_wrap(use_gpu)
+
+            if args.mlperf_logging:
+                previous_iteration_time = None
+
+            for j, (X, lS_o, lS_i, T) in enumerate(train_ld):
+                if j == 0 and args.save_onnx:
+                    (X_onnx, lS_o_onnx, lS_i_onnx) = (X, lS_o, lS_i)
+
+                if j < skip_upto_batch:
                     continue
 
                 if args.mlperf_logging:
-                    previous_iteration_time = None
+                    current_time = time_wrap(use_gpu)
+                    if previous_iteration_time:
+                        iteration_time = current_time - previous_iteration_time
+                    else:
+                        iteration_time = 0
+                    previous_iteration_time = current_time
+                    # if prof and prof_support_suspend_resume and j == prof_start_iter: prof.resume()
+                    # if prof and prof_support_suspend_resume and j == prof_end_iter: prof.suspend()
+                else:
+                    # ext_dist.barrier()
+                    # if prof and prof_support_suspend_resume and j >= prof_start_iter and j < prof_end_iter: prof.resume()
+                    t1 = time_wrap(use_gpu)
+
+                # early exit if nbatches was set by the user and has been exceeded
+                if nbatches > 0 and j >= nbatches:
+                    break
+                '''
+                # debug prints
+                print("input and targets")
+                print(X.detach().cpu().numpy())
+                print([np.diff(S_o.detach().cpu().tolist()
+                       + list(lS_i[i].shape)).tolist() for i, S_o in enumerate(lS_o)])
+                print([S_i.detach().cpu().numpy().tolist() for S_i in lS_i])
+                print(T.detach().cpu().numpy())
+                '''
+
+                # forward pass
+                Z = dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device)
+
+                # loss
+                E = loss_fn_wrap(Z, T, use_gpu, use_ipex, device)
+                '''
+                # debug prints
+                print("output and loss")
+                print(Z.detach().cpu().numpy())
+                print(E.detach().cpu().numpy())
+                '''
+                # compute loss and accuracy
+                L = E.detach().cpu().numpy()  # numpy array
+                S = Z.detach().cpu().numpy()  # numpy array
+                T = T.detach().cpu().numpy()  # numpy array
+                mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
+                A = np.sum((np.round(S, 0) == T).astype(np.uint8))
+
+                if not args.inference_only:
+                    # scaled error gradient propagation
+                    # (where we do not accumulate gradients across mini-batches)
+                    if args.optimizer == 1 or args.optimizer == 3:
+                        optimizer_dense.zero_grad()
+                        optimizer_sparse.zero_grad()
+                    else:
+                        optimizer.zero_grad()
+                    # backward pass
+                    E.backward()
+                    # debug prints (check gradient norm)
+                    # for l in mlp.layers:
+                    #     if hasattr(l, 'weight'):
+                    #          print(l.weight.grad.norm().item())
+
+                    # optimizer
+                    if args.optimizer == 1 or args.optimizer == 3:
+                        optimizer_dense.step()
+                        optimizer_sparse.step()
+                    else:
+                        optimizer.step()
+                    lr_scheduler.step()
 
-                for j, inputBatch in enumerate(train_ld):
-                    if j == 0 and args.save_onnx:
-                        X_onnx, lS_o_onnx, lS_i_onnx, _, _, _ = unpack_batch(inputBatch)
+                if args.mlperf_logging:
+                    total_time += iteration_time
+                else:
+                    t2 = time_wrap(use_gpu)
+                    total_time += t2 - t1
+                total_accu += A
+                total_loss += L * mbs
+                total_iter += 1
+                total_samp += mbs
+
+                should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)
+                should_test = (
+                    (args.test_freq > 0)
+                    and (args.data_generation == "dataset")
+                    and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
+                )
 
-                    if j < skip_upto_batch:
-                        continue
+                # print time, loss and accuracy
+                if should_print or should_test:
+                    gT = 1000.0 * total_time / total_iter if args.print_time else -1
+                    total_time = 0
 
-                    X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)
+                    gA = total_accu / total_samp
+                    total_accu = 0
 
-                    if args.mlperf_logging:
-                        current_time = time_wrap(use_gpu)
-                        if previous_iteration_time:
-                            iteration_time = current_time - previous_iteration_time
-                        else:
-                            iteration_time = 0
-                        previous_iteration_time = current_time
-                    else:
-                        t1 = time_wrap(use_gpu)
-
-                    # early exit if nbatches was set by the user and has been exceeded
-                    if nbatches > 0 and j >= nbatches:
-                        break
+                    gL = total_loss / total_samp
+                    total_loss = 0
 
-                    # Skip the batch if batch size not multiple of total ranks
-                    if ext_dist.my_size > 1 and X.size(0) % ext_dist.my_size != 0:
-                        print(
-                            "Warning: Skiping the batch %d with size %d"
-                            % (j, X.size(0))
+                    str_run_type = "inference" if args.inference_only else "training"
+                    print(
+                        "Finished {} it {}/{} of epoch {}, {:.2f} ms/it, ".format(
+                            str_run_type, j + 1, nbatches, k, gT
                         )
-                        continue
-
-                    mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
-
-                    # forward pass
-                    Z = dlrm_wrap(
-                        X,
-                        lS_o,
-                        lS_i,
-                        use_gpu,
-                        device,
-                        ndevices=ndevices,
+                        + "loss {:.6f}, accuracy {:3.3f} %".format(gL, gA * 100)
                     )
+                    # Uncomment the line below to print out the total time with overhead
+                    # print("Accumulated time so far: {}" \
+                    # .format(time_wrap(use_gpu) - accum_time_begin))
+                    total_iter = 0
+                    total_samp = 0
+
+                # testing
+                if should_test and not args.inference_only:
+                    test_start = time.time()
+                    epoch_num_float = (j + 1) / len(train_ld) + k + 1
+                    mlperf_logger.barrier()
+                    mlperf_logger.log_start(key=mlperf_logger.constants.EVAL_START,
+                                            metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
 
-                    if ext_dist.my_size > 1:
-                        T = T[ext_dist.get_my_slice(mbs)]
-                        W = W[ext_dist.get_my_slice(mbs)]
-
-                    # loss
-                    E = loss_fn_wrap(Z, T, use_gpu, device)
-
-                    # compute loss and accuracy
-                    L = E.detach().cpu().numpy()  # numpy array
-                    # training accuracy is not disabled
-                    # S = Z.detach().cpu().numpy()  # numpy array
-                    # T = T.detach().cpu().numpy()  # numpy array
-
-                    # # print("res: ", S)
-
-                    # # print("j, train: BCE ", j, L)
-
-                    # mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
-                    # A = np.sum((np.round(S, 0) == T).astype(np.uint8))
-
-                    with record_function("DLRM backward"):
-                        # scaled error gradient propagation
-                        # (where we do not accumulate gradients across mini-batches)
-                        if (args.mlperf_logging and (j + 1) % args.mlperf_grad_accum_iter == 0) or not args.mlperf_logging:
-                            optimizer.zero_grad()
-                        # backward pass
-                        E.backward()
-
-                        # optimizer
-                        if (args.mlperf_logging and (j + 1) % args.mlperf_grad_accum_iter == 0) or not args.mlperf_logging:
-                            optimizer.step()
-                            lr_scheduler.step()
-
+                    # don't measure training iter time in a test iteration
                     if args.mlperf_logging:
-                        total_time += iteration_time
-                    else:
-                        t2 = time_wrap(use_gpu)
-                        total_time += t2 - t1
+                        previous_iteration_time = None
 
-                    total_loss += L * mbs
-                    total_iter += 1
-                    total_samp += mbs
+                    test_accu = 0
+                    test_loss = 0
+                    test_samp = 0
 
-                    should_print = ((j + 1) % args.print_freq == 0) or (
-                        j + 1 == nbatches
-                    )
-                    should_test = (
-                        (args.test_freq > 0)
-                        and (args.data_generation in ["dataset", "random"])
-                        and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
-                    )
+                    accum_test_time_begin = time_wrap(use_gpu)
+                    if args.mlperf_logging:
+                        scores = []
+                        targets = []
 
-                    # print time, loss and accuracy
-                    if should_print or should_test:
-                        gT = 1000.0 * total_time / total_iter if args.print_time else -1
-                        total_time = 0
+                    for i, (X_test, lS_o_test, lS_i_test, T_test) in enumerate(test_ld):
+                        # early exit if nbatches was set by the user and was exceeded
+                        if nbatches > 0 and i >= nbatches:
+                            break
 
-                        train_loss = total_loss / total_samp
-                        total_loss = 0
+                        t1_test = time_wrap(use_gpu)
 
-                        str_run_type = (
-                            "inference" if args.inference_only else "training"
+                        # forward pass
+                        Z_test = dlrm_wrap(
+                            X_test, lS_o_test, lS_i_test, use_gpu, use_ipex, device
                         )
+                        if args.mlperf_logging:
+                            if ext_dist.my_size > 1:
+                                Z_test = ext_dist.all_gather(Z_test, None)
+                                T_test = ext_dist.all_gather(T_test, None)
+                            S_test = Z_test.detach().cpu().numpy()  # numpy array
+                            T_test = T_test.detach().cpu().numpy()  # numpy array
+                            scores.append(S_test)
+                            targets.append(T_test)
+                        else:
+                            # loss
+                            E_test = loss_fn_wrap(Z_test, T_test, use_gpu, use_ipex, device)
 
-                        wall_time = ""
-                        if args.print_wall_time:
-                            wall_time = " ({})".format(time.strftime("%H:%M"))
+                            # compute loss and accuracy
+                            L_test = E_test.detach().cpu().numpy()  # numpy array
+                            S_test = Z_test.detach().cpu().numpy()  # numpy array
+                            T_test = T_test.detach().cpu().numpy()  # numpy array
+                            mbs_test = T_test.shape[0]  # = mini_batch_size except last
+                            A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))
+                            test_accu += A_test
+                            test_loss += L_test * mbs_test
+                            test_samp += mbs_test
 
-                        print(
-                            "Finished {} it {}/{} of epoch {}, {:.2f} ms/it,".format(
-                                str_run_type, j + 1, nbatches, k, gT
-                            )
-                            + " loss {:.6f}".format(train_loss)
-                            + wall_time,
-                            flush=True,
-                        )
+                        t2_test = time_wrap(use_gpu)
 
-                        log_iter = nbatches * k + j + 1
-                        writer.add_scalar("Train/Loss", train_loss, log_iter)
+                    if args.mlperf_logging:
+                        scores = np.concatenate(scores, axis=0)
+                        targets = np.concatenate(targets, axis=0)
 
-                        total_iter = 0
-                        total_samp = 0
+                        validation_results = {}
+                        if args.use_ipex:
+                            validation_results['roc_auc'], validation_results['loss'], validation_results['accuracy'] = \
+                                core.roc_auc_score(torch.from_numpy(targets).reshape(-1), torch.from_numpy(scores).reshape(-1))
+                        else:
+                            metrics = {
+                                'loss' : sklearn.metrics.log_loss,
+                                'recall' : lambda y_true, y_score:
+                                sklearn.metrics.recall_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'precision' : lambda y_true, y_score:
+                                sklearn.metrics.precision_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'f1' : lambda y_true, y_score:
+                                sklearn.metrics.f1_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'ap' : sklearn.metrics.average_precision_score,
+                                'roc_auc' : sklearn.metrics.roc_auc_score,
+                                'accuracy' : lambda y_true, y_score:
+                                sklearn.metrics.accuracy_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                            }
+
+                            # print("Compute time for validation metric : ", end="")
+                            # first_it = True
+                            for metric_name, metric_function in metrics.items():
+                                # if first_it:
+                                #     first_it = False
+                                # else:
+                                #     print(", ", end="")
+                                # metric_compute_start = time_wrap(False)
+                                validation_results[metric_name] = metric_function(
+                                    targets,
+                                    scores
+                                )
+                                # metric_compute_end = time_wrap(False)
+                                # met_time = metric_compute_end - metric_compute_start
+                                # print("{} {:.4f}".format(metric_name, 1000 * (met_time)),
+                                #      end="")
+
+                        # print(" ms")
+                        gA_test = validation_results['accuracy']
+                        gL_test = validation_results['loss']
+                    else:
+                        gA_test = test_accu / test_samp
+                        gL_test = test_loss / test_samp
 
-                    # testing
-                    if should_test:
-                        epoch_num_float = (j + 1) / len(train_ld) + k + 1
-                        if args.mlperf_logging:
-                            mlperf_logger.barrier()
-                            mlperf_logger.log_start(
-                                key=mlperf_logger.constants.EVAL_START,
-                                metadata={
-                                    mlperf_logger.constants.EPOCH_NUM: epoch_num_float
-                                },
-                            )
+                    is_best = gA_test > best_gA_test
+                    
+                    if args.mlperf_logging:
+                        is_best = validation_results['roc_auc'] > best_auc_test
+                        if is_best:
+                            best_auc_test = validation_results['roc_auc']
 
-                        # don't measure training iter time in a test iteration
-                        if args.mlperf_logging:
-                            previous_iteration_time = None
+                        mlperf_logger.log_event(key=mlperf_logger.constants.EVAL_ACCURACY,
+                                                value=float(validation_results['roc_auc']),
+                                                metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
                         print(
                             "Testing at - {}/{} of epoch {},".format(j + 1, nbatches, k)
-                        )
-                        model_metrics_dict, is_best = inference(
-                            args,
-                            dlrm,
-                            best_acc_test,
-                            best_auc_test,
-                            test_ld,
-                            device,
-                            use_gpu,
-                            log_iter,
-                        )
-
-                        if (
-                            is_best
-                            and not (args.save_model == "")
-                            and not args.inference_only
-                        ):
-                            model_metrics_dict["epoch"] = k
-                            model_metrics_dict["iter"] = j + 1
-                            model_metrics_dict["train_loss"] = train_loss
-                            model_metrics_dict["total_loss"] = total_loss
-                            model_metrics_dict[
-                                "opt_state_dict"
-                            ] = optimizer.state_dict()
-                            print("Saving model to {}".format(args.save_model))
-                            torch.save(model_metrics_dict, args.save_model)
-
-                        if args.mlperf_logging:
-                            mlperf_logger.barrier()
-                            mlperf_logger.log_end(
-                                key=mlperf_logger.constants.EVAL_STOP,
-                                metadata={
-                                    mlperf_logger.constants.EPOCH_NUM: epoch_num_float
-                                },
+                            + " loss {:.6f},".format(
+                                validation_results['loss']
                             )
-
-                        # Uncomment the line below to print out the total time with overhead
-                        # print("Total test time for this group: {}" \
-                        # .format(time_wrap(use_gpu) - accum_test_time_begin))
-
-                        if (
-                            args.mlperf_logging
-                            and (args.mlperf_acc_threshold > 0)
-                            and (best_acc_test > args.mlperf_acc_threshold)
-                        ):
-                            print(
-                                "MLPerf testing accuracy threshold "
-                                + str(args.mlperf_acc_threshold)
-                                + " reached, stop training"
+                            + " auc {:.4f}, best auc {:.4f},".format(
+                                validation_results['roc_auc'],
+                                best_auc_test
                             )
-                            break
-
-                        if (
-                            args.mlperf_logging
-                            and (args.mlperf_auc_threshold > 0)
-                            and (best_auc_test > args.mlperf_auc_threshold)
-                        ):
-                            print(
-                                "MLPerf testing auc threshold "
-                                + str(args.mlperf_auc_threshold)
-                                + " reached, stop training"
+                            + " accuracy {:3.3f} %, best accuracy {:3.3f} %".format(
+                                validation_results['accuracy'] * 100,
+                                best_gA_test * 100
                             )
-                            if args.mlperf_logging:
-                                mlperf_logger.barrier()
-                                mlperf_logger.log_end(
-                                    key=mlperf_logger.constants.RUN_STOP,
-                                    metadata={
-                                        mlperf_logger.constants.STATUS: mlperf_logger.constants.SUCCESS
-                                    },
-                                )
-                            break
-
-                if args.mlperf_logging:
-                    mlperf_logger.barrier()
-                    mlperf_logger.log_end(
-                        key=mlperf_logger.constants.EPOCH_STOP,
-                        metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},
-                    )
+                        )
+                    else:
+                        print(
+                            "Testing at - {}/{} of epoch {},".format(j + 1, nbatches, 0)
+                            + " loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %".format(
+                                gL_test, gA_test * 100, best_gA_test * 100
+                            )
+                        )
                     mlperf_logger.barrier()
-                    mlperf_logger.log_end(
-                        key=mlperf_logger.constants.BLOCK_STOP,
-                        metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1)},
-                    )
-                k += 1  # nepochs
-            if args.mlperf_logging and best_auc_test <= args.mlperf_auc_threshold:
-                mlperf_logger.barrier()
-                mlperf_logger.log_end(
-                    key=mlperf_logger.constants.RUN_STOP,
-                    metadata={
-                        mlperf_logger.constants.STATUS: mlperf_logger.constants.ABORTED
-                    },
-                )
-        else:
-            print("Testing for inference only")
-            inference(
-                args,
-                dlrm,
-                best_acc_test,
-                best_auc_test,
-                test_ld,
-                device,
-                use_gpu,
-            )
-
-    # profiling
-    if args.enable_profiling:
-        time_stamp = str(datetime.datetime.now()).replace(" ", "_")
-        with open("dlrm_s_pytorch" + time_stamp + "_shape.prof", "w") as prof_f:
-            prof_f.write(
-                prof.key_averages(group_by_input_shape=True).table(
-                    sort_by="self_cpu_time_total"
-                )
-            )
-        with open("dlrm_s_pytorch" + time_stamp + "_total.prof", "w") as prof_f:
-            prof_f.write(prof.key_averages().table(sort_by="self_cpu_time_total"))
-        prof.export_chrome_trace("dlrm_s_pytorch" + time_stamp + ".json")
-        # print(prof.key_averages().table(sort_by="cpu_time_total"))
+                    mlperf_logger.log_end(key=mlperf_logger.constants.EVAL_STOP,
+                                          metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
+
+                    # Uncomment the line below to print out the total time with overhead
+                    # print("Total test time for this group: {}" \
+                    # .format(time_wrap(use_gpu) - accum_test_time_begin))
+                    # if ext_dist.dist.get_rank()==0:
+
+                    file1 = open("/home/vmagent/app/hydro.ai/modelzoo/dlrm/dlrm/best_auc.txt",'w')
+                    file1.writelines(str(best_auc_test))
+                    file1.close()
+
+                    if (args.mlperf_logging
+                        and (args.mlperf_acc_threshold > 0)
+                        and (best_gA_test > args.mlperf_acc_threshold)):
+                        print("MLPerf testing accuracy threshold "
+                              + str(args.mlperf_acc_threshold)
+                              + " reached, stop training")
+                        break
 
-    # plot compute graph
-    if args.plot_compute_graph:
-        sys.exit(
-            "ERROR: Please install pytorchviz package in order to use the"
-            + " visualization. Then, uncomment its import above as well as"
-            + " three lines below and run the code again."
-        )
-        # V = Z.mean() if args.inference_only else E
-        # dot = make_dot(V, params=dict(dlrm.named_parameters()))
-        # dot.render('dlrm_s_pytorch_graph') # write .pdf file
+                    if (args.mlperf_logging
+                        and (args.mlperf_auc_threshold > 0)
+                        and (best_auc_test > args.mlperf_auc_threshold)):
+                        print("MLPerf testing auc threshold "
+                              + str(args.mlperf_auc_threshold)
+                              + " reached, stop training")
+                        train_end = time.time()
+                        total_time = train_end - train_start
+                        print(F"Total Time:{total_time}")
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_end(key=mlperf_logger.constants.RUN_STOP,
+                                              metadata={
+                                                  mlperf_logger.constants.STATUS: mlperf_logger.constants.SUCCESS})
 
-    # test prints
-    if not args.inference_only and args.debug_mode:
-        print("updated parameters (weights and bias):")
-        for param in dlrm.parameters():
-            print(param.detach().cpu().numpy())
-
-    # export the model in onnx
-    if args.save_onnx:
-        """
-        # workaround 1: tensor -> list
-        if torch.is_tensor(lS_i_onnx):
-            lS_i_onnx = [lS_i_onnx[j] for j in range(len(lS_i_onnx))]
-        # workaound 2: list -> tensor
-        lS_i_onnx = torch.stack(lS_i_onnx)
-        """
-        # debug prints
-        # print("inputs", X_onnx, lS_o_onnx, lS_i_onnx)
-        # print("output", dlrm_wrap(X_onnx, lS_o_onnx, lS_i_onnx, use_gpu, device))
-        dlrm_pytorch_onnx_file = "dlrm_s_pytorch.onnx"
-        batch_size = X_onnx.shape[0]
-        print("X_onnx.shape", X_onnx.shape)
-        if torch.is_tensor(lS_o_onnx):
-            print("lS_o_onnx.shape", lS_o_onnx.shape)
-        else:
-            for oo in lS_o_onnx:
-                print("oo.shape", oo.shape)
-        if torch.is_tensor(lS_i_onnx):
-            print("lS_i_onnx.shape", lS_i_onnx.shape)
-        else:
-            for ii in lS_i_onnx:
-                print("ii.shape", ii.shape)
-
-        # name inputs and outputs
-        o_inputs = (
-            ["offsets"]
-            if torch.is_tensor(lS_o_onnx)
-            else ["offsets_" + str(i) for i in range(len(lS_o_onnx))]
-        )
-        i_inputs = (
-            ["indices"]
-            if torch.is_tensor(lS_i_onnx)
-            else ["indices_" + str(i) for i in range(len(lS_i_onnx))]
-        )
-        all_inputs = ["dense_x"] + o_inputs + i_inputs
-        # debug prints
-        print("inputs", all_inputs)
-
-        # create dynamic_axis dictionaries
-        do_inputs = (
-            [{"offsets": {1: "batch_size"}}]
-            if torch.is_tensor(lS_o_onnx)
-            else [
-                {"offsets_" + str(i): {0: "batch_size"}} for i in range(len(lS_o_onnx))
-            ]
-        )
-        di_inputs = (
-            [{"indices": {1: "batch_size"}}]
-            if torch.is_tensor(lS_i_onnx)
-            else [
-                {"indices_" + str(i): {0: "batch_size"}} for i in range(len(lS_i_onnx))
-            ]
-        )
-        dynamic_axes = {"dense_x": {0: "batch_size"}, "pred": {0: "batch_size"}}
-        for do in do_inputs:
-            dynamic_axes.update(do)
-        for di in di_inputs:
-            dynamic_axes.update(di)
-        # debug prints
-        print(dynamic_axes)
-        # export model
-        torch.onnx.export(
-            dlrm,
-            (X_onnx, lS_o_onnx, lS_i_onnx),
-            dlrm_pytorch_onnx_file,
-            verbose=True,
-            opset_version=11,
-            input_names=all_inputs,
-            output_names=["pred"],
-            dynamic_axes=dynamic_axes,
+                        break
+                    test_end = time.time()
+                    print(F"Test time:{test_end - test_start}")
+                    ext_dist.barrier()
+                    
+                    # if (j>1) and (j+1)%13600 ==0:
+                    #     print(F"Fineshed 13600 steps training")
+                    #     break
+
+            mlperf_logger.barrier()
+            mlperf_logger.log_end(key=mlperf_logger.constants.EPOCH_STOP,
+                                  metadata={mlperf_logger.constants.EPOCH_NUM: k + 1})
+            mlperf_logger.barrier()
+            mlperf_logger.log_end(key=mlperf_logger.constants.BLOCK_STOP,
+                                  metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: k + 1})
+            k += 1  # nepochs
+    train_end = time.time()
+    total_time = train_end - train_start
+    print(F"Total Time:{total_time}")
+    if not (args.save_model == ""):
+        save_start = time.time()
+        print("Saving model to {}".format(args.save_model))
+        dlrm.to(torch.device("cpu"))
+        torch.save(
+            {
+                "epoch": k,
+                "nepochs": args.nepochs,
+                "nbatches": nbatches,
+                "nbatches_test": nbatches_test,
+                "iter": j + 1,
+                "state_dict": dlrm.state_dict(),
+                "train_acc": gA,
+                "train_loss": gL,
+                "test_acc": gA_test,
+                "test_loss": gL_test,
+                "total_loss": total_loss,
+                "total_accu": total_accu
+            },
+            os.path.join(args.save_model, "dlrm_s_pytorch_" + str(dlrm.rank) + "_best.pkl"),
         )
-        # recover the model back
-        dlrm_pytorch_onnx = onnx.load("dlrm_s_pytorch.onnx")
-        # check the onnx model
-        onnx.checker.check_model(dlrm_pytorch_onnx)
-    total_time_end = time_wrap(use_gpu)
-
-
-if __name__ == "__main__":
-    run()
+        print("Saved model to {}".format(args.save_model))
\ No newline at end of file
diff --git a/dlrm_s_pytorch_compress.py b/dlrm_s_pytorch_compress.py
new file mode 100644
index 0000000..6e177ec
--- /dev/null
+++ b/dlrm_s_pytorch_compress.py
@@ -0,0 +1,1681 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+#
+# Description: an implementation of a deep learning recommendation model (DLRM)
+# The model input consists of dense and sparse features. The former is a vector
+# of floating point values. The latter is a list of sparse indices into
+# embedding tables, which consist of vectors of floating point values.
+# The selected vectors are passed to mlp networks denoted by triangles,
+# in some cases the vectors are interacted through operators (Ops).
+#
+# output:
+#                         vector of values
+# model:                        |
+#                              /\
+#                             /__\
+#                               |
+#       _____________________> Op  <___________________
+#     /                         |                      \
+#    /\                        /\                      /\
+#   /__\                      /__\           ...      /__\
+#    |                          |                       |
+#    |                         Op                      Op
+#    |                    ____/__\_____           ____/__\____
+#    |                   |_Emb_|____|__|    ...  |_Emb_|__|___|
+# input:
+# [ dense features ]     [sparse indices] , ..., [sparse indices]
+#
+# More precise definition of model layers:
+# 1) fully connected layers of an mlp
+# z = f(y)
+# y = Wx + b
+#
+# 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])
+# z = Op(e1,...,ek)
+# obtain vectors e1=E[:,p1], ..., ek=E[:,pk]
+#
+# 3) Operator Op can be one of the following
+# Sum(e1,...,ek) = e1 + ... + ek
+# Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]
+# Cat(e1,...,ek) = [e1', ..., ek']'
+# where ' denotes transpose operation
+#
+# References:
+# [1] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
+# Narayanan Sundaram, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
+# Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii,
+# Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,
+# Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,
+# Misha Smelyanskiy, "Deep Learning Recommendation Model for Personalization and
+# Recommendation Systems", CoRR, arXiv:1906.00091, 2019
+
+from __future__ import absolute_import, division, print_function, unicode_literals
+
+# For model compression
+import distiller
+
+# miscellaneous
+import builtins
+import functools
+# import bisect
+# import shutil
+import time
+import json
+import math
+
+from numpy.core.fromnumeric import compress
+# data generation
+import dlrm_data_pytorch as dp
+
+# numpy
+import numpy as np
+
+# onnx
+# The onnx import causes deprecation warnings every time workers
+# are spawned during testing. So, we filter out those warnings.
+import warnings
+with warnings.catch_warnings():
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+import onnx
+
+# pytorch
+import torch
+import torch.nn as nn
+from torch.nn.parallel.parallel_apply import parallel_apply
+from torch.nn.parallel.replicate import replicate
+from torch.nn.parallel.scatter_gather import gather, scatter
+
+# For distributed run
+import extend_distributed as ext_dist
+
+try:
+    import intel_pytorch_extension as ipex
+    from intel_pytorch_extension import core
+except:
+    pass
+from lamb_bin import Lamb, log_lamb_rs
+
+# quotient-remainder trick
+from tricks.qr_embedding_bag import QREmbeddingBag
+# mixed-dimension trick
+from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
+
+import sklearn.metrics
+import mlperf_logger
+
+# from torchviz import make_dot
+# import torch.nn.functional as Functional
+# from torch.nn.parameter import Parameter
+
+from torch.optim.lr_scheduler import _LRScheduler
+
+exc = getattr(builtins, "IOError", "FileNotFoundError")
+
+class LRPolicyScheduler(_LRScheduler):
+    def __init__(self, optimizer, num_warmup_steps, decay_start_step, num_decay_steps):
+        self.num_warmup_steps = num_warmup_steps
+        self.decay_start_step = decay_start_step
+        self.decay_end_step = decay_start_step + num_decay_steps
+        self.num_decay_steps = num_decay_steps
+
+        if self.decay_start_step < self.num_warmup_steps:
+            sys.exit("Learning rate warmup must finish before the decay starts")
+
+        if isinstance(optimizer, tuple):
+            for opt in optimizer:
+                super(LRPolicyScheduler, self).__init__(opt)
+        else:
+            super(LRPolicyScheduler, self).__init__(optimizer)
+
+    def get_lr(self):
+        step_count = self._step_count
+        if step_count < self.num_warmup_steps:
+            # warmup
+            scale = 1.0 - (self.num_warmup_steps - step_count) / self.num_warmup_steps
+            lr = [base_lr * scale for base_lr in self.base_lrs]
+            self.last_lr = lr
+        elif self.decay_start_step <= step_count and step_count < self.decay_end_step:
+            # decay
+            decayed_steps = step_count - self.decay_start_step
+            scale = ((self.num_decay_steps - decayed_steps) / self.num_decay_steps) ** 2
+            min_lr = 0.0000001
+            lr = [max(min_lr, base_lr * scale) for base_lr in self.base_lrs]
+            self.last_lr = lr
+        else:
+            if self.num_decay_steps > 0:
+                # freeze at last, either because we're after decay
+                # or because we're between warmup and decay
+                lr = self.last_lr
+            else:
+                # do not adjust
+                lr = self.base_lrs
+        return lr
+
+
+class Cast(nn.Module):
+     __constants__ = ['to_dtype']
+ 
+     def __init__(self, to_dtype):
+         super(Cast, self).__init__()
+         self.to_dtype = to_dtype
+ 
+     def forward(self, input):
+         if input.is_mkldnn:
+             return input.to_dense(self.to_dtype)
+         else:
+             return input.to(self.to_dtype)
+ 
+     def extra_repr(self):
+         return 'to(%s)' % self.to_dtype
+
+ 
+### define dlrm in PyTorch ###
+class DLRM_Net(nn.Module):
+    def create_mlp(self, ln, sigmoid_layer):
+        # build MLP layer by layer
+        layers = nn.ModuleList()
+        for i in range(0, ln.size - 1):
+            n = ln[i]
+            m = ln[i + 1]
+
+            # construct fully connected operator
+            if self.use_ipex: #and self.bf16:
+                LL = ipex.IpexMLPLinear(int(n), int(m), bias=True, output_stays_blocked=(i < ln.size - 2), default_blocking=32)
+            else:
+                LL = nn.Linear(int(n), int(m), bias=True)
+
+            # initialize the weights
+            # with torch.no_grad():
+            # custom Xavier input, output or two-sided fill
+            mean = 0.0  # std_dev = np.sqrt(variance)
+            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)
+            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)
+            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))
+            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)
+            # approach 1
+            LL.weight.data = torch.tensor(W, requires_grad=True)
+            LL.bias.data = torch.tensor(bt, requires_grad=True)
+            # approach 2
+            # LL.weight.data.copy_(torch.tensor(W))
+            # LL.bias.data.copy_(torch.tensor(bt))
+            # approach 3
+            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)
+            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)
+
+            if self.bf16 and ipex.is_available():
+                LL.to(torch.bfloat16)
+            # prepack weight for IPEX Linear
+            if hasattr(LL, 'reset_weight_shape'):
+                if self.bf16:
+                    LL.reset_weight_shape(block_for_dtype=torch.bfloat16)
+                else:
+                    LL.reset_weight_shape(block_for_dtype=torch.float32)
+
+            layers.append(LL)
+
+            # construct sigmoid or relu operator
+            if i == sigmoid_layer:
+                if self.bf16:
+                    layers.append(Cast(torch.float32))
+                layers.append(nn.Sigmoid())
+            else:
+                if self.use_ipex: #and self.bf16:
+                    LL.set_activation_type('relu')
+                else:
+                    layers.append(nn.ReLU())
+
+        # approach 1: use ModuleList
+        # return layers
+        # approach 2: use Sequential container to wrap all layers
+        return torch.nn.Sequential(*layers)
+
+    def create_emb(self, m, ln, local_ln_emb_sparse=None, ln_emb_dense=None):
+        emb_l = nn.ModuleList()
+        # save the numpy random state
+        np_rand_state = np.random.get_state()
+        emb_dense = nn.ModuleList()
+        emb_sparse = nn.ModuleList()
+        embs = range(len(ln))
+        if local_ln_emb_sparse or ln_emb_dense:
+            embs = local_ln_emb_sparse + ln_emb_dense
+        for i in embs:
+            # Use per table random seed for Embedding initialization
+            np.random.seed(self.l_emb_seeds[i])
+            n = ln[i]
+            # construct embedding operator
+            if self.qr_flag and n > self.qr_threshold:
+                EE = QREmbeddingBag(n, m, self.qr_collisions,
+                    operation=self.qr_operation, mode="sum", sparse=True)
+            elif self.md_flag:
+                base = max(m)
+                _m = m[i] if n > self.md_threshold else base
+                EE = PrEmbeddingBag(n, _m, base)
+                # use np initialization as below for consistency...
+                W = np.random.uniform(
+                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)
+                ).astype(np.float32)
+                EE.embs.weight.data = torch.tensor(W, requires_grad=True)
+
+            else:
+                # initialize embeddings
+                # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))
+                W = np.random.uniform(
+                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
+                ).astype(np.float32)
+                # approach 1
+                if n >= self.sparse_dense_boundary:
+                    #n = 39979771
+                    m_sparse = 16
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m_sparse)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m_sparse, mode="sum", sparse=True, _weight=torch.tensor(W, requires_grad=True))
+                else:
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m, mode="sum", sparse=False, _weight=torch.tensor(W, requires_grad=True))
+                # approach 2
+                # EE.weight.data.copy_(torch.tensor(W))
+                # approach 3
+                # EE.weight = Parameter(torch.tensor(W),requires_grad=True)
+                if self.bf16 and ipex.is_available():
+                    EE.to(torch.bfloat16)
+               
+            if ext_dist.my_size > 1:
+                if n >= self.sparse_dense_boundary:
+                    emb_sparse.append(EE)
+                else:
+                    emb_dense.append(EE)
+
+            emb_l.append(EE)
+
+        # Restore the numpy random state
+        np.random.set_state(np_rand_state)
+        return emb_l, emb_dense, emb_sparse
+
+    def __init__(
+        self,
+        m_spa=None,
+        ln_emb=None,
+        ln_bot=None,
+        ln_top=None,
+        arch_interaction_op=None,
+        arch_interaction_itself=False,
+        sigmoid_bot=-1,
+        sigmoid_top=-1,
+        sync_dense_params=True,
+        loss_threshold=0.0,
+        ndevices=-1,
+        qr_flag=False,
+        qr_operation="mult",
+        qr_collisions=0,
+        qr_threshold=200,
+        md_flag=False,
+        md_threshold=200,
+        bf16=False,
+        use_ipex=False,
+        sparse_dense_boundary = 2048
+    ):
+        super(DLRM_Net, self).__init__()
+
+        if (
+            (m_spa is not None)
+            and (ln_emb is not None)
+            and (ln_bot is not None)
+            and (ln_top is not None)
+            and (arch_interaction_op is not None)
+        ):
+
+            # save arguments
+            self.ndevices = ndevices
+            self.output_d = 0
+            self.parallel_model_batch_size = -1
+            self.parallel_model_is_not_prepared = True
+            self.arch_interaction_op = arch_interaction_op
+            self.arch_interaction_itself = arch_interaction_itself
+            self.sync_dense_params = sync_dense_params
+            self.loss_threshold = loss_threshold
+            self.bf16 = bf16
+            self.use_ipex = use_ipex
+            self.sparse_dense_boundary = sparse_dense_boundary
+            # create variables for QR embedding if applicable
+            self.qr_flag = qr_flag
+            if self.qr_flag:
+                self.qr_collisions = qr_collisions
+                self.qr_operation = qr_operation
+                self.qr_threshold = qr_threshold
+            # create variables for MD embedding if applicable
+            self.md_flag = md_flag
+            if self.md_flag:
+                self.md_threshold = md_threshold
+
+            # generate np seeds for Emb table initialization
+            self.l_emb_seeds = np.random.randint(low=0, high=100000, size=len(ln_emb))
+
+            #If running distributed, get local slice of embedding tables
+            if ext_dist.my_size > 1:
+                n_emb = len(ln_emb)
+                self.n_global_emb = n_emb
+                self.rank = ext_dist.dist.get_rank()
+                self.ln_emb_dense = [i for i in range(n_emb) if ln_emb[i] < self.sparse_dense_boundary]
+                self.ln_emb_sparse = [i for i in range(n_emb) if ln_emb[i] >= self.sparse_dense_boundary]
+                n_emb_sparse = len(self.ln_emb_sparse)
+                self.n_local_emb_sparse, self.n_sparse_emb_per_rank = ext_dist.get_split_lengths(n_emb_sparse)
+                self.local_ln_emb_sparse_slice = ext_dist.get_my_slice(n_emb_sparse)
+                self.local_ln_emb_sparse = self.ln_emb_sparse[self.local_ln_emb_sparse_slice]
+            # create operators
+            if ndevices <= 1:
+                if ext_dist.my_size > 1:
+                    _, self.emb_dense, self.emb_sparse = self.create_emb(m_spa, ln_emb, self.local_ln_emb_sparse, self.ln_emb_dense)
+                else:
+                    self.emb_l, _, _ = self.create_emb(m_spa, ln_emb)
+
+            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)
+            self.top_l = self.create_mlp(ln_top, sigmoid_top)
+
+    def apply_mlp(self, x, layers):
+        # approach 1: use ModuleList
+        # for layer in layers:
+        #     x = layer(x)
+        # return x
+        # approach 2: use Sequential container to wrap all layers
+        need_padding = self.use_ipex and x.size(0) % 2 == 1 #and self.bf16
+        if need_padding:
+            x = torch.nn.functional.pad(input=x, pad=(0,0,0,1), mode='constant', value=0)
+            ret = layers(x)
+            return(ret[:-1,:])
+        else:
+            return layers(x)
+
+    def apply_emb(self, lS_o, lS_i, emb_l):
+        # WARNING: notice that we are processing the batch at once. We implicitly
+        # assume that the data is laid out such that:
+        # 1. each embedding is indexed with a group of sparse indices,
+        #   corresponding to a single lookup
+        # 2. for each embedding the lookups are further organized into a batch
+        # 3. for a list of embedding tables there is a list of batched lookups
+
+        ly = []
+        for k, sparse_index_group_batch in enumerate(lS_i):
+            sparse_offset_group_batch = lS_o[k]
+
+            # embedding lookup
+            # We are using EmbeddingBag, which implicitly uses sum operator.
+            # The embeddings are represented as tall matrices, with sum
+            # happening vertically across 0 axis, resulting in a row vector
+            E = emb_l[k]
+            V = E(sparse_index_group_batch, sparse_offset_group_batch)
+
+            ly.append(V)
+
+        # print(ly)
+        return ly
+#if self.bf16:
+    def interact_features(self, x, ly):
+        x = x.to(ly[0].dtype)
+        if self.arch_interaction_op == "dot":
+            if self.bf16 or self.use_ipex:
+                T = [x] + ly
+                R = ipex.interaction(*T)
+            else:
+                # concatenate dense and sparse features
+                (batch_size, d) = x.shape
+                T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
+                # perform a dot product
+                Z = torch.bmm(T, torch.transpose(T, 1, 2))
+                # append dense feature with the interactions (into a row vector)
+                # approach 1: all
+                # Zflat = Z.view((batch_size, -1))
+                # approach 2: unique
+                _, ni, nj = Z.shape
+                # approach 1: tril_indices
+                # offset = 0 if self.arch_interaction_itself else -1
+                # li, lj = torch.tril_indices(ni, nj, offset=offset)
+                # approach 2: custom
+                offset = 1 if self.arch_interaction_itself else 0
+                li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
+                lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
+                Zflat = Z[:, li, lj]
+                # concatenate dense features and interactions
+                R = torch.cat([x] + [Zflat], dim=1)
+        elif self.arch_interaction_op == "cat":
+            # concatenation features (into a row vector)
+            R = torch.cat([x] + ly, dim=1)
+        else:
+            sys.exit(
+                "ERROR: --arch-interaction-op="
+                + self.arch_interaction_op
+                + " is not supported"
+            )
+
+        return R
+
+    def forward(self, dense_x, lS_o, lS_i):
+        if self.bf16:
+            dense_x = dense_x.bfloat16()
+        if ext_dist.my_size > 1:
+            return self.distributed_forward(dense_x, lS_o, lS_i)
+        elif self.ndevices <= 1:
+            return self.sequential_forward(dense_x, lS_o, lS_i)
+        else:
+            return self.parallel_forward(dense_x, lS_o, lS_i)
+
+    def sequential_forward(self, dense_x, lS_o, lS_i):
+        # process dense features (using bottom mlp), resulting in a row vector
+        x = self.apply_mlp(dense_x, self.bot_l)
+        # debug prints
+        # print("intermediate")
+        # print(x.detach().cpu().numpy())
+
+        # process sparse features(using embeddings), resulting in a list of row vectors
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
+        # for y in ly:
+        #     print(y.detach().cpu().numpy())
+
+        # interact features (dense and sparse)
+        z = self.interact_features(x, ly)
+        # print(z.detach().cpu().numpy())
+
+        # obtain probability of a click (using top mlp)
+        p = self.apply_mlp(z, self.top_l)
+
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))
+        else:
+            z = p
+
+        return z
+
+    def distributed_forward(self, dense_x, lS_o, lS_i):
+        batch_size = dense_x.size()[0]
+        # WARNING: # of ranks must be <= batch size in distributed_forward call
+        if batch_size < ext_dist.my_size:
+            sys.exit("ERROR: batch_size (%d) must be larger than number of ranks (%d)" % (batch_size, ext_dist.my_size))
+
+        lS_o_dense = [lS_o[i]  for i in self.ln_emb_dense]
+        lS_i_dense = [lS_i[i] for i in self.ln_emb_dense]
+        lS_o_sparse = [lS_o[i] for i in self.ln_emb_sparse]  # partition sparse table in one group
+        lS_i_sparse = [lS_i[i] for i in self.ln_emb_sparse]
+
+        lS_i_sparse = ext_dist.shuffle_data(lS_i_sparse)
+        g_i_sparse = [lS_i_sparse[:, i * batch_size:(i + 1) * batch_size].reshape(-1) for i in range(len(self.local_ln_emb_sparse))]
+        offset = torch.arange(batch_size * ext_dist.my_size).to(device)
+        g_o_sparse = [offset for i in range(self.n_local_emb_sparse)]
+
+        if (len(self.local_ln_emb_sparse) != len(g_o_sparse)) or (len(self.local_ln_emb_sparse) != len(g_i_sparse)):
+           sys.exit("ERROR 0 : corrupted model input detected in distributed_forward call")
+        # sparse embeddings
+        ly_sparse = self.apply_emb(g_o_sparse, g_i_sparse, self.emb_sparse)
+        a2a_req = ext_dist.alltoall(ly_sparse, self.n_sparse_emb_per_rank)
+        # bottom mlp
+        x = self.apply_mlp(dense_x, self.bot_l)
+        # dense embeddings
+        ly_dense = self.apply_emb(lS_o_dense, lS_i_dense, self.emb_dense)
+        ly_sparse = a2a_req.wait()
+        ly_sparse2 = []
+        for i in range(len(ly_sparse)):
+            ly_sparse2.append(ly_sparse[i].repeat(1,4))
+        del ly_sparse
+        #ly_sparse ""= torch.cat(ly_sparse,1)
+        ly = ly_dense + list(ly_sparse2)
+
+        # interactions
+        z = self.interact_features(x, ly)
+        # top mlp
+        p = self.apply_mlp(z, self.top_l)
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z = torch.clamp(
+                p, min=self.loss_threshold, max=(1.0 - self.loss_threshold)
+            )
+        else:
+            z = p
+
+        return z
+ 
+    def parallel_forward(self, dense_x, lS_o, lS_i):
+        ### prepare model (overwrite) ###
+        # WARNING: # of devices must be >= batch size in parallel_forward call
+        batch_size = dense_x.size()[0]
+        ndevices = min(self.ndevices, batch_size, len(self.emb_l))
+        device_ids = range(ndevices)
+        # WARNING: must redistribute the model if mini-batch size changes(this is common
+        # for last mini-batch, when # of elements in the dataset/batch size is not even
+        if self.parallel_model_batch_size != batch_size:
+            self.parallel_model_is_not_prepared = True
+
+        if self.parallel_model_is_not_prepared or self.sync_dense_params:
+            # replicate mlp (data parallelism)
+            self.bot_l_replicas = replicate(self.bot_l, device_ids)
+            self.top_l_replicas = replicate(self.top_l, device_ids)
+            self.parallel_model_batch_size = batch_size
+
+        if self.parallel_model_is_not_prepared:
+            # distribute embeddings (model parallelism)
+            t_list = []
+            for k, emb in enumerate(self.emb_l):
+                d = torch.device("cuda:" + str(k % ndevices))
+                emb.to(d)
+                t_list.append(emb.to(d))
+            self.emb_l = nn.ModuleList(t_list)
+            self.parallel_model_is_not_prepared = False
+
+        ### prepare input (overwrite) ###
+        # scatter dense features (data parallelism)
+        # print(dense_x.device)
+        dense_x = scatter(dense_x, device_ids, dim=0)
+        # distribute sparse features (model parallelism)
+        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):
+            sys.exit("ERROR: corrupted model input detected in parallel_forward call")
+
+        t_list = []
+        i_list = []
+        for k, _ in enumerate(self.emb_l):
+            d = torch.device("cuda:" + str(k % ndevices))
+            t_list.append(lS_o[k].to(d))
+            i_list.append(lS_i[k].to(d))
+        lS_o = t_list
+        lS_i = i_list
+
+        ### compute results in parallel ###
+        # bottom mlp
+        # WARNING: Note that the self.bot_l is a list of bottom mlp modules
+        # that have been replicated across devices, while dense_x is a tuple of dense
+        # inputs that has been scattered across devices on the first (batch) dimension.
+        # The output is a list of tensors scattered across devices according to the
+        # distribution of dense_x.
+        x = parallel_apply(self.bot_l_replicas, dense_x, None, device_ids)
+        # debug prints
+        # print(x)
+
+        # embeddings
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
+        # debug prints
+        # print(ly)
+
+        # butterfly shuffle (implemented inefficiently for now)
+        # WARNING: Note that at this point we have the result of the embedding lookup
+        # for the entire batch on each device. We would like to obtain partial results
+        # corresponding to all embedding lookups, but part of the batch on each device.
+        # Therefore, matching the distribution of output of bottom mlp, so that both
+        # could be used for subsequent interactions on each device.
+        if len(self.emb_l) != len(ly):
+            sys.exit("ERROR: corrupted intermediate result in parallel_forward call")
+
+        t_list = []
+        for k, _ in enumerate(self.emb_l):
+            d = torch.device("cuda:" + str(k % ndevices))
+            y = scatter(ly[k], device_ids, dim=0)
+            t_list.append(y)
+        # adjust the list to be ordered per device
+        ly = list(map(lambda y: list(y), zip(*t_list)))
+        # debug prints
+        # print(ly)
+
+        # interactions
+        z = []
+        for k in range(ndevices):
+            zk = self.interact_features(x[k], ly[k])
+            z.append(zk)
+        # debug prints
+        # print(z)
+
+        # top mlp
+        # WARNING: Note that the self.top_l is a list of top mlp modules that
+        # have been replicated across devices, while z is a list of interaction results
+        # that by construction are scattered across devices on the first (batch) dim.
+        # The output is a list of tensors scattered across devices according to the
+        # distribution of z.
+        p = parallel_apply(self.top_l_replicas, z, None, device_ids)
+
+        ### gather the distributed results ###
+        p0 = gather(p, self.output_d, dim=0)
+
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z0 = torch.clamp(
+                p0, min=self.loss_threshold, max=(1.0 - self.loss_threshold)
+            )
+        else:
+            z0 = p0
+
+        return z0
+
+
+if __name__ == "__main__":
+    # the reference implementation doesn't clear the cache currently
+    # but the submissions are required to do that
+    mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)
+
+    mlperf_logger.log_start(key=mlperf_logger.constants.INIT_START, log_all_ranks=True)
+
+    ### import packages ###
+    import sys
+    import os
+    import argparse
+
+    ### parse arguments ###
+    parser = argparse.ArgumentParser(
+        description="Train Deep Learning Recommendation Model (DLRM)"
+    )
+    # model related parameters
+    parser.add_argument("--arch-sparse-feature-size", type=int, default=2)
+    parser.add_argument("--arch-embedding-size", type=str, default="4-3-2")
+    # j will be replaced with the table number
+    parser.add_argument("--arch-mlp-bot", type=str, default="4-3-2")
+    parser.add_argument("--arch-mlp-top", type=str, default="4-2-1")
+    parser.add_argument("--arch-interaction-op", type=str, default="dot")
+    parser.add_argument("--arch-interaction-itself", action="store_true", default=False)
+    # embedding table options
+    parser.add_argument("--md-flag", action="store_true", default=False)
+    parser.add_argument("--md-threshold", type=int, default=200)
+    parser.add_argument("--md-temperature", type=float, default=0.3)
+    parser.add_argument("--md-round-dims", action="store_true", default=False)
+    parser.add_argument("--qr-flag", action="store_true", default=False)
+    parser.add_argument("--qr-threshold", type=int, default=200)
+    parser.add_argument("--qr-operation", type=str, default="mult")
+    parser.add_argument("--qr-collisions", type=int, default=4)
+    # activations and loss
+    parser.add_argument("--activation-function", type=str, default="relu")
+    parser.add_argument("--loss-function", type=str, default="mse")  # or bce or wbce
+    parser.add_argument("--loss-weights", type=str, default="1.0-1.0")  # for wbce
+    parser.add_argument("--loss-threshold", type=float, default=0.0)  # 1.0e-7
+    parser.add_argument("--round-targets", type=bool, default=False)
+    # data
+    parser.add_argument("--data-size", type=int, default=1)
+    parser.add_argument("--num-batches", type=int, default=0)
+    parser.add_argument(
+        "--data-generation", type=str, default="random"
+    )  # synthetic or dataset
+    parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
+    parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
+    parser.add_argument("--raw-data-file", type=str, default="")
+    parser.add_argument("--processed-data-file", type=str, default="")
+    parser.add_argument("--data-randomize", type=str, default="total")  # or day or none
+    parser.add_argument("--data-trace-enable-padding", type=bool, default=False)
+    parser.add_argument("--max-ind-range", type=int, default=-1)
+    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
+    parser.add_argument("--num-indices-per-lookup", type=int, default=10)
+    parser.add_argument("--num-indices-per-lookup-fixed", type=bool, default=False)
+    parser.add_argument("--num-workers", type=int, default=0)
+    parser.add_argument("--memory-map", action="store_true", default=False)
+    # training
+    parser.add_argument("--mini-batch-size", type=int, default=1)
+    parser.add_argument("--nepochs", type=int, default=1)
+    parser.add_argument("--learning-rate", type=float, default=0.01)
+    parser.add_argument("--print-precision", type=int, default=5)
+    parser.add_argument("--numpy-rand-seed", type=int, default=123)
+    parser.add_argument("--sync-dense-params", type=bool, default=True)
+    # inference
+    parser.add_argument("--inference-only", action="store_true", default=False)
+    # onnx
+    parser.add_argument("--save-onnx", action="store_true", default=False)
+    # gpu
+    parser.add_argument("--use-gpu", action="store_true", default=False)
+    # distributed run
+    parser.add_argument("--dist-backend", type=str, default="")
+    # debugging and profiling
+    parser.add_argument("--print-freq", type=int, default=1)
+    parser.add_argument("--test-freq", type=int, default=-1)
+    parser.add_argument("--test-mini-batch-size", type=int, default=-1)
+    parser.add_argument("--test-num-workers", type=int, default=-1)
+    parser.add_argument("--print-time", action="store_true", default=False)
+    parser.add_argument("--debug-mode", action="store_true", default=False)
+    parser.add_argument("--enable-profiling", action="store_true", default=False)
+    parser.add_argument("--plot-compute-graph", action="store_true", default=False)
+    parser.add_argument("--profiling-start-iter", type=int, default=50)
+    parser.add_argument("--profiling-num-iters", type=int, default=100)
+    # store/load model
+    parser.add_argument("--out-dir", type=str, default=".")
+    parser.add_argument("--save-model", type=str, default="")
+    parser.add_argument("--load-model", type=str, default="")
+    # mlperf logging (disables other output and stops early)
+    parser.add_argument("--mlperf-logging", action="store_true", default=False)
+    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
+    parser.add_argument("--mlperf-acc-threshold", type=float, default=0.0)
+    # stop at target AUC Terabyte (no subsampling) 0.8025
+    parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
+    parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
+    parser.add_argument("--mlperf-bin-shuffle", action='store_true', default=False)
+    # LR policy
+    parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
+    parser.add_argument("--lr-decay-start-step", type=int, default=0)
+    parser.add_argument("--lr-num-decay-steps", type=int, default=0)
+    # embedding table is sparse table only if sparse_dense_boundary >= 2048
+    parser.add_argument("--sparse-dense-boundary", type=int, default=2048)
+    # bf16 option
+    parser.add_argument("--bf16", action='store_true', default=False)
+    # ipex option
+    parser.add_argument("--use-ipex", action="store_true", default=False)
+    # lamb
+    parser.add_argument("--optimizer", type=int, default=0, help='optimizer:[0:sgd, 1:lamb/sgd, 2:adagrad, 3:sparseadam]')
+    # distiller option
+    parser.add_argument("--model-compression-type", type=str, default=None)
+    parser.add_argument("--compression-file", type=str, default="./model_compression/dlrm.schedule_agp.yaml")
+
+    args = parser.parse_args()
+
+    ext_dist.init_distributed(backend=args.dist_backend)
+
+    if args.mlperf_logging:
+        print('command line args: ', json.dumps(vars(args)))
+
+    ### some basic setup ###
+    np.random.seed(args.numpy_rand_seed)
+    np.set_printoptions(precision=args.print_precision)
+    torch.set_printoptions(precision=args.print_precision)
+    torch.manual_seed(args.numpy_rand_seed)
+
+    if (args.test_mini_batch_size < 0):
+        # if the parameter is not set, use the training batch size
+        args.test_mini_batch_size = args.mini_batch_size
+    if (args.test_num_workers < 0):
+        # if the parameter is not set, use the same parameter for training
+        args.test_num_workers = args.num_workers
+    if (args.mini_batch_size % ext_dist.my_size !=0 or args.test_mini_batch_size % ext_dist.my_size != 0):
+        print("Either test minibatch (%d) or train minibatch (%d) does not split across %d ranks" % (args.test_mini_batch_size, args.mini_batch_size, ext_dist.my_size))
+        sys.exit(1)
+
+    use_gpu = args.use_gpu and torch.cuda.is_available()
+    use_ipex = args.use_ipex
+    if use_gpu:
+        torch.cuda.manual_seed_all(args.numpy_rand_seed)
+        torch.backends.cudnn.deterministic = True
+        if ext_dist.my_size > 1:
+            ngpus = torch.cuda.device_count()  # 1
+            if ext_dist.my_local_size > torch.cuda.device_count():
+                print("Not sufficient GPUs available... local_size = %d, ngpus = %d" % (ext_dist.my_local_size, ngpus))
+                sys.exit(1)
+            ngpus = 1
+            device = torch.device("cuda", ext_dist.my_local_rank)
+        else:
+            device = torch.device("cuda", 0)
+            ngpus = torch.cuda.device_count()  # 1
+        print("Using {} GPU(s)...".format(ngpus))
+    elif use_ipex:
+        device = torch.device("dpcpp")
+        print("Using IPEX...")
+    else:
+        device = torch.device("cpu")
+        print("Using CPU...")
+
+    ### prepare training data ###
+    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
+    # input data
+
+    mlperf_logger.barrier()
+    mlperf_logger.log_end(key=mlperf_logger.constants.INIT_STOP)
+    mlperf_logger.barrier()
+    mlperf_logger.log_start(key=mlperf_logger.constants.RUN_START)
+    mlperf_logger.barrier()
+
+    if (args.data_generation == "dataset"):
+        train_data, train_ld, test_data, test_ld = \
+            dp.make_criteo_data_and_loaders(args)
+        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+        nbatches_test = len(test_ld)
+
+        ln_emb = train_data.counts
+        # enforce maximum limit on number of vectors per embedding
+        if args.max_ind_range > 0:
+            ln_emb = np.array(list(map(
+                lambda x: x if x < args.max_ind_range else args.max_ind_range,
+                ln_emb
+            )))
+        m_den = train_data.m_den
+        ln_bot[0] = m_den
+
+    else:
+        # input and target at random
+        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
+        m_den = ln_bot[0]
+        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+
+    ### parse command line arguments ###
+    m_spa = args.arch_sparse_feature_size
+    num_fea = ln_emb.size + 1  # num sparse + num dense features
+    m_den_out = ln_bot[ln_bot.size - 1]
+    if args.arch_interaction_op == "dot":
+        # approach 1: all
+        # num_int = num_fea * num_fea + m_den_out
+        # approach 2: unique
+        if args.arch_interaction_itself:
+            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
+        else:
+            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
+    elif args.arch_interaction_op == "cat":
+        num_int = num_fea * m_den_out
+    else:
+        sys.exit(
+            "ERROR: --arch-interaction-op="
+            + args.arch_interaction_op
+            + " is not supported"
+        )
+    arch_mlp_top_adjusted = str(num_int) + "-" + args.arch_mlp_top
+    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep="-")
+
+    # sanity check: feature sizes and mlp dimensions must match
+    if m_den != ln_bot[0]:
+        sys.exit(
+            "ERROR: arch-dense-feature-size "
+            + str(m_den)
+            + " does not match first dim of bottom mlp "
+            + str(ln_bot[0])
+        )
+    if args.qr_flag:
+        if args.qr_operation == "concat" and 2 * m_spa != m_den_out:
+            sys.exit(
+                "ERROR: 2 arch-sparse-feature-size "
+                + str(2 * m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+                + " (note that the last dim of bottom mlp must be 2x the embedding dim)"
+            )
+        if args.qr_operation != "concat" and m_spa != m_den_out:
+            sys.exit(
+                "ERROR: arch-sparse-feature-size "
+                + str(m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+            )
+    else:
+        if m_spa != m_den_out:
+            sys.exit(
+                "ERROR: arch-sparse-feature-size "
+                + str(m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+            )
+    if num_int != ln_top[0]:
+        sys.exit(
+            "ERROR: # of feature interactions "
+            + str(num_int)
+            + " does not match first dimension of top mlp "
+            + str(ln_top[0])
+        )
+
+    # assign mixed dimensions if applicable
+    if args.md_flag:
+        m_spa = md_solver(
+            torch.tensor(ln_emb),
+            args.md_temperature,  # alpha
+            d0=m_spa,
+            round_dim=args.md_round_dims
+        ).tolist()
+
+    # test prints (model arch)
+    if args.debug_mode:
+        print("model arch:")
+        print(
+            "mlp top arch "
+            + str(ln_top.size - 1)
+            + " layers, with input to output dimensions:"
+        )
+        print(ln_top)
+        print("# of interactions")
+        print(num_int)
+        print(
+            "mlp bot arch "
+            + str(ln_bot.size - 1)
+            + " layers, with input to output dimensions:"
+        )
+        print(ln_bot)
+        print("# of features (sparse and dense)")
+        print(num_fea)
+        print("dense feature size")
+        print(m_den)
+        print("sparse feature size")
+        print(m_spa)
+        print(
+            "# of embeddings (= # of sparse features) "
+            + str(ln_emb.size)
+            + ", with dimensions "
+            + str(m_spa)
+            + "x:"
+        )
+        print(ln_emb)
+
+        print("data (inputs and targets):")
+        for j, (X, lS_o, lS_i, T) in enumerate(train_ld):
+            # early exit if nbatches was set by the user and has been exceeded
+            if nbatches > 0 and j >= nbatches:
+                break
+
+            print("mini-batch: %d" % j)
+            print(X.detach().cpu().numpy())
+            # transform offsets to lengths when printing
+            print(
+                [
+                    np.diff(
+                        S_o.detach().cpu().tolist() + list(lS_i[i].shape)
+                    ).tolist()
+                    for i, S_o in enumerate(lS_o)
+                ]
+            )
+            print([S_i.detach().cpu().tolist() for S_i in lS_i])
+            print(T.detach().cpu().numpy())
+
+    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
+
+    ### construct the neural network specified above ###
+    # WARNING: to obtain exactly the same initialization for
+    # the weights we need to start from the same random seed.
+    # np.random.seed(args.numpy_rand_seed)
+    print('Creating the model...')
+    dlrm = DLRM_Net(
+        m_spa,
+        ln_emb,
+        ln_bot,
+        ln_top,
+        arch_interaction_op=args.arch_interaction_op,
+        arch_interaction_itself=args.arch_interaction_itself,
+        sigmoid_bot=-1,
+        sigmoid_top=ln_top.size - 2,
+        sync_dense_params=args.sync_dense_params,
+        loss_threshold=args.loss_threshold,
+        ndevices=ndevices,
+        qr_flag=args.qr_flag,
+        qr_operation=args.qr_operation,
+        qr_collisions=args.qr_collisions,
+        qr_threshold=args.qr_threshold,
+        md_flag=args.md_flag,
+        md_threshold=args.md_threshold,
+        sparse_dense_boundary=args.sparse_dense_boundary,
+        bf16 = args.bf16,
+        use_ipex = args.use_ipex
+    )
+    
+    print('Model created!')
+    # test prints
+    if args.debug_mode:
+        print("initial parameters (weights and bias):")
+        #for param in dlrm.parameters():
+        #    print(param.detach().cpu().numpy())
+        # print(dlrm)
+
+    if args.use_ipex:
+       dlrm = dlrm.to(device)
+       print(dlrm, device, args.use_ipex)
+
+    if use_gpu:
+        # Custom Model-Data Parallel
+        # the mlps are replicated and use data parallelism, while
+        # the embeddings are distributed and use model parallelism
+        dlrm = dlrm.to(device)  # .cuda()
+        if dlrm.ndevices > 1:
+            dlrm.emb_l = dlrm.create_emb(m_spa, ln_emb)
+
+    if ext_dist.my_size > 1:
+        if use_gpu:
+            device_ids = [ext_dist.my_local_rank]
+            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l, device_ids=device_ids)
+            dlrm.top_l = ext_dist.DDP(dlrm.top_l, device_ids=device_ids)
+        else:
+            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l)
+            dlrm.top_l = ext_dist.DDP(dlrm.top_l)
+            for i in range(len(dlrm.emb_dense)):
+                dlrm.emb_dense[i] = ext_dist.DDP(dlrm.emb_dense[i])
+
+    # specify the loss function
+    if args.loss_function == "mse":
+        loss_fn = torch.nn.MSELoss(reduction="mean")
+    elif args.loss_function == "bce":
+        loss_fn = torch.nn.BCELoss(reduction="mean")
+    elif args.loss_function == "wbce":
+        loss_ws = torch.tensor(np.fromstring(args.loss_weights, dtype=float, sep="-"))
+        loss_fn = torch.nn.BCELoss(reduction="none")
+    else:
+        sys.exit("ERROR: --loss-function=" + args.loss_function + " is not supported")
+
+    for name, params in dlrm.named_parameters():
+            print('{}:{}:{}'.format(name, params.size(), params.dtype))
+    compression_scheduler = None
+
+    if not args.inference_only:
+        # specify the optimizer algorithm
+        optimizer_list = ([torch.optim.SGD, ([Lamb, False], torch.optim.SGD),
+                           torch.optim.Adagrad, ([torch.optim.Adam, None], torch.optim.SparseAdam)],
+                          [ipex.SplitSGD, ([Lamb, True], ipex.SplitSGD)])
+        optimizers = optimizer_list[args.bf16 and ipex.is_available()][args.optimizer]
+        print('Chosen optimizer(s): %s' % str(optimizers))
+
+        if ext_dist.my_size == 1:
+            if len(optimizers) == 1:
+                optimizer = optimizers(dlrm.parameters(), lr=args.learning_rate)
+            else:
+                optimizer_dense = optimizers[0][0]([
+                    {"params": dlrm.bot_l.parameters(), "lr": args.learning_rate},
+                    {"params": dlrm.top_l.parameters(), "lr": args.learning_rate}
+                ], lr=args.learning_rate)
+                if optimizers[0][1] is not None:
+                    optimizer_dense.set_bf16(optimizers[0][1])
+                optimizer_sparse = optimizers[1]([
+                    {"params": [p for emb in dlrm.emb_l for p in emb.parameters()], "lr": args.learning_rate},
+                ], lr=args.learning_rate)
+                optimizer = (optimizer_dense, optimizer_sparse)
+        else:
+            if len(optimizers) == 1:
+                optimizer = optimizers([
+                    {"params": [p for emb in dlrm.emb_sparse for p in emb.parameters()],
+                     "lr": args.learning_rate / ext_dist.my_size},
+                    {"params": [p for emb in dlrm.emb_dense for p in emb.parameters()], "lr": args.learning_rate},
+                    {"params": dlrm.bot_l.parameters(), "lr": args.learning_rate},
+                    {"params": dlrm.top_l.parameters(), "lr": args.learning_rate}
+                ], lr=args.learning_rate)
+            else:
+                optimizer_dense = optimizers[0][0]([
+                    {"params": [p for emb in dlrm.emb_dense for p in emb.parameters()], "lr": args.learning_rate},
+                    {"params": dlrm.bot_l.parameters(), "lr": args.learning_rate},
+                    {"params": dlrm.top_l.parameters(), "lr": args.learning_rate}
+                ], lr=args.learning_rate, bf16=args.bf16)
+                optimizer_sparse = optimizers[1]([
+                    {"params": [p for emb in dlrm.emb_sparse for p in emb.parameters()],
+                     "lr": args.learning_rate / ext_dist.my_size},
+                ], lr=args.learning_rate)
+                optimizer = (optimizer_dense, optimizer_sparse)
+        lr_scheduler = LRPolicyScheduler(optimizer, args.lr_num_warmup_steps, args.lr_decay_start_step,
+                                      args.lr_num_decay_steps)
+        # load the model compression configuration
+        if args.model_compression_type is not None:
+            compression_scheduler = distiller.file_config(dlrm, optimizer, args.compression_file, compression_scheduler)
+
+    ### main loop ###
+    def time_wrap(use_gpu):
+        if use_gpu:
+            torch.cuda.synchronize()
+        return time.time()
+
+    def dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device):
+        if use_gpu or use_ipex:  # .cuda()
+            # lS_i can be either a list of tensors or a stacked tensor.
+            # Handle each case below:
+            lS_i = [S_i.to(device) for S_i in lS_i] if isinstance(lS_i, list) \
+                else lS_i.to(device)
+            lS_o = [S_o.to(device) for S_o in lS_o] if isinstance(lS_o, list) \
+                else lS_o.to(device)
+            return dlrm(
+                X.to(device),
+                lS_o,
+                lS_i
+            )
+        else:
+            return dlrm(X, lS_o, lS_i)
+
+    def loss_fn_wrap(Z, T, use_gpu, use_ipex, device):
+        if args.loss_function == "mse" or args.loss_function == "bce":
+            if use_gpu or use_ipex:
+                return loss_fn(Z, T.to(device))
+            else:
+                return loss_fn(Z, T)
+        elif args.loss_function == "wbce":
+            if use_gpu:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T).to(device)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            else:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            loss_sc_ = loss_ws_ * loss_fn_
+            # debug prints
+            # print(loss_ws_)
+            # print(loss_fn_)
+            return loss_sc_.mean()
+
+    # training or inference
+    best_gA_test = 0
+    best_auc_test = 0
+    skip_upto_epoch = 0
+    skip_upto_batch = 0
+    total_time = 0
+    total_loss = 0
+    total_accu = 0
+    total_iter = 0
+    total_samp = 0
+    k = 0
+
+    mini_num_batchs = 1
+
+    train_ld_iter = enumerate(train_ld)
+
+    mlperf_logger.mlperf_submission_log('dlrm')
+    mlperf_logger.log_event(key=mlperf_logger.constants.SEED, value=args.numpy_rand_seed)
+    mlperf_logger.log_event(key=mlperf_logger.constants.GLOBAL_BATCH_SIZE, value=args.mini_batch_size)
+
+    # Load model is specified
+    if not (args.load_model == ""):
+        print("Loading saved model {}".format(args.load_model))
+        if use_gpu:
+            if dlrm.ndevices > 1:
+                # NOTE: when targeting inference on multiple GPUs,
+                # load the model as is on CPU or GPU, with the move
+                # to multiple GPUs to be done in parallel_forward
+                ld_model = torch.load(args.load_model)
+            else:
+                # NOTE: when targeting inference on single GPU,
+                # note that the call to .to(device) has already happened
+                ld_model = torch.load(
+                    args.load_model,
+                    map_location=torch.device('cuda')
+                    # map_location=lambda storage, loc: storage.cuda(0)
+                )
+        else:
+            # when targeting inference on CPU
+            ld_model = torch.load(args.load_model, map_location=torch.device('cpu'))
+        dlrm.load_state_dict(ld_model["state_dict"])
+        ld_j = ld_model["iter"]
+        ld_k = ld_model["epoch"]
+        ld_nepochs = ld_model["nepochs"]
+        ld_nbatches = ld_model["nbatches"]
+        ld_nbatches_test = ld_model["nbatches_test"]
+        ld_gA = ld_model["train_acc"]
+        ld_gL = ld_model["train_loss"]
+        ld_total_loss = ld_model["total_loss"]
+        ld_total_accu = ld_model["total_accu"]
+        ld_gA_test = ld_model["test_acc"]
+        ld_gL_test = ld_model["test_loss"]
+        if not args.inference_only:
+            optimizer.load_state_dict(ld_model["opt_state_dict"])
+            best_gA_test = ld_gA_test
+            total_loss = ld_total_loss
+            total_accu = ld_total_accu
+            skip_upto_epoch = ld_k  # epochs
+            skip_upto_batch = ld_j  # batches
+        else:
+            args.print_freq = ld_nbatches
+            args.test_freq = 0
+
+        print(
+            "Saved at: epoch = {:d}/{:d}, batch = {:d}/{:d}, ntbatch = {:d}".format(
+                ld_k, ld_nepochs, ld_j, ld_nbatches, ld_nbatches_test
+            )
+        )
+        print(
+            "Training state: loss = {:.6f}, accuracy = {:3.3f} %".format(
+                ld_gL, ld_gA * 100
+            )
+        )
+        print(
+            "Testing state: loss = {:.6f}, accuracy = {:3.3f} %".format(
+                ld_gL_test, ld_gA_test * 100
+            )
+        )
+
+    ext_dist.barrier()
+    print("time/loss/accuracy (if enabled):")
+
+    # LR is logged twice for now because of a compliance checker bug
+    mlperf_logger.log_event(key=mlperf_logger.constants.OPT_BASE_LR, value=args.learning_rate)
+    mlperf_logger.log_event(key=mlperf_logger.constants.OPT_LR_WARMUP_STEPS,
+                            value=args.lr_num_warmup_steps)
+
+    # use logging keys from the official HP table and not from the logging library
+    mlperf_logger.log_event(key='sgd_opt_base_learning_rate', value=args.learning_rate)
+    mlperf_logger.log_event(key='lr_decay_start_steps', value=args.lr_decay_start_step)
+    mlperf_logger.log_event(key='sgd_opt_learning_rate_decay_steps', value=args.lr_num_decay_steps)
+    mlperf_logger.log_event(key='sgd_opt_learning_rate_decay_poly_power', value=2)
+
+    # record_shapes=True
+    # if hasattr(torch.autograd.profiler.profile, "resume"):
+    #     prof_support_suspend_resume = True
+    #     prof_arg_dict = {"start_suspended": True}
+    # else:
+    #     prof_support_suspend_resume = False
+    #     prof_arg_dict = { }
+
+    # prof_start_iter = args.profiling_start_iter
+    # prof_end_iter = prof_start_iter + args.profiling_num_iters
+    train_start = time.time()
+    # with torch.autograd.profiler.profile(args.enable_profiling, use_gpu, record_shapes=record_shapes, **prof_arg_dict) as prof:
+
+    with torch.autograd.profiler.profile(args.enable_profiling, use_gpu) as prof:
+        while k < args.nepochs:
+            mlperf_logger.barrier()
+            mlperf_logger.log_start(key=mlperf_logger.constants.BLOCK_START,
+                                    metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1),
+                                              mlperf_logger.constants.EPOCH_COUNT: 1})
+            mlperf_logger.barrier()
+            mlperf_logger.log_start(key=mlperf_logger.constants.EPOCH_START,
+                                    metadata={mlperf_logger.constants.EPOCH_NUM: k + 1})
+
+            if k < skip_upto_epoch:
+                continue
+
+            if compression_scheduler:
+                compression_scheduler.on_epoch_begin(epoch=k)
+
+            accum_time_begin = time_wrap(use_gpu)
+
+            if args.mlperf_logging:
+                previous_iteration_time = None
+
+            if compression_scheduler:
+                mini_num_batchs = steps_per_epoch = math.ceil(len(train_ld) / args.nepochs)
+            else:
+                mini_num_batchs = len(train_ld)
+
+            for j, (X, lS_o, lS_i, T) in train_ld_iter:
+                mini_j = j % mini_num_batchs
+                if j == 0 and args.save_onnx:
+                    (X_onnx, lS_o_onnx, lS_i_onnx) = (X, lS_o, lS_i)
+
+                if j < skip_upto_batch:
+                    continue
+
+                if args.mlperf_logging:
+                    current_time = time_wrap(use_gpu)
+                    if previous_iteration_time:
+                        iteration_time = current_time - previous_iteration_time
+                    else:
+                        iteration_time = 0
+                    previous_iteration_time = current_time
+                    # if prof and prof_support_suspend_resume and j == prof_start_iter: prof.resume()
+                    # if prof and prof_support_suspend_resume and j == prof_end_iter: prof.suspend()
+                else:
+                    # ext_dist.barrier()
+                    # if prof and prof_support_suspend_resume and j >= prof_start_iter and j < prof_end_iter: prof.resume()
+                    t1 = time_wrap(use_gpu)
+
+                # early exit if nbatches was set by the user and has been exceeded
+                if nbatches > 0 and j >= nbatches:
+                    break
+                '''
+                # debug prints
+                print("input and targets")
+                print(X.detach().cpu().numpy())
+                print([np.diff(S_o.detach().cpu().tolist()
+                       + list(lS_i[i].shape)).tolist() for i, S_o in enumerate(lS_o)])
+                print([S_i.detach().cpu().numpy().tolist() for S_i in lS_i])
+                print(T.detach().cpu().numpy())
+                '''
+
+                if compression_scheduler and not args.inference_only:
+                    compression_scheduler.on_minibatch_begin(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, optimizer=optimizer)
+
+                # forward pass
+                Z = dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device)
+
+                # loss
+                E = loss_fn_wrap(Z, T, use_gpu, use_ipex, device)
+                '''
+                # debug prints
+                print("output and loss")
+                print(Z.detach().cpu().numpy())
+                print(E.detach().cpu().numpy())
+                '''
+                # compute loss and accuracy
+                L = E.detach().cpu().numpy()  # numpy array
+                S = Z.detach().cpu().numpy()  # numpy array
+                T = T.detach().cpu().numpy()  # numpy array
+                mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
+                A = np.sum((np.round(S, 0) == T).astype(np.uint8))
+
+                if not args.inference_only:
+                    # scaled error gradient propagation
+                    # (where we do not accumulate gradients across mini-batches)
+                    if compression_scheduler:
+                        if args.optimizer == 1 or args.optimizer == 3:
+                            compression_scheduler.before_backward_pass(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, loss=L, optimizer=optimizer_dense)
+                            compression_scheduler.before_backward_pass(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, loss=L, optimizer=optimizer_sparse)
+                        else:
+                            compression_scheduler.before_backward_pass(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, loss=L, optimizer=optimizer)
+
+                    if args.optimizer == 1 or args.optimizer == 3:
+                        optimizer_dense.zero_grad()
+                        optimizer_sparse.zero_grad()
+                    else:
+                        optimizer.zero_grad()
+                    # backward pass
+                    E.backward()
+                    if compression_scheduler:
+                        if args.optimizer == 1 or args.optimizer == 3:
+                            compression_scheduler.before_parameter_optimization(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, optimizer=optimizer_dense)
+                            compression_scheduler.before_parameter_optimization(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, optimizer=optimizer_sparse)
+                        else:
+                            compression_scheduler.before_parameter_optimization(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, optimizer=optimizer)
+                    # debug prints (check gradient norm)
+                    # for l in mlp.layers:
+                    #     if hasattr(l, 'weight'):
+                    #          print(l.weight.grad.norm().item())
+
+                    # optimizer
+                    if args.optimizer == 1 or args.optimizer == 3:
+                        optimizer_dense.step()
+                        optimizer_sparse.step()
+                    else:
+                        optimizer.step()
+                    lr_scheduler.step()
+
+                    if compression_scheduler:
+                        compression_scheduler.on_minibatch_end(epoch=k, minibatch_id=mini_j, minibatches_per_epoch=steps_per_epoch, optimizer=optimizer)
+
+                if args.mlperf_logging:
+                    total_time += iteration_time
+                else:
+                    t2 = time_wrap(use_gpu)
+                    total_time += t2 - t1
+                total_accu += A
+                total_loss += L * mbs
+                total_iter += 1
+                total_samp += mbs
+
+                should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)
+                should_test = (
+                    (args.test_freq > 0)
+                    and (args.data_generation == "dataset")
+                    and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
+                )
+
+                # print time, loss and accuracy
+                if should_print or should_test:
+                    gT = 1000.0 * total_time / total_iter if args.print_time else -1
+                    total_time = 0
+
+                    gA = total_accu / total_samp
+                    total_accu = 0
+
+                    gL = total_loss / total_samp
+                    total_loss = 0
+
+                    str_run_type = "inference" if args.inference_only else "training"
+                    if compression_scheduler:
+                        print(
+                            "Finished {} it {}/{} of epoch {}, {:.2f} ms/it, ".format(
+                                str_run_type, mini_j + 1, mini_num_batchs, k, gT
+                            )
+                            + "loss {:.6f}, accuracy {:3.3f} %".format(gL, gA * 100)
+                        )
+                    else:
+                        print(
+                            "Finished {} it {}/{} of epoch {}, {:.2f} ms/it, ".format(
+                                str_run_type, j + 1, nbatches, k, gT
+                            )
+                            + "loss {:.6f}, accuracy {:3.3f} %".format(gL, gA * 100)
+                        )
+                    # Uncomment the line below to print out the total time with overhead
+                    # print("Accumulated time so far: {}" \
+                    # .format(time_wrap(use_gpu) - accum_time_begin))
+                    total_iter = 0
+                    total_samp = 0
+
+                # testing
+                if should_test and not args.inference_only:
+                    epoch_num_float = (j + 1) / len(train_ld) + k + 1
+                    mlperf_logger.barrier()
+                    mlperf_logger.log_start(key=mlperf_logger.constants.EVAL_START,
+                                            metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
+
+                    # don't measure training iter time in a test iteration
+                    if args.mlperf_logging:
+                        previous_iteration_time = None
+
+                    test_accu = 0
+                    test_loss = 0
+                    test_samp = 0
+
+                    accum_test_time_begin = time_wrap(use_gpu)
+                    if args.mlperf_logging:
+                        scores = []
+                        targets = []
+
+                    for i, (X_test, lS_o_test, lS_i_test, T_test) in enumerate(test_ld):
+                        # early exit if nbatches was set by the user and was exceeded
+                        if nbatches > 0 and i >= nbatches:
+                            break
+
+                        t1_test = time_wrap(use_gpu)
+
+                        # forward pass
+                        Z_test = dlrm_wrap(
+                            X_test, lS_o_test, lS_i_test, use_gpu, use_ipex, device
+                        )
+                        if args.mlperf_logging:
+                            if ext_dist.my_size > 1:
+                                Z_test = ext_dist.all_gather(Z_test, None)
+                                T_test = ext_dist.all_gather(T_test, None)
+                            S_test = Z_test.detach().cpu().numpy()  # numpy array
+                            T_test = T_test.detach().cpu().numpy()  # numpy array
+                            scores.append(S_test)
+                            targets.append(T_test)
+                        else:
+                            # loss
+                            E_test = loss_fn_wrap(Z_test, T_test, use_gpu, use_ipex, device)
+
+                            # compute loss and accuracy
+                            L_test = E_test.detach().cpu().numpy()  # numpy array
+                            S_test = Z_test.detach().cpu().numpy()  # numpy array
+                            T_test = T_test.detach().cpu().numpy()  # numpy array
+                            mbs_test = T_test.shape[0]  # = mini_batch_size except last
+                            A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))
+                            test_accu += A_test
+                            test_loss += L_test * mbs_test
+                            test_samp += mbs_test
+
+                        t2_test = time_wrap(use_gpu)
+
+                    if args.mlperf_logging:
+                        scores = np.concatenate(scores, axis=0)
+                        targets = np.concatenate(targets, axis=0)
+
+                        validation_results = {}
+                        if args.use_ipex:
+                            validation_results['roc_auc'], validation_results['loss'], validation_results['accuracy'] = \
+                                core.roc_auc_score(torch.from_numpy(targets).reshape(-1), torch.from_numpy(scores).reshape(-1))
+                        else:
+                            metrics = {
+                                'loss' : sklearn.metrics.log_loss,
+                                'recall' : lambda y_true, y_score:
+                                sklearn.metrics.recall_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'precision' : lambda y_true, y_score:
+                                sklearn.metrics.precision_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'f1' : lambda y_true, y_score:
+                                sklearn.metrics.f1_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                                'ap' : sklearn.metrics.average_precision_score,
+                                'roc_auc' : sklearn.metrics.roc_auc_score,
+                                'accuracy' : lambda y_true, y_score:
+                                sklearn.metrics.accuracy_score(
+                                    y_true=y_true,
+                                    y_pred=np.round(y_score)
+                                ),
+                            }
+
+                            # print("Compute time for validation metric : ", end="")
+                            # first_it = True
+                            for metric_name, metric_function in metrics.items():
+                                # if first_it:
+                                #     first_it = False
+                                # else:
+                                #     print(", ", end="")
+                                # metric_compute_start = time_wrap(False)
+                                validation_results[metric_name] = metric_function(
+                                    targets,
+                                    scores
+                                )
+                                # metric_compute_end = time_wrap(False)
+                                # met_time = metric_compute_end - metric_compute_start
+                                # print("{} {:.4f}".format(metric_name, 1000 * (met_time)),
+                                #      end="")
+
+                        # print(" ms")
+                        gA_test = validation_results['accuracy']
+                        gL_test = validation_results['loss']
+                    else:
+                        gA_test = test_accu / test_samp
+                        gL_test = test_loss / test_samp
+
+                    is_best = gA_test > best_gA_test
+                    
+                    dlrm.to(torch.device("cpu"))
+                    if is_best:
+                        best_gA_test = gA_test
+                        if not (args.save_model == ""):
+                            print("Saving model to {}".format(args.save_model))
+                            torch.save(
+                                {
+                                    "epoch": k,
+                                    "nepochs": args.nepochs,
+                                    "nbatches": nbatches,
+                                    "nbatches_test": nbatches_test,
+                                    "iter": j + 1,
+                                    "state_dict": dlrm.state_dict(),
+                                    "train_acc": gA,
+                                    "train_loss": gL,
+                                    "test_acc": gA_test,
+                                    "test_loss": gL_test,
+                                    "total_loss": total_loss,
+                                    "total_accu": total_accu,
+                                },
+                                os.path.join(args.save_model, "dlrm_s_pytorch_" + str(dlrm.rank) + "_best.pkl")
+                            )
+                    else:
+                        if not (args.save_model == ""):
+                            torch.save(
+                                {
+                                    "epoch": k,
+                                    "nepochs": args.nepochs,
+                                    "nbatches": nbatches,
+                                    "nbatches_test": nbatches_test,
+                                    "iter": j + 1,
+                                    "state_dict": dlrm.state_dict(),
+                                    "train_acc": gA,
+                                    "train_loss": gL,
+                                    "test_acc": gA_test,
+                                    "test_loss": gL_test,
+                                    "total_loss": total_loss,
+                                    "total_accu": total_accu,
+                                },
+                                os.path.join(args.save_model, "dlrm_s_pytorch_" + str(dlrm.rank) + ".pkl")
+                            )
+                    dlrm.to(device)
+
+                    if args.mlperf_logging:
+                        is_best = validation_results['roc_auc'] > best_auc_test
+                        if is_best:
+                            best_auc_test = validation_results['roc_auc']
+
+                        mlperf_logger.log_event(key=mlperf_logger.constants.EVAL_ACCURACY,
+                                                value=float(validation_results['roc_auc']),
+                                                metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
+                        print(
+                            "Testing at - {}/{} of epoch {},".format(j + 1, nbatches, k)
+                            + " loss {:.6f},".format(
+                                validation_results['loss']
+                            )
+                            + " auc {:.4f}, best auc {:.4f},".format(
+                                validation_results['roc_auc'],
+                                best_auc_test
+                            )
+                            + " accuracy {:3.3f} %, best accuracy {:3.3f} %".format(
+                                validation_results['accuracy'] * 100,
+                                best_gA_test * 100
+                            )
+                        )
+                    else:
+                        print(
+                            "Testing at - {}/{} of epoch {},".format(j + 1, nbatches, 0)
+                            + " loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %".format(
+                                gL_test, gA_test * 100, best_gA_test * 100
+                            )
+                        )
+                    mlperf_logger.barrier()
+                    mlperf_logger.log_end(key=mlperf_logger.constants.EVAL_STOP,
+                                          metadata={mlperf_logger.constants.EPOCH_NUM: epoch_num_float})
+
+                    # Uncomment the line below to print out the total time with overhead
+                    # print("Total test time for this group: {}" \
+                    # .format(time_wrap(use_gpu) - accum_test_time_begin))
+
+                    if (args.mlperf_logging
+                        and (args.mlperf_acc_threshold > 0)
+                        and (best_gA_test > args.mlperf_acc_threshold)):
+                        print("MLPerf testing accuracy threshold "
+                              + str(args.mlperf_acc_threshold)
+                              + " reached, stop training")
+                        break
+
+                    if (args.mlperf_logging
+                        and (args.mlperf_auc_threshold > 0)
+                        and (best_auc_test > args.mlperf_auc_threshold)):
+                        print("MLPerf testing auc threshold "
+                              + str(args.mlperf_auc_threshold)
+                              + " reached, stop training")
+                        train_end = time.time()
+                        total_time = train_end - train_start
+                        print(F"Total Time:{total_time}")
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_end(key=mlperf_logger.constants.RUN_STOP,
+                                              metadata={
+                                                  mlperf_logger.constants.STATUS: mlperf_logger.constants.SUCCESS})
+                                
+                        break
+                    #ext_dist.barrier()
+                if mini_j + 1 >= mini_num_batchs:
+                    break
+
+            if compression_scheduler:
+                compression_scheduler.on_epoch_end(epoch=k, optimizer=optimizer, metrics={'min': total_loss, 'max': total_accu})
+
+            mlperf_logger.barrier()
+            mlperf_logger.log_end(key=mlperf_logger.constants.EPOCH_STOP,
+                                  metadata={mlperf_logger.constants.EPOCH_NUM: k + 1})
+            mlperf_logger.barrier()
+            mlperf_logger.log_end(key=mlperf_logger.constants.BLOCK_STOP,
+                                  metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: k + 1})
+            k += 1  # nepochs
+    train_end = time.time()
+    total_time = train_end - train_start
+    print(F"Total Time:{total_time}")
+    if args.enable_profiling:
+        print(prof.key_averages().table(sort_by="cpu_time_total"))
+
+    if args.mlperf_logging and best_auc_test <= args.mlperf_auc_threshold:
+        mlperf_logger.barrier()
+        mlperf_logger.log_end(key=mlperf_logger.constants.RUN_STOP,
+                              metadata={mlperf_logger.constants.STATUS: mlperf_logger.constants.ABORTED})
+
+    # profiling
+    if args.enable_profiling:
+        with open("dlrm_s_pytorch.prof", "w") as prof_f:
+            prof_f.write(prof.key_averages().table(sort_by="cpu_time_total"))
+            prof.export_chrome_trace("./dlrm_s_pytorch.json")
+        # print(prof.key_averages().table(sort_by="cpu_time_total"))
+
+    # plot compute graph
+    if args.plot_compute_graph:
+        sys.exit(
+            "ERROR: Please install pytorchviz package in order to use the"
+            + " visualization. Then, uncomment its import above as well as"
+            + " three lines below and run the code again."
+        )
+        # V = Z.mean() if args.inference_only else E
+        # dot = make_dot(V, params=dict(dlrm.named_parameters()))
+        # dot.render('dlrm_s_pytorch_graph') # write .pdf file
+
+    # test prints
+    if not args.inference_only and args.debug_mode:
+        print("updated parameters (weights and bias):")
+        for param in dlrm.parameters():
+            print(param.detach().cpu().numpy())
+
+    # export the model in onnx
+    if args.save_onnx:
+        dlrm_pytorch_onnx_file = "dlrm_s_pytorch.onnx"
+        torch.onnx.export(
+            dlrm, (X_onnx, lS_o_onnx, lS_i_onnx), dlrm_pytorch_onnx_file, verbose=True, use_external_data_format=True
+        )
+        # recover the model back
+        dlrm_pytorch_onnx = onnx.load("dlrm_s_pytorch.onnx")
+        # check the onnx model
+        onnx.checker.check_model(dlrm_pytorch_onnx)
diff --git a/dlrm_s_pytorch_inference.py b/dlrm_s_pytorch_inference.py
new file mode 100644
index 0000000..6029b80
--- /dev/null
+++ b/dlrm_s_pytorch_inference.py
@@ -0,0 +1,1125 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+#
+# Description: an implementation of a deep learning recommendation model (DLRM)
+# The model input consists of dense and sparse features. The former is a vector
+# of floating point values. The latter is a list of sparse indices into
+# embedding tables, which consist of vectors of floating point values.
+# The selected vectors are passed to mlp networks denoted by triangles,
+# in some cases the vectors are interacted through operators (Ops).
+#
+# output:
+#                         vector of values
+# model:                        |
+#                              /\
+#                             /__\
+#                               |
+#       _____________________> Op  <___________________
+#     /                         |                      \
+#    /\                        /\                      /\
+#   /__\                      /__\           ...      /__\
+#    |                          |                       |
+#    |                         Op                      Op
+#    |                    ____/__\_____           ____/__\____
+#    |                   |_Emb_|____|__|    ...  |_Emb_|__|___|
+# input:
+# [ dense features ]     [sparse indices] , ..., [sparse indices]
+#
+# More precise definition of model layers:
+# 1) fully connected layers of an mlp
+# z = f(y)
+# y = Wx + b
+#
+# 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])
+# z = Op(e1,...,ek)
+# obtain vectors e1=E[:,p1], ..., ek=E[:,pk]
+#
+# 3) Operator Op can be one of the following
+# Sum(e1,...,ek) = e1 + ... + ek
+# Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]
+# Cat(e1,...,ek) = [e1', ..., ek']'
+# where ' denotes transpose operation
+#
+# References:
+# [1] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
+# Narayanan Sundaram, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
+# Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii,
+# Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,
+# Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,
+# Misha Smelyanskiy, "Deep Learning Recommendation Model for Personalization and
+# Recommendation Systems", CoRR, arXiv:1906.00091, 2019
+
+from __future__ import absolute_import, division, print_function, unicode_literals
+
+# miscellaneous
+import builtins
+import functools
+# import bisect
+# import shutil
+import time
+import json
+# data generation
+import dlrm_data_pytorch as dp
+
+# numpy
+import numpy as np
+
+# onnx
+# The onnx import causes deprecation warnings every time workers
+# are spawned during testing. So, we filter out those warnings.
+import warnings
+with warnings.catch_warnings():
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+import onnx
+
+# pytorch
+import torch
+import torch.nn as nn
+from torch.nn.parallel.parallel_apply import parallel_apply
+from torch.nn.parallel.replicate import replicate
+from torch.nn.parallel.scatter_gather import gather, scatter
+
+# For distributed run
+import extend_distributed as ext_dist
+
+try:
+    import intel_pytorch_extension as ipex
+    from intel_pytorch_extension import core
+except:
+    pass
+from lamb_bin import Lamb, log_lamb_rs
+
+# quotient-remainder trick
+from tricks.qr_embedding_bag import QREmbeddingBag
+# mixed-dimension trick
+from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
+
+import sklearn.metrics
+import mlperf_logger
+
+# from torchviz import make_dot
+# import torch.nn.functional as Functional
+# from torch.nn.parameter import Parameter
+
+from torch.optim.lr_scheduler import _LRScheduler
+
+exc = getattr(builtins, "IOError", "FileNotFoundError")
+
+
+class Cast(nn.Module):
+     __constants__ = ['to_dtype']
+ 
+     def __init__(self, to_dtype):
+         super(Cast, self).__init__()
+         self.to_dtype = to_dtype
+ 
+     def forward(self, input):
+         if input.is_mkldnn:
+             return input.to_dense(self.to_dtype)
+         else:
+             return input.to(self.to_dtype)
+ 
+     def extra_repr(self):
+         return 'to(%s)' % self.to_dtype
+
+ 
+### define dlrm in PyTorch ###
+class DLRM_Net(nn.Module):
+    def create_mlp(self, ln, sigmoid_layer):
+        # build MLP layer by layer
+        layers = nn.ModuleList()
+        for i in range(0, ln.size - 1):
+            n = ln[i]
+            m = ln[i + 1]
+
+            # construct fully connected operator
+            if self.use_ipex and self.bf16:
+                LL = ipex.IpexMLPLinear(int(n), int(m), bias=True, output_stays_blocked=(i < ln.size - 2), default_blocking=32)
+            else:
+                LL = nn.Linear(int(n), int(m), bias=True)
+
+            # initialize the weights
+            # with torch.no_grad():
+            # custom Xavier input, output or two-sided fill
+            mean = 0.0  # std_dev = np.sqrt(variance)
+            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)
+            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)
+            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))
+            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)
+            # approach 1
+            LL.weight.data = torch.tensor(W, requires_grad=True)
+            LL.bias.data = torch.tensor(bt, requires_grad=True)
+            # approach 2
+            # LL.weight.data.copy_(torch.tensor(W))
+            # LL.bias.data.copy_(torch.tensor(bt))
+            # approach 3
+            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)
+            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)
+
+            if self.bf16 and ipex.is_available():
+                LL.to(torch.bfloat16)
+            # prepack weight for IPEX Linear
+            if hasattr(LL, 'reset_weight_shape'):
+                LL.reset_weight_shape(block_for_dtype=torch.bfloat16)
+
+            layers.append(LL)
+
+            # construct sigmoid or relu operator
+            if i == sigmoid_layer:
+                if self.bf16:
+                    layers.append(Cast(torch.float32))
+                layers.append(nn.Sigmoid())
+            else:
+                if self.use_ipex and self.bf16:
+                    LL.set_activation_type('relu')
+                else:
+                    layers.append(nn.ReLU())
+
+        # approach 1: use ModuleList
+        # return layers
+        # approach 2: use Sequential container to wrap all layers
+        return torch.nn.Sequential(*layers)
+
+    def create_emb(self, m, ln, local_ln_emb_sparse=None, ln_emb_dense=None):
+        emb_l = nn.ModuleList()
+        # save the numpy random state
+        np_rand_state = np.random.get_state()
+        emb_dense = nn.ModuleList()
+        emb_sparse = nn.ModuleList()
+        embs = range(len(ln))
+        if local_ln_emb_sparse or ln_emb_dense:
+            embs = local_ln_emb_sparse + ln_emb_dense
+        for i in embs:
+            # Use per table random seed for Embedding initialization
+            np.random.seed(self.l_emb_seeds[i])
+            n = ln[i]
+            # construct embedding operator
+            if self.qr_flag and n > self.qr_threshold:
+                EE = QREmbeddingBag(n, m, self.qr_collisions,
+                    operation=self.qr_operation, mode="sum", sparse=True)
+            elif self.md_flag:
+                base = max(m)
+                _m = m[i] if n > self.md_threshold else base
+                EE = PrEmbeddingBag(n, _m, base)
+                # use np initialization as below for consistency...
+                W = np.random.uniform(
+                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)
+                ).astype(np.float32)
+                EE.embs.weight.data = torch.tensor(W, requires_grad=True)
+
+            else:
+                # initialize embeddings
+                # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))
+                W = np.random.uniform(
+                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
+                ).astype(np.float32)
+                # approach 1
+                if n >= self.sparse_dense_boundary:
+                    #n = 39979771
+                    m_sparse = 16
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m_sparse)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m_sparse, mode="sum", sparse=True, _weight=torch.tensor(W, requires_grad=True))
+                else:
+                    W = np.random.uniform(
+                        low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
+                    ).astype(np.float32)
+                    EE = nn.EmbeddingBag(n, m, mode="sum", sparse=False, _weight=torch.tensor(W, requires_grad=True))
+                # approach 2
+                # EE.weight.data.copy_(torch.tensor(W))
+                # approach 3
+                # EE.weight = Parameter(torch.tensor(W),requires_grad=True)
+                if self.bf16 and ipex.is_available():
+                    EE.to(torch.bfloat16)
+               
+            if ext_dist.my_size > 1:
+                if n >= self.sparse_dense_boundary:
+                    emb_sparse.append(EE)
+                else:
+                    emb_dense.append(EE)
+
+            emb_l.append(EE)
+
+        # Restore the numpy random state
+        np.random.set_state(np_rand_state)
+        return emb_l, emb_dense, emb_sparse
+
+    def __init__(
+        self,
+        m_spa=None,
+        ln_emb=None,
+        ln_bot=None,
+        ln_top=None,
+        arch_interaction_op=None,
+        arch_interaction_itself=False,
+        sigmoid_bot=-1,
+        sigmoid_top=-1,
+        sync_dense_params=True,
+        loss_threshold=0.0,
+        ndevices=-1,
+        qr_flag=False,
+        qr_operation="mult",
+        qr_collisions=0,
+        qr_threshold=200,
+        md_flag=False,
+        md_threshold=200,
+        bf16=False,
+        use_ipex=False,
+        sparse_dense_boundary = 2048
+    ):
+        super(DLRM_Net, self).__init__()
+
+        if (
+            (m_spa is not None)
+            and (ln_emb is not None)
+            and (ln_bot is not None)
+            and (ln_top is not None)
+            and (arch_interaction_op is not None)
+        ):
+
+            # save arguments
+            self.ndevices = ndevices
+            self.output_d = 0
+            self.parallel_model_batch_size = -1
+            self.parallel_model_is_not_prepared = True
+            self.arch_interaction_op = arch_interaction_op
+            self.arch_interaction_itself = arch_interaction_itself
+            self.sync_dense_params = sync_dense_params
+            self.loss_threshold = loss_threshold
+            self.bf16 = bf16
+            self.use_ipex = use_ipex
+            self.sparse_dense_boundary = sparse_dense_boundary
+            # create variables for QR embedding if applicable
+            self.qr_flag = qr_flag
+            if self.qr_flag:
+                self.qr_collisions = qr_collisions
+                self.qr_operation = qr_operation
+                self.qr_threshold = qr_threshold
+            # create variables for MD embedding if applicable
+            self.md_flag = md_flag
+            if self.md_flag:
+                self.md_threshold = md_threshold
+
+            # generate np seeds for Emb table initialization
+            self.l_emb_seeds = np.random.randint(low=0, high=100000, size=len(ln_emb))
+
+            #If running distributed, get local slice of embedding tables
+            if ext_dist.my_size > 1:
+                n_emb = len(ln_emb)
+                self.n_global_emb = n_emb
+                self.rank = ext_dist.dist.get_rank()
+                self.ln_emb_dense = [i for i in range(n_emb) if ln_emb[i] < self.sparse_dense_boundary]
+                self.ln_emb_sparse = [i for i in range(n_emb) if ln_emb[i] >= self.sparse_dense_boundary]
+                n_emb_sparse = len(self.ln_emb_sparse)
+                self.n_local_emb_sparse, self.n_sparse_emb_per_rank = ext_dist.get_split_lengths(n_emb_sparse)
+                self.local_ln_emb_sparse_slice = ext_dist.get_my_slice(n_emb_sparse)
+                self.local_ln_emb_sparse = self.ln_emb_sparse[self.local_ln_emb_sparse_slice]
+            # create operators
+            if ndevices <= 1:
+                if ext_dist.my_size > 1:
+                    _, self.emb_dense, self.emb_sparse = self.create_emb(m_spa, ln_emb, self.local_ln_emb_sparse, self.ln_emb_dense)
+                else:
+                    self.emb_l, _, _ = self.create_emb(m_spa, ln_emb)
+
+            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)
+            self.top_l = self.create_mlp(ln_top, sigmoid_top)
+
+    def apply_mlp(self, x, layers):
+        # approach 1: use ModuleList
+        # for layer in layers:
+        #     x = layer(x)
+        # return x
+        # approach 2: use Sequential container to wrap all layers
+        need_padding = self.use_ipex and self.bf16 and x.size(0) % 2 == 1
+        if need_padding:
+            x = torch.nn.functional.pad(input=x, pad=(0,0,0,1), mode='constant', value=0)
+            ret = layers(x)
+            return(ret[:-1,:])
+        else:
+            return layers(x)
+
+    def apply_emb(self, lS_o, lS_i, emb_l):
+        # WARNING: notice that we are processing the batch at once. We implicitly
+        # assume that the data is laid out such that:
+        # 1. each embedding is indexed with a group of sparse indices,
+        #   corresponding to a single lookup
+        # 2. for each embedding the lookups are further organized into a batch
+        # 3. for a list of embedding tables there is a list of batched lookups
+
+        ly = []
+        for k, sparse_index_group_batch in enumerate(lS_i):
+            sparse_offset_group_batch = lS_o[k]
+
+            # embedding lookup
+            # We are using EmbeddingBag, which implicitly uses sum operator.
+            # The embeddings are represented as tall matrices, with sum
+            # happening vertically across 0 axis, resulting in a row vector
+            E = emb_l[k]
+            V = E(sparse_index_group_batch, sparse_offset_group_batch)
+
+            ly.append(V)
+
+        # print(ly)
+        return ly
+#if self.bf16:
+    def interact_features(self, x, ly):
+        x = x.to(ly[0].dtype)
+        if self.arch_interaction_op == "dot":
+            if self.bf16:
+                T = [x] + ly
+                R = ipex.interaction(*T)
+            else:
+                # concatenate dense and sparse features
+                (batch_size, d) = x.shape
+                T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
+                # perform a dot product
+                Z = torch.bmm(T, torch.transpose(T, 1, 2))
+                # append dense feature with the interactions (into a row vector)
+                # approach 1: all
+                # Zflat = Z.view((batch_size, -1))
+                # approach 2: unique
+                _, ni, nj = Z.shape
+                # approach 1: tril_indices
+                # offset = 0 if self.arch_interaction_itself else -1
+                # li, lj = torch.tril_indices(ni, nj, offset=offset)
+                # approach 2: custom
+                offset = 1 if self.arch_interaction_itself else 0
+                li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
+                lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
+                Zflat = Z[:, li, lj]
+                # concatenate dense features and interactions
+                R = torch.cat([x] + [Zflat], dim=1)
+        elif self.arch_interaction_op == "cat":
+            # concatenation features (into a row vector)
+            R = torch.cat([x] + ly, dim=1)
+        else:
+            sys.exit(
+                "ERROR: --arch-interaction-op="
+                + self.arch_interaction_op
+                + " is not supported"
+            )
+
+        return R
+
+    def forward(self, dense_x, lS_o, lS_i):
+        if self.bf16:
+            dense_x = dense_x.bfloat16()
+        if ext_dist.my_size > 1:
+            return self.distributed_forward(dense_x, lS_o, lS_i)
+        elif self.ndevices <= 1:
+            return self.sequential_forward(dense_x, lS_o, lS_i)
+        else:
+            return self.parallel_forward(dense_x, lS_o, lS_i)
+
+    def sequential_forward(self, dense_x, lS_o, lS_i):
+        # process dense features (using bottom mlp), resulting in a row vector
+        x = self.apply_mlp(dense_x, self.bot_l)
+        # debug prints
+        # print("intermediate")
+        # print(x.detach().cpu().numpy())
+
+        # process sparse features(using embeddings), resulting in a list of row vectors
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
+        # for y in ly:
+        #     print(y.detach().cpu().numpy())
+
+        # interact features (dense and sparse)
+        z = self.interact_features(x, ly)
+        # print(z.detach().cpu().numpy())
+
+        # obtain probability of a click (using top mlp)
+        p = self.apply_mlp(z, self.top_l)
+
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))
+        else:
+            z = p
+
+        return z
+
+    def distributed_forward(self, dense_x, lS_o, lS_i):
+        batch_size = dense_x.size()[0]
+        # WARNING: # of ranks must be <= batch size in distributed_forward call
+        if batch_size < ext_dist.my_size:
+            sys.exit("ERROR: batch_size (%d) must be larger than number of ranks (%d)" % (batch_size, ext_dist.my_size))
+
+        lS_o_dense = [lS_o[i]  for i in self.ln_emb_dense]
+        lS_i_dense = [lS_i[i] for i in self.ln_emb_dense]
+        lS_o_sparse = [lS_o[i] for i in self.ln_emb_sparse]  # partition sparse table in one group
+        lS_i_sparse = [lS_i[i] for i in self.ln_emb_sparse]
+
+        lS_i_sparse = ext_dist.shuffle_data(lS_i_sparse)
+        g_i_sparse = [lS_i_sparse[:, i * batch_size:(i + 1) * batch_size].reshape(-1) for i in range(len(self.local_ln_emb_sparse))]
+        offset = torch.arange(batch_size * ext_dist.my_size).to(device)
+        g_o_sparse = [offset for i in range(self.n_local_emb_sparse)]
+
+        if (len(self.local_ln_emb_sparse) != len(g_o_sparse)) or (len(self.local_ln_emb_sparse) != len(g_i_sparse)):
+           sys.exit("ERROR 0 : corrupted model input detected in distributed_forward call")
+        # sparse embeddings
+        ly_sparse = self.apply_emb(g_o_sparse, g_i_sparse, self.emb_sparse)
+        a2a_req = ext_dist.alltoall(ly_sparse, self.n_sparse_emb_per_rank)
+        # bottom mlp
+        x = self.apply_mlp(dense_x, self.bot_l)
+        # dense embeddings
+        ly_dense = self.apply_emb(lS_o_dense, lS_i_dense, self.emb_dense)
+        ly_sparse = a2a_req.wait()
+        ly_sparse2 = []
+        for i in range(len(ly_sparse)):
+            ly_sparse2.append(ly_sparse[i].repeat(1,4))
+        del ly_sparse
+        #ly_sparse ""= torch.cat(ly_sparse,1)
+        ly = ly_dense + list(ly_sparse2)
+        # interactions
+        z = self.interact_features(x, ly)
+        # top mlp
+        p = self.apply_mlp(z, self.top_l)
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z = torch.clamp(
+                p, min=self.loss_threshold, max=(1.0 - self.loss_threshold)
+            )
+        else:
+            z = p
+
+        return z
+ 
+    def parallel_forward(self, dense_x, lS_o, lS_i):
+        ### prepare model (overwrite) ###
+        # WARNING: # of devices must be >= batch size in parallel_forward call
+        batch_size = dense_x.size()[0]
+        ndevices = min(self.ndevices, batch_size, len(self.emb_l))
+        device_ids = range(ndevices)
+        # WARNING: must redistribute the model if mini-batch size changes(this is common
+        # for last mini-batch, when # of elements in the dataset/batch size is not even
+        if self.parallel_model_batch_size != batch_size:
+            self.parallel_model_is_not_prepared = True
+
+        if self.parallel_model_is_not_prepared or self.sync_dense_params:
+            # replicate mlp (data parallelism)
+            self.bot_l_replicas = replicate(self.bot_l, device_ids)
+            self.top_l_replicas = replicate(self.top_l, device_ids)
+            self.parallel_model_batch_size = batch_size
+
+        if self.parallel_model_is_not_prepared:
+            # distribute embeddings (model parallelism)
+            t_list = []
+            for k, emb in enumerate(self.emb_l):
+                d = torch.device("cuda:" + str(k % ndevices))
+                emb.to(d)
+                t_list.append(emb.to(d))
+            self.emb_l = nn.ModuleList(t_list)
+            self.parallel_model_is_not_prepared = False
+
+        ### prepare input (overwrite) ###
+        # scatter dense features (data parallelism)
+        # print(dense_x.device)
+        dense_x = scatter(dense_x, device_ids, dim=0)
+        # distribute sparse features (model parallelism)
+        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):
+            sys.exit("ERROR: corrupted model input detected in parallel_forward call")
+
+        t_list = []
+        i_list = []
+        for k, _ in enumerate(self.emb_l):
+            d = torch.device("cuda:" + str(k % ndevices))
+            t_list.append(lS_o[k].to(d))
+            i_list.append(lS_i[k].to(d))
+        lS_o = t_list
+        lS_i = i_list
+
+        ### compute results in parallel ###
+        # bottom mlp
+        # WARNING: Note that the self.bot_l is a list of bottom mlp modules
+        # that have been replicated across devices, while dense_x is a tuple of dense
+        # inputs that has been scattered across devices on the first (batch) dimension.
+        # The output is a list of tensors scattered across devices according to the
+        # distribution of dense_x.
+        x = parallel_apply(self.bot_l_replicas, dense_x, None, device_ids)
+        # debug prints
+        # print(x)
+
+        # embeddings
+        ly = self.apply_emb(lS_o, lS_i, self.emb_l)
+        # debug prints
+        # print(ly)
+
+        # butterfly shuffle (implemented inefficiently for now)
+        # WARNING: Note that at this point we have the result of the embedding lookup
+        # for the entire batch on each device. We would like to obtain partial results
+        # corresponding to all embedding lookups, but part of the batch on each device.
+        # Therefore, matching the distribution of output of bottom mlp, so that both
+        # could be used for subsequent interactions on each device.
+        if len(self.emb_l) != len(ly):
+            sys.exit("ERROR: corrupted intermediate result in parallel_forward call")
+
+        t_list = []
+        for k, _ in enumerate(self.emb_l):
+            d = torch.device("cuda:" + str(k % ndevices))
+            y = scatter(ly[k], device_ids, dim=0)
+            t_list.append(y)
+        # adjust the list to be ordered per device
+        ly = list(map(lambda y: list(y), zip(*t_list)))
+        # debug prints
+        # print(ly)
+
+        # interactions
+        z = []
+        for k in range(ndevices):
+            zk = self.interact_features(x[k], ly[k])
+            z.append(zk)
+        # debug prints
+        # print(z)
+
+        # top mlp
+        # WARNING: Note that the self.top_l is a list of top mlp modules that
+        # have been replicated across devices, while z is a list of interaction results
+        # that by construction are scattered across devices on the first (batch) dim.
+        # The output is a list of tensors scattered across devices according to the
+        # distribution of z.
+        p = parallel_apply(self.top_l_replicas, z, None, device_ids)
+
+        ### gather the distributed results ###
+        p0 = gather(p, self.output_d, dim=0)
+
+        # clamp output if needed
+        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
+            z0 = torch.clamp(
+                p0, min=self.loss_threshold, max=(1.0 - self.loss_threshold)
+            )
+        else:
+            z0 = p0
+
+        return z0
+
+
+if __name__ == "__main__":
+    # the reference implementation doesn't clear the cache currently
+    # but the submissions are required to do that
+    mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)
+
+    mlperf_logger.log_start(key=mlperf_logger.constants.INIT_START, log_all_ranks=True)
+
+    ### import packages ###
+    import sys
+    import os
+    import argparse
+
+    ### parse arguments ###
+    parser = argparse.ArgumentParser(
+        description="Train Deep Learning Recommendation Model (DLRM)"
+    )
+    # model related parameters
+    parser.add_argument("--arch-sparse-feature-size", type=int, default=2)
+    parser.add_argument("--arch-embedding-size", type=str, default="4-3-2")
+    # j will be replaced with the table number
+    parser.add_argument("--arch-mlp-bot", type=str, default="4-3-2")
+    parser.add_argument("--arch-mlp-top", type=str, default="4-2-1")
+    parser.add_argument("--arch-interaction-op", type=str, default="dot")
+    parser.add_argument("--arch-interaction-itself", action="store_true", default=False)
+    # embedding table options
+    parser.add_argument("--md-flag", action="store_true", default=False)
+    parser.add_argument("--md-threshold", type=int, default=200)
+    parser.add_argument("--md-temperature", type=float, default=0.3)
+    parser.add_argument("--md-round-dims", action="store_true", default=False)
+    parser.add_argument("--qr-flag", action="store_true", default=False)
+    parser.add_argument("--qr-threshold", type=int, default=200)
+    parser.add_argument("--qr-operation", type=str, default="mult")
+    parser.add_argument("--qr-collisions", type=int, default=4)
+    # activations and loss
+    parser.add_argument("--activation-function", type=str, default="relu")
+    parser.add_argument("--loss-function", type=str, default="mse")  # or bce or wbce
+    parser.add_argument("--loss-weights", type=str, default="1.0-1.0")  # for wbce
+    parser.add_argument("--loss-threshold", type=float, default=0.0)  # 1.0e-7
+    parser.add_argument("--round-targets", type=bool, default=False)
+    # data
+    parser.add_argument("--data-size", type=int, default=1)
+    parser.add_argument("--num-batches", type=int, default=0)
+    parser.add_argument(
+        "--data-generation", type=str, default="random"
+    )  # synthetic or dataset
+    parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
+    parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
+    parser.add_argument("--raw-data-file", type=str, default="")
+    parser.add_argument("--processed-data-file", type=str, default="")
+    parser.add_argument("--data-randomize", type=str, default="total")  # or day or none
+    parser.add_argument("--data-trace-enable-padding", type=bool, default=False)
+    parser.add_argument("--max-ind-range", type=int, default=-1)
+    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
+    parser.add_argument("--num-indices-per-lookup", type=int, default=10)
+    parser.add_argument("--num-indices-per-lookup-fixed", type=bool, default=False)
+    parser.add_argument("--num-workers", type=int, default=0)
+    parser.add_argument("--memory-map", action="store_true", default=False)
+    # training
+    parser.add_argument("--mini-batch-size", type=int, default=1)
+    parser.add_argument("--nepochs", type=int, default=1)
+    parser.add_argument("--learning-rate", type=float, default=0.01)
+    parser.add_argument("--print-precision", type=int, default=5)
+    parser.add_argument("--numpy-rand-seed", type=int, default=123)
+    parser.add_argument("--sync-dense-params", type=bool, default=True)
+    # inference
+    parser.add_argument("--inference-only", action="store_true", default=False)
+    # onnx
+    parser.add_argument("--save-onnx", action="store_true", default=False)
+    # gpu
+    parser.add_argument("--use-gpu", action="store_true", default=False)
+    # distributed run
+    parser.add_argument("--dist-backend", type=str, default="")
+    # debugging and profiling
+    parser.add_argument("--print-freq", type=int, default=1)
+    parser.add_argument("--test-freq", type=int, default=-1)
+    parser.add_argument("--test-mini-batch-size", type=int, default=-1)
+    parser.add_argument("--test-num-workers", type=int, default=-1)
+    parser.add_argument("--print-time", action="store_true", default=False)
+    parser.add_argument("--debug-mode", action="store_true", default=False)
+    parser.add_argument("--enable-profiling", action="store_true", default=False)
+    parser.add_argument("--plot-compute-graph", action="store_true", default=False)
+    parser.add_argument("--profiling-start-iter", type=int, default=50)
+    parser.add_argument("--profiling-num-iters", type=int, default=100)
+    # store/load model
+    parser.add_argument("--out-dir", type=str, default=".")
+    parser.add_argument("--save-model", type=str, default="")
+    parser.add_argument("--load-model", type=str, default="")
+    # mlperf logging (disables other output and stops early)
+    parser.add_argument("--mlperf-logging", action="store_true", default=False)
+    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
+    parser.add_argument("--mlperf-acc-threshold", type=float, default=0.0)
+    # stop at target AUC Terabyte (no subsampling) 0.8025
+    parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
+    parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
+    parser.add_argument("--mlperf-bin-shuffle", action='store_true', default=False)
+    # LR policy
+    parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
+    parser.add_argument("--lr-decay-start-step", type=int, default=0)
+    parser.add_argument("--lr-num-decay-steps", type=int, default=0)
+    # embedding table is sparse table only if sparse_dense_boundary >= 2048
+    parser.add_argument("--sparse-dense-boundary", type=int, default=2048)
+    # bf16 option
+    parser.add_argument("--bf16", action='store_true', default=False)
+    # ipex option
+    parser.add_argument("--use-ipex", action="store_true", default=False)
+    # lamb
+    parser.add_argument("--optimizer", type=int, default=0, help='optimizer:[0:sgd, 1:lamb/sgd, 2:adagrad, 3:sparseadam]')
+    parser.add_argument("--lamblr", type=float, default=0.01, help='lr for lamb')
+    args = parser.parse_args()
+
+    ext_dist.init_distributed(backend=args.dist_backend)
+
+    if args.mlperf_logging:
+        print('command line args: ', json.dumps(vars(args)))
+
+    ### some basic setup ###
+    np.random.seed(args.numpy_rand_seed)
+    np.set_printoptions(precision=args.print_precision)
+    torch.set_printoptions(precision=args.print_precision)
+    torch.manual_seed(args.numpy_rand_seed)
+
+    if (args.test_mini_batch_size < 0):
+        # if the parameter is not set, use the training batch size
+        args.test_mini_batch_size = args.mini_batch_size
+    if (args.test_num_workers < 0):
+        # if the parameter is not set, use the same parameter for training
+        args.test_num_workers = args.num_workers
+    if (args.mini_batch_size % ext_dist.my_size !=0 or args.test_mini_batch_size % ext_dist.my_size != 0):
+        print("Either test minibatch (%d) or train minibatch (%d) does not split across %d ranks" % (args.test_mini_batch_size, args.mini_batch_size, ext_dist.my_size))
+        sys.exit(1)
+
+    use_gpu = args.use_gpu and torch.cuda.is_available()
+    use_ipex = args.use_ipex
+    if use_gpu:
+        torch.cuda.manual_seed_all(args.numpy_rand_seed)
+        torch.backends.cudnn.deterministic = True
+        if ext_dist.my_size > 1:
+            ngpus = torch.cuda.device_count()  # 1
+            if ext_dist.my_local_size > torch.cuda.device_count():
+                print("Not sufficient GPUs available... local_size = %d, ngpus = %d" % (ext_dist.my_local_size, ngpus))
+                sys.exit(1)
+            ngpus = 1
+            device = torch.device("cuda", ext_dist.my_local_rank)
+        else:
+            device = torch.device("cuda", 0)
+            ngpus = torch.cuda.device_count()  # 1
+        print("Using {} GPU(s)...".format(ngpus))
+    elif use_ipex:
+        device = torch.device("dpcpp")
+        print("Using IPEX...")
+    else:
+        device = torch.device("cpu")
+        print("Using CPU...")
+
+    ### prepare training data ###
+    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
+    # input data
+
+    mlperf_logger.barrier()
+    mlperf_logger.log_end(key=mlperf_logger.constants.INIT_STOP)
+    mlperf_logger.barrier()
+    mlperf_logger.log_start(key=mlperf_logger.constants.RUN_START)
+    mlperf_logger.barrier()
+
+    if (args.data_generation == "dataset"):
+        test_data, test_ld = \
+            dp.make_criteo_data_and_loaders_test(args)
+        nbatches_test = len(test_ld)
+        ln_emb = test_data.counts
+        # enforce maximum limit on number of vectors per embedding
+        if args.max_ind_range > 0:
+            ln_emb = np.array(list(map(
+                lambda x: x if x < args.max_ind_range else args.max_ind_range,
+                ln_emb
+            )))
+        m_den = test_data.m_den
+        ln_bot[0] = m_den
+
+    else:
+        # input and target at random
+        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
+        m_den = ln_bot[0]
+        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
+        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
+
+    ### parse command line arguments ###
+    m_spa = args.arch_sparse_feature_size
+    num_fea = ln_emb.size + 1  # num sparse + num dense features
+    m_den_out = ln_bot[ln_bot.size - 1]
+    if args.arch_interaction_op == "dot":
+        # approach 1: all
+        # num_int = num_fea * num_fea + m_den_out
+        # approach 2: unique
+        if args.arch_interaction_itself:
+            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
+        else:
+            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
+    elif args.arch_interaction_op == "cat":
+        num_int = num_fea * m_den_out
+    else:
+        sys.exit(
+            "ERROR: --arch-interaction-op="
+            + args.arch_interaction_op
+            + " is not supported"
+        )
+    arch_mlp_top_adjusted = str(num_int) + "-" + args.arch_mlp_top
+    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep="-")
+
+    # sanity check: feature sizes and mlp dimensions must match
+    if m_den != ln_bot[0]:
+        sys.exit(
+            "ERROR: arch-dense-feature-size "
+            + str(m_den)
+            + " does not match first dim of bottom mlp "
+            + str(ln_bot[0])
+        )
+    if args.qr_flag:
+        if args.qr_operation == "concat" and 2 * m_spa != m_den_out:
+            sys.exit(
+                "ERROR: 2 arch-sparse-feature-size "
+                + str(2 * m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+                + " (note that the last dim of bottom mlp must be 2x the embedding dim)"
+            )
+        if args.qr_operation != "concat" and m_spa != m_den_out:
+            sys.exit(
+                "ERROR: arch-sparse-feature-size "
+                + str(m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+            )
+    else:
+        if m_spa != m_den_out:
+            sys.exit(
+                "ERROR: arch-sparse-feature-size "
+                + str(m_spa)
+                + " does not match last dim of bottom mlp "
+                + str(m_den_out)
+            )
+    if num_int != ln_top[0]:
+        sys.exit(
+            "ERROR: # of feature interactions "
+            + str(num_int)
+            + " does not match first dimension of top mlp "
+            + str(ln_top[0])
+        )
+
+    # assign mixed dimensions if applicable
+    if args.md_flag:
+        m_spa = md_solver(
+            torch.tensor(ln_emb),
+            args.md_temperature,  # alpha
+            d0=m_spa,
+            round_dim=args.md_round_dims
+        ).tolist()
+   
+    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
+
+    ### construct the neural network specified above ###
+    # WARNING: to obtain exactly the same initialization for
+    # the weights we need to start from the same random seed.
+    # np.random.seed(args.numpy_rand_seed)
+    print('Creating the model...')
+    dlrm = DLRM_Net(
+        m_spa,
+        ln_emb,
+        ln_bot,
+        ln_top,
+        arch_interaction_op=args.arch_interaction_op,
+        arch_interaction_itself=args.arch_interaction_itself,
+        sigmoid_bot=-1,
+        sigmoid_top=ln_top.size - 2,
+        sync_dense_params=args.sync_dense_params,
+        loss_threshold=args.loss_threshold,
+        ndevices=ndevices,
+        qr_flag=args.qr_flag,
+        qr_operation=args.qr_operation,
+        qr_collisions=args.qr_collisions,
+        qr_threshold=args.qr_threshold,
+        md_flag=args.md_flag,
+        md_threshold=args.md_threshold,
+        sparse_dense_boundary=args.sparse_dense_boundary,
+        bf16 = args.bf16,
+        use_ipex = args.use_ipex
+    )
+    
+    print('Model created!')
+    # test prints
+    if args.debug_mode:
+        print("initial parameters (weights and bias):")
+        for param in dlrm.parameters():
+            print(param.detach().cpu().numpy())
+        # print(dlrm)
+
+    if args.use_ipex:
+       dlrm = dlrm.to(device)
+       print(dlrm, device, args.use_ipex)
+
+    if use_gpu:
+        # Custom Model-Data Parallel
+        # the mlps are replicated and use data parallelism, while
+        # the embeddings are distributed and use model parallelism
+        dlrm = dlrm.to(device)  # .cuda()
+        if dlrm.ndevices > 1:
+            dlrm.emb_l = dlrm.create_emb(m_spa, ln_emb)
+
+    if ext_dist.my_size > 1:
+        if use_gpu:
+            device_ids = [ext_dist.my_local_rank]
+            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l, device_ids=device_ids)
+            dlrm.top_l = ext_dist.DDP(dlrm.top_l, device_ids=device_ids)
+        else:
+            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l)
+            dlrm.top_l = ext_dist.DDP(dlrm.top_l)
+            for i in range(len(dlrm.emb_dense)):
+                dlrm.emb_dense[i] = ext_dist.DDP(dlrm.emb_dense[i])
+
+    # specify the loss function
+    if args.loss_function == "mse":
+        loss_fn = torch.nn.MSELoss(reduction="mean")
+    elif args.loss_function == "bce":
+        loss_fn = torch.nn.BCELoss(reduction="mean")
+    elif args.loss_function == "wbce":
+        loss_ws = torch.tensor(np.fromstring(args.loss_weights, dtype=float, sep="-"))
+        loss_fn = torch.nn.BCELoss(reduction="none")
+    else:
+        sys.exit("ERROR: --loss-function=" + args.loss_function + " is not supported")
+    ### main loop ###
+    def time_wrap(use_gpu):
+        if use_gpu:
+            torch.cuda.synchronize()
+        return time.time()
+
+    def dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device):
+        if use_gpu or use_ipex:  # .cuda()
+            # lS_i can be either a list of tensors or a stacked tensor.
+            # Handle each case below:
+            lS_i = [S_i.to(device) for S_i in lS_i] if isinstance(lS_i, list) \
+                else lS_i.to(device)
+            lS_o = [S_o.to(device) for S_o in lS_o] if isinstance(lS_o, list) \
+                else lS_o.to(device)
+            return dlrm(
+                X.to(device),
+                lS_o,
+                lS_i
+            )
+        else:
+            return dlrm(X, lS_o, lS_i)
+
+    def loss_fn_wrap(Z, T, use_gpu, use_ipex, device):
+        if args.loss_function == "mse" or args.loss_function == "bce":
+            if use_gpu or use_ipex:
+                return loss_fn(Z, T.to(device))
+            else:
+                return loss_fn(Z, T)
+        elif args.loss_function == "wbce":
+            if use_gpu:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T).to(device)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            else:
+                loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T)
+                loss_fn_ = loss_fn(Z, T.to(device))
+            loss_sc_ = loss_ws_ * loss_fn_
+            # debug prints
+            # print(loss_ws_)
+            # print(loss_fn_)
+            return loss_sc_.mean()
+
+    # training or inference
+    best_gA_test = 0
+    best_auc_test = 0
+    skip_upto_epoch = 0
+    skip_upto_batch = 0
+    total_time = 0
+    total_loss = 0
+    total_accu = 0
+    total_iter = 0
+    total_samp = 0
+    k = 0
+
+
+    # Load model is specified
+    if not (args.load_model == ""):
+        print("Loading saved model {}".format(args.load_model))
+        if use_gpu:
+            if dlrm.ndevices > 1:
+                # NOTE: when targeting inference on multiple GPUs,
+                # load the model as is on CPU or GPU, with the move
+                # to multiple GPUs to be done in parallel_forward
+                ld_model = torch.load(args.load_model)
+            else:
+                # NOTE: when targeting inference on single GPU,
+                # note that the call to .to(device) has already happened
+                ld_model = torch.load(
+                    args.load_model,
+                    map_location=torch.device('cuda')
+                    # map_location=lambda storage, loc: storage.cuda(0)
+                )
+        else:
+            # when targeting inference on CPU
+            ld_model = torch.load(os.path.join(args.load_model, "dlrm_s_pytorch_" + str(dlrm.rank) + "_best.pkl"), map_location=torch.device('cpu'))
+        dlrm.load_state_dict(ld_model["state_dict"])
+
+    if args.use_ipex:
+       dlrm = dlrm.to(device)
+       print(dlrm, device, args.use_ipex)
+    ext_dist.barrier()
+    print("Start inference==========================:")
+
+    test_start = time.time()
+
+    if args.mlperf_logging:
+        scores = []
+        targets = []
+
+    for i, (X_test, lS_o_test, lS_i_test, T_test) in enumerate(test_ld):
+
+        # forward pass
+        Z_test = dlrm_wrap(
+            X_test, lS_o_test, lS_i_test, use_gpu, use_ipex, device
+        )
+        if args.mlperf_logging:
+            if ext_dist.my_size > 1:
+                Z_test = ext_dist.all_gather(Z_test, None)
+                T_test = ext_dist.all_gather(T_test, None)
+            S_test = Z_test.detach().cpu().numpy()  # numpy array
+            T_test = T_test.detach().cpu().numpy()  # numpy array
+            scores.append(S_test)
+            targets.append(T_test)
+        else:
+            # loss
+            E_test = loss_fn_wrap(Z_test, T_test, use_gpu, use_ipex, device)
+
+            # compute loss and accuracy
+            L_test = E_test.detach().cpu().numpy()  # numpy array
+            S_test = Z_test.detach().cpu().numpy()  # numpy array
+            T_test = T_test.detach().cpu().numpy()  # numpy array
+            mbs_test = T_test.shape[0]  # = mini_batch_size except last
+            A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))
+            test_accu += A_test
+            test_loss += L_test * mbs_test
+            test_samp += mbs_test
+
+        t2_test = time_wrap(use_gpu)
+
+    if args.mlperf_logging:
+        scores = np.concatenate(scores, axis=0)
+        targets = np.concatenate(targets, axis=0)
+
+        validation_results = {}
+        if args.use_ipex:
+            validation_results['roc_auc'], validation_results['loss'], validation_results['accuracy'] = \
+                core.roc_auc_score(torch.from_numpy(targets).reshape(-1), torch.from_numpy(scores).reshape(-1))
+        else:
+            metrics = {
+                'loss' : sklearn.metrics.log_loss,
+                'recall' : lambda y_true, y_score:
+                sklearn.metrics.recall_score(
+                    y_true=y_true,
+                    y_pred=np.round(y_score)
+                ),
+                'precision' : lambda y_true, y_score:
+                sklearn.metrics.precision_score(
+                    y_true=y_true,
+                    y_pred=np.round(y_score)
+                ),
+                'f1' : lambda y_true, y_score:
+                sklearn.metrics.f1_score(
+                    y_true=y_true,
+                    y_pred=np.round(y_score)
+                ),
+                'ap' : sklearn.metrics.average_precision_score,
+                'roc_auc' : sklearn.metrics.roc_auc_score,
+                'accuracy' : lambda y_true, y_score:
+                sklearn.metrics.accuracy_score(
+                    y_true=y_true,
+                    y_pred=np.round(y_score)
+                ),
+            }
+
+            # print("Compute time for validation metric : ", end="")
+            # first_it = True
+            for metric_name, metric_function in metrics.items():
+                # if first_it:
+                #     first_it = False
+                # else:
+                #     print(", ", end="")
+                # metric_compute_start = time_wrap(False)
+                validation_results[metric_name] = metric_function(
+                    targets,
+                    scores
+                )
+                # metric_compute_end = time_wrap(False)
+                # met_time = metric_compute_end - metric_compute_start
+                # print("{} {:.4f}".format(metric_name, 1000 * (met_time)),
+                #      end="")
+
+        # print(" ms")
+        gA_test = validation_results['accuracy']
+        gL_test = validation_results['loss']
+    else:
+        gA_test = test_accu / test_samp
+        gL_test = test_loss / test_samp
+
+
+
+
+    if args.mlperf_logging:
+        print(
+            " loss {:.6f},".format(
+                validation_results['loss']
+            )
+            + " auc {:.4f}".format(
+                validation_results['roc_auc']
+            )
+            + " accuracy {:3.3f} %".format(
+                validation_results['accuracy'] * 100
+            )
+        )
+    test_end = time.time()
+    print(F"Test time:{test_end - test_start}")
+    print(F"Total results length:{len(scores)}")
+#     results = {'pred':scores.tolist(),'targets':targets.tolist()}
+#     with open("results.json", "w") as f:
+#         json.dump(results, f, sort_keys=True, indent=4)
+    ext_dist.barrier()
diff --git a/extend_distributed.py b/extend_distributed.py
index 1f2c8a5..8eaa16e 100644
--- a/extend_distributed.py
+++ b/extend_distributed.py
@@ -1,196 +1,124 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-#
-import builtins
 import os
-import sys
-
+import builtins
+import numpy as np
 import torch
-import torch.distributed as dist
 from torch.autograd import Function
-from torch.autograd.profiler import record_function
 from torch.nn.parallel import DistributedDataParallel as DDP
-
-
+import torch.distributed as dist
+torch_ccl = False
 try:
     import torch_ccl
+    torch_ccl = True
 except ImportError as e:
-    # print(e)
+    #print(e)
     torch_ccl = False
-
-try:
-    import torch_ucc
-except ImportError as e:
-    torch_ucc = False
-
+print(f"torch_ccl is {torch_ccl}")
 
 my_rank = -1
 my_size = -1
 my_local_rank = -1
 my_local_size = -1
 alltoall_supported = False
-a2a_impl = os.environ.get("DLRM_ALLTOALL_IMPL", "")
+allgatherv_supported = False
+a2a_impl = os.environ.get('DLRM_ALLTOALL_IMPL', '')
 
 myreq = None
 
-
-def env2int(env_list, default=-1):
+def env2int(env_list, default = -1):
     for e in env_list:
         val = int(os.environ.get(e, -1))
-        if val >= 0:
-            return val
+        if val >= 0: return val
     return default
 
-
 def get_my_slice(n):
+    my_size = dist.get_world_size()
+    my_rank = dist.get_rank()
     k, m = divmod(n, my_size)
-    return slice(
-        my_rank * k + min(my_rank, m), (my_rank + 1) * k + min(my_rank + 1, m), 1
-    )
-
+    return slice(my_rank * k + min(my_rank, m), (my_rank+1) * k + min(my_rank+1, m), 1)
 
 def get_split_lengths(n):
+    my_size = dist.get_world_size()
     k, m = divmod(n, my_size)
     if m == 0:
         splits = None
         my_len = k
     else:
-        splits = [(k + 1) if i < m else k for i in range(my_size)]
+        my_rank = dist.get_rank()
+        splits = [(k+1) if i < m else k for i in range(my_size)]
         my_len = splits[my_rank]
     return (my_len, splits)
 
-
-def init_distributed(rank=-1, local_rank=-1, size=-1, use_gpu=False, backend=""):
+def init_distributed(rank = -1, size = -1, backend=''):
     global myreq
-    global my_rank
+    #global my_rank
     global my_size
     global my_local_rank
     global my_local_size
     global a2a_impl
     global alltoall_supported
+    global allgatherv_supported
 
     # guess MPI ranks from env (works for IMPI, OMPI and MVAPICH2)
-    num_mpi_ranks = env2int(
-        ["PMI_SIZE", "OMPI_COMM_WORLD_SIZE", "MV2_COMM_WORLD_SIZE", "WORLD_SIZE"]
-    )
-    if backend == "" and num_mpi_ranks > 1:
-        if torch_ccl and env2int(["CCL_WORKER_COUNT"]) > 0:
-            backend = "ccl"
-        elif use_gpu and dist.is_nccl_available():
-            backend = "nccl"
+    num_mpi_ranks = env2int(['PMI_SIZE', 'OMPI_COMM_WORLD_SIZE', 'MV2_COMM_WORLD_SIZE', 'WORLD_SIZE'])
+    if backend == '' and num_mpi_ranks > 1:
+        if torch_ccl and env2int(['CCL_WORKER_COUNT']) > 0:
+            backend = 'ccl'
         elif dist.is_mpi_available():
-            backend = "mpi"
+            backend = 'mpi'
         else:
-            print(
-                "WARNING: MPI multi-process launch detected but PyTorch MPI backend not available."
-            )
-            backend = "gloo"
-
-    if backend != "":
-        # guess Rank and size
+            print("WARNING: MPI multi-process launch detected but PyTorch MPI backend not available.")
+            backend = 'gloo'
+    if backend != '':
+        #guess Rank and size
         if rank == -1:
-            rank = env2int(
-                ["PMI_RANK", "OMPI_COMM_WORLD_RANK", "MV2_COMM_WORLD_RANK", "RANK"], 0
-            )
+            rank = env2int(['PMI_RANK', 'OMPI_COMM_WORLD_RANK', 'MV2_COMM_WORLD_RANK', 'RANK'], 0)
         if size == -1:
-            size = env2int(
-                [
-                    "PMI_SIZE",
-                    "OMPI_COMM_WORLD_SIZE",
-                    "MV2_COMM_WORLD_SIZE",
-                    "WORLD_SIZE",
-                ],
-                1,
-            )
-        if not os.environ.get("RANK", None) and rank != -1:
-            os.environ["RANK"] = str(rank)
-        if not os.environ.get("WORLD_SIZE", None) and size != -1:
-            os.environ["WORLD_SIZE"] = str(size)
-        if not os.environ.get("MASTER_PORT", None):
-            os.environ["MASTER_PORT"] = "29500"
-        if not os.environ.get("MASTER_ADDR", None):
-            local_size = env2int(
-                [
-                    "MPI_LOCALNRANKS",
-                    "OMPI_COMM_WORLD_LOCAL_SIZE",
-                    "MV2_COMM_WORLD_LOCAL_SIZE",
-                ],
-                1,
-            )
-            if local_size != size and backend != "mpi":
-                print(
-                    "Warning: Looks like distributed multinode run but MASTER_ADDR env not set, using '127.0.0.1' as default"
-                )
-                print(
-                    "If this run hangs, try exporting rank 0's hostname as MASTER_ADDR"
-                )
-            os.environ["MASTER_ADDR"] = "127.0.0.1"
-
+            size = env2int(['PMI_SIZE', 'OMPI_COMM_WORLD_SIZE', 'MV2_COMM_WORLD_SIZE', 'WORLD_SIZE'], 1)
+        if not os.environ.get('RANK', None) and rank != -1: os.environ['RANK'] = str(rank)
+        if not os.environ.get('WORLD_SIZE', None) and size != -1: os.environ['WORLD_SIZE'] = str(size)
+        if not os.environ.get('MASTER_PORT', None): os.environ['MASTER_PORT'] = '29500'
+        if not os.environ.get('MASTER_ADDR', None):
+            local_size = env2int(['MPI_LOCALNRANKS', 'OMPI_COMM_WORLD_LOCAL_SIZE', 'MV2_COMM_WORLD_LOCAL_SIZE'], 1)
+            if local_size != size and backend != 'mpi':
+                print("Warning: Looks like distributed multinode run but MASTER_ADDR env not set, using '127.0.0.1' as default")
+                print("If this run hangs, try exporting rank 0's hostname as MASTER_ADDR")
+            os.environ['MASTER_ADDR'] = '127.0.0.1'
     if size > 1:
-        if local_rank == -1:
-            my_local_rank = env2int(
-                [
-                    "MPI_LOCALRANKID",
-                    "OMPI_COMM_WORLD_LOCAL_RANK",
-                    "MV2_COMM_WORLD_LOCAL_RANK",
-                    "LOCAL_RANK",
-                ],
-                0,
-            )
-        else:
-            my_local_rank = local_rank
-        my_local_size = env2int(
-            [
-                "MPI_LOCALNRANKS",
-                "OMPI_COMM_WORLD_LOCAL_SIZE",
-                "MV2_COMM_WORLD_LOCAL_SIZE",
-            ],
-            1,
-        )
-        if use_gpu:
-            if my_local_size > torch.cuda.device_count():
-                print(
-                    "Not sufficient GPUs available... local_size = %d, ngpus = %d"
-                    % (my_local_size, torch.cuda.device_count())
-                )
-                sys.exit(1)
-            torch.cuda.set_device(my_local_rank)
+        print(F"world_size:{size},rank:{rank}")
         dist.init_process_group(backend, rank=rank, world_size=size)
         my_rank = dist.get_rank()
         my_size = dist.get_world_size()
-        if my_rank == 0:
-            print("Running on %d ranks using %s backend" % (my_size, backend))
-        if hasattr(dist, "all_to_all_single"):
+        my_local_rank = env2int(['MPI_LOCALRANKID', 'OMPI_COMM_WORLD_LOCAL_RANK', 'MV2_COMM_WORLD_LOCAL_RANK'], 0)
+        my_local_size = env2int(['MPI_LOCALNRANKS', 'OMPI_COMM_WORLD_LOCAL_SIZE', 'MV2_COMM_WORLD_LOCAL_SIZE'], 1)
+        if my_rank == 0: print("Running on %d ranks using %s backend" % (my_size, backend))
+        if backend == 'ccl':
+            print("Using CCL_ATL_TRANSPORT=%s" % os.environ.get('CCL_ATL_TRANSPORT', '(default)'))
+            print("Using CCL_ATL_SHM=%s" % os.environ.get('CCL_ATL_SHM', '(default)'))
+        if hasattr(dist, 'all_to_all_single'):
             try:
-                t = torch.zeros([4])
-                if use_gpu:
-                    t = t.cuda()
-                dist.all_to_all_single(t, t)
+               # dist.all_to_all_single(torch.empty([0]), torch.empty([0]))
                 alltoall_supported = True
-            except RuntimeError as err:
-                print("fail to enable all_to_all_single primitive: %s" % err)
-        if a2a_impl == "alltoall" and alltoall_supported == False:
-            print(
-                "Requested DLRM_ALLTOALL_IMPL=%s but backend %s does not support it, use scatter/gather based alltoall"
-                % (a2a_impl, backend)
-            )
-            a2a_impl = "scatter"
-        if a2a_impl != "":
-            print("Using DLRM_ALLTOALL_IMPL=%s" % a2a_impl)
+            except RuntimeError:
+                pass
+        if a2a_impl == 'alltoall' and alltoall_supported == False:
+            print("Requested DLRM_ALLTOALL_IMPL=%s but backend %s does not support it, use scatter/gather based alltoall" % (a2a_impl, backend))
+            a2a_impl = 'scatter'
+        if a2a_impl != '': print("Using DLRM_ALLTOALL_IMPL=%s" % a2a_impl)
+        try:
+            x = torch.ones([my_rank])
+            y = torch.zeros([(my_size*(my_size-1))//2])
+            y = list(y.split([r for r in range(my_size)]))
+            dist.all_gather(y, x)
+            allgatherv_supported = True
+        except RuntimeError:
+            pass
     else:
         my_rank = 0
         my_size = 1
         my_local_rank = 0
         my_local_size = 1
-    print_all(
-        "world size: %d, current rank: %d, local rank: %d"
-        % (my_size, my_rank, my_local_rank)
-    )
     myreq = Request()
 
-
 class Request(object):
     def __init__(self):
         self.req = None
@@ -203,44 +131,32 @@ class Request(object):
         self.tensor = None
         return ret
 
-
 class All2All_ScatterList_Req(Function):
     @staticmethod
-    def forward(ctx, a2a_info, *inputs):
+    def forward(ctx, a2ai, *inputs):
         global myreq
-        batch_split_lengths = (
-            a2a_info.global_batch_partition_slices
-            if a2a_info.global_batch_partition_slices
-            else a2a_info.local_batch_num
-        )
-        table_split_lengths = (
-            a2a_info.global_table_wise_parition_slices
-            if a2a_info.global_table_wise_parition_slices
-            else [a2a_info.local_table_num] * my_size
-        )
+        my_rank = dist.get_rank()
+        #print("All2All_ScatterList_Req:forward")
+        mb_split_lengths = a2ai.gNS if a2ai.gNS else a2ai.lN
+        emb_split_lengths = a2ai.gSS if a2ai.gSS else [a2ai.lS] * my_size
         gather_list = []
         req_list = []
         for i in range(my_size):
-            for j in range(table_split_lengths[i]):
-                out_tensor = inputs[0].new_empty(
-                    [a2a_info.local_batch_num, a2a_info.emb_dim]
-                )
-                scatter_list = (
-                    list(inputs[j].split(batch_split_lengths, dim=0))
-                    if i == my_rank
-                    else []
-                )
+            for j in range(emb_split_lengths[i]):
+                out_tensor = inputs[0].new_empty([a2ai.lN, a2ai.E])
+                scatter_list = list(inputs[j].split(mb_split_lengths, dim = 0)) if i == my_rank else []
                 req = dist.scatter(out_tensor, scatter_list, src=i, async_op=True)
                 gather_list.append(out_tensor)
                 req_list.append(req)
         myreq.req = req_list
         myreq.tensor = tuple(gather_list)
-        myreq.a2a_info = a2a_info
+        myreq.a2ai = a2ai
         return myreq.tensor
 
     @staticmethod
     def backward(ctx, *grad_output):
         global myreq
+        #print("All2All_ScatterList_Req:backward")
         for r in myreq.req:
             r.wait()
         myreq.req = None
@@ -253,7 +169,8 @@ class All2All_ScatterList_Wait(Function):
     @staticmethod
     def forward(ctx, *output):
         global myreq
-        ctx.a2a_info = myreq.a2a_info
+        #print("All2All_Scatter_Wait:forward")
+        ctx.a2ai = myreq.a2ai
         for r in myreq.req:
             r.wait()
         myreq.req = None
@@ -263,32 +180,18 @@ class All2All_ScatterList_Wait(Function):
     @staticmethod
     def backward(ctx, *grad_output):
         global myreq
-        a2a_info = ctx.a2a_info
+        my_rank = dist.get_rank()
+        a2ai = ctx.a2ai
         grad_output = [t.contiguous() for t in grad_output]
-        batch_split_lengths = (
-            a2a_info.global_batch_partition_slices
-            if a2a_info.global_batch_partition_slices
-            else [a2a_info.local_batch_num] * my_size
-        )
-        per_rank_table_splits = (
-            a2a_info.global_table_wise_parition_slices
-            if a2a_info.global_table_wise_parition_slices
-            else [a2a_info.local_table_num] * my_size
-        )
-        grad_inputs = [
-            grad_output[0].new_empty([ctx.a2a_info.batch_size, ctx.a2a_info.emb_dim])
-            for _ in range(a2a_info.local_table_num)
-        ]
+        mb_split_lengths = a2ai.gNS if a2ai.gNS else [a2ai.lN] * my_size
+        per_rank_split_lengths = a2ai.gSS if a2ai.gSS else [a2ai.lS] * my_size
+        grad_inputs = [grad_output[0].new_empty([ctx.a2ai.N, ctx.a2ai.E]) for _ in range(a2ai.lS)]
         req_list = []
         ind = 0
         for i in range(my_size):
-            for j in range(per_rank_table_splits[i]):
-                gather_list = (
-                    list(grad_inputs[j].split(batch_split_lengths, dim=0))
-                    if i == my_rank
-                    else None
-                )
-                req = dist.gather(grad_output[ind], gather_list, dst=i, async_op=True)
+            for j in range(per_rank_split_lengths[i]):
+                gather_list = list(grad_inputs[j].split(mb_split_lengths, dim = 0)) if i == my_rank else None
+                req = dist.gather(grad_output[ind], gather_list, dst = i, async_op=True)
                 req_list.append(req)
                 ind += 1
         myreq.req = req_list
@@ -296,47 +199,39 @@ class All2All_ScatterList_Wait(Function):
         return tuple(grad_output)
 
 
+
 class All2All_Scatter_Req(Function):
     @staticmethod
-    def forward(ctx, a2a_info, *inputs):
+    def forward(ctx, a2ai, *inputs):
         global myreq
-        batch_split_lengths = (
-            a2a_info.global_batch_partition_slices
-            if a2a_info.global_batch_partition_slices
-            else a2a_info.local_batch_num
-        )
-        table_split_lengths = (
-            a2a_info.global_table_wise_parition_slices
-            if a2a_info.global_table_wise_parition_slices
-            else [a2a_info.local_table_num] * my_size
-        )
+        #print("All2All_Scatter_Req:forward")
+        my_rank = dist.get_rank()
+        mb_split_lengths = a2ai.gNS if a2ai.gNS else a2ai.lN
+        emb_split_lengths = a2ai.gSS if a2ai.gSS else [a2ai.lS] * my_size
         input = torch.cat(inputs, dim=1)
-        scatter_list = list(input.split(batch_split_lengths, dim=0))
+        scatter_list = list(input.split(mb_split_lengths, dim=0))
         gather_list = []
         req_list = []
         for i in range(my_size):
-            out_tensor = input.new_empty(
-                [a2a_info.local_batch_num, table_split_lengths[i] * a2a_info.emb_dim]
-            )
-            req = dist.scatter(
-                out_tensor, scatter_list if i == my_rank else [], src=i, async_op=True
-            )
+            out_tensor = input.new_empty([a2ai.lN, emb_split_lengths[i] * a2ai.E])
+            req = dist.scatter(out_tensor, scatter_list if i == my_rank else [], src=i, async_op=True)
             gather_list.append(out_tensor)
             req_list.append(req)
         myreq.req = req_list
         myreq.tensor = tuple(gather_list)
-        myreq.a2a_info = a2a_info
-        ctx.a2a_info = a2a_info
+        myreq.a2ai = a2ai
+        ctx.a2ai = a2ai
         return myreq.tensor
 
     @staticmethod
     def backward(ctx, *grad_output):
         global myreq
+        #print("All2All_Scatter_Req:backward")
         for r in myreq.req:
             r.wait()
         myreq.req = None
         grad_input = myreq.tensor
-        grad_inputs = grad_input.split(ctx.a2a_info.emb_dim, dim=1)
+        grad_inputs = grad_input.split(ctx.a2ai.E, dim=1)
         myreq.tensor = None
         return (None, *grad_inputs)
 
@@ -345,7 +240,8 @@ class All2All_Scatter_Wait(Function):
     @staticmethod
     def forward(ctx, *output):
         global myreq
-        ctx.a2a_info = myreq.a2a_info
+        #print("All2All_Scatter_Wait:forward")
+        ctx.a2ai = myreq.a2ai
         for r in myreq.req:
             r.wait()
         myreq.req = None
@@ -355,31 +251,19 @@ class All2All_Scatter_Wait(Function):
     @staticmethod
     def backward(ctx, *grad_output):
         global myreq
+        my_rank = dist.get_rank()
+        #print("All2All_Scatter_Wait:backward")
         assert len(grad_output) == my_size
         scatter_list = [t.contiguous() for t in grad_output]
-        a2a_info = ctx.a2a_info
-        batch_split_lengths = (
-            a2a_info.global_batch_partition_slices
-            if a2a_info.global_batch_partition_slices
-            else a2a_info.local_batch_num
-        )
-        table_split_lengths = (
-            a2a_info.global_table_wise_parition_slices
-            if a2a_info.global_table_wise_parition_slices
-            else [a2a_info.local_table_num] * my_size
-        )
-        grad_input = grad_output[0].new_empty(
-            [a2a_info.batch_size, a2a_info.emb_dim * a2a_info.local_table_num]
-        )
-        gather_list = list(grad_input.split(batch_split_lengths, dim=0))
+        a2ai = ctx.a2ai
+        mb_split_lengths = a2ai.gNS if a2ai.gNS else a2ai.lN
+        emb_split_lengths = a2ai.gSS if a2ai.gSS else [a2ai.lS] * my_size
+        grad_input = grad_output[0].new_empty([a2ai.N, a2ai.E*a2ai.lS])
+        gather_list = list(grad_input.split(mb_split_lengths, dim=0))
         req_list = []
         for i in range(my_size):
-            req = dist.gather(
-                scatter_list[i],
-                gather_list if i == my_rank else [],
-                dst=i,
-                async_op=True,
-            )
+            #req = dist.scatter(gather_list[i], scatter_list if i == my_rank else [], src=i, async_op=True)
+            req = dist.gather(scatter_list[i], gather_list if i == my_rank else [], dst=i, async_op=True)
             req_list.append(req)
         myreq.req = req_list
         myreq.tensor = grad_input
@@ -388,112 +272,77 @@ class All2All_Scatter_Wait(Function):
 
 class All2All_Req(Function):
     @staticmethod
-    def forward(ctx, a2a_info, *inputs):
+    def forward(ctx, a2ai, *inputs):
         global myreq
-        with record_function("DLRM alltoall_req_fwd_single"):
-            batch_split_lengths = a2a_info.global_batch_partition_slices
-            if batch_split_lengths:
-                batch_split_lengths = [
-                    m * a2a_info.emb_dim * a2a_info.local_table_num
-                    for m in batch_split_lengths
-                ]
-            table_split_lengths = a2a_info.global_table_wise_parition_slices
-            if table_split_lengths:
-                table_split_lengths = [
-                    a2a_info.local_batch_num * e * a2a_info.emb_dim
-                    for e in table_split_lengths
-                ]
-            input = torch.cat(inputs, dim=1).view([-1])
-            output = input.new_empty(
-                [
-                    a2a_info.global_table_num
-                    * a2a_info.local_batch_num
-                    * a2a_info.emb_dim
-                ]
-            )
-            req = dist.all_to_all_single(
-                output, input, table_split_lengths, batch_split_lengths, async_op=True
-            )
-
-            myreq.req = req
-            myreq.tensor = []
-            myreq.tensor.append(output)
-            myreq.tensor = tuple(myreq.tensor)
-            a2a_info.batch_split_lengths = batch_split_lengths
-            a2a_info.table_split_lengths = table_split_lengths
-            myreq.a2a_info = a2a_info
-            ctx.a2a_info = a2a_info
-            return myreq.tensor
+        #print("All2All_Req:forward")
+        mb_split_lengths = a2ai.gNS
+        if mb_split_lengths: mb_split_lengths = [m * a2ai.lS * a2ai.E for m in mb_split_lengths]
+        emb_split_lengths = a2ai.gSS
+        if emb_split_lengths: emb_split_lengths = [a2ai.lN * e * a2ai.E for e in emb_split_lengths]
+        input = torch.cat(inputs, dim=1).view([-1])
+        output = input.new_empty([a2ai.S*a2ai.lN*a2ai.E])
+        req = dist.all_to_all_single(output, input, emb_split_lengths, mb_split_lengths, async_op=True)
+        myreq.req = req
+        myreq.tensor = []
+        myreq.tensor.append(output)
+        myreq.tensor = tuple(myreq.tensor)
+        a2ai.mb_split_lengths = mb_split_lengths
+        a2ai.emb_split_lengths = emb_split_lengths
+        myreq.a2ai = a2ai
+        ctx.a2ai = a2ai
+        return myreq.tensor
 
     @staticmethod
     def backward(ctx, *grad_output):
         global myreq
-        with record_function("DLRM alltoall_req_bwd_single"):
-            a2a_info = ctx.a2a_info
-            myreq.req.wait()
-            myreq.req = None
-            grad_input = myreq.tensor
-            grad_inputs = grad_input.view([a2a_info.batch_size, -1]).split(
-                a2a_info.emb_dim, dim=1
-            )
-            grad_inputs = [gin.contiguous() for gin in grad_inputs]
-            myreq.tensor = None
-            return (None, *grad_inputs)
+        #print("All2All_Req:backward")
+        a2ai = ctx.a2ai
+        myreq.req.wait()
+        myreq.req = None
+        grad_input = myreq.tensor
+        grad_inputs = grad_input.view([a2ai.N, -1]).split(a2ai.E, dim=1)
+        grad_inputs = [gin.contiguous() for gin in grad_inputs]
+        myreq.tensor = None
+        return (None, *grad_inputs)
 
 
 class All2All_Wait(Function):
     @staticmethod
     def forward(ctx, *output):
         global myreq
-        with record_function("DLRM alltoall_wait_fwd_single"):
-            a2a_info = myreq.a2a_info
-            ctx.a2a_info = a2a_info
-            myreq.req.wait()
-            myreq.req = None
-            myreq.tensor = None
-            table_split_lengths = (
-                a2a_info.table_split_lengths
-                if a2a_info.table_split_lengths
-                else a2a_info.local_table_num
-                * a2a_info.local_batch_num
-                * a2a_info.emb_dim
-            )
-            outputs = output[0].split(table_split_lengths)
-            outputs = tuple(
-                [out.view([a2a_info.local_batch_num, -1]) for out in outputs]
-            )
-            return outputs
+        #print("All2All_Wait:forward")
+        a2ai = myreq.a2ai
+        ctx.a2ai = a2ai
+        myreq.req.wait()
+        myreq.req = None
+        myreq.tensor = None
+        emb_split_lengths = a2ai.emb_split_lengths if a2ai.emb_split_lengths else a2ai.lS * a2ai.lN * a2ai.E
+        outputs = output[0].split(emb_split_lengths)
+        outputs = tuple([out.view([a2ai.lN, -1]) for out in outputs])
+        return outputs
 
     @staticmethod
     def backward(ctx, *grad_outputs):
         global myreq
-        with record_function("DLRM alltoall_wait_bwd_single"):
-            a2a_info = ctx.a2a_info
-            grad_outputs = [gout.contiguous().view([-1]) for gout in grad_outputs]
-            grad_output = torch.cat(grad_outputs)
-            grad_input = grad_output.new_empty(
-                [a2a_info.batch_size * a2a_info.local_table_num * a2a_info.emb_dim]
-            )
-            req = dist.all_to_all_single(
-                grad_input,
-                grad_output,
-                a2a_info.batch_split_lengths,
-                a2a_info.table_split_lengths,
-                async_op=True,
-            )
-            myreq.req = req
-            myreq.tensor = grad_input
-            return (grad_output,)
-
+        #print("All2All_Wait:backward")
+        a2ai = ctx.a2ai
+        grad_outputs = [gout.contiguous().view([-1]) for gout in grad_outputs]
+        grad_output = torch.cat(grad_outputs)
+        grad_input = grad_output.new_empty([a2ai.N * a2ai.lS * a2ai.E])
+        req = dist.all_to_all_single(grad_input, grad_output, a2ai.mb_split_lengths, a2ai.emb_split_lengths, async_op=True)
+        myreq.req = req
+        myreq.tensor = grad_input
+        return (grad_output,)
 
 class AllGather(Function):
+
     @staticmethod
     def forward(ctx, input, global_lengths, dim=0):
         if not isinstance(global_lengths, (list, tuple)):
             global_lengths = [global_lengths] * my_size
-
-        assert len(global_lengths) == my_size
-        assert global_lengths[my_rank] == input.size(dim)
+        my_rank = dist.get_rank()
+        assert(len(global_lengths) == my_size)
+        assert(global_lengths[my_rank] == input.size(dim))
         local_start = sum(global_lengths[:my_rank])
 
         output_size = list(input.size())
@@ -511,8 +360,8 @@ class AllGather(Function):
         else:
             gather_list = [torch.empty_like(input) for _ in range(my_size)]
             gather_list = []
-            for length in global_lengths:
-                output_size[dim] = length
+            for l in global_lengths:
+                output_size[dim] = l
                 gather_list.append(input.new_empty(output_size))
 
         dist.all_gather(gather_list, input)
@@ -533,71 +382,49 @@ class AllGather(Function):
 
         return (grad_input, None, None)
 
-
 class All2AllInfo(object):
     pass
 
-
-def alltoall(inputs, per_rank_table_splits):
+def alltoall(inputs, per_rank_split_lengths):
     global myreq
-    batch_size, emb_dim = inputs[0].size()
-    a2a_info = All2AllInfo()
-    a2a_info.local_table_num = len(inputs)
-    a2a_info.global_table_wise_parition_slices = per_rank_table_splits
-    (
-        a2a_info.local_batch_num,
-        a2a_info.global_batch_partition_slices,
-    ) = get_split_lengths(batch_size)
-    a2a_info.emb_dim = emb_dim
-    a2a_info.batch_size = batch_size
-    a2a_info.global_table_num = (
-        sum(per_rank_table_splits)
-        if per_rank_table_splits
-        else a2a_info.local_table_num * my_size
-    )
-
-    if a2a_impl == "" and alltoall_supported or a2a_impl == "alltoall":
-        # print("Using All2All_Req")
-        output = All2All_Req.apply(a2a_info, *inputs)
+    N, E = inputs[0].size()
+    a2ai = All2AllInfo()
+    a2ai.lS = len(inputs)
+    a2ai.gSS = per_rank_split_lengths
+    a2ai.lN, a2ai.gNS = get_split_lengths(N)
+    a2ai.E = E
+    a2ai.N = N
+    a2ai.S = sum(per_rank_split_lengths) if per_rank_split_lengths else a2ai.lS * my_size
+    if a2a_impl == '' and alltoall_supported or a2a_impl == 'alltoall':
+        output = All2All_Req.apply(a2ai, *inputs)
         myreq.WaitFunction = All2All_Wait
-    elif a2a_impl == "" or a2a_impl == "scatter":
-        # print("Using All2All_Scatter_Req")
-        output = All2All_Scatter_Req.apply(a2a_info, *inputs)
+    elif a2a_impl == '' or a2a_impl == 'scatter':
+        #print("Using All2All_Scatter_Req")
+        output = All2All_Scatter_Req.apply(a2ai, *inputs)
         myreq.WaitFunction = All2All_Scatter_Wait
-    elif a2a_impl == "scatter_list":
-        # print("Using All2All_ScatterList_Req")
-        output = All2All_ScatterList_Req.apply(a2a_info, *inputs)
+    elif a2a_impl == 'scatter_list':
+        #print("Using All2All_ScatterList_Req")
+        output = All2All_ScatterList_Req.apply(a2ai, *inputs)
         myreq.WaitFunction = All2All_ScatterList_Wait
     else:
-        print(
-            "Unknown value set for DLRM_ALLTOALL_IMPL (%s), "
-            "please use one of [alltoall, scatter, scatter_list]" % a2a_impl
-        )
+        print("Unknown value set for DLRM_ALLTOALL_IMPL (%s), please use one of [alltoall, scatter, scatter_list]" % a2a_impl) 
     return myreq
 
+def shuffle_data(inputs):
+    input = torch.cat(inputs)
+    output = input.new_empty(input.size())
+    req = dist.all_to_all_single(output, input) 
+    output = output.reshape(my_size, -1)
+    return output
+    
 
 def all_gather(input, lengths, dim=0):
-    if not lengths:
-        lengths = [input.size(0)] * my_size
+    #print("lengths: ", lengths)
+    if not lengths: lengths = [input.size(0)] * my_size
     return AllGather.apply(input, lengths, dim)
 
-
 def barrier():
     if my_size > 1:
         dist.barrier()
 
 
-# Override builtin print function to print only from rank 0
-orig_print = builtins.print
-
-
-def rank0_print(*args, **kwargs):
-    if my_rank <= 0 or kwargs.get("print_all", False):
-        orig_print(*args, **kwargs)
-
-
-builtins.print = rank0_print
-
-# Allow printing from all rank with explicit print_all
-def print_all(*args, **kwargs):
-    orig_print(*args, **kwargs)
diff --git a/input/dist_emb_0.log b/input/dist_emb_0.log
deleted file mode 100644
index 7a8c1b7..0000000
--- a/input/dist_emb_0.log
+++ /dev/null
@@ -1,3 +0,0 @@
-1, 2, 3, 4, 5, 6
-0, 1, 3, 4, 5
-0.55, 0.64, 0.82, 0.91, 1.0
diff --git a/input/dist_emb_1.log b/input/dist_emb_1.log
deleted file mode 100644
index 7a8c1b7..0000000
--- a/input/dist_emb_1.log
+++ /dev/null
@@ -1,3 +0,0 @@
-1, 2, 3, 4, 5, 6
-0, 1, 3, 4, 5
-0.55, 0.64, 0.82, 0.91, 1.0
diff --git a/input/dist_emb_2.log b/input/dist_emb_2.log
deleted file mode 100644
index 7a8c1b7..0000000
--- a/input/dist_emb_2.log
+++ /dev/null
@@ -1,3 +0,0 @@
-1, 2, 3, 4, 5, 6
-0, 1, 3, 4, 5
-0.55, 0.64, 0.82, 0.91, 1.0
diff --git a/input/trace.log b/input/trace.log
deleted file mode 100644
index 4d33e55..0000000
--- a/input/trace.log
+++ /dev/null
@@ -1 +0,0 @@
-1, 2, 3, 4, 5, 3, 4, 1, 1, 6, 3
diff --git a/lamb_bin.py b/lamb_bin.py
new file mode 100644
index 0000000..dce74f3
--- /dev/null
+++ b/lamb_bin.py
@@ -0,0 +1,155 @@
+"""Lamb optimizer."""
+
+import collections
+import math
+
+import torch
+from tensorboardX import SummaryWriter
+from torch.optim import Optimizer
+
+
+def log_lamb_rs(optimizer: Optimizer, event_writer: SummaryWriter, token_count: int):
+    """Log a histogram of trust ratio scalars in across layers."""
+    results = collections.defaultdict(list)
+    for group in optimizer.param_groups:
+        for p in group['params']:
+            state = optimizer.state[p]
+            for i in ('weight_norm', 'adam_norm', 'trust_ratio'):
+                if i in state:
+                    results[i].append(state[i])
+
+    for k, v in results.items():
+        event_writer.add_histogram(f'lamb/{k}', torch.tensor(v), token_count)
+
+class Lamb(Optimizer):
+    r"""Implements Lamb algorithm.
+
+    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.
+
+    Arguments:
+        params (iterable): iterable of parameters to optimize or dicts defining
+            parameter groups
+        lr (float, optional): learning rate (default: 1e-3)
+        betas (Tuple[float, float], optional): coefficients used for computing
+            running averages of gradient and its square (default: (0.9, 0.999))
+        eps (float, optional): term added to the denominator to improve
+            numerical stability (default: 1e-8)
+        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
+        adam (bool, optional): always use trust ratio = 1, which turns this into
+            Adam. Useful for comparison purposes.
+
+    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:
+        https://arxiv.org/abs/1904.00962
+    """
+
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,
+                 weight_decay=0, adam=False, bf16=False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay)
+        self.adam = adam
+        self.bf16 = bf16
+        beta1 = betas[0]
+        beta2 = betas[1]
+        
+        super(Lamb, self).__init__(params, defaults)
+
+    def set_bf16(self, bf16=False):
+        self.bf16 = bf16
+
+    def step(self, closure=None):
+        """Performs a single optimization step.
+
+        Arguments:
+            closure (callable, optional): A closure that reevaluates the model
+                and returns the loss.
+        """
+        loss = None
+        if closure is not None:
+            loss = closure()
+
+        for group in self.param_groups:
+            for p in group['params']:
+                if p.grad is None:
+                    continue
+                grad = p.grad.data
+                if grad.is_sparse:
+                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')
+
+                state = self.state[p]
+
+                # State initialization
+                if len(state) == 0:
+                    state['step'] = 0
+                    # Exponential moving average of gradient values
+                    state['exp_avg'] = torch.zeros_like(p.data, dtype=torch.float32)
+                    # Exponential moving average of squared gradient values
+                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=torch.float32)
+                    if self.bf16:
+                        # additional fp32 version of master weights
+                        state['data_fp32'] = p.data.to(torch.float32)
+
+                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
+                beta1, beta2 = group['betas']
+                if self.bf16:
+                    grad_fp32 = grad.to(torch.float32)
+                    data_fp32 = state['data_fp32']
+
+                state['step'] += 1
+
+
+                # Decay the first and second moment running average coefficient
+                if self.bf16:
+                    # m_t
+                    exp_avg.mul_(beta1).add_(1 - beta1, grad_fp32)
+                    # v_t
+                    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad_fp32, grad_fp32)     
+                else:
+                    # m_t
+                    exp_avg.mul_(beta1).add_(1 - beta1, grad)
+                    # v_t
+                    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
+                exp_avg = exp_avg/(1 - beta1)
+                exp_avg_sq = exp_avg_sq/(1 - beta2)
+
+                # Paper v3 does not use debiasing.
+                # bias_correction1 = 1 - beta1 ** state['step']
+                # bias_correction2 = 1 - beta2 ** state['step']
+                # Apply bias to lr to avoid broadcast.
+                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1
+
+                weight_norm = data_fp32.pow(2).sum().sqrt().clamp(0, 10) if self.bf16 \
+                    else p.data.pow(2).sum().sqrt().clamp(0, 10)
+                #r_t+l * p_data
+                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])
+                if group['weight_decay'] != 0:
+                    if self.bf16:
+                        adam_step.add_(group['weight_decay'], data_fp32)
+                    else:
+                        adam_step.add_(group['weight_decay'], p.data)
+
+                adam_norm = adam_step.pow(2).sum().sqrt()
+                if weight_norm == 0 or adam_norm == 0:
+                    trust_ratio = 1
+                else:
+                    trust_ratio = weight_norm / adam_norm
+                state['weight_norm'] = weight_norm
+                state['adam_norm'] = adam_norm
+                state['trust_ratio'] = trust_ratio
+                if self.adam:
+                    trust_ratio = 1
+
+                if self.bf16:
+                    data_fp32.add_(-step_size * trust_ratio, adam_step)
+                    p.data = data_fp32.to(torch.bfloat16)
+                else:
+                    p.data.add_(-step_size * trust_ratio, adam_step)
+
+        return loss
diff --git a/launch.py b/launch.py
new file mode 100644
index 0000000..a7241d0
--- /dev/null
+++ b/launch.py
@@ -0,0 +1,650 @@
+from __future__ import absolute_import, division, print_function, unicode_literals
+import sys
+import platform
+import subprocess
+import os
+from os.path import expanduser
+import re
+import glob
+import numpy as np
+from argparse import ArgumentParser, REMAINDER
+from argparse import RawTextHelpFormatter
+import logging
+import psutil
+
+logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+logger = logging.getLogger(__name__)
+
+r"""
+This is a script for launching PyTorch training and inference on Intel Xeon CPU with optimal configurations.
+Now, single instance inference/training, multi-instance inference/training and distributed training 
+with oneCCL backend is enabled.
+
+To get the peak performance on Intel Xeon CPU, the script optimizes the configuration of thread and memory 
+management. For thread management, the script configures thread affinity and the preload of Intel OMP library. 
+For memory management, it configures NUMA binding and preload optimized memory allocation library (e.g. tcmalloc, jemalloc).
+ 
+**How to use this module:**
+
+*** Single instance inference/training *** 
+
+1. Run single-instance inference or training on a single node with all CPU sockets.
+
+::
+
+   >>> python -m intel_pytorch_extension.launch script.py args
+
+2. Run single-instance inference or training on a single CPU socket.
+
+::
+
+   >>> python -m intel_pytorch_extension.launch --socket_id 1 script.py args
+
+*** Multi-instance inference *** 
+
+1. Multi-instance 
+   By default, one instance per socket. if you want to set the instance numbers and core per instance,  
+   --nintances and  --ncore_per_instance should be set. 
+
+   
+   >>> python -m intel_pytorch_extension.launch --multi_instance python_script args
+
+   eg: on CLX8280 with 14 instance, 4 cores per instance 
+::
+
+   >>> python -m intel_pytorch_extension.launch --multi_instance --nintances 14 --ncore_per_instance 4 python_script args
+
+
+*** Distributed Training ***
+
+spawns up multiple distributed training processes on each of the training nodes. For intel_pytorch_extension, oneCCL 
+is used as the communication backend and MPI used to launch multi-proc. To get the better 
+performance, you should specify the different cores for oneCCL communication and computation 
+process seperately. This tool can automatically set these ENVs(such as I_MPI_PIN_DOMIN) and launch
+multi-proc for you.   
+
+The utility can be used for single-node distributed training, in which one or
+more processes per node will be spawned.  It can also be used in
+multi-node distributed training, by spawning up multiple processes on each node
+for well-improved multi-node distributed training performance as well.
+
+
+1. Single-Node multi-process distributed training
+
+::
+
+    >>> python  -m intel_pytorch_extension.launch --distributed  python_script  --arg1 --arg2 --arg3 and all other
+                arguments of your training script
+
+2. Multi-Node multi-process distributed training: (e.g. two nodes)
+
+
+rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*
+
+::
+
+    >>> python -m intel_pytorch_extension.launch --distributed --nproc_per_node=xxx
+               --nnodes=2 --hostfile hostfile python_sript --arg1 --arg2 --arg3 
+               and all other arguments of your training script)
+
+
+3. To look up what optional arguments this module offers:
+
+::
+
+    >>> python -m intel_pytorch_extension.launch --help
+
+*** Memory allocator  ***
+
+"--enable_tcmalloc" and "--enable_jemalloc" can be used to enable different memory allcator. 
+
+"""
+
+class CPUinfo():
+    def __init__(self):
+
+        self.cpuinfo = []
+        if platform.system() == "Windows":
+            raise RuntimeError("Windows platform is not supported!!!")
+        elif platform.system() == "Linux":
+            args = ["lscpu", "--parse=CPU,Core,Socket,Node"]
+            lscpu_info = subprocess.check_output(args, universal_newlines=True).split("\n")
+
+            # Get information about  cpu, core, socket and node
+            for line in lscpu_info:
+                pattern = r"^([\d]+,[\d]+,[\d]+,[\d]+)"
+                regex_out = re.search(pattern, line)
+                if regex_out:
+                    self.cpuinfo.append(regex_out.group(1).strip().split(","))
+            self._get_socket_info()
+
+    def _get_socket_info(self):
+
+        self.socket_physical_cores = [] #socket_id is index
+        self.socket_logical_cores = []  #socket_id is index
+        self.sockets =  int(max([line[2] for line in self.cpuinfo])) + 1
+        for socket_id in range(self.sockets):
+            cur_socket_physical_core = []
+            cur_socket_logical_core = []
+            for line in self.cpuinfo:
+                if socket_id == int(line[2]):
+                    if line[1] not in cur_socket_physical_core:
+                        cur_socket_physical_core.append(line[1])
+                    cur_socket_logical_core.append(line[0])
+            self.socket_physical_cores.append(cur_socket_physical_core)
+            self.socket_logical_cores.append(cur_socket_logical_core)
+
+
+    def socket_nums(self):
+        return self.sockets
+
+    def physical_core_nums(self):
+        return len(self.socket_physical_cores) * len(self.socket_physical_cores[0])
+
+    def logical_core_nums(self):
+        return len(self.socket_logical_cores) * len(self.socket_logical_cores[0])
+    
+    def get_socket_physical_cores(self, socket_id):
+        if socket_id < 0 or socket_id > self.sockets - 1:
+            logger.error("Invalid socket id")
+        return self.socket_physical_cores[socket_id]
+
+    def get_socket_logical_cores(self, socket_id):
+        if socket_id < 0 or socket_id > self.sockets - 1:
+            logger.error("Invalid socket id")
+        return self.socket_logical_cores[socket_id]
+
+    def get_all_physical_cores(self):
+        return np.array(self.socket_physical_cores).flatten().tolist()
+    
+    def get_all_logical_cores(self):
+        return np.array(self.socket_logical_cores).flatten().tolist()
+              
+
+def set_mpi_pin_domain(args):
+    '''
+    I_MPI_PIN_DOMAIN specify the cores used for every MPI process. 
+    The first ccl_worker_count cores of every rank for ccl communication
+    and the other cores will be used to do computation.
+    For example: on CascadeLake 8280 CPU, 2 ranks on one node. ccl_worker_count=4
+    CCL_WORKER_COUNT=4
+    CCL_WORKER_AFFINITY="0,1,2,3,28,29,30,31"
+    I_MPI_PIN_DOMAIN=[0xffffff0,0xffffff0000000]
+    '''
+    cpuinfo = CPUinfo()
+    ppn = args.nproc_per_node
+    total_cores = cpuinfo.physical_core_nums()
+    if args.use_logical_core:
+        total_cores = cpuinfo.logcal_core_nums()
+    cores_per_rank = total_cores // ppn
+    pin_domain = "["
+    for proc in range(ppn):
+        domain_binary = 0
+        begin = proc * cores_per_rank + args.ccl_worker_count
+        end = proc * cores_per_rank + cores_per_rank -1 
+        for i in range(begin, end + 1):
+            domain_binary |= (1 << i)
+        pin_domain += hex(domain_binary) + ","
+    return pin_domain + "]"
+
+def set_ccl_worker_affinity(args):
+    '''
+    computation and communication use different cores when using oneCCL
+    backend for distributed training. we use first ccl_worker_count cores of 
+    every rank for ccl communication
+    '''
+    cpuinfo = CPUinfo()
+    ppn = args.nproc_per_node
+    total_cores = cpuinfo.physical_core_nums()
+    if args.use_logical_core:
+        total_cores = cpuinfo.logcal_core_nums()
+    cores_per_rank = total_cores // ppn
+    affinity = ''
+    for proc in range(ppn):
+        for ccl_worker in range(args.ccl_worker_count):
+            affinity += str(proc * cores_per_rank + ccl_worker)+ "," 
+    os.environ["CCL_WORKER_AFFINITY"] = affinity
+
+
+def add_lib_preload(lib_type=None):
+    '''
+    Enale TCMalloc/JeMalloc/iomp 
+    '''
+    library_paths = []
+    if "CONDA_PREFIX" in os.environ:
+        library_paths.append(os.environ["CONDA_PREFIX"] + "/lib/")
+    
+    library_paths += ["{}/.local/lib/".format(expanduser("~")), "/usr/local/lib/",
+                     "/usr/local/lib64/", "/usr/lib/", "/usr/lib64/"]
+    lib_find = False
+    for lib_path in library_paths:
+        library_file = lib_path + "lib" + lib_type + ".so"
+        matches = glob.glob(library_file)
+        if len(matches) > 0:
+            if "LD_PRELOAD" in os.environ:
+                os.environ["LD_PRELOAD"] = matches[0] + ":" + os.environ["LD_PRELOAD"]
+            else:
+                os.environ["LD_PRELOAD"] = matches[0]
+            lib_find = True
+            break
+    return lib_find
+
+def set_memory_allocator(args):
+    if args.enable_tcmalloc and args.enable_jemalloc:
+        logger.error("Unable to enable TCMalloc and JEMalloc at the same time")
+        exit(-1)
+
+    if args.enable_tcmalloc: 
+        find_tc = add_lib_preload(lib_type="tcmalloc")
+        if not find_tc:
+            logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or  /.local/lib/"
+               " or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+               "~/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+               .format("TCmalloc", "tcmalloc", expanduser("~")))
+        else:
+            logger.info("Use TCMalloc memory allocator")
+
+    elif args.enable_jemalloc:
+        find_je = add_lib_preload(lib_type="jemalloc")
+        if not find_je:
+            logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or  /.local/lib/"
+               " or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+               "~/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+               .format("JeMalloc", "jemalloc", expanduser("~")))
+        else:
+            logger.info("Use JeMallocl memory allocator")
+
+    elif args.use_default_allocator:
+        pass
+
+    else:
+        find_tc = add_lib_preload(lib_type="tcmalloc")
+        if find_tc:
+            logger.info("Use TCMalloc memory allocator")
+            return 
+        find_je = add_lib_preload(lib_type="jemalloc")
+        if find_je:
+            logger.info("Use JeMallocl memory allocator")
+            return 
+        logger.warning("Both TCMalloc and JeMalloc are not fount in $CONDA_PREFIX/lib or  /.local/lib/"
+                       " or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+                       "~/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance"
+                       .format(expanduser("~")))
+         
+def set_multi_thread_and_allcator(args):
+    
+    set_memory_allocator(args)
+    if "OMP_NUM_THREADS" not in os.environ:
+        os.environ["OMP_NUM_THREADS"] = str(args.ncore_per_instance)
+    elif "OMP_NUM_THREADS" in os.environ:
+        args.ncore_per_instance = int(os.environ["OMP_NUM_THREADS"])
+    
+    if "KMP_AFFINITY" not in os.environ:
+        os.environ["KMP_AFFINITY"] = args.kmp_affinity
+    
+    if "KMP_BLOCKTIME" not in os.environ:
+        os.environ["KMP_BLOCKTIME"] = "1"
+    
+    if "DNNL_PRIMITIVE_CACHE_CAPACITY" not in os.environ:    
+       os.environ["DNNL_PRIMITIVE_CACHE_CAPACITY"] = '1024'
+
+    logger.info("OMP_NUM_THREADS={} ".format(os.environ["OMP_NUM_THREADS"]))
+    logger.info("KMP_AFFINITY={}".format(os.environ["KMP_AFFINITY"]))
+    logger.info("KMP_BLOCKTIME={}".format(os.environ["KMP_BLOCKTIME"]))
+    logger.info("DNNL_PRIMITIVE_CACHE_CAPACITY={}".format(os.environ["DNNL_PRIMITIVE_CACHE_CAPACITY"]))
+     
+    if args.enable_iomp:
+        find_iomp = add_lib_preload(lib_type="iomp")
+        if not find_iomp:
+            logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or  /.local/lib/"
+               " or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+               "~/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+               .format("iomp", "iomp", expanduser("~")))
+        else:
+            logger.info("User iomp") 
+ 
+def launch(args):
+    '''
+    single-instance / multi-instance launcher  
+    ''' 
+    processes = []
+    cores = []
+ 
+    cpuinfo = CPUinfo()
+    if args.core_list:#user specify what cores will be used by params
+        cores = args.core_list.strip().split(",")
+        if args.ncore_per_instance == -1:
+            logger.error("please specify the '--ncore_per_instance' if you have pass the --core_list params")
+            exit(-1) 
+        elif args.ninstances > 1 and args.ncore_per_instance * args.ninstances < len(cores):
+            logger.warning("only first {} cores will be used, but you specify {} cores in core_list".format
+                  (args.ncore_per_instance * args.ninstances, len(cores)))
+        else:
+            args.ninstances = len(cores) // args.ncore_per_instance
+    else:
+        if args.use_logical_core:
+            if args.socket_id != -1:
+                cores = cpuinfo.get_socket_logical_cores(args.socket_id) 
+            else:
+                cores = cpuinfo.get_all_logical_cores()            
+        else:
+            if args.socket_id != -1:
+                cores = cpuinfo.get_socket_physical_cores(args.socket_id)
+            else:
+                cores = cpuinfo.get_all_physical_cores()      
+        if not args.multi_instance and args.ninstances == -1 and args.ncore_per_instance == -1:
+            args.ninstances = 1;
+            args.ncore_per_instance = len(cores)
+        elif args.multi_instance and args.ninstances == -1 and args.ncore_per_instance == -1:
+            args.throughput_performance = True
+        elif args.ncore_per_instance == -1 and args.ninstances != -1:
+            args.ncore_per_instance = len(cores) // args.ninstances
+        elif args.ncore_per_instance != -1 and args.ninstances == -1:
+            args.ninstances = len(cores) // args.ncore_per_instance
+        else:
+            if args.ninstances * args.ncore_per_instance > len(cores):
+                logger.error("Please make sure ninstances * ncore_per_instance <= total_cores")
+                exit(-1)
+        if args.latency_performance:
+            if args.ncore_per_instance !=4:
+               logger.warning("latency_performance is a specail mode, args.ncore_per_instance can only be set to be 4")
+            args.ncore_per_instance = 4
+            cores = cpuinfo.get_all_physical_cores()
+            args.ninstances = len(cores) // args.ncore_per_instance
+
+        if args.throughput_performance:
+            args.ninstances = cpuinfo.socket_nums()
+            cores = cpuinfo.get_all_physical_cores()
+            args.ncore_per_instance = len(cores) // args.ninstances
+
+    os.environ["LAUNCH_CMD"] = "#"
+    set_multi_thread_and_allcator(args)
+    for i in range(args.ninstances):
+       cmd = []
+       cur_process_cores = ""
+       if not args.disable_numactl:
+           cmd = ["numactl"]
+           for core in cores[i * args.ncore_per_instance:(i + 1) * args.ncore_per_instance]:
+               cur_process_cores = cur_process_cores + str(core) + ","
+           numa_params = "-C {} ".format(cur_process_cores[:-1])
+           cmd.extend(numa_params.split())
+       with_python = not args.no_python
+       if with_python:
+           cmd.append(sys.executable)
+       if args.module:
+           cmd.append("-m")
+       cmd.append(args.program)
+       cmd.extend(args.program_args)
+       os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
+       process = subprocess.Popen(cmd, env=os.environ)
+       processes.append(process)
+    os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
+    for process in processes:
+        process.wait()
+        if process.returncode != 0:
+            raise subprocess.CalledProcessError(returncode=process.returncode,
+                                                cmd=cmd) 
+    
+def mpi_dist_launch(args):
+    '''
+    Set ENVs and launch MPI process for distributed training.
+    '''
+    if args.nnodes > 1 and not os.path.exists(args.hostfile):
+        raise ValueError("hostfile is necessary when you use multi-node distributed training,"
+                          "Please create hostfile which include the ip list you used for distributed running")
+    elif args.nnodes > 1:
+        ipv4_addr_pattern = r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
+        ip_list = []
+        with open(args.hostfile) as f:
+             for line in f:
+                 line = line.strip().strip("\n")
+                 is_valid = re.match(ipv4_addr_pattern, line)
+                 if not is_valid:
+                     logger.error("{} is not valid IPV4 address".format(line))
+                     exit(-1)
+                 else:
+                     ip_list.append(line)
+        if len(ip_list) < args.nnodes:
+            logger.error("The number of IP {} should greater than nnodes parameters {}".format(len(ip_list), args.nnodes))
+            exit(-1)
+        master_check = False
+        dic = psutil.net_if_addrs()
+        for adapter in dic:
+            snicList = dic[adapter]
+            for snic in snicList:
+                if snic.address == ip_list[0]:
+                    master_check = True
+        if not master_check:
+           logger.error("MASTER_ADDR is not right. Please make sure the first ip {} in your hostfile is the current node".format(ip_list[0]))
+           exit(-1)
+ 
+        logger.info("Begin to validate the ip connect")
+        args.master_addr = ip_list[0]
+        for ip in ip_list[1:]:
+            completed_process = subprocess.run("ssh -o PasswordAuthentication=no {} ':'".format(ip), shell=True)
+            if completed_process.returncode != 0:
+                logger.error("Passwordless SSH login to {} failed, please make sure you have setup SSH public key right") 
+                exit(-1)
+            else:
+                logger.info("connection from master node {} to slave node {} is OK".format(args.master_addr, ip))
+
+    set_memory_allocator(args)
+    # set distributed related environmental variables
+    os.environ["MASTER_ADDR"] = args.master_addr
+    os.environ["MASTER_PORT"] = str(args.master_port)
+    if "I_MPI_PIN_DOMAIN" not in os.environ:
+         mpi_pin_domain = set_mpi_pin_domain(args)
+    else:
+         mpi_pin_domain = os.environ["I_MPI_PIN_DOMAIN"]
+    
+    cpuinfo = CPUinfo()
+    ppn = args.nproc_per_node 
+    total_cores = len(cpuinfo.get_all_physical_cores())
+    cores_per_rank = total_cores // ppn
+    
+    if "OMP_NUM_THREADS" not in os.environ:
+        opm_num_threads = cores_per_rank - args.ccl_worker_count
+    else:
+        opm_num_threads = os.environ["OMP_NUM_THREADS"]
+
+    os.environ["CCL_WORKER_COUNT"] = str(args.ccl_worker_count)
+
+    if "CCL_WORKER_AFFINITY" not in os.environ:
+        set_ccl_worker_affinity(args)
+
+    if "CCL_ATL_TRANSPORT" not in os.environ:
+        os.environ["CCL_ATL_TRANSPORT"] = "ofi"
+    
+    if args.enable_iomp:
+        find_iomp = add_lib_preload(lib_type="iomp")
+        if not find_iomp:
+            logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or  /.local/lib/"
+               " or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+               "~/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+               .format("iomp", "iomp", expanduser("~")))
+        else:
+             logger.info("Enale iomp by set LD_PRELOAD")
+
+    logger.info("MASTER_ADDR={}".format(args.master_addr))
+    logger.info("MASTER_PORT={}".format(args.master_port))
+    logger.info("I_MPI_PIN_DOMAIN={}".format(mpi_pin_domain))
+    logger.info("OMP_NUM_THREADS={} ".format(opm_num_threads))
+    logger.info("CCL_WORKER_COUNT={}".format(args.ccl_worker_count))
+    logger.info("CCL_WORKER_AFFINITY={}".format(os.environ["CCL_WORKER_AFFINITY"]))
+
+    os.environ["LAUNCH_CMD"] = "#"
+    cmd = ['mpiexec.hydra']
+    mpi_config = "-l -np {} -ppn {} -genv I_MPI_PIN_DOMAIN={} -genv OMP_NUM_THREADS={} ".format(args.nnodes*args.nproc_per_node,
+                  args.nproc_per_node,  mpi_pin_domain, opm_num_threads)
+    mpi_config += args.more_mpi_parms
+    if args.nnodes > 1:
+        mpi_config += " -hostfile {}".format(args.hostfile)
+    cmd.extend(mpi_config.split())
+    with_python = not args.no_python
+    if with_python:
+        cmd.append(sys.executable)
+        cmd.append("-u")
+    if args.module:
+        cmd.append("-m")
+    cmd.append(args.program)
+    cmd.extend(args.program_args)
+    process = subprocess.Popen(cmd, env=os.environ)
+    process.wait()
+    os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
+    os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
+
+def add_distributed_training_params(parser):
+    
+    cpuinfo = CPUinfo()
+    socket_nums = cpuinfo.socket_nums()
+
+    group = parser.add_argument_group("Distributed Training Parameters With oneCCL backend")
+    group.add_argument("--nnodes", metavar='\b', type=int, default=1,
+                        help="The number of nodes to use for distributed "
+                             "training")
+    group.add_argument("--nproc_per_node", metavar='\b', type=int, default=socket_nums,
+                        help="The number of processes to launch on each node")
+    #ccl control 
+    group.add_argument("--ccl_worker_count", metavar='\b', default=4, type=int,
+                        help="Core numbers per rank used for ccl communication")
+    #mpi control
+    group.add_argument("--master_addr", metavar='\b', default="127.0.0.1", type=str,
+                        help="Master node (rank 0)'s address, should be either "
+                             "the IP address or the hostname of node 0, for "
+                             "single node multi-proc training, the "
+                             "--master_addr can simply be 127.0.0.1")
+    group.add_argument("--master_port", metavar='\b', default=29500, type=int,
+                        help="Master node (rank 0)'s free port that needs to "
+                             "be used for communication during distributed "
+                             "training")
+    group.add_argument("--hostfile", metavar='\b', default="hostfile", type=str,
+                        help="Hostfile is necessary for multi-node multi-proc "
+                              "training. hostfile includes the node address list "
+                              "node address which should be either the IP address"
+                              "or the hostname.")
+    group.add_argument("--more_mpi_parms", metavar='\b', default="", type=str,
+                        help="User can pass more parameters for mpiexec.hydra "
+                              "except for -np -ppn -hostfile and -genv I_MPI_PIN_DOMAIN")
+
+def add_memory_allocator_params(parser):
+
+    group = parser.add_argument_group("Memory Allocator Parameters") 
+        #allocator control
+    group.add_argument("--enable_tcmalloc", action='store_true', default=False,
+                        help="Enable tcmalloc allocator")
+    group.add_argument("--enable_jemalloc", action='store_true', default=False,
+                        help="Enable jemalloc allocator")
+    group.add_argument("--use_default_allocator",  action='store_true', default=False,
+                        help="Use default memory allocator")
+        
+def add_multi_instance_params(parser):
+    
+    group = parser.add_argument_group("Multi-instance Parameters")
+     #multi-instance control
+    group.add_argument("--ncore_per_instance", metavar='\b', default=-1, type=int, 
+                         help="Cores per instance")
+    group.add_argument("--ninstances", metavar='\b', default=-1, type=int,
+                         help="For multi-instance, you should give the cores number you used for per insantance.")
+    group.add_argument("--latency_performance", action='store_true', default=False,
+                         help="By detault 4 core per instance and use all physical cores")
+    group.add_argument("--throughput_performance", action='store_true', default=False,
+                         help="By default one instance per socket and use all physical cores")
+    group.add_argument("--socket_id", metavar='\b', default=-1, type=int,
+                         help="Socket id for multi-instance, by default all sockets will be used")
+    group.add_argument("--use_logical_core", action='store_true', default=False,
+                         help="Whether only use physical cores")
+    group.add_argument("--disable_numactl",  action='store_true', default=False,
+                         help="Disable numactl")
+    group.add_argument("--core_list", metavar='\b', default=None, type=str,
+                         help="Specify the core list as 'core_id, core_id, ....', otherwise, all the cores will be used.")
+ 
+def add_kmp_iomp_params(parser): 
+
+    group = parser.add_argument_group("KMP/IOMP Affinity Parameters") 
+    group.add_argument("--kmp_affinity", metavar='\b', default="granularity=fine,compact,1,0", type=str,
+                        help="KMP_AFFINITY setup, environment variable has higher priority than this args."
+                             "defualt value is : granularity=fine,compact,1,0")
+    group.add_argument("--enable_iomp", action='store_true', default=False,
+                        help="Enable iomp and libiomp.so will be add to LD_PRELOAD") 
+   
+
+def parse_args():
+    """
+    Helper function parsing the command line options
+    @retval ArgumentParser
+    """
+    parser = ArgumentParser(description="This is a script for launching PyTorch training and inference on Intel Xeon CPU "
+                                        "with optimal configurations. Now, single instance inference/training, multi-instance "
+                                        "inference/training and distributed training with oneCCL backend is enabled. "
+                                        "To get the peak performance on Intel Xeon CPU, the script optimizes the configuration "
+                                        "of thread and memory management. For thread management, the script configures thread "
+                                        "affinity and the preload of Intel OMP library. For memory management, it configures " 
+                                        "NUMA binding and preload optimized memory allocation library (e.g. tcmalloc, jemalloc) "
+                                        "\n################################# Basic usage ############################# \n"
+                                        "\n 1. single instance\n" 
+                                         "\n   >>> python -m intel_pytorch_extension.launch python_script args \n"
+                                        "\n2. multi-instance \n"
+                                        "\n    >>> python -m intel_pytorch_extension.launch --multi_instance python_script args\n"
+                                        "\n3. Single-Node multi-process distributed training\n"
+                                        "\n    >>> python  -m intel_pytorch_extension.launch --distributed  python_script args\n"
+                                        "\n4. Multi-Node multi-process distributed training: (e.g. two nodes)\n"
+                                        "\n   rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*\n"
+                                        "\n   >>> python -m intel_pytorch_extension.launch --distributed --nproc_per_node=2\n"
+                                        "\n       --nnodes=2 --hostfile hostfile python_script args\n",
+                                        formatter_class=RawTextHelpFormatter)
+    
+    parser.add_argument("--multi_instance", action='store_true', default=False,
+                        help="Enable multi-instance, by default one instance per socket")  
+
+    parser.add_argument('--distributed', action='store_true', default=False,
+                    help='Enable distributed training.')
+    parser.add_argument("-m", "--module", default=False, action="store_true",
+                        help="Changes each process to interpret the launch script "
+                             "as a python module, executing with the same behavior as"
+                             "'python -m'.")
+
+    parser.add_argument("--no_python", default=False, action="store_true",
+                        help="Do not prepend the --program script with \"python\" - just exec "
+                             "it directly. Useful when the script is not a Python script.")
+    add_memory_allocator_params(parser)
+    add_kmp_iomp_params(parser)
+     
+    add_distributed_training_params(parser)
+    add_multi_instance_params(parser)
+    # positional
+    parser.add_argument("program", type=str,
+                        help="The full path to the proram/script to be launched. "
+                             "followed by all the arguments for the script")
+
+    # rest from the training program
+    parser.add_argument('program_args', nargs=REMAINDER)
+    return parser.parse_args()
+
+def main():
+
+    env_before = set(os.environ.keys())
+    if platform.system() == "Windows":
+        raise RuntimeError("Windows platform is not supported!!!")
+
+    args = parse_args()
+
+    if args.distributed and args.multi_instance:
+        raise RuntimeError("Either args.distributed or args.multi_instance should be set")
+    
+    if args.latency_performance and args.throughput_performance:
+        raise RuntimeError("Either args.latency_performance or args.throughput_performance  should be set")
+
+    if args.nnodes > 1:
+        args.distributed = True
+
+    if args.distributed:
+        mpi_dist_launch(args)
+    else:
+        launch(args)
+
+    for x in sorted(set(os.environ.keys()) - env_before):
+        logger.debug(f'{x}={os.environ[x]}')
+ 
+if __name__ == "__main__":
+    main()
+
diff --git a/mlperf_logger.py b/mlperf_logger.py
index efce1d3..ab6b9a3 100644
--- a/mlperf_logger.py
+++ b/mlperf_logger.py
@@ -8,14 +8,13 @@
 Utilities for MLPerf logging
 """
 import os
+
+from mlperf_logging import mllog
+from mlperf_logging.mllog import constants
+
 import torch
 
-try:
-    from mlperf_logging import mllog
-    from mlperf_logging.mllog import constants
-    _MLLOGGER = mllog.get_mllogger()
-except ImportError as error:
-        print("Unable to import mlperf_logging, ", error)
+_MLLOGGER = mllog.get_mllogger()
 
 
 def log_start(*args, **kwargs):
@@ -62,9 +61,9 @@ def barrier():
     Calls all_reduce on dummy tensor and synchronizes with GPU.
     """
     if torch.distributed.is_available() and torch.distributed.is_initialized():
-        torch.distributed.all_reduce(torch.cuda.FloatTensor(1))
-        torch.cuda.synchronize()
-
+        #torch.distributed.all_reduce(torch.cuda.FloatTensor(1))
+        #torch.cuda.synchronize()
+        torch.distributed.barrier()
 
 def get_rank():
     """
@@ -103,7 +102,7 @@ def mlperf_submission_log(benchmark):
 
     log_event(
         key=constants.SUBMISSION_PLATFORM,
-        value='reference_implementation')
+        value=f'reference_implementation')
 
     log_event(
         key=constants.SUBMISSION_ENTRY,
diff --git a/model_compression/AGP_Structure/dlrm.schedule_agp_1.yaml b/model_compression/AGP_Structure/dlrm.schedule_agp_1.yaml
new file mode 100755
index 0000000..d0d50f8
--- /dev/null
+++ b/model_compression/AGP_Structure/dlrm.schedule_agp_1.yaml
@@ -0,0 +1,68 @@
+version: 1
+pruners:
+  low_pruner:
+    class: L1RankedStructureParameterPruner_AGP
+    initial_sparsity : 0.01
+    final_sparsity: 0.50
+    group_type: Rows
+    weights: [bot_l.module.0.weight,
+              bot_l.module.2.weight,
+              bot_l.module.4.weight,
+              bot_l.module.6.weight,
+              top_l.module.0.weight,
+              top_l.module.2.weight,
+              top_l.module.4.weight,
+              top_l.module.6.weight,
+              top_l.module.8.weight]
+
+  fine_pruner:
+    class:  AutomatedGradualPruner
+    initial_sparsity : 0.01
+    final_sparsity: 0.50
+    weights: [bot_l.module.0.weight,
+              bot_l.module.2.weight,
+              bot_l.module.4.weight,
+              bot_l.module.6.weight,
+              top_l.module.0.weight,
+              top_l.module.2.weight,
+              top_l.module.4.weight,
+              top_l.module.6.weight,
+              top_l.module.8.weight]
+
+#lr_schedulers:
+#  pruning_lr:
+#    class: StepLR
+#    step_size: 50
+#    gamma: 0.10
+
+
+#extensions:
+#  net_thinner:
+#      class: 'FilterRemover'
+#      thinning_func_str: remove_filters
+#      arch: 'resnet20_cifar'
+#      dataset: 'cifar10'
+
+policies:
+  - pruner:
+      instance_name : low_pruner
+    starting_epoch: 0
+    ending_epoch: 2
+    frequency: 1
+
+  - pruner:
+      instance_name : fine_pruner
+    starting_epoch: 2
+    ending_epoch: 4
+    frequency: 1
+
+# After completing the pruning, we perform network thinning and continue fine-tuning.
+  #- extension:
+  #    instance_name: net_thinner
+  #  epochs: [22]
+
+  #- lr_scheduler:
+  #    instance_name: pruning_lr
+  #  starting_epoch: 0
+  #  ending_epoch: 400
+  #  frequency: 1
diff --git a/model_compression/AGP_Structure/dlrm.schedule_agp_2.yaml b/model_compression/AGP_Structure/dlrm.schedule_agp_2.yaml
new file mode 100755
index 0000000..af068a6
--- /dev/null
+++ b/model_compression/AGP_Structure/dlrm.schedule_agp_2.yaml
@@ -0,0 +1,68 @@
+version: 1
+pruners:
+  low_pruner:
+    class: L1RankedStructureParameterPruner_AGP
+    initial_sparsity : 0.01
+    final_sparsity: 0.99
+    group_type: Rows
+    weights: [bot_l.module.0.weight,
+              bot_l.module.1.weight,
+              bot_l.module.2.weight,
+              bot_l.module.3.weight,
+              top_l.module.0.weight,
+              top_l.module.1.weight,
+              top_l.module.2.weight,
+              top_l.module.3.weight,
+              top_l.module.4.weight]
+
+  fine_pruner:
+    class:  AutomatedGradualPruner
+    initial_sparsity : 0.01
+    final_sparsity: 0.99
+    weights: [bot_l.module.0.weight,
+              bot_l.module.1.weight,
+              bot_l.module.2.weight,
+              bot_l.module.3.weight,
+              top_l.module.0.weight,
+              top_l.module.1.weight,
+              top_l.module.2.weight,
+              top_l.module.3.weight,
+              top_l.module.4.weight]
+
+#lr_schedulers:
+#  pruning_lr:
+#    class: StepLR
+#    step_size: 50
+#    gamma: 0.10
+
+
+#extensions:
+#  net_thinner:
+#      class: 'FilterRemover'
+#      thinning_func_str: remove_filters
+#      arch: 'resnet20_cifar'
+#      dataset: 'cifar10'
+
+policies:
+  - pruner:
+      instance_name : low_pruner
+    starting_epoch: 0
+    ending_epoch: 2
+    frequency: 1
+
+  - pruner:
+      instance_name : fine_pruner
+    starting_epoch: 2
+    ending_epoch: 4
+    frequency: 1
+
+# After completing the pruning, we perform network thinning and continue fine-tuning.
+  #- extension:
+  #    instance_name: net_thinner
+  #  epochs: [22]
+
+  #- lr_scheduler:
+  #    instance_name: pruning_lr
+  #  starting_epoch: 0
+  #  ending_epoch: 400
+  #  frequency: 1
diff --git a/model_compression/AGP_Weight/dlrm.schedule_agp.yaml b/model_compression/AGP_Weight/dlrm.schedule_agp.yaml
new file mode 100755
index 0000000..f175206
--- /dev/null
+++ b/model_compression/AGP_Weight/dlrm.schedule_agp.yaml
@@ -0,0 +1,96 @@
+# This schedule performs element-wise (fine grain) pruning, following the Automated Gradual Pruner (Zhu-Gupta) schedule.
+#
+# time python3 compress_classifier.py -a=alexnet --lr=0.005 -p=50 ../../../data.imagenet -j 22 --epochs 90 --pretrained --compress=../agp-pruning/alexnet.schedule_agp.yaml
+#
+# Parameters:
+#
+# +----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
+# |    | Name                      | Shape            |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
+# |----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
+# |  0 | features.module.0.weight  | (64, 3, 11, 11)  |         23232 |          23232 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.15003 | -0.00002 |    0.10000 |
+# |  1 | features.module.3.weight  | (192, 64, 5, 5)  |        307200 |         116736 |    0.00000 |    0.00000 |  0.00000 |  2.19727 |  0.00000 |   62.00000 | 0.04665 | -0.00245 |    0.02222 |
+# |  2 | features.module.6.weight  | (384, 192, 3, 3) |        663552 |         232244 |    0.00000 |    0.00000 |  0.00000 |  8.85824 |  0.00000 |   64.99988 | 0.03270 | -0.00179 |    0.01627 |
+# |  3 | features.module.8.weight  | (256, 384, 3, 3) |        884736 |         504300 |    0.00000 |    0.00000 |  0.00000 |  0.44861 |  0.00000 |   42.99995 | 0.02758 | -0.00193 |    0.01720 |
+# |  4 | features.module.10.weight | (256, 256, 3, 3) |        589824 |         336200 |    0.00000 |    0.00000 |  0.00000 |  0.68512 |  0.00000 |   42.99995 | 0.02836 | -0.00284 |    0.01795 |
+# |  5 | classifier.1.weight       | (4096, 9216)     |      37748736 |        3397387 |    0.00000 |    0.21973 |  0.00000 |  0.21973 |  0.00000 |   91.00000 | 0.00567 | -0.00017 |    0.00151 |
+# |  6 | classifier.4.weight       | (4096, 4096)     |      16777216 |        1509950 |    0.21973 |    3.93066 |  0.00000 |  3.93066 |  0.00000 |   91.00000 | 0.00798 | -0.00054 |    0.00217 |
+# |  7 | classifier.6.weight       | (1000, 4096)     |       4096000 |        1024000 |    2.97852 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   75.00000 | 0.01688 |  0.00052 |    0.00753 |
+# |  8 | Total sparsity:           | -                |      61090496 |        7144049 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.30579 | 0.00000 |  0.00000 |    0.00000 |
+# +----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
+# Total sparsity: 88.31
+#
+# --- validate (epoch=89)-----------
+# 128116 samples (256 per mini-batch)
+# Epoch: [89][   50/  500]    Loss 2.180059    Top1 51.671875    Top5 74.250000
+# Epoch: [89][  100/  500]    Loss 2.174012    Top1 51.851562    Top5 74.386719
+# Epoch: [89][  150/  500]    Loss 2.187923    Top1 51.520833    Top5 74.184896
+# Epoch: [89][  200/  500]    Loss 2.192574    Top1 51.433594    Top5 74.156250
+# Epoch: [89][  250/  500]    Loss 2.184967    Top1 51.459375    Top5 74.279688
+# Epoch: [89][  300/  500]    Loss 2.184608    Top1 51.440104    Top5 74.239583
+# Epoch: [89][  350/  500]    Loss 2.179280    Top1 51.537946    Top5 74.357143
+# Epoch: [89][  400/  500]    Loss 2.182293    Top1 51.491211    Top5 74.354492
+# Epoch: [89][  450/  500]    Loss 2.182311    Top1 51.440104    Top5 74.333333
+# Epoch: [89][  500/  500]    Loss 2.183890    Top1 51.459375    Top5 74.328125
+# ==> Top1: 51.456    Top5: 74.326    Loss: 2.185
+#
+# Saving checkpoint
+# --- test ---------------------
+# 50000 samples (256 per mini-batch)
+# Test: [   50/  195]    Loss 1.493478    Top1 63.070312    Top5 85.898438
+# Test: [  100/  195]    Loss 1.643213    Top1 60.539063    Top5 83.589844
+# Test: [  150/  195]    Loss 1.836551    Top1 57.466146    Top5 80.273438
+# ==> Top1: 56.528    Top5: 79.352    Loss: 1.897
+#
+#
+# Log file for this run: /data/home/cvds_lab/nzmora/private-distiller/examples/classifier_compression/logs/2018.03.30-142316/2018.03.30-142316.log
+#
+# real    664m25.914s
+# user    13391m25.914s
+# sys     1569m37.119s
+
+version: 1
+pruners:
+  bot_l1_pruner:
+    class: 'AutomatedGradualPruner'
+    initial_sparsity : 0.01
+    final_sparsity: 0.50
+    weights: ['bot_l.module.1.weight', 'bot_l.module.1.bias']
+
+  top_l1_pruner:
+    class: 'AutomatedGradualPruner'
+    initial_sparsity: 0.01
+    final_sparsity: 0.50
+    weights: ['top_l.module.0.weight', 'top_l.module.0.bias']
+
+  top_l2_pruner:
+    class: 'AutomatedGradualPruner'
+    initial_sparsity : 0.01
+    final_sparsity: 0.50
+    weights: ['top_l.module.1.weight', top_l.module.1.bias]
+
+#lr_schedulers:
+  # Learning rate decay scheduler
+#   pruning_lr:
+#     class: ExponentialLR
+#     gamma: 0.9
+
+
+policies:
+  - pruner:
+      instance_name : 'bot_l1_pruner'
+    starting_epoch: 1
+    ending_epoch: 10
+    frequency: 1
+
+  - pruner:
+      instance_name : 'top_l1_pruner'
+    starting_epoch: 1
+    ending_epoch: 10
+    frequency: 1
+
+  - pruner:
+      instance_name : 'top_l2_pruner'
+    starting_epoch: 1
+    ending_epoch: 10
+    frequency: 1
+
diff --git a/model_compression/AGP_Weight/dlrm.schedule_sensivity.yaml b/model_compression/AGP_Weight/dlrm.schedule_sensivity.yaml
new file mode 100755
index 0000000..19a0368
--- /dev/null
+++ b/model_compression/AGP_Weight/dlrm.schedule_sensivity.yaml
@@ -0,0 +1,89 @@
+#
+# This schedule is an example of "Iterative Pruning" for Alexnet/Imagent, as
+# described in chapter 3 of Song Han's PhD dissertation: "EFFICIENT METHODS AND
+# HARDWARE FOR DEEP LEARNING"
+#
+# The pruning policy uses multiple pruning phases.  Each pruning phase is
+# followed by a retraining phase.
+# In this particular policy, pruning is scheduled every 2 epochs.
+# After 38/2 pruning phases, pruning ends and the only retraining continues.
+#
+# time python3 compress_classifier.py -a alexnet --lr 0.005 -p 50 ../../../data.imagenet -j=44 --epochs=90 --pretrained --compress=../sensitivity-pruning/alexnet.schedule_sensitivity.yaml
+#
+# Parameters:
+#
+# +----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
+# |    | Name                      | Shape            |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
+# |----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
+# |  0 | features.module.0.weight  | (64, 3, 11, 11)  |         23232 |          13373 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.43716 | 0.14381 | -0.00002 |    0.08794 |
+# |  1 | features.module.3.weight  | (192, 64, 5, 5)  |        307200 |         115322 |    0.00000 |    0.00000 |  0.00000 |  2.04264 |  0.00000 |   62.46029 | 0.04702 | -0.00248 |    0.02286 |
+# |  2 | features.module.6.weight  | (384, 192, 3, 3) |        663552 |         256454 |    0.00000 |    0.00000 |  0.00000 |  6.13742 |  0.00000 |   61.35133 | 0.03354 | -0.00184 |    0.01803 |
+# |  3 | features.module.8.weight  | (256, 384, 3, 3) |        884736 |         315278 |    0.00000 |    0.00000 |  0.00000 |  7.02922 |  0.00000 |   64.36474 | 0.02647 | -0.00168 |    0.01423 |
+# |  4 | features.module.10.weight | (256, 256, 3, 3) |        589824 |         186861 |    0.00000 |    0.00000 |  0.00000 | 15.72266 |  0.00000 |   68.31919 | 0.02714 | -0.00245 |    0.01408 |
+# |  5 | classifier.1.weight       | (4096, 9216)     |      37748736 |        3395124 |    0.00000 |    0.21973 |  0.00000 |  0.21973 |  0.00000 |   91.00599 | 0.00589 | -0.00020 |    0.00168 |
+# |  6 | classifier.4.weight       | (4096, 4096)     |      16777216 |        1783541 |    0.21973 |    3.49121 |  0.00000 |  3.49121 |  0.00000 |   89.36927 | 0.00849 | -0.00066 |    0.00263 |
+# |  7 | classifier.6.weight       | (1000, 4096)     |       4096000 |         993134 |    3.39355 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   75.75356 | 0.01718 |  0.00029 |    0.00777 |
+# |  8 | Total sparsity:           | -                |      61090496 |        7059087 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.44487 | 0.00000 |  0.00000 |    0.00000 |
+# +----+---------------------------+------------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
+# Total sparsity: 88.44
+#
+# --- validate (epoch=89)-----------
+# 128116 samples (256 per mini-batch)
+# Epoch: [89][   50/  500]    Loss 2.149753    Top1 51.976562    Top5 74.859375
+# Epoch: [89][  100/  500]    Loss 2.154934    Top1 51.941406    Top5 74.550781
+# Epoch: [89][  150/  500]    Loss 2.159868    Top1 51.880208    Top5 74.513021
+# Epoch: [89][  200/  500]    Loss 2.158245    Top1 51.875000    Top5 74.597656
+# Epoch: [89][  250/  500]    Loss 2.150266    Top1 51.920313    Top5 74.667187
+# Epoch: [89][  300/  500]    Loss 2.152199    Top1 51.933594    Top5 74.682292
+# Epoch: [89][  350/  500]    Loss 2.152126    Top1 51.952009    Top5 74.684152
+# Epoch: [89][  400/  500]    Loss 2.153599    Top1 51.949219    Top5 74.648438
+# Epoch: [89][  450/  500]    Loss 2.151281    Top1 52.046875    Top5 74.703993
+# Epoch: [89][  500/  500]    Loss 2.149620    Top1 52.032031    Top5 74.765625
+# ==> Top1: 52.029    Top5: 74.767    Loss: 2.150
+#
+# Saving checkpoint
+# --- test ---------------------
+# 50000 samples (256 per mini-batch)
+# Test: [   50/  195]    Loss 1.484814    Top1 63.328125    Top5 85.820312
+# Test: [  100/  195]    Loss 1.636993    Top1 60.835938    Top5 83.617188
+# Test: [  150/  195]    Loss 1.832027    Top1 57.713542    Top5 80.330729
+# ==> Top1: 56.762    Top5: 79.340    Loss: 1.892
+#
+#
+# Log file for this run: /data/home/cvds_lab/nzmora/private-distiller/examples/classifier_compression/logs/2018.04.08-154509/2018.04.08-154509.log
+#
+# real    646m54.061s
+# user    14899m29.068s
+# sys     1901m19.958s
+
+version: 1
+pruners:
+  pruner1:
+    class: 'SensitivityPruner'
+    sensitivities:
+      'features.module.0.weight': 0.25
+      'features.module.3.weight': 0.35
+      'features.module.6.weight': 0.40
+      'features.module.8.weight': 0.45
+      'features.module.10.weight': 0.55
+      'classifier.1.weight': 0.875
+      'classifier.4.weight': 0.875
+      'classifier.6.weight': 0.625
+
+lr_schedulers:
+   pruning_lr:
+     class: ExponentialLR
+     gamma: 0.9
+
+policies:
+  - pruner:
+      instance_name : 'pruner1'
+    starting_epoch: 0
+    ending_epoch: 38
+    frequency: 2
+
+  - lr_scheduler:
+      instance_name: pruning_lr
+    starting_epoch: 24
+    ending_epoch: 200
+    frequency: 1
diff --git a/model_compression/__init__.py b/model_compression/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/model_compression/hyparameters.py b/model_compression/hyparameters.py
new file mode 100644
index 0000000..4c08876
--- /dev/null
+++ b/model_compression/hyparameters.py
@@ -0,0 +1,101 @@
+import argparse
+
+class hyparams:
+    ### parse arguments ###
+    parser = argparse.ArgumentParser(
+        description="Train Deep Learning Recommendation Model (DLRM)"
+    )
+    # model related parameters
+    parser.add_argument("--arch-sparse-feature-size", type=int, default=2)
+    parser.add_argument("--arch-embedding-size", type=str, default="4-3-2")
+    # j will be replaced with the table number
+    parser.add_argument("--arch-mlp-bot", type=str, default="4-3-2")
+    parser.add_argument("--arch-mlp-top", type=str, default="4-2-1")
+    parser.add_argument("--arch-interaction-op", type=str, default="dot")
+    parser.add_argument("--arch-interaction-itself", action="store_true", default=False)
+    # embedding table options
+    parser.add_argument("--md-flag", action="store_true", default=False)
+    parser.add_argument("--md-threshold", type=int, default=200)
+    parser.add_argument("--md-temperature", type=float, default=0.3)
+    parser.add_argument("--md-round-dims", action="store_true", default=False)
+    parser.add_argument("--qr-flag", action="store_true", default=False)
+    parser.add_argument("--qr-threshold", type=int, default=200)
+    parser.add_argument("--qr-operation", type=str, default="mult")
+    parser.add_argument("--qr-collisions", type=int, default=4)
+    # activations and loss
+    parser.add_argument("--activation-function", type=str, default="relu")
+    parser.add_argument("--loss-function", type=str, default="mse")  # or bce or wbce
+    parser.add_argument("--loss-weights", type=str, default="1.0-1.0")  # for wbce
+    parser.add_argument("--loss-threshold", type=float, default=0.0)  # 1.0e-7
+    parser.add_argument("--round-targets", type=bool, default=False)
+    # data
+    parser.add_argument("--data-size", type=int, default=1)
+    parser.add_argument("--num-batches", type=int, default=0)
+    parser.add_argument(
+        "--data-generation", type=str, default="random"
+    )  # synthetic or dataset
+    parser.add_argument("--data-trace-file", type=str, default="./input/dist_emb_j.log")
+    parser.add_argument("--data-set", type=str, default="kaggle")  # or terabyte
+    parser.add_argument("--raw-data-file", type=str, default="")
+    parser.add_argument("--processed-data-file", type=str, default="")
+    parser.add_argument("--data-randomize", type=str, default="total")  # or day or none
+    parser.add_argument("--data-trace-enable-padding", type=bool, default=False)
+    parser.add_argument("--max-ind-range", type=int, default=-1)
+    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
+    parser.add_argument("--num-indices-per-lookup", type=int, default=10)
+    parser.add_argument("--num-indices-per-lookup-fixed", type=bool, default=False)
+    parser.add_argument("--num-workers", type=int, default=0)
+    parser.add_argument("--memory-map", action="store_true", default=False)
+    # training
+    parser.add_argument("--mini-batch-size", type=int, default=1)
+    parser.add_argument("--nepochs", type=int, default=1)
+    parser.add_argument("--learning-rate", type=float, default=0.01)
+    parser.add_argument("--print-precision", type=int, default=5)
+    parser.add_argument("--numpy-rand-seed", type=int, default=123)
+    parser.add_argument("--sync-dense-params", type=bool, default=True)
+    # inference
+    parser.add_argument("--inference-only", action="store_true", default=False)
+    # onnx
+    parser.add_argument("--save-onnx", action="store_true", default=False)
+    # gpu
+    parser.add_argument("--use-gpu", action="store_true", default=False)
+    # distributed run
+    parser.add_argument("--dist-backend", type=str, default="")
+    # debugging and profiling
+    parser.add_argument("--print-freq", type=int, default=1)
+    parser.add_argument("--test-freq", type=int, default=-1)
+    parser.add_argument("--test-mini-batch-size", type=int, default=-1)
+    parser.add_argument("--test-num-workers", type=int, default=-1)
+    parser.add_argument("--print-time", action="store_true", default=False)
+    parser.add_argument("--debug-mode", action="store_true", default=False)
+    parser.add_argument("--enable-profiling", action="store_true", default=False)
+    parser.add_argument("--plot-compute-graph", action="store_true", default=False)
+    parser.add_argument("--profiling-start-iter", type=int, default=50)
+    parser.add_argument("--profiling-num-iters", type=int, default=100)
+    # store/load model
+    parser.add_argument("--out-dir", type=str, default=".")
+    parser.add_argument("--save-model", type=str, default="")
+    parser.add_argument("--load-model", type=str, default="")
+    # mlperf logging (disables other output and stops early)
+    parser.add_argument("--mlperf-logging", action="store_true", default=False)
+    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
+    parser.add_argument("--mlperf-acc-threshold", type=float, default=0.0)
+    # stop at target AUC Terabyte (no subsampling) 0.8025
+    parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
+    parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
+    parser.add_argument("--mlperf-bin-shuffle", action='store_true', default=False)
+    # LR policy
+    parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
+    parser.add_argument("--lr-decay-start-step", type=int, default=0)
+    parser.add_argument("--lr-num-decay-steps", type=int, default=0)
+    # embedding table is sparse table only if sparse_dense_boundary >= 2048
+    parser.add_argument("--sparse-dense-boundary", type=int, default=2048)
+    # bf16 option
+    parser.add_argument("--bf16", action='store_true', default=False)
+    # ipex option
+    parser.add_argument("--use-ipex", action="store_true", default=False)
+    # lamb
+    parser.add_argument("--optimizer", type=int, default=0, help='optimizer:[0:sgd, 1:lamb/sgd, 2:adagrad, 3:sparseadam]')
+    # distiller option
+    parser.add_argument("--model-compression-type", type=str, default=None)
+    parser.add_argument("--compression-file", type=str, default="./model_compression/dlrm.schedule_agp.yaml")
\ No newline at end of file
diff --git a/optim/rwsadagrad.py b/optim/rwsadagrad.py
deleted file mode 100644
index 95381ec..0000000
--- a/optim/rwsadagrad.py
+++ /dev/null
@@ -1,122 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-
-import torch
-from torch.optim import Optimizer
-
-
-class RWSAdagrad(Optimizer):
-    """Implements Row Wise Sparse Adagrad algorithm.
-
-    Arguments:
-        params (iterable): iterable of parameters to optimize or dicts defining
-            parameter groups
-        lr (float, optional): learning rate (default: 1e-2)
-        lr_decay (float, optional): learning rate decay (default: 0)
-        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
-        eps (float, optional): term added to the denominator to improve
-            numerical stability (default: 1e-10)
-
-    """
-
-    def __init__(self, params, lr=1e-2, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0.0, eps=1e-10):
-        if not 0.0 <= lr:
-            raise ValueError("Invalid learning rate: {}".format(lr))
-        if not 0.0 <= lr_decay:
-            raise ValueError("Invalid lr_decay value: {}".format(lr_decay))
-        if not 0.0 <= weight_decay:
-            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
-        if not 0.0 <= initial_accumulator_value:
-            raise ValueError("Invalid initial_accumulator_value value: {}".format(initial_accumulator_value))
-        if not 0.0 <= eps:
-            raise ValueError("Invalid epsilon value: {}".format(eps))
-
-        self.defaults = dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay,
-                        initial_accumulator_value=initial_accumulator_value)
-        super(RWSAdagrad, self).__init__(params, self.defaults)
-
-        self.momentum_initialized = False
-
-        for group in self.param_groups:
-            for p in group['params']:
-                self.state[p]['step'] = 0
-
-    def share_memory(self):
-        for group in self.param_groups:
-            for p in group['params']:
-                state = self.state[p]
-                if p.grad.data.is_sparse:
-                    state['momentum'].share_memory_()
-                else:
-                    state['sum'].share_memory_()
-
-    def step(self, closure=None):
-        """Performs a single optimization step.
-
-        Arguments:
-            closure (callable, optional): A closure that reevaluates the model
-                and returns the loss.
-        """
-        loss = None
-        if closure is not None:
-            loss = closure()
-
-        for group in self.param_groups:
-            for p in group['params']:
-                if p.grad is None:
-                    continue
-
-                if not self.momentum_initialized :
-                    if p.grad.data.is_sparse:
-                        self.state[p]['momentum'] = torch.full(
-                            [p.data.shape[0]],
-                            self.defaults["initial_accumulator_value"],
-                            dtype=torch.float32,
-                        )
-                    else:
-                        self.state[p]['sum'] = torch.full_like(p.data,
-                            self.defaults["initial_accumulator_value"],
-                            dtype=torch.float32,
-                        )
-
-                grad = p.grad
-                state = self.state[p]
-
-                state['step'] += 1
-
-                if group['weight_decay'] != 0:
-                    if p.grad.data.is_sparse:
-                        raise RuntimeError("weight_decay option is not compatible with sparse gradients")
-                    grad = grad.add(group['weight_decay'], p.data)
-
-                clr = group['lr'] / (1.0 + (state['step'] - 1.0) * group['lr_decay'])
-
-                if grad.is_sparse:
-                    grad = grad.coalesce()  # the update is non-linear so indices must be unique
-                    grad_indices = grad._indices()
-                    grad_values = grad._values()
-                    size = grad.size()
-
-                    def make_sparse(values, row_wise):
-                        constructor = grad.new
-                        matrix_size = [size[0]] if row_wise else size
-                        return constructor(grad_indices, values, matrix_size)
-
-                    if grad_values.numel() > 0:
-                        momentum_update = make_sparse(grad_values.pow(2).mean(dim=1), True)
-                        state['momentum'].add_(momentum_update)  # update momentum
-                        std = state['momentum'].sparse_mask(momentum_update.coalesce())
-                        std_values = std._values().sqrt_().add_(group['eps'])
-                        p.data.add_(make_sparse(grad_values / std_values.view(std_values.size()[0], 1), False), alpha=-clr)
-
-                else:
-                    state['sum'].addcmul_(grad, grad, value=1.0)
-                    std = state['sum'].sqrt().add_(group['eps'])
-                    p.data.addcdiv_(grad, std, value=-clr)
-
-        self.momentum_initialized = True
-
-        return loss
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 83ab042..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-future
-numpy
-onnx
-pydot
-torch
-torchviz
-scikit-learn
-tqdm
-torchrec-nightly
-torchx-nightly
diff --git a/run_and_time_launch.sh b/run_and_time_launch.sh
new file mode 100755
index 0000000..7135bb7
--- /dev/null
+++ b/run_and_time_launch.sh
@@ -0,0 +1,7 @@
+source ~/.local/env/setvars.sh
+DATA_PATH = 
+seed_num=$(date +%s)
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY="granularity=fine,compact,1,0"
+
+python  -u  ./launch.py --distributed --nproc_per_node=2 --nnodes=4 --hostfile ./hosts --master_addr="10***"  dlrm_s_pytorch.py --train-data-path=$DATA_PATH +"/train_data.bin" --eval-data-path=$DATA_PATH + "/test_data.bin" --day-feature-count=$DATA_PATH + "/day_fea_count.npz" --arch-sparse-feature-size=64 --arch-mlp-bot="13-128-64" --arch-mlp-top="256-128-1" --max-ind-range=40000000 --data-generation=dataset --data-set=terabyte --raw-data-file=$DATA_PATH/day --processed-data-file=$DATA_PATH/terabyte_processed.npz --loss-function=bce --round-targets=True --bf16 --num-workers=0 --test-num-workers=0 --use-ipex --optimizer=1 --dist-backend=ccl --learning-rate=16 --mini-batch-size=262144 --print-freq=16 --print-time --test-freq=800 --sparse-dense-boundary=403346 --test-mini-batch-size=131072 --lamblr=30 --lr-num-warmup-steps=4000 --lr-decay-start-step=5760 --lr-num-decay-steps=27000 --memory-map --mlperf-logging --mlperf-auc-threshold=0.9 --mlperf-bin-loader --mlperf-bin-shuffle --numpy-rand-seed=12345 $dlrm_extra_option 2>&1 | tee run_terabyte_${seed_num}.log
diff --git a/run_compress.sh b/run_compress.sh
new file mode 100755
index 0000000..3766f37
--- /dev/null
+++ b/run_compress.sh
@@ -0,0 +1,18 @@
+source ~/.local/env/setvars.sh
+seed_num=$(date +%s)
+#export CCL_ATL_SHM=0
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY="granularity=fine,compact,1,0"
+
+
+#Four nodes
+#python -u ./launch.py --distributed --nproc_per_node=2 --nnodes=1 --hostfile ./hosts --master_addr="10.0.0.44" ./dlrm_s_pytorch_lamb_sparselamb_test.py --arch-sparse-feature-size=64 --arch-mlp-bot="13-128-64" --arch-mlp-top="256-128-1" --max-ind-range=40000000  --data-generation=dataset --data-set=terabyte --raw-data-file=$DATA_PATH/day --processed-data-file=$DATA_PATH/terabyte_processed.npz --loss-function=bce --round-targets=True --bf16 --num-workers=0 --test-num-workers=0 --use-ipex --optimizer=1 --dist-backend=ccl --learning-rate=16 --mini-batch-size=65536 --print-freq=16 --print-time --test-freq=800 --sparse-dense-boundary=403346 --test-mini-batch-size=4096 --lr-num-warmup-steps=4000 --lr-decay-start-step=5760 --lr-num-decay-steps=27000 --memory-map --mlperf-logging --mlperf-auc-threshold=0.8025 --mlperf-bin-loader --mlperf-bin-shuffle --numpy-rand-seed=12345 --save-model "./model/" $dlrm_extra_option 2>&1 | tee run_${seed_num}_03_30_2021.log
+
+# run model compression
+python -u ./launch.py --distributed --nproc_per_node=2 --nnodes=1 --hostfile ./hosts --master_addr="10.0.0.44" --master_port="29501" ./dlrm_s_pytorch_compress.py --arch-sparse-feature-size=64 --arch-mlp-bot="13-512-256-128-64" --arch-mlp-top="1024-1024-512-256-1" --max-ind-range=40000000  --data-generation=dataset --data-set=terabyte --raw-data-file=$DATA_PATH/day --processed-data-file=$DATA_PATH/terabyte_processed.npz --loss-function=bce --round-targets=True --num-workers=0 --nepochs 4 --test-num-workers=0 --use-ipex --optimizer=1 --dist-backend=ccl --learning-rate=16 --mini-batch-size=65536 --print-freq=16 --print-time --test-freq=800 --sparse-dense-boundary=403346 --test-mini-batch-size=4096 --lr-num-warmup-steps=4000 --lr-decay-start-step=5760 --lr-num-decay-steps=27000 --memory-map --mlperf-logging --mlperf-auc-threshold=-1 --mlperf-bin-loader --mlperf-bin-shuffle --numpy-rand-seed=12345 --save-model "./model_compression/model/compress/AGP_Structure/test2" --model-compression-type "AGP" --compression-file "./model_compression/AGP_Structure/dlrm.schedule_agp_2.yaml" $dlrm_extra_option 2>&1 | tee ./model_compression/model/compress/AGP_Structure/test2/run_model_09_2021_test.log
+
+&&
+
+#run compression analysis
+python -u ./launch.py --distributed --nproc_per_node=2 --nnodes=1 --hostfile ./hosts --master_addr="10.0.0.44" ./analysis_model.py --arch-sparse-feature-size=64 --arch-mlp-bot="13-512-256-128-64" --arch-mlp-top="1024-1024-512-256-1" --max-ind-range=40000000  --data-generation=dataset --data-set=terabyte --raw-data-file=$DATA_PATH/day --processed-data-file=$DATA_PATH/terabyte_processed.npz --loss-function=bce --round-targets=True --num-workers=0 --test-num-workers=0 --use-ipex --optimizer=1 --dist-backend=ccl --learning-rate=16 --nepochs 10 --mini-batch-size=65536 --print-freq=16 --print-time --test-freq=800 --sparse-dense-boundary=403346 --test-mini-batch-size=4096 --lr-num-warmup-steps=4000 --lr-decay-start-step=5760 --lr-num-decay-steps=27000 --memory-map --mlperf-logging --mlperf-auc-threshold=0.8025 --mlperf-bin-loader --mlperf-bin-shuffle --numpy-rand-seed=12345 > ./model_compression/model/compress/AGP_Structure/test2/analysis_10_19_2021.log 2>&1 &
+
diff --git a/test/dlrm_s_test.sh b/test/dlrm_s_test.sh
deleted file mode 100755
index e504545..0000000
--- a/test/dlrm_s_test.sh
+++ /dev/null
@@ -1,47 +0,0 @@
-#!/bin/bash
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-#
-#WARNING: must have compiled PyTorch and caffe2
-
-#check if extra argument is passed to the test
-if [[ $# == 1 ]]; then
-    dlrm_extra_option=$1
-else
-    dlrm_extra_option=""
-fi
-#echo $dlrm_extra_option
-
-dlrm_py="python dlrm_s_pytorch.py"
-dlrm_c2="python dlrm_s_caffe2.py"
-
-echo "Running commands ..."
-#run pytorch
-echo $dlrm_py
-$dlrm_py --mini-batch-size=1 --data-size=1 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ppp1
-$dlrm_py --mini-batch-size=2 --data-size=4 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ppp2
-$dlrm_py --mini-batch-size=2 --data-size=5 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ppp3
-$dlrm_py --mini-batch-size=2 --data-size=5 --nepochs=3 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ppp4
-
-#run caffe2
-echo $dlrm_c2
-$dlrm_c2 --mini-batch-size=1 --data-size=1 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ccc1
-$dlrm_c2 --mini-batch-size=2 --data-size=4 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ccc2
-$dlrm_c2 --mini-batch-size=2 --data-size=5 --nepochs=1 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ccc3
-$dlrm_c2 --mini-batch-size=2 --data-size=5 --nepochs=3 --arch-interaction-op=dot --learning-rate=0.1 --debug-mode $dlrm_extra_option > ccc4
-
-echo "Checking results ..."
-#check results
-#WARNING: correct test will have no difference in numeric values
-#(but might have some verbal difference, e.g. due to warnnings)
-#in the output file
-echo "diff test1 (no numeric values in the output = SUCCESS)"
-diff ccc1 ppp1
-echo "diff test2 (no numeric values in the output = SUCCESS)"
-diff ccc2 ppp2
-echo "diff test3 (no numeric values in the output = SUCCESS)"
-diff ccc3 ppp3
-echo "diff test4 (no numeric values in the output = SUCCESS)"
-diff ccc4 ppp4
diff --git a/tools/visualize.py b/tools/visualize.py
old mode 100755
new mode 100644
index f16504c..7a219ee
--- a/tools/visualize.py
+++ b/tools/visualize.py
@@ -1,1030 +1,619 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-#
-#
-# This script performs the visualization of the embedding tables created in
-# DLRM during the training procedure. We use two popular techniques for
-# visualization: umap (https://umap-learn.readthedocs.io/en/latest/) and
-# tsne (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).
-# These links also provide instructions on how to install these packages
-# in different environments.
-#
-# Warning: the size of the data to be visualized depends on the RAM on your machine.
-#
-#
-# Connand line examples:
-#
-# Full analysis of embeddings and data representations for Criteo Kaggle data:
-# $python ./tools/visualize.py --data-set=kaggle --load-model=../dlrm-2020-05-25/criteo.pytorch-e-0-i-110591 
-#         --raw-data-file=../../criteo/input/train.txt --skip-categorical-analysis 
-#         --processed-data-file=../../criteo/input/kaggleAdDisplayChallenge_processed.npz
-#
-#
-# To run just the analysis of categoricala data for Criteo Kaggle data set:
-# $python ./tools/visualize.py --data-set=kaggle --load-model=../dlrm-2020-05-25/criteo.pytorch-e-0-i-110591 \
-#         --raw-data-file=../../criteo/input/train.txt --data-randomize=none --processed-data-file=../../criteo/input/kaggleAdDisplayChallenge_processed.npz \
-#         --skip-embedding --skip-data-plots
-#
-#
-# The following command line arguments are available to the user:
-#
-#    --load-model                   - DLRM model file
-#    --data-set                     - one of ["kaggle", "terabyte"]
-#    --max-ind-range                - max index range used during the traning
-#    --output-dir                   - output directory, if not specified, it will be traeted from the model and datset names
-#    --max-umap-size                - max number of points to visualize using UMAP, default=50000
-#    --use-tsne                     - use T-SNE
-#    --max-tsne-size                - max number of points to visualize using T-SNE, default=1000)
-#    --skip-embedding               - skips analysis of embedding tables
-#    --umap-metric                  - metric for UMAP 
-#    --skip-data-plots              - skips data plots
-#    --skip-categorical-analysis    - skips categorical analysis
-# 
-#    # data file related
-#    --raw-data-file
-#    --processed-data-file
-#    --data-sub-sample-rate
-#    --data-randomize
-#    --memory-map
-#    --mini-batch-size
-#    --num-workers
-#    --test-mini-batch-size
-#    --test-num-workers
-#    --num-batches    
-#    --mlperf-logging
-
-import os
-import sys
-import argparse
-import numpy as np
-import umap
-import hdbscan
-import json
-import torch
-import math
-import matplotlib
-import matplotlib.pyplot as plt
-import collections
-
-from sklearn.metrics import accuracy_score
-from sklearn.metrics import f1_score
-from sklearn.metrics import precision_score
-from sklearn.metrics import recall_score
-
-from sklearn import manifold
-
-import dlrm_data_pytorch as dp
-from dlrm_s_pytorch import DLRM_Net
-
-
-def visualize_embeddings_umap(emb_l, 
-                              output_dir    = "",
-                              max_size      = 500000, 
-                              umap_metric   = "euclidean",
-                              cat_counts    = None,
-                              use_max_count = True):
-
-    for k in range(0, len(emb_l)):
-
-        E = emb_l[k].weight.detach().cpu().numpy()
-        print("umap", E.shape)
-
-        # create histogram of norms
-        bins = 50
-        norms = [np.linalg.norm(E[i], ord=2) for i in range(0,E.shape[0])]
-#        plt.hist(norms, bins = bins)
-#        plt.title("Cat norm hist var. "+str(k))
-        hist, bins = np.histogram(norms, bins=bins)
-        logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))
-
-        plt.figure(figsize=(8,8))
-        plt.title("Categorical norms: " + str(k) + " cardinality " + str(len(cat_counts[k])))
-        plt.hist(norms, bins=logbins)
-        plt.xscale("log")
-#        plt.legend()
-        plt.savefig(output_dir+"/cat-norm-histogram-"+str(k)+".png")
-        plt.close()
-
-        if E.shape[0] < 20:
-            print("Skipping small embedding")
-            continue
-
-        n_vis = min(max_size, E.shape[0])
-        min_cnt = 0
-        
-#        reducer = umap.UMAP(random_state=42, n_neighbors=25, min_dist=0.1)
-        reducer = umap.UMAP(random_state=42, metric=umap_metric)
-        
-        if use_max_count is False or n_vis == E.shape[0]:
-            Y = reducer.fit_transform(E[:n_vis,:])
-        else:
-            
-            # select values with couns > 1
-            done  = False
-            min_cnt = 1
-            while done == False:
-                el_cnt = (cat_counts[k] > min_cnt).sum()
-                if el_cnt <= max_size:
-                    done = True
-                else:
-                    min_cnt = min_cnt+1
-           
-            E1= []
-            for i in range(0, E.shape[0]):
-                if cat_counts[k][i] > min_cnt:
-                    E1.append(E[i,:])
-            
-            print("max_count_len", len(E1), "mincount", min_cnt)
-            Y = reducer.fit_transform(np.array(E1))
-
-            n_vis = len(E1)
-
-        plt.figure(figsize=(8,8))
-        
-        linewidth = 0
-        size      = 1
-        
-        if Y.shape[0] < 2500:
-            linewidth = 1 
-            size      = 5
-
-        if cat_counts is None:
-            plt.scatter(-Y[:,0], -Y[:,1], s=size, marker=".", linewidth=linewidth)
-        else:
-            #print(cat_counts[k])
-            n_disp = min(len(cat_counts[k]), Y.shape[0])
-            cur_max = math.log(max(cat_counts[k]))
-            norm_cat_count = [math.log(cat_counts[k][i]+1)/cur_max for i in range(0, len(cat_counts[k]))]
-            plt.scatter(-Y[0:n_disp,0], -Y[0:n_disp,1], s=size, marker=".", linewidth=linewidth, c=np.array(norm_cat_count)[0:n_disp], cmap="viridis")
-            plt.colorbar()
-            
-        plt.title("UMAP: categorical var. " + str(k) + "  (" + str(n_vis) + " of " + str(E.shape[0]) + ", min count " + str(min_cnt) + ")")
-        plt.savefig(output_dir + "/cat-" + str(k) + "-" + str(n_vis) + "-of-" + str(E.shape[0]) + "-umap.png")
-        plt.close()
-
-
-def visualize_embeddings_tsne(emb_l, 
-                              output_dir = "",
-                              max_size   = 10000):
-
-    for k in range(0, len(emb_l)):
-
-        E = emb_l[k].weight.detach().cpu()    
-        print("tsne", E.shape)
-
-        if E.shape[0] < 20:
-            print("Skipping small embedding")
-            continue
-
-        n_vis = min(max_size, E.shape[0])
-        
-        tsne = manifold.TSNE(init="pca", random_state=0, method="exact")
-    
-        Y = tsne.fit_transform(E[:n_vis,:])
-
-        plt.figure(figsize=(8, 8))
-
-        linewidth = 0
-        if Y.shape[0] < 5000:
-            linewidth = 1 
-
-        plt.scatter(-Y[:,0], -Y[:,1], s=1, marker=".", linewidth=linewidth)
-        
-        plt.title("TSNE: categorical var. " + str(k) + "  (" + str(n_vis) + " of " + str(E.shape[0]) + ")")
-        plt.savefig(output_dir + "/cat-" + str(k) + "-" + str(n_vis) + "-of-" + str(E.shape[0]) + "-tsne.png")
-        plt.close()
-
-
-def analyse_categorical_data(X_cat, n_days=10, output_dir=""):
-
-    # analyse categorical variables
-    n_vec = len(X_cat)
-    n_cat = len(X_cat[0])
-    n_days = n_days
-    
-    print("n_vec", n_vec, "n_cat", n_cat)
-#    for c in train_data.X_cat:
-#        print(n_cat, c)
-
-    all_cat = np.array(X_cat)
-    print("all_cat.shape", all_cat.shape)
-    day_size = all_cat.shape[0]/n_days
-
-    for i in range(0,n_cat):
-        l_d   = []
-        l_s1  = []
-        l_s2  = []
-        l_int = []
-        l_rem = []
-
-        cat = all_cat[:,i]
-        print("cat", i, cat.shape)
-        for d in range(1,n_days):
-            offset = int(d*day_size)
-            #print(offset)
-            cat1 = cat[:offset]
-            cat2 = cat[offset:]
-
-            s1 = set(cat1)
-            s2 = set(cat2)
-
-            intersect = list(s1 & s2) 
-            #print(intersect)
-            l_d.append(d)
-            l_s1.append(len(s1))
-            l_s2.append(len(s2))
-            l_int.append(len(intersect))
-            l_rem.append((len(s1)-len(intersect)))
-
-            print(d, ",", len(s1), ",", len(s2), ",", len(intersect), ",", (len(s1)-len(intersect)))
-
-        print("spit",    l_d)
-        print("before",  l_s1)
-        print("after",   l_s2)
-        print("inters.", l_int)
-        print("removed", l_rem)
-
-        plt.figure(figsize=(8,8))
-        plt.plot(l_d, l_s1,  "g", label="before")
-        plt.plot(l_d, l_s2,  "r", label="after")
-        plt.plot(l_d, l_int, "b", label="intersect")
-        plt.plot(l_d, l_rem, "y", label="removed")
-        plt.title("categorical var. "+str(i))
-        plt.legend()
-        plt.savefig(output_dir+"/cat-"+str(i).zfill(3)+".png")
-        plt.close()
-
-
-def analyse_categorical_counts(X_cat, emb_l=None, output_dir=""):
-
-    # analyse categorical variables
-    n_vec = len(X_cat)
-    n_cat = len(X_cat[0])
-    
-    print("n_vec", n_vec, "n_cat", n_cat)
-#    for c in train_data.X_cat:
-#        print(n_cat, c)
-
-    all_cat = np.array(X_cat)
-    print("all_cat.shape", all_cat.shape)
-
-    all_counts = []
-
-    for i in range(0,n_cat):
-        
-        cat = all_cat[:,i]
-        if emb_l is None:
-            s      = set(cat)
-            counts = np.zeros((len(s)))
-            print("cat", i, cat.shape, len(s))
-        else:
-            s = emb_l[i].weight.detach().cpu().shape[0]
-            counts = np.zeros((s))
-            print("cat", i, cat.shape, s)
-
-        for d in range(0,n_vec):
-            cv = int(cat[d])
-            counts[cv] = counts[cv]+1
-
-        all_counts.append(counts)
-
-        if emb_l is None:
-            plt.figure(figsize=(8,8))
-            plt.plot(counts)
-            plt.title("Categorical var "+str(i) + " cardinality " + str(len(counts)))
-            #        plt.legend()
-        else:
-            E = emb_l[i].weight.detach().cpu().numpy()
-            norms = [np.linalg.norm(E[i], ord=2) for i in range(0,E.shape[0])]
-
-            fig, (ax0, ax1) = plt.subplots(2, 1)
-            fig.suptitle("Categorical variable: " + str(i)+" cardinality "+str(len(counts)))
-
-            ax0.plot(counts)
-            ax0.set_yscale("log")
-            ax0.set_title("Counts", fontsize=10)
-    
-            ax1.plot(norms)
-            ax1.set_title("Norms", fontsize=10)
-
-        plt.savefig(output_dir+"/cat_counts-"+str(i).zfill(3)+".png")
-        plt.close()
-    
-    return all_counts
-    
-
-def dlrm_output_wrap(dlrm, X, lS_o, lS_i, T):
-
-    all_feat_vec = []
-    all_cat_vec  = []
-    x_vec        = None
-    t_out        = None
-    c_out        = None
-    z_out        = []
-    p_out        = None
-
-    z_size = len(dlrm.top_l)
-
-    x = dlrm.apply_mlp(X, dlrm.bot_l)
-    # debug prints
-    #print("intermediate")
-    #print(x[0].detach().cpu().numpy())
-    x_vec = x[0].detach().cpu().numpy()
-    all_feat_vec.append(x_vec)
-#    all_X.append(x[0].detach().cpu().numpy())
-
-    # process sparse features(using embeddings), resulting in a list of row vectors
-    ly = dlrm.apply_emb(lS_o, lS_i, dlrm.emb_l)
-
-    for e in ly:
-        #print(e.detach().cpu().numpy())
-        all_feat_vec.append(e[0].detach().cpu().numpy())
-        all_cat_vec.append(e[0].detach().cpu().numpy())
-
-    all_feat_vec= np.concatenate(all_feat_vec, axis=0)
-    all_cat_vec= np.concatenate(all_cat_vec, axis=0)
-
-#    all_features.append(all_feat_vec)
-#    all_cat.append(all_cat_vec)
-    t_out = int(T.detach().cpu().numpy()[0,0])
-#    all_T.append(int(T.detach().cpu().numpy()[0,0]))
-
-    z = dlrm.interact_features(x, ly)
-    # print(z.detach().cpu().numpy())
-#    z_out = z.detach().cpu().numpy().flatten()
-    z_out.append(z.detach().cpu().numpy().flatten())
-#    all_z[0].append(z.detach().cpu().numpy().flatten())
-
-        # obtain probability of a click (using top mlp)
-#        print(dlrm.top_l)
-#        p = dlrm.apply_mlp(z, dlrm.top_l)
-
-    for i in range(0, z_size):
-        z = dlrm.top_l[i](z)
-
-#        if i < z_size-1:
-#            curr_z = z.detach().cpu().numpy().flatten()
-        z_out.append(z.detach().cpu().numpy().flatten())
-#            all_z[i+1].append(curr_z)
-#            print("z append", i)
-            
-#        print("z",i, z.detach().cpu().numpy().flatten().shape)
-
-    p = z
-
-    # clamp output if needed
-    if 0.0 < dlrm.loss_threshold and dlrm.loss_threshold < 1.0:
-        z = torch.clamp(p, min=dlrm.loss_threshold, max=(1.0 - dlrm.loss_threshold))
-    else:
-        z = p
-
-    class_thresh = 0.0 #-0.25
-    zp = z.detach().cpu().numpy()[0,0]+ class_thresh
-    
-    p_out = int(zp+0.5)
-    if p_out > 1:
-        p_out = 1
-    if p_out < 0:
-        p_out = 0
-
-#    all_pred.append(int(z.detach().cpu().numpy()[0,0]+0.5))
-
-    #print(int(z.detach().cpu().numpy()[0,0]+0.5))
-    if int(p_out) == t_out:
-        c_out = 0
-    else:
-        c_out = 1
-
-    return all_feat_vec, x_vec, all_cat_vec, t_out, c_out, z_out, p_out
-
-
-def create_umap_data(dlrm, data_ld, max_size=50000, offset=0,  info=""):
-    
-    all_features = []
-    all_X        = []
-    all_cat      = []
-    all_T        = []
-    all_c        = []
-    all_z        = []
-    all_pred     = []
-    
-    z_size = len(dlrm.top_l)
-    print("z_size", z_size)
-    for i in range(0, z_size):
-        all_z.append([])
-    
-    for j, (X, lS_o, lS_i, T) in enumerate(data_ld):
-
-        if j < offset:
-            continue
-        
-        if j >= max_size+offset:
-            break
-        
-        af, x, cat, t, c, z, p = dlrm_output_wrap(dlrm, X, lS_o, lS_i, T)
-       
-        all_features.append(af)
-        all_X.append(x)
-        all_cat.append(cat)
-        all_T.append(t)
-        all_c.append(c)
-        all_pred.append(p)
-        
-        for i in range(0, z_size):
-            all_z[i].append(z[i])
-
-#    # calculate classifier metrics 
-    ac = accuracy_score(all_T, all_pred)
-    f1 = f1_score(all_T, all_pred)
-    ps = precision_score(all_T, all_pred)
-    rc = recall_score(all_T, all_pred)
-
-    print(info, "accuracy", ac, "f1", f1, "precision", ps, "recall", rc)
-
-    return all_features, all_X, all_cat, all_T, all_z, all_c, all_pred
-
-
-def plot_all_data_3(umap_Y,
-                    umap_T,
-                    train_Y          = None, 
-                    train_T          = None, 
-                    test_Y           = None, 
-                    test_T           = None, 
-                    total_train_size = "", 
-                    total_test_size  = "", 
-                    info             = "",
-                    output_dir       = "",
-                    orig_space_dim   = 0):
-    
-    size = 1
-    colors = ["red","green"]
-
-    fig, (ax0, ax1, ax2) = plt.subplots(1, 3)
-    fig.suptitle("UMAP: " + info + " space dim "+str(orig_space_dim))
-
-    ax0.scatter(umap_Y[:,0], umap_Y[:,1], s=size, c=umap_T, cmap=matplotlib.colors.ListedColormap(colors), marker=".", linewidth=0)
-    ax0.set_title("UMAP ("+str(len(umap_T))+" of "+ total_train_size+")", fontsize=7)
-    
-    if train_Y is not None and train_T is not None:
-        ax1.scatter(train_Y[:,0], train_Y[:,1], s=size, c=train_T, cmap=matplotlib.colors.ListedColormap(colors), marker=".", linewidth=0)
-        ax1.set_title("Train ("+str(len(train_T))+" of "+ total_train_size+")", fontsize=7)
-
-    if test_Y is not None and test_T is not None:
-        ax2.scatter(test_Y[:,0], test_Y[:,1], s=size, c=test_T, cmap=matplotlib.colors.ListedColormap(colors), marker=".", linewidth=0)
-        ax2.set_title("Test ("+str(len(test_T))+" of "+ total_test_size+")", fontsize=7)
-
-    plt.savefig(output_dir+"/"+info+"-umap.png")
-    plt.close()
-
-
-def plot_one_class_3(umap_Y,
-                     umap_T,
-                     train_Y,
-                     train_T,
-                     test_Y, 
-                     test_T, 
-                     target           = 0, 
-                     col              = "red", 
-                     total_train_size = "", 
-                     total_test_size  = "", 
-                     info             = "",
-                     output_dir       = "",
-                     orig_space_dim   = 0):
-    
-    size = 1
-    
-    fig, (ax0, ax1, ax2) = plt.subplots(1, 3)
-    fig.suptitle("UMAP: "+ info + " space dim "+str(orig_space_dim))
-
-    ind_l_umap     = [i for i,x in enumerate(umap_T) if x == target]
-    Y_umap_l       = np.array([umap_Y[i,:] for i in ind_l_umap])
-
-    ax0.scatter(Y_umap_l[:,0], Y_umap_l[:,1], s=size, c=col, marker=".", linewidth=0)
-    ax0.set_title("UMAP, ("+str(len(umap_T))+" of "+ total_train_size+")", fontsize=7)
-    
-    if train_Y is not None and train_T is not None:
-        ind_l_test = [i for i,x in enumerate(train_T) if x == target]
-        Y_test_l   = np.array([train_Y[i,:] for i in ind_l_test])
-        
-        ax1.scatter(Y_test_l[:,0], Y_test_l[:,1], s=size, c=col, marker=".", linewidth=0)
-        ax1.set_title("Train, ("+str(len(train_T))+" of "+ total_train_size+")", fontsize=7)
-
-    if test_Y is not None and test_T is not None:
-        ind_l_test = [i for i,x in enumerate(test_T) if x == target]
-        Y_test_l   = np.array([test_Y[i,:] for i in ind_l_test])
-
-        ax2.scatter(Y_test_l[:,0], Y_test_l[:,1], s=size, c=col, marker=".", linewidth=0)
-        ax2.set_title("Test, ("+str(len(test_T))+" of "+ total_test_size+")", fontsize=7)
-
-    plt.savefig(output_dir+"/"+info+"-umap.png")
-    plt.close()
-
-
-def visualize_umap_data(umap_Y,
-                        umap_T,
-                        umap_C,
-                        umap_P,
-                        train_Y, 
-                        train_T, 
-                        train_C,
-                        train_P,
-                        test_Y           = None,
-                        test_T           = None, 
-                        test_C           = None,
-                        test_P           = None,
-                        total_train_size = "", 
-                        total_test_size  = "",  
-                        info             = "",
-                        output_dir       = "",
-                        orig_space_dim   = 0):
-
-    # all classes
-    plot_all_data_3(umap_Y           = umap_Y,
-                    umap_T           = umap_T,
-                    train_Y          = train_Y,
-                    train_T          = train_T, 
-                    test_Y           = test_Y, 
-                    test_T           = test_T, 
-                    total_train_size = total_train_size,
-                    total_test_size  = total_test_size,
-                    info             = info,
-                    output_dir       = output_dir,
-                    orig_space_dim   = orig_space_dim)
-
-    # all predictions
-    plot_all_data_3(umap_Y           = umap_Y,
-                    umap_T           = umap_P,
-                    train_Y          = train_Y,
-                    train_T          = train_P, 
-                    test_Y           = test_Y, 
-                    test_T           = test_P, 
-                    total_train_size = total_train_size,
-                    total_test_size  = total_test_size,
-                    info             = info+", all-predictions",
-                    output_dir       = output_dir,
-                    orig_space_dim   = orig_space_dim)
-
-    
-    # class 0
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_T,
-                     train_Y          = train_Y,
-                     train_T          = train_T,
-                     test_Y           = test_Y, 
-                     test_T           = test_T, 
-                     target           = 0, 
-                     col              = "red", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info+" class " + str(0),
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-    # class 1
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_T,
-                     train_Y          = train_Y,
-                     train_T          = train_T,
-                     test_Y           = test_Y, 
-                     test_T           = test_T, 
-                     target           = 1, 
-                     col              = "green", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info + " class " + str(1),
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-    # correct classification
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_C,
-                     train_Y          = train_Y,
-                     train_T          = train_C,
-                     test_Y           = test_Y, 
-                     test_T           = test_C, 
-                     target           = 0, 
-                     col              = "green", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info + " correct ",
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-    # errors
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_C,
-                     train_Y          = train_Y,
-                     train_T          = train_C,
-                     test_Y           = test_Y, 
-                     test_T           = test_C, 
-                     target           = 1, 
-                     col              = "red", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info + " errors ",
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-    # prediction 0
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_P,
-                     train_Y          = train_Y,
-                     train_T          = train_P,
-                     test_Y           = test_Y, 
-                     test_T           = test_P, 
-                     target           = 0, 
-                     col              = "red", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info + " predict-0 ",
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-    # prediction 1
-    plot_one_class_3(umap_Y           = umap_Y,
-                     umap_T           = umap_P,
-                     train_Y          = train_Y,
-                     train_T          = train_P,
-                     test_Y           = test_Y, 
-                     test_T           = test_P, 
-                     target           = 1, 
-                     col              = "green", 
-                     total_train_size = total_train_size, 
-                     total_test_size  = total_test_size, 
-                     info             = info + " predict-1 ",
-                     output_dir       = output_dir,
-                     orig_space_dim   = orig_space_dim)
-
-def hdbscan_clustering(umap_data, train_data, test_data, info="", output_dir=""):
-
-    clusterer       = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500, prediction_data=True)
-    umap_labels     = clusterer.fit_predict(umap_data)
-    train_labels, _ = hdbscan.approximate_predict(clusterer, train_data)
-    test_labels,  _ = hdbscan.approximate_predict(clusterer, test_data)
-
-    fig, ((ax00, ax01, ax02), (ax10, ax11, ax12)) = plt.subplots(2, 3)
-    fig.suptitle("HDBSCAN clastering: "+ info )
-
-    # plot umap data
-    umap_clustered = (umap_labels >= 0)
-    umap_coll = collections.Counter(umap_clustered)
-    print("umap_clustered", umap_coll)
-#    print("umap_data", umap_data.shape)
-#    print("~umap_clustered", umap_clustered.count(False), ~umap_clustered)
-    ax00.scatter(umap_data[~umap_clustered, 0],
-                 umap_data[~umap_clustered, 1],
-                 c=(0.5, 0.5, 0.5),
-                 s=0.1,
-                 alpha=0.5)
-    ax00.set_title("UMAP Outliers " + str(umap_coll[False]), fontsize=7)
-    ax10.scatter(umap_data[umap_clustered, 0],
-                 umap_data[umap_clustered, 1],
-                 c=umap_labels[umap_clustered],
-                 s=0.1,
-                 cmap="Spectral")
-    ax10.set_title("UMAP Inliers " + str(umap_coll[True]), fontsize=7)
-    
-    # plot train data
-    train_clustered = (train_labels >= 0)
-    train_coll = collections.Counter(train_clustered)
-    ax01.scatter(train_data[~train_clustered, 0],
-                 train_data[~train_clustered, 1],
-                 c=(0.5, 0.5, 0.5),
-                 s=0.1,
-                 alpha=0.5)
-    ax01.set_title("Train Outliers " + str(train_coll[False]), fontsize=7)
-    ax11.scatter(train_data[train_clustered, 0],
-                 train_data[train_clustered, 1],
-                 c=train_labels[train_clustered],
-                 s=0.1,
-                 cmap="Spectral")
-    ax11.set_title("Train Inliers " + str(train_coll[True]), fontsize=7)
-    
-    # plot test data
-    test_clustered = (test_labels >= 0)
-    test_coll = collections.Counter(test_clustered)
-    ax02.scatter(test_data[~test_clustered, 0],
-                 test_data[~test_clustered, 1],
-                 c=(0.5, 0.5, 0.5),
-                 s=0.1,
-                 alpha=0.5)
-    ax02.set_title("Tets Outliers " + str(test_coll[False]), fontsize=7)
-    ax12.scatter(test_data[test_clustered, 0],
-                 test_data[test_clustered, 1],
-                 c=test_labels[test_clustered],
-                 s=0.1,
-                 cmap="Spectral")
-    ax12.set_title("Test Inliers " + str(test_coll[True]), fontsize=7)
-    
-    plt.savefig(output_dir+"/"+info+"-hdbscan.png")
-    plt.close()
-
-
-def visualize_all_data_umap(dlrm, 
-                            train_ld, 
-                            test_ld       = None, 
-                            max_umap_size = 50000,
-                            output_dir    = "",
-                            umap_metric   = "euclidean"):
-
-    data_ratio = 1
-    
-    print("creating umap data")
-    umap_train_feat, umap_train_X, umap_train_cat, umap_train_T, umap_train_z, umap_train_c, umap_train_p = create_umap_data(dlrm=dlrm, data_ld=train_ld, max_size=max_umap_size, offset=0, info="umap")
-    
-    # transform train and test data
-    train_feat, train_X, train_cat, train_T, train_z, train_c, train_p = create_umap_data(dlrm=dlrm, data_ld=train_ld, max_size=max_umap_size*data_ratio, offset=max_umap_size, info="train")
-    test_feat,  test_X,  test_cat,  test_T,  test_z,  test_c,  test_p  = create_umap_data(dlrm=dlrm, data_ld=test_ld,  max_size=max_umap_size*data_ratio, offset=0,             info="test")
-
-    print("umap_train_feat", np.array(umap_train_feat).shape)
-    reducer_all_feat = umap.UMAP(random_state=42, metric=umap_metric)
-    umap_feat_Y = reducer_all_feat.fit_transform(umap_train_feat)
-
-    train_feat_Y = reducer_all_feat.transform(train_feat)
-    test_feat_Y  = reducer_all_feat.transform(test_feat)
-    
-    visualize_umap_data(umap_Y           = umap_feat_Y,
-                        umap_T           = umap_train_T,
-                        umap_C           = umap_train_c,
-                        umap_P           = umap_train_p,
-                        train_Y          = train_feat_Y, 
-                        train_T          = train_T, 
-                        train_C          = train_c,
-                        train_P          = train_p,
-                        test_Y           = test_feat_Y,
-                        test_T           = test_T, 
-                        test_C           = test_c,
-                        test_P           = test_p,
-                        total_train_size = str(len(train_ld)), 
-                        total_test_size  = str(len(test_ld)), 
-                        info             = "all-features",
-                        output_dir       = output_dir,
-                        orig_space_dim   = np.array(umap_train_feat).shape[1])
-
-    hdbscan_clustering(umap_data  = umap_feat_Y, 
-                       train_data = train_feat_Y, 
-                       test_data  = test_feat_Y, 
-                       info       = "umap-all-features", 
-                       output_dir = output_dir)
-
-#    hdbscan_clustering(umap_data  = np.array(umap_train_feat), 
-#                       train_data = np.array(train_feat), 
-#                       test_data  = np.array(test_feat), 
-#                       info       = "all-features", 
-#                       output_dir = output_dir)
-
-    print("umap_train_X", np.array(umap_train_X).shape)
-    reducer_X = umap.UMAP(random_state=42, metric=umap_metric)
-    umap_X_Y = reducer_X.fit_transform(umap_train_X)
-
-    train_X_Y = reducer_X.transform(train_X)
-    test_X_Y  = reducer_X.transform(test_X)
-
-    visualize_umap_data(umap_Y           = umap_X_Y,
-                        umap_T           = umap_train_T,
-                        umap_C           = umap_train_c,
-                        umap_P           = umap_train_p,
-                        train_Y          = train_X_Y, 
-                        train_T          = train_T, 
-                        train_C          = train_c,
-                        train_P          = train_p,
-                        test_Y           = test_X_Y,
-                        test_T           = test_T, 
-                        test_C           = test_c,
-                        test_P           = test_p,
-                        total_train_size = str(len(train_ld)), 
-                        total_test_size  = str(len(test_ld)), 
-                        info             = "cont-features",
-                        output_dir       = output_dir,
-                        orig_space_dim   = np.array(umap_train_X).shape[1])
-
-    print("umap_train_cat", np.array(umap_train_cat).shape)
-    reducer_cat = umap.UMAP(random_state=42, metric=umap_metric)
-    umap_cat_Y = reducer_cat.fit_transform(umap_train_cat)
-
-    train_cat_Y = reducer_cat.transform(train_cat)
-    test_cat_Y  = reducer_cat.transform(test_cat)
-
-    visualize_umap_data(umap_Y           = umap_cat_Y,
-                        umap_T           = umap_train_T,
-                        umap_C           = umap_train_c,
-                        umap_P           = umap_train_p,
-                        train_Y          = train_cat_Y, 
-                        train_T          = train_T, 
-                        train_C          = train_c,
-                        train_P          = train_p,
-                        test_Y           = test_cat_Y,
-                        test_T           = test_T, 
-                        test_C           = test_c,
-                        test_P           = test_p,
-                        total_train_size = str(len(train_ld)), 
-                        total_test_size  = str(len(test_ld)), 
-                        info             = "cat-features",
-                        output_dir       = output_dir,
-                        orig_space_dim   = np.array(umap_train_cat).shape[1])
-
-    # UMAP for z data
-    for i in range(0,len(umap_train_z)):
-        print("z", i, np.array(umap_train_z[i]).shape)
-        reducer_z = umap.UMAP(random_state=42, metric=umap_metric)
-        umap_z_Y = reducer_z.fit_transform(umap_train_z[i])
-
-        train_z_Y = reducer_z.transform(train_z[i])
-        test_z_Y  = reducer_z.transform(test_z[i])
-
-        visualize_umap_data(umap_Y           = umap_z_Y,
-                            umap_T           = umap_train_T,
-                            umap_C           = umap_train_c,
-                            umap_P           = umap_train_p,
-                            train_Y          = train_z_Y, 
-                            train_T          = train_T, 
-                            train_C          = train_c,
-                            train_P          = train_p,
-                            test_Y           = test_z_Y,
-                            test_T           = test_T, 
-                            test_C           = test_c,
-                            test_P           = test_p,
-                            total_train_size = str(len(train_ld)), 
-                            total_test_size  = str(len(test_ld)), 
-                            info             = "z-features-"+str(i),
-                            output_dir       = output_dir,
-                            orig_space_dim   = np.array(umap_train_z[i]).shape[1])
-
-
-def analyze_model_data(output_dir,
-                       dlrm,
-                       train_ld,
-                       test_ld,
-                       train_data,
-                       skip_embedding            = False,
-                       use_tsne                  = False,
-                       max_umap_size             = 50000,
-                       max_tsne_size             = 10000,
-                       skip_categorical_analysis = False,
-                       skip_data_plots           = False,
-                       umap_metric               = "euclidean"):
-
-    if not os.path.exists(output_dir):
-        os.makedirs(output_dir)
-
-    if skip_embedding is False:
-
-        cat_counts = None
-        
-        cat_counts = analyse_categorical_counts(X_cat=train_data.X_cat, emb_l=dlrm.emb_l, output_dir=output_dir)
-
-        visualize_embeddings_umap(emb_l       = dlrm.emb_l,
-                                  output_dir  = output_dir,
-                                  max_size    = max_umap_size,
-                                  umap_metric = umap_metric,
-                                  cat_counts  = cat_counts)
-
-        if use_tsne is True:
-            visualize_embeddings_tsne(emb_l      = dlrm.emb_l,
-                                      output_dir = output_dir,
-                                      max_size   = max_tsne_size)
-
-    # data visualization and analysis
-    if skip_data_plots is False:
-        visualize_all_data_umap(dlrm=dlrm, train_ld=train_ld, test_ld=test_ld, max_umap_size=max_umap_size, output_dir=output_dir, umap_metric=umap_metric)
-
-    # analyse categorical variables
-    if skip_categorical_analysis is False and args.data_randomize == "none":
-        analyse_categorical_data(X_cat=train_data.X_cat, n_days=10, output_dir=output_dir)
-
-
-
-if __name__ == "__main__":
-
-    output_dir = ""
-    
-    ### parse arguments ###
-    parser = argparse.ArgumentParser(
-        description="Exploratory DLRM analysis"
-    )
-
-    parser.add_argument("--load-model", type=str, default="")
-    parser.add_argument("--data-set", choices=["kaggle", "terabyte"], help="dataset")
-#    parser.add_argument("--dataset-path", required=True, help="path to the dataset")
-    parser.add_argument("--max-ind-range", type=int, default=-1)
-#    parser.add_argument("--mlperf-bin-loader", action="store_true", default=False)
-    parser.add_argument("--output-dir", type=str, default="")
-    parser.add_argument("--skip-embedding", action="store_true", default=False)
-    parser.add_argument("--umap-metric", type=str, default="euclidean")
-    parser.add_argument("--skip-data-plots", action="store_true", default=False)
-    parser.add_argument("--skip-categorical-analysis", action="store_true", default=False)
-    
-    # umap relatet
-    parser.add_argument("--max-umap-size", type=int, default=50000)
-    # tsne related
-    parser.add_argument("--use-tsne", action="store_true", default=False)
-    parser.add_argument("--max-tsne-size", type=int, default=1000)
-    # data file related
-    parser.add_argument("--raw-data-file", type=str, default="")
-    parser.add_argument("--processed-data-file", type=str, default="")
-    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
-    parser.add_argument("--data-randomize", type=str, default="total")  # none, total or day or none
-    parser.add_argument("--memory-map", action="store_true", default=False)
-    parser.add_argument("--mini-batch-size", type=int, default=1)
-    parser.add_argument("--num-workers", type=int, default=0)
-    parser.add_argument("--test-mini-batch-size", type=int, default=1)
-    parser.add_argument("--test-num-workers", type=int, default=0)
-    parser.add_argument("--num-batches", type=int, default=0)    
-    # mlperf logging (disables other output and stops early)
-    parser.add_argument("--mlperf-logging", action="store_true", default=False)
-
-    args = parser.parse_args()
-
-    print("command line args: ", json.dumps(vars(args)))
-
-    if output_dir == "":
-        output_dir = args.data_set+"-"+os.path.split(args.load_model)[-1]+"-vis_all"
-    print("output_dir:", output_dir)
-    
-    if args.data_set == "kaggle":
-        # 1. Criteo Kaggle Display Advertisement Challenge Dataset (see ./bench/dlrm_s_criteo_kaggle.sh)
-        m_spa=16
-        ln_emb=np.array([1460,583,10131227,2202608,305,24,12517,633,3,93145,5683,8351593,3194,27,14992,5461306,10,5652,2173,4,7046547,18,15,286181,105,142572])
-        ln_bot=np.array([13,512,256,64,16])
-        ln_top=np.array([367,512,256,1])
-        
-    elif args.dataset == "terabyte":
-
-        if args.max_ind_range == 10000000:
-            # 2. Criteo Terabyte (see ./bench/dlrm_s_criteo_terabyte.sh [--sub-sample=0.875] --max-in-range=10000000)
-            m_spa=64
-            ln_emb=np.array([9980333,36084,17217,7378,20134,3,7112,1442,61, 9758201,1333352,313829,10,2208,11156,122,4,970,14, 9994222, 7267859, 9946608,415421,12420,101, 36])
-            ln_bot=np.array([13,512,256,64])
-            ln_top=np.array([415,512,512,256,1])
-        elif args.max_ind_range == 40000000:
-            # 3. Criteo Terabyte MLPerf training (see ./bench/run_and_time.sh --max-in-range=40000000)
-            m_spa=128
-            ln_emb=np.array([39884406,39043,17289,7420,20263,3,7120,1543,63,38532951,2953546,403346,10,2208,11938,155,4,976,14,39979771,25641295,39664984,585935,12972,108,36])
-            ln_bot=np.array([13,512,256,128])
-            ln_top=np.array([479,1024,1024,512,256,1])
-        else:
-            raise ValueError("only --max-in-range 10M or 40M is supported")
-    else:
-        raise ValueError("only kaggle|terabyte dataset options are supported")
-
-    # check input parameters
-    if args.data_randomize != "none" and args.skip_categorical_analysis is not True:
-        print("Incorrect option for categoricat analysis, use:  --data-randomize=none")
-        sys.exit(-1)
-
-    dlrm = DLRM_Net(
-            m_spa,
-            ln_emb,
-            ln_bot,
-            ln_top,
-            arch_interaction_op="dot",
-            arch_interaction_itself=False,
-            sigmoid_bot=-1,
-            sigmoid_top=ln_top.size - 2,
-            sync_dense_params=True,
-            loss_threshold=0.0,
-            ndevices=-1,
-            qr_flag=False,
-            qr_operation=None,
-            qr_collisions=None,
-            qr_threshold=None,
-            md_flag=False,
-            md_threshold=None,
-        )
-
-    # Load model is specified
-    if not (args.load_model == ""):
-        print("Loading saved model {}".format(args.load_model))
-
-        ld_model = torch.load(args.load_model, map_location=torch.device("cpu"))
-        dlrm.load_state_dict(ld_model["state_dict"])
-
-        print("Model loaded", args.load_model)
-        #print(dlrm)
-
-    z_size = len(dlrm.top_l)
-    for i in range(0, z_size):
-         print("z", i, dlrm.top_l[i])
-
-    # load data
-    train_data = None
-    test_data  = None
-    
-    if args.raw_data_file is not "" or args.processed_data_file is not "":
-        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
-
-    analyze_model_data(output_dir                = output_dir,
-                       dlrm                      = dlrm,
-                       train_ld                  = train_ld,
-                       test_ld                   = test_ld,
-                       train_data                = train_data,
-                       skip_embedding            = args.skip_embedding,
-                       use_tsne                  = args.use_tsne,
-                       max_umap_size             = args.max_umap_size,
-                       max_tsne_size             = args.max_tsne_size,
-                       skip_categorical_analysis = args.skip_categorical_analysis,
-                       skip_data_plots           = args.skip_data_plots,
-                       umap_metric               = args.umap_metric)
-
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+#
+#
+# This script performs the visualization of the embedding tables created in
+# DLRM during the training procedure. We use two popular techniques for
+# visualization: umap (https://umap-learn.readthedocs.io/en/latest/) and
+# tsne (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).
+# These links also provide instructions on how to install these packages
+# in different environments.
+#
+# Warning: the size of the data to be visualized depends on the RAM on your machine.
+#
+#
+# A sample run of the code, with a kaggle model is shown below
+# $python ./tools/visualize.py --dataset=kaggle --load-model=./input/dlrm_kaggle.pytorch
+#
+#
+# The following command line arguments are available to the user:
+#
+#    --load-model      - DLRM model file
+#    --data-set        - one of ["kaggle", "terabyte"]
+#    --max-ind-range   - max index range used during the traning
+#    --output-dir      - output directory where output plots will be written, default will be on of these: ["kaggle_vis", "terabyte_vis"]
+#    --max-umap-size   - max number of points to visualize using UMAP, default=50000
+#    --use-tsne        - use T-SNE
+#    --max-tsne-size   - max number of points to visualize using T-SNE, default=1000)
+#
+
+import os
+import argparse
+import numpy as np
+import umap
+import json
+import torch
+import matplotlib
+import matplotlib.pyplot as plt
+
+from sklearn.metrics import accuracy_score
+from sklearn.metrics import f1_score
+from sklearn.metrics import precision_score
+from sklearn.metrics import recall_score
+
+from sklearn import manifold
+
+import dlrm_data_pytorch as dp
+from dlrm_s_pytorch import DLRM_Net
+
+
+def visualize_embeddings_umap(emb_l,
+                              output_dir    = "",
+                              max_size = 500000):
+
+    for k in range(0, len(emb_l)):
+
+        E = emb_l[k].weight.detach().cpu()
+        print("umap", E.shape)
+
+        if E.shape[0] < 20:
+            print("Skipping small embedding")
+            continue
+
+        n_vis = min(max_size, E.shape[0])
+
+        # reducer = umap.UMAP(random_state=42, n_neighbors=25, min_dist=0.1)
+        reducer = umap.UMAP(random_state=42)
+        Y = reducer.fit_transform(E[:n_vis,:])
+
+        plt.figure(figsize=(8,8))
+
+        linewidth = 0
+        size      = 1
+
+        if Y.shape[0] < 2500:
+            linewidth = 1
+            size      = 5
+
+        plt.scatter(-Y[:,0], -Y[:,1], s=size, marker='.', linewidth=linewidth)
+
+        plt.title("UMAP: categorical var. "+str(k)+"  ("+str(n_vis)+" of "+str(E.shape[0])+")")
+        plt.savefig(output_dir+"/cat-"+str(k)+"-"+str(n_vis)+"-of-"+str(E.shape[0])+"-umap.png")
+        plt.close()
+
+
+def visualize_embeddings_tsne(emb_l,
+                              output_dir = "",
+                              max_size   = 10000):
+
+    for k in range(0, len(emb_l)):
+
+        E = emb_l[k].weight.detach().cpu()
+        print("tsne", E.shape)
+
+        if E.shape[0] < 20:
+            print("Skipping small embedding")
+            continue
+
+        n_vis = min(max_size, E.shape[0])
+
+        tsne = manifold.TSNE(init='pca', random_state=0, method='exact')
+
+        Y = tsne.fit_transform(E[:n_vis,:])
+
+        plt.figure(figsize=(8,8))
+
+        linewidth = 0
+        if Y.shape[0] < 5000:
+            linewidth = 1
+
+        plt.scatter(-Y[:,0], -Y[:,1], s=1, marker='.', linewidth=linewidth)
+
+        plt.title("TSNE: categorical var. "+str(k)+"  ("+str(n_vis)+" of "+str(E.shape[0])+")")
+        plt.savefig(output_dir+"/cat-"+str(k)+"-"+str(n_vis)+"-of-"+str(E.shape[0])+"-tsne.png")
+        plt.close()
+
+
+def create_vis_data(dlrm, data_ld, max_size=50000, info=''):
+
+    all_features = []
+    all_X        = []
+    all_cat      = []
+    all_T        = []
+    all_c        = []
+    all_z        = []
+    all_pred     = []
+
+    z_size = len(dlrm.top_l)
+    print('z_size', z_size)
+    for i in range(0, z_size):
+        all_z.append([])
+
+    for j, (X, lS_o, lS_i, T) in enumerate(data_ld):
+
+        if j >= max_size:
+            break
+
+        all_feat_vec = []
+        all_cat_vec  = []
+
+        x = dlrm.apply_mlp(X, dlrm.bot_l)
+        # debug prints
+        #print("intermediate")
+        #print(x[0].detach().cpu().numpy())
+        all_feat_vec.append(x[0].detach().cpu().numpy())
+        all_X.append(x[0].detach().cpu().numpy())
+
+        # process sparse features(using embeddings), resulting in a list of row vectors
+        ly = dlrm.apply_emb(lS_o, lS_i, dlrm.emb_l)
+
+        for e in ly:
+            #print(e.detach().cpu().numpy())
+            all_feat_vec.append(e[0].detach().cpu().numpy())
+            all_cat_vec.append(e[0].detach().cpu().numpy())
+
+        all_feat_vec= np.concatenate(all_feat_vec, axis=0)
+        all_cat_vec= np.concatenate(all_cat_vec, axis=0)
+
+        all_features.append(all_feat_vec)
+        all_cat.append(all_cat_vec)
+        all_T.append(int(T.detach().cpu().numpy()[0,0]))
+
+        z = dlrm.interact_features(x, ly)
+        # print(z.detach().cpu().numpy())
+        all_z[0].append(z.detach().cpu().numpy().flatten())
+
+        # obtain probability of a click (using top mlp)
+        # print(dlrm.top_l)
+        # p = dlrm.apply_mlp(z, dlrm.top_l)
+
+        for i in range(0, z_size):
+            z = dlrm.top_l[i](z)
+
+            if i < z_size-1:
+                curr_z = z.detach().cpu().numpy().flatten()
+                all_z[i+1].append(curr_z)
+
+            # print('z',i, z.detach().cpu().numpy().flatten().shape)
+
+        p = z
+
+        # clamp output if needed
+        if 0.0 < dlrm.loss_threshold and dlrm.loss_threshold < 1.0:
+            z = torch.clamp(p, min=dlrm.loss_threshold, max=(1.0 - dlrm.loss_threshold))
+        else:
+            z = p
+
+        all_pred.append(int(z.detach().cpu().numpy()[0,0]+0.5))
+
+        #print(int(z.detach().cpu().numpy()[0,0]+0.5))
+        if int(z.detach().cpu().numpy()[0,0]+0.5) == int(T.detach().cpu().numpy()[0,0]):
+            all_c.append(0)
+        else:
+            all_c.append(1)
+
+    # calculate classifier metrics
+    ac = accuracy_score(all_T, all_pred)
+    f1 = f1_score(all_T, all_pred)
+    ps = precision_score(all_T, all_pred)
+    rc = recall_score(all_T, all_pred)
+
+    print(info, 'accuracy', ac, 'f1', f1, 'precision', ps, 'recall', rc)
+
+    return all_features, all_X, all_cat, all_T, all_z, all_c
+
+def plot_all_data(Y_train_data,
+                  train_labels,
+                  Y_test_data,
+                  test_labels,
+                  total_train_size = '',
+                  total_test_size  = '',
+                  info             = '',
+                  output_dir       = ''):
+
+    size = 1
+    colors = ['red','green']
+
+    fig, (ax1, ax2) = plt.subplots(1, 2)
+    fig.suptitle('UMAP: ' + info)
+
+    ax1.scatter(-Y_train_data[:,0], -Y_train_data[:,1], s=size, c=train_labels, cmap=matplotlib.colors.ListedColormap(colors), marker='.', linewidth=0)
+    ax1.title.set_text('Train ('+str(len(train_labels))+' of '+ total_train_size+')')
+    if test_data is not None and test_labels is not None:
+        ax2.scatter(-Y_test_data[:,0], -Y_test_data[:,1], s=size, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors), marker='.', linewidth=0)
+        ax2.title.set_text('Test ('+str(len(test_labels))+' of '+ total_test_size+')')
+
+    plt.savefig(output_dir+"/"+info+'-umap.png')
+    plt.close()
+
+
+def plot_one_class(Y_train_data,
+                   train_labels,
+                   Y_test_data,
+                   test_labels,
+                   label            = 0,
+                   col              = 'red',
+                   total_train_size = '',
+                   total_test_size  = '',
+                   info             = '',
+                   output_dir       = ''):
+
+    size = 1
+
+    fig, (ax1, ax2) = plt.subplots(1, 2)
+    fig.suptitle('UMAP: '+ info )
+
+    ind_l_train     = [i for i,x in enumerate(train_labels) if x == label]
+    Y_train_l       = np.array([Y_train_data[i,:] for i in ind_l_train])
+
+    ax1.scatter(-Y_train_l[:,0], -Y_train_l[:,1], s=size, c=col, marker='.', linewidth=0)
+    ax1.title.set_text('Train, ('+str(len(train_labels))+' of '+ total_train_size+')')
+    if Y_test_data is not None and test_labels is not None:
+        ind_l_test = [i for i,x in enumerate(test_labels) if x == label]
+        Y_test_l   = np.array([Y_test_data[i,:] for i in ind_l_test])
+
+        ax2.scatter(-Y_test_l[:,0], -Y_test_l[:,1], s=size, c=col, marker='.', linewidth=0)
+        ax2.title.set_text('Test, ('+str(len(test_labels))+' of '+ total_test_size+')')
+
+    plt.savefig(output_dir+"/"+info+'-umap.png')
+    plt.close()
+
+
+def visualize_umap(train_data,
+                   train_c,
+                   train_targets,
+                   test_data       = None,
+                   test_c          = None,
+                   test_targets    = None,
+                   total_train_size = '',
+                   total_test_size  = '',
+                   info             = '',
+                   output_dir       = ''):
+
+#    reducer = umap.UMAP(random_state=42, n_neighbors=25, min_dist=0.1)
+    reducer = umap.UMAP(random_state=42)
+    train_Y = reducer.fit_transform(train_data)
+
+    if test_data is not None and test_targets is not None:
+        test_Y = reducer.transform(test_data)
+
+    # all classes
+    plot_all_data(Y_train_data     = train_Y,
+                  train_labels     = train_targets,
+                  Y_test_data      = test_Y,
+                  test_labels      = test_targets,
+                  total_train_size = total_train_size,
+                  total_test_size  = total_test_size,
+                  info             = info,
+                  output_dir       = output_dir)
+
+    # class 0
+    plot_one_class(Y_train_data     = train_Y,
+                   train_labels     = train_targets,
+                   Y_test_data      = test_Y,
+                   test_labels      = test_targets,
+                   label            = 0,
+                   col              = 'red',
+                   total_train_size = total_train_size,
+                   total_test_size  = total_test_size,
+                   info             = info+' class ' + str(0),
+                   output_dir       = output_dir)
+
+    # class 1
+    plot_one_class(Y_train_data     = train_Y,
+                   train_labels     = train_targets,
+                   Y_test_data      = test_Y,
+                   test_labels      = test_targets,
+                   label            = 1,
+                   col              = 'green',
+                   total_train_size = total_train_size,
+                   total_test_size  = total_test_size,
+                   info             = info + ' class ' + str(1),
+                   output_dir       = output_dir)
+
+    # correct classification
+    plot_one_class(Y_train_data     = train_Y,
+                   train_labels     = train_c,
+                   Y_test_data      = test_Y,
+                   test_labels      = test_c,
+                   label            = 0,
+                   col              = 'green',
+                   total_train_size = total_train_size,
+                   total_test_size  = total_test_size,
+                   info             = info + ' correct ',
+                   output_dir       = output_dir)
+
+    # errors
+    plot_one_class(Y_train_data     = train_Y,
+                   train_labels     = train_c,
+                   Y_test_data      = test_Y,
+                   test_labels      = test_c,
+                   label            = 1,
+                   col              = 'red',
+                   total_train_size = total_train_size,
+                   total_test_size  = total_test_size,
+                   info             = info + ' errors ',
+                   output_dir       = output_dir)
+
+
+
+def visualize_data_umap(dlrm,
+                        train_data_ld,
+                        test_data_ld  = None,
+                        max_umap_size = 50000,
+                        output_dir    = ''):
+
+    train_feat, train_X, train_cat, train_T, train_z, train_c = create_vis_data(dlrm=dlrm, data_ld=train_data_ld, max_size=max_umap_size, info='train')
+
+    test_feat = None
+    test_X    = None
+    test_cat  = None
+    test_T    = None
+
+    if test_data_ld is not None:
+        test_feat, test_X, test_cat, test_T, test_z, test_c = create_vis_data(dlrm=dlrm, data_ld=test_data_ld, max_size=max_umap_size, info='test')
+
+    visualize_umap(train_data       = train_feat,
+                   train_targets    = train_T,
+                   train_c          = train_c,
+                   test_data        = test_feat,
+                   test_c           = test_c,
+                   test_targets     = test_T,
+                   total_train_size = str(len(train_data_ld)),
+                   total_test_size  = str(len(test_data_ld)),
+                   info             = 'all-features',
+                   output_dir       = output_dir)
+
+    visualize_umap(train_data       = train_X,
+                   train_c          = train_c,
+                   train_targets    = train_T,
+                   test_data        = test_X,
+                   test_c           = test_c,
+                   test_targets     = test_T,
+                   total_train_size = str(len(train_data_ld)),
+                   total_test_size  = str(len(test_data_ld)),
+                   info             = 'cont-features',
+                   output_dir       = output_dir)
+
+    visualize_umap(train_data       = train_cat,
+                   train_c          = train_c,
+                   train_targets    = train_T,
+                   test_data        = test_cat,
+                   test_c           = test_c,
+                   test_targets     = test_T,
+                   total_train_size = str(len(train_data_ld)),
+                   total_test_size  = str(len(test_data_ld)),
+                   info             = 'cat-features',
+                   output_dir       = output_dir)
+
+    # UMAP for z data
+    for i in range(0,len(test_z)):
+        visualize_umap(train_data       = train_z[i],
+                       train_targets    = train_T,
+                       train_c          = train_c,
+                       test_data        = test_z[i],
+                       test_c           = test_c,
+                       test_targets     = test_T,
+                       total_train_size = str(len(train_data_ld)),
+                       total_test_size  = str(len(test_data_ld)),
+                       info             = 'z-data-'+str(i),
+                       output_dir       = output_dir)
+
+
+
+def analyse_categorical_data(X_cat, n_days=10, output_dir=""):
+
+    # analyse categorical variables
+    n_vec = len(X_cat)
+    n_cat = len(X_cat[0])
+    n_days = n_days
+
+    print('n_vec', n_vec, 'n_cat', n_cat)
+#    for c in train_data.X_cat:
+#        print(n_cat, c)
+
+    all_cat = np.array(X_cat)
+    print('all_cat.shape', all_cat.shape)
+    day_size = all_cat.shape[0]/n_days
+
+    for i in range(0,n_cat):
+        l_d   = []
+        l_s1  = []
+        l_s2  = []
+        l_int = []
+        l_rem = []
+
+        cat = all_cat[:,i]
+        print('cat', i, cat.shape)
+        for d in range(1,n_days):
+            offset = int(d*day_size)
+            #print(offset)
+            cat1 = cat[:offset]
+            cat2 = cat[offset:]
+
+            s1 = set(cat1)
+            s2 = set(cat2)
+
+            intersect = list(s1 & s2)
+            #print(intersect)
+            l_d.append(d)
+            l_s1.append(len(s1))
+            l_s2.append(len(s2))
+            l_int.append(len(intersect))
+            l_rem.append((len(s1)-len(intersect)))
+
+            print(d, ',', len(s1), ',', len(s2), ',', len(intersect), ',', (len(s1)-len(intersect)))
+
+        print("spit",    l_d)
+        print("before",  l_s1)
+        print("after",   l_s2)
+        print("inters.", l_int)
+        print("removed", l_rem)
+
+        plt.figure(figsize=(8,8))
+        plt.plot(l_d, l_s1,  'g', label='before')
+        plt.plot(l_d, l_s2,  'r', label='after')
+        plt.plot(l_d, l_int, 'b', label='intersect')
+        plt.plot(l_d, l_rem, 'y', label='removed')
+        plt.title("categorical var. "+str(i))
+        plt.legend()
+        plt.savefig(output_dir+"/cat-"+str(i).zfill(3)+".png")
+        plt.close()
+
+
+def analyze_model_data(output_dir,
+                       dlrm,
+                       train_ld,
+                       test_ld,
+                       skip_embedding            = False,
+                       use_tsne                  = False,
+                       max_umap_size             = 50000,
+                       max_tsne_size             = 10000,
+                       skip_categorical_analysis = False,
+                       skip_data_plots           = False):
+
+    if not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+
+    if skip_embedding == False:
+        visualize_embeddings_umap(emb_l      = dlrm.emb_l,
+                                  output_dir = output_dir,
+                                  max_size   = max_umap_size)
+
+        if use_tsne == True:
+            visualize_embeddings_tsne(emb_l      = dlrm.emb_l,
+                                      output_dir = output_dir,
+                                      max_size   = max_tsne_size)
+
+    # data visualization and analysis
+    if skip_data_plots == False:
+        visualize_data_umap(dlrm=dlrm, train_data_ld=train_ld, test_data_ld=test_ld, max_umap_size=max_umap_size, output_dir=output_dir)
+
+    # analyse categorical variables
+    if skip_categorical_analysis == False:
+        analyse_categorical_data(X_cat=train_data.X_cat, n_days=10, output_dir=output_dir)
+
+
+if __name__ == "__main__":
+
+    output_dir = ""
+
+    ### parse arguments ###
+    parser = argparse.ArgumentParser(
+        description="Exploratory DLRM analysis"
+    )
+
+    parser.add_argument("--load-model", type=str, default="")
+    parser.add_argument("--data-set", choices=["kaggle", "terabyte"], help="dataset")
+    # parser.add_argument("--dataset-path", required=True, help="path to the dataset")
+    parser.add_argument("--max-ind-range", type=int, default=-1)
+    # parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
+    parser.add_argument("--output-dir", type=str, default="")
+    parser.add_argument("--skip-embedding", action='store_true', default=False)
+    parser.add_argument("--skip-data-plots", action='store_true', default=False)
+    parser.add_argument("--skip-categorical-analysis", action='store_true', default=False)
+
+    # umap relatet
+    parser.add_argument("--max-umap-size", type=int, default=50000)
+    # tsne related
+    parser.add_argument("--use-tsne", action='store_true', default=False)
+    parser.add_argument("--max-tsne-size", type=int, default=1000)
+    # data file related
+    parser.add_argument("--raw-data-file", type=str, default="")
+    parser.add_argument("--processed-data-file", type=str, default="")
+    parser.add_argument("--data-sub-sample-rate", type=float, default=0.0)  # in [0, 1]
+    parser.add_argument("--data-randomize", type=str, default="none")  # total or day or none
+    parser.add_argument("--memory-map", action="store_true", default=False)
+    parser.add_argument("--mini-batch-size", type=int, default=1)
+    parser.add_argument("--num-workers", type=int, default=0)
+    parser.add_argument("--test-mini-batch-size", type=int, default=1)
+    parser.add_argument("--test-num-workers", type=int, default=0)
+    parser.add_argument("--num-batches", type=int, default=0)
+    # mlperf logging (disables other output and stops early)
+    parser.add_argument("--mlperf-logging", action="store_true", default=False)
+
+    args = parser.parse_args()
+
+    print('command line args: ', json.dumps(vars(args)))
+
+    if output_dir == "":
+        output_dir = args.data_set+"_vis_all"
+    print('output_dir:', output_dir)
+
+    if args.data_set == "kaggle":
+        # 1. Criteo Kaggle Display Advertisement Challenge Dataset (see ./bench/dlrm_s_criteo_kaggle.sh)
+        m_spa=16
+        ln_emb=np.array([1460,583,10131227,2202608,305,24,12517,633,3,93145,5683,8351593,3194,27,14992,5461306,10,5652,2173,4,7046547,18,15,286181,105,142572])
+        ln_bot=np.array([13,512,256,64,16])
+        ln_top=np.array([367,512,256,1])
+
+    elif args.dataset == "terabyte":
+
+        if args.max_ind_range == 10000000:
+            # 2. Criteo Terabyte (see ./bench/dlrm_s_criteo_terabyte.sh [--sub-sample=0.875] --max-in-range=10000000)
+            m_spa=64
+            ln_emb=np.array([9980333,36084,17217,7378,20134,3,7112,1442,61,9758201,1333352,313829,10,2208,11156,122,4,970,14, 9994222, 7267859, 9946608,415421,12420,101, 36])
+            ln_bot=np.array([13,512,256,64])
+            ln_top=np.array([415,512,512,256,1])
+        elif args.max_ind_range == 40000000:
+            # 3. Criteo Terabyte MLPerf training (see ./bench/run_and_time.sh --max-in-range=40000000)
+            m_spa=128
+            ln_emb=np.array([39884406,39043,17289,7420,20263,3,7120,1543,63,38532951,2953546,403346,10,2208,11938,155,4,976,14,39979771,25641295,39664984,585935,12972,108,36])
+            ln_bot=np.array([13,512,256,128])
+            ln_top=np.array([479,1024,1024,512,256,1])
+        else:
+            raise ValueError("only --max-in-range 10M or 40M is supported")
+    else:
+        raise ValueError("only kaggle|terabyte dataset options are supported")
+
+    dlrm = DLRM_Net(
+            m_spa,
+            ln_emb,
+            ln_bot,
+            ln_top,
+            arch_interaction_op="dot",
+            arch_interaction_itself=False,
+            sigmoid_bot=-1,
+            sigmoid_top=ln_top.size - 2,
+            sync_dense_params=True,
+            loss_threshold=0.0,
+            ndevices=-1,
+            qr_flag=False,
+            qr_operation=None,
+            qr_collisions=None,
+            qr_threshold=None,
+            md_flag=False,
+            md_threshold=None,
+        )
+
+    # Load model is specified
+    if not (args.load_model == ""):
+        print("Loading saved model {}".format(args.load_model))
+
+        ld_model = torch.load(args.load_model, map_location=torch.device('cpu'))
+        dlrm.load_state_dict(ld_model["state_dict"])
+
+        print("Model loaded", args.load_model)
+        #print(dlrm)
+
+    # load data
+    train_data = None
+    train_ld   = None
+    test_data  = None
+    test_ld    = None
+
+    if args.raw_data_file is not "" or args.processed_data_file is not "":
+        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
+
+    analyze_model_data(output_dir                = output_dir,
+                       dlrm                      = dlrm,
+                       train_ld                  = train_ld,
+                       test_ld                   = test_ld,
+                       skip_embedding            = args.skip_embedding,
+                       use_tsne                  = args.use_tsne,
+                       max_umap_size             = args.max_umap_size,
+                       max_tsne_size             = args.max_tsne_size,
+                       skip_categorical_analysis = args.skip_categorical_analysis,
+                       skip_data_plots           = args.skip_data_plots)
diff --git a/torchrec_dlrm/README.MD b/torchrec_dlrm/README.MD
deleted file mode 100644
index c4fbaca..0000000
--- a/torchrec_dlrm/README.MD
+++ /dev/null
@@ -1,115 +0,0 @@
-# TorchRec DLRM Example
-
-`dlrm_main.py` trains, validates, and tests a [Deep Learning Recommendation Model](https://arxiv.org/abs/1906.00091) (DLRM) with TorchRec. The DLRM model contains both data parallel components (e.g. multi-layer perceptrons & interaction arch) and model parallel components (e.g. embedding tables). The DLRM model is pipelined so that dataloading, data-parallel to model-parallel comms, and forward/backward are overlapped. Can be run with either a random dataloader or [Criteo 1 TB click logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).
-
-It has been tested on the following cloud instance types:
-| Cloud  | Instance Type       | GPUs             | vCPUs | Memory (GB) |
-| ------ | ------------------- | ---------------- | ----- | ----------- |
-| AWS    | p4d.24xlarge        | 8 x A100 (40GB)  | 96    | 1152        |
-| Azure  | Standard_ND96asr_v4 | 8 x A100 (40GB)  | 96    | 900         |
-| GCP    | a2-megagpu-16g      | 16 x A100 (40GB) | 96    | 1300        |
-
-# Running
-
-## Install dependencies
-`pip install tqdm torchmetrics`
-
-## Torchx
-We recommend using [torchx](https://pytorch.org/torchx/main/quickstart.html) to run. Here we use the [DDP builtin](https://pytorch.org/torchx/main/components/distributed.html)
-
-1. pip install torchx
-2. (optional) setup a slurm or kubernetes cluster
-3.
-    a. locally: `torchx run -s local_cwd dist.ddp -j 1x2 --script dlrm_main.py`
-    b. remotely: `torchx run -s slurm dist.ddp -j 1x8 --script dlrm_main.py`
-
-## TorchRun
-You can also use [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
-* e.g. `torchrun --nnodes 1 --nproc_per_node 2 --rdzv_backend c10d --rdzv_endpoint localhost --rdzv_id 54321 --role trainer dlrm_main.py`
-
-
-## Preliminary Training Results
-
-**Setup:**
-* Dataset: Criteo 1TB Click Logs dataset
-* CUDA 11.0, NCCL 2.10.3.
-* AWS p4d24xlarge instances, each with 8 40GB NVIDIA A100s.
-
-**Results**
-
-Common settings across all runs:
-
-```
---num_embeddings_per_feature "45833188,36746,17245,7413,20243,3,7114,1441,62,29275261,1572176,345138,10,2209,11267,128,4,974,14,48937457,11316796,40094537,452104,12606,104,35" --embedding_dim_size 128 --pin_memory --over_arch_layer_sizes "1024,1024,512,256,1" --dense_arch_layer_sizes "512,256,128" --epochs 1 --change_lr --shuffle_batches --learning_rate 15.0
-```
-
-|Number of GPUs|Collective Size of Embedding Tables (GiB)|Local Batch Size|Global Batch Size|AUROC over Val Set After 1 Epoch|AUROC Over Test Set After 1 Epoch|Train Records/Second|Time to Train 1 Epoch | Unique Flags |
---- | --- | --- | --- | --- | --- | --- | --- | ---
-|1|91.10|16384|16384|0.8025434017181396|0.8026024103164673|~740,065 rec/s| 1h35m29s | `--batch_size 16384 --lr_change_point 0.65 --lr_after_change_point 0.035` |
-|4|91.10|4096|16384|0.8030692934989929|0.8030484914779663|~1,458,176 rec/s| 48m39s | `--batch_size 4096 --lr_change_point 0.80 --lr_after_change_point 0.20` |
-|8|91.10|2048|16384|0.802501916885376|0.8025660514831543|~1,671,168 rec/s| 43m24s | `--batch_size 2048 --lr_change_point 0.80 --lr_after_change_point 0.20` |
-|8|91.10|8192|65536|0.7996258735656738|0.7996508479118347|~5,373,952 rec/s| 13m40s | `--batch_size 8192`|
-
-QPS (train record/second) is calculated by using the following formula: `x it/s * local_batch_size * num_gpus`. The `it/s`
-can be found within the logs of the training results.
-
-The final row, using 8 GPUs with a batch size of 8192, was not tuned to hit the MLPerf benchmark but is shown to
-highlight the QPS (train record/second) achievable with torchrec.
-
-The `change_lr` parameter activates the variable learning rate schedule. `lr_after_change_point` is a parameter that we use to dictate the point
-at which we'll shift the learning rate to the value specified by `lr_change_point`. We found that having a high learning rate to start (e.g. 15.0) and dropping to a smaller learning rate (e.g. 0.20) near the end of the first epoch (e.g. 80% through) helped us converge faster to the 0.8025 MLPerf AUROC metric.
-
-**Reproduce**
-
-Run the following command to reproduce the results for a single node (8 GPUs) on AWS. This command makes use of the `aws_component.py` script.
-
-Ensure to:
-- set $PATH_TO_1TB_NUMPY_FILES to the path with the pre-processed .npy files of the Criteo 1TB dataset.
-- set $TRAIN_QUEUE to the partition that handles training jobs
-
-Example command:
-```
-torchx run --scheduler slurm --scheduler_args partition=$TRAIN_QUEUE,time=5:00:00 aws_component.py:run_dlrm_main --num_trainers=8 -- --pin_memory --batch_size 2048 --epochs 1 --num_embeddings_per_feature "45833188,36746,17245,7413,20243,3,7114,1441,62,29275261,1572176,345138,10,2209,11267,128,4,974,14,48937457,11316796,40094537,452104,12606,104,35" --embedding_dim 128 --dense_arch_layer_sizes "512,256,128" --over_arch_layer_sizes "1024,1024,512,256,1" --in_memory_binary_criteo_path $PATH_TO_1TB_NUMPY_FILES --learning_rate 15.0 --shuffle_batches --change_lr --lr_change_point 0.80 --lr_after_change_point 0.20
-```
-Upon scheduling the job, there should be an output that looks like this:
-
-```
-warnings.warn(
-slurm://torchx/14731
-torchx 2022-01-07 21:06:59 INFO     Launched app: slurm://torchx/14731
-torchx 2022-01-07 21:06:59 INFO     AppStatus:
-  msg: ''
-  num_restarts: -1
-  roles: []
-  state: UNKNOWN (7)
-  structured_error_msg: <NONE>
-  ui_url: null
-
-torchx 2022-01-07 21:06:59 INFO     Job URL: None
-```
-
-In this example, the job was launched to: `slurm://torchx/14731`.
-
-Run the following commands to check the status of your job and read the logs:
-
-```
-# Status should be "RUNNING" if properly scheduled
-torchx status slurm://torchx/14731
-
-# Log file was automatically created in the directory where you launched the job from
-cat slurm-14731.out
-
-```
-
-The results from the training can be found in the log file (e.g. `slurm-14731.out`).
-
-**Debugging**
-
-The `--validation_freq_within_epoch x` parameter can be used to print the AUROC every `x` iterations through an epoch.
-
-The in-memory dataloader can take approximately 20-30 minutes to load the data into memory before training starts. The
-`--mmap_mode` parameter can be used to load data from disk which reduces start-up time for training at the cost
-of QPS.
-
-**TODO/Work In Progress**
-* Write section on how to pre-process the dataset.
diff --git a/torchrec_dlrm/__init__.py b/torchrec_dlrm/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/torchrec_dlrm/aws_component.py b/torchrec_dlrm/aws_component.py
deleted file mode 100644
index e2caa6a..0000000
--- a/torchrec_dlrm/aws_component.py
+++ /dev/null
@@ -1,44 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import os
-
-import torchx.specs as specs
-from torchx.components.dist import ddp
-from torchx.specs.api import Resource
-
-
-def run_dlrm_main(num_trainers: int = 8, *script_args: str) -> specs.AppDef:
-    """
-    Args:
-        num_trainers: The number of trainers to use.
-        script_args: A variable number of parameters to provide dlrm_main.py.
-    """
-    cwd = os.getcwd()
-    entrypoint = os.path.join(cwd, "dlrm_main.py")
-
-    user = os.environ.get("USER")
-    image = f"/data/home/{user}"
-
-    if num_trainers > 8 and num_trainers % 8 != 0:
-        raise ValueError(
-            "Trainer jobs spanning multiple hosts must be in multiples of 8."
-        )
-    nproc_per_node = 8 if num_trainers >= 8 else num_trainers
-    num_replicas = max(num_trainers // 8, 1)
-
-    return ddp(
-        *script_args,
-        name="train_dlrm",
-        image=image,
-        # AWS p4d instance (https://aws.amazon.com/ec2/instance-types/p4/).
-        cpu=96,
-        gpu=8,
-        memMB=-1,
-        script=entrypoint,
-        j=f"{num_replicas}x{nproc_per_node}",
-    )
diff --git a/torchrec_dlrm/data/__init__.py b/torchrec_dlrm/data/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/torchrec_dlrm/data/dlrm_dataloader.py b/torchrec_dlrm/data/dlrm_dataloader.py
deleted file mode 100644
index 03e88d9..0000000
--- a/torchrec_dlrm/data/dlrm_dataloader.py
+++ /dev/null
@@ -1,130 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import argparse
-import os
-from typing import List
-
-from torch import distributed as dist
-from torch.utils.data import DataLoader
-from torchrec.datasets.criteo import (
-    CAT_FEATURE_COUNT,
-    DEFAULT_CAT_NAMES,
-    DEFAULT_INT_NAMES,
-    DAYS,
-    InMemoryBinaryCriteoIterDataPipe,
-)
-from torchrec.datasets.random import RandomRecDataset
-
-STAGES = ["train", "val", "test"]
-
-
-def _get_random_dataloader(
-    args: argparse.Namespace,
-) -> DataLoader:
-    return DataLoader(
-        RandomRecDataset(
-            keys=DEFAULT_CAT_NAMES,
-            batch_size=args.batch_size,
-            hash_size=args.num_embeddings,
-            hash_sizes=args.num_embeddings_per_feature
-            if hasattr(args, "num_embeddings_per_feature")
-            else None,
-            manual_seed=args.seed if hasattr(args, "seed") else None,
-            ids_per_feature=1,
-            num_dense=len(DEFAULT_INT_NAMES),
-        ),
-        batch_size=None,
-        batch_sampler=None,
-        pin_memory=args.pin_memory,
-        num_workers=0,
-    )
-
-
-def _get_in_memory_dataloader(
-    args: argparse.Namespace,
-    stage: str,
-) -> DataLoader:
-    files = os.listdir(args.in_memory_binary_criteo_path)
-
-    def is_final_day(s: str) -> bool:
-        return f"day_{DAYS - 1}" in s
-
-    if stage == "train":
-        # Train set gets all data except from the final day.
-        files = list(filter(lambda s: not is_final_day(s), files))
-        rank = dist.get_rank()
-        world_size = dist.get_world_size()
-    else:
-        # Validation set gets the first half of the final day's samples. Test set get
-        # the other half.
-        files = list(filter(is_final_day, files))
-        rank = (
-            dist.get_rank()
-            if stage == "val"
-            else dist.get_rank() + dist.get_world_size()
-        )
-        world_size = dist.get_world_size() * 2
-
-    stage_files: List[List[str]] = [
-        sorted(
-            map(
-                lambda x: os.path.join(args.in_memory_binary_criteo_path, x),
-                filter(lambda s: kind in s, files),
-            )
-        )
-        for kind in ["dense", "sparse", "labels"]
-    ]
-    dataloader = DataLoader(
-        InMemoryBinaryCriteoIterDataPipe(
-            *stage_files,  # pyre-ignore[6]
-            batch_size=args.batch_size,
-            rank=rank,
-            world_size=world_size,
-            shuffle_batches=args.shuffle_batches,
-            hashes=args.num_embeddings_per_feature
-            if args.num_embeddings is None
-            else ([args.num_embeddings] * CAT_FEATURE_COUNT),
-        ),
-        batch_size=None,
-        pin_memory=args.pin_memory,
-        collate_fn=lambda x: x,
-    )
-    return dataloader
-
-
-def get_dataloader(args: argparse.Namespace, backend: str, stage: str) -> DataLoader:
-    """
-    Gets desired dataloader from dlrm_main command line options. Currently, this
-    function is able to return either a DataLoader wrapped around a RandomRecDataset or
-    a Dataloader wrapped around an InMemoryBinaryCriteoIterDataPipe.
-
-    Args:
-        args (argparse.Namespace): Command line options supplied to dlrm_main.py's main
-            function.
-        backend (str): "nccl" or "gloo".
-        stage (str): "train", "val", or "test".
-
-    Returns:
-        dataloader (DataLoader): PyTorch dataloader for the specified options.
-
-    """
-    stage = stage.lower()
-    if stage not in STAGES:
-        raise ValueError(f"Supplied stage was {stage}. Must be one of {STAGES}.")
-
-    args.pin_memory = (
-        (backend == "nccl") if not hasattr(args, "pin_memory") else args.pin_memory
-    )
-
-    if (
-        not hasattr(args, "in_memory_binary_criteo_path")
-        or args.in_memory_binary_criteo_path is None
-    ):
-        return _get_random_dataloader(args)
-    else:
-        return _get_in_memory_dataloader(args, stage)
diff --git a/torchrec_dlrm/dlrm_main.py b/torchrec_dlrm/dlrm_main.py
deleted file mode 100644
index 7025b45..0000000
--- a/torchrec_dlrm/dlrm_main.py
+++ /dev/null
@@ -1,566 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import argparse
-import itertools
-import os
-import sys
-from dataclasses import dataclass, field
-from typing import cast, Iterator, List, Optional, Tuple
-
-import torch
-import torchmetrics as metrics
-from pyre_extensions import none_throws
-from torch import nn, distributed as dist
-from torch.utils.data import DataLoader
-from torchrec import EmbeddingBagCollection
-from torchrec.datasets.criteo import (
-    DEFAULT_CAT_NAMES,
-    DEFAULT_INT_NAMES,
-    TOTAL_TRAINING_SAMPLES,
-)
-from torchrec.datasets.utils import Batch
-from torchrec.distributed import TrainPipelineSparseDist
-from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder
-from torchrec.distributed.model_parallel import DistributedModelParallel
-from torchrec.distributed.types import ModuleSharder
-from torchrec.modules.embedding_configs import EmbeddingBagConfig
-from torchrec.optim.keyed import CombinedOptimizer, KeyedOptimizerWrapper
-from tqdm import tqdm
-from fbgemm_gpu.split_embedding_configs import EmbOptimType as OptimType
-
-
-# OSS import
-try:
-    # pyre-ignore[21]
-    # @manual=//pytorch/benchmark/torchrec_dlrm/data:dlrm_dataloader
-    from data.dlrm_dataloader import get_dataloader, STAGES
-
-    # pyre-ignore[21]
-    # @manual=//pytorch/benchmark/torchrec_dlrm/modules:dlrm_train
-    from modules.dlrm_train import DLRMTrain
-except ImportError:
-    pass
-
-# internal import
-try:
-    from .data.dlrm_dataloader import (  # noqa F811
-        get_dataloader,
-        STAGES,
-    )
-    from .modules.dlrm_train import DLRMTrain  # noqa F811
-except ImportError:
-    pass
-
-TRAIN_PIPELINE_STAGES = 3  # Number of stages in TrainPipelineSparseDist.
-
-
-def parse_args(argv: List[str]) -> argparse.Namespace:
-    parser = argparse.ArgumentParser(description="torchrec dlrm example trainer")
-    parser.add_argument(
-        "--epochs", type=int, default=1, help="number of epochs to train"
-    )
-    parser.add_argument(
-        "--batch_size", type=int, default=32, help="batch size to use for training"
-    )
-    parser.add_argument(
-        "--limit_train_batches",
-        type=int,
-        default=None,
-        help="number of train batches",
-    )
-    parser.add_argument(
-        "--limit_val_batches",
-        type=int,
-        default=None,
-        help="number of validation batches",
-    )
-    parser.add_argument(
-        "--limit_test_batches",
-        type=int,
-        default=None,
-        help="number of test batches",
-    )
-    parser.add_argument(
-        "--dataset_name",
-        type=str,
-        default="criteo_1t",
-        help="dataset for experiment, current support criteo_1tb, criteo_kaggle",
-    )
-    parser.add_argument(
-        "--num_embeddings",
-        type=int,
-        default=100_000,
-        help="max_ind_size. The number of embeddings in each embedding table. Defaults"
-        " to 100_000 if num_embeddings_per_feature is not supplied.",
-    )
-    parser.add_argument(
-        "--num_embeddings_per_feature",
-        type=str,
-        default=None,
-        help="Comma separated max_ind_size per sparse feature. The number of embeddings"
-        " in each embedding table. 26 values are expected for the Criteo dataset.",
-    )
-    parser.add_argument(
-        "--dense_arch_layer_sizes",
-        type=str,
-        default="512,256,64",
-        help="Comma separated layer sizes for dense arch.",
-    )
-    parser.add_argument(
-        "--over_arch_layer_sizes",
-        type=str,
-        default="512,512,256,1",
-        help="Comma separated layer sizes for over arch.",
-    )
-    parser.add_argument(
-        "--embedding_dim",
-        type=int,
-        default=64,
-        help="Size of each embedding.",
-    )
-    parser.add_argument(
-        "--undersampling_rate",
-        type=float,
-        help="Desired proportion of zero-labeled samples to retain (i.e. undersampling zero-labeled rows)."
-        " Ex. 0.3 indicates only 30pct of the rows with label 0 will be kept."
-        " All rows with label 1 will be kept. Value should be between 0 and 1."
-        " When not supplied, no undersampling occurs.",
-    )
-    parser.add_argument(
-        "--seed",
-        type=int,
-        help="Random seed for reproducibility.",
-    )
-    parser.add_argument(
-        "--pin_memory",
-        dest="pin_memory",
-        action="store_true",
-        help="Use pinned memory when loading data.",
-    )
-    parser.add_argument(
-        "--mmap_mode",
-        dest="mmap_mode",
-        action="store_true",
-        help="--mmap_mode mmaps the dataset."
-        " That is, the dataset is kept on disk but is accessed as if it were in memory."
-        " --mmap_mode is intended mostly for faster debugging. Use --mmap_mode to bypass"
-        " preloading the dataset when preloading takes too long or when there is "
-        " insufficient memory available to load the full dataset.",
-    )
-    parser.add_argument(
-        "--in_memory_binary_criteo_path",
-        type=str,
-        default=None,
-        help="Path to a folder containing the binary (npy) files for the Criteo dataset."
-        " When supplied, InMemoryBinaryCriteoIterDataPipe is used.",
-    )
-    parser.add_argument(
-        "--learning_rate",
-        type=float,
-        default=15.0,
-        help="Learning rate.",
-    )
-    parser.add_argument(
-        "--shuffle_batches",
-        dest="shuffle_batches",
-        action="store_true",
-        help="Shuffle each batch during training.",
-    )
-    parser.add_argument(
-        "--validation_freq_within_epoch",
-        type=int,
-        default=None,
-        help="Frequency at which validation will be run within an epoch.",
-    )
-    parser.add_argument(
-        "--change_lr",
-        dest="change_lr",
-        action="store_true",
-        help="Flag to determine whether learning rate should be changed part way through training.",
-    )
-    parser.add_argument(
-        "--lr_change_point",
-        type=float,
-        default=0.80,
-        help="The point through training at which learning rate should change to the value set by"
-        " lr_after_change_point. The default value is 0.80 which means that 80% through the total iterations (totaled"
-        " across all epochs), the learning rate will change.",
-    )
-    parser.add_argument(
-        "--lr_after_change_point",
-        type=float,
-        default=0.20,
-        help="Learning rate after change point in first epoch.",
-    )
-    parser.set_defaults(
-        pin_memory=None,
-        mmap_mode=None,
-        shuffle_batches=None,
-        change_lr=None,
-    )
-    parser.add_argument(
-        "--adagrad",
-        dest="adagrad",
-        action="store_true",
-        help="Flag to determine if adagrad optimizer should be used.",
-    )
-    return parser.parse_args(argv)
-
-
-def _evaluate(
-    limit_batches: Optional[int],
-    train_pipeline: TrainPipelineSparseDist,
-    iterator: Iterator[Batch],
-    next_iterator: Iterator[Batch],
-    stage: str,
-) -> Tuple[float, float]:
-    """
-    Evaluates model. Computes and prints metrics including AUROC and Accuracy. Helper
-    function for train_val_test.
-
-    Args:
-        limit_batches (Optional[int]): number of batches.
-        train_pipeline (TrainPipelineSparseDist): pipelined model.
-        iterator (Iterator[Batch]): Iterator used for val/test batches.
-        next_iterator (Iterator[Batch]): Iterator used for the next phase (either train
-            if there are more epochs to train on or test if all epochs are complete).
-            Used to queue up the next TRAIN_PIPELINE_STAGES - 1 batches before
-            train_val_test switches to the next phase. This is done so that when the
-            next phase starts, the first output train_pipeline generates an output for
-            is the 1st batch for that phase.
-        stage (str): "val" or "test".
-
-    Returns:
-        Tuple[float, float]: auroc and accuracy result
-    """
-    model = train_pipeline._model
-    model.eval()
-    device = train_pipeline._device
-    if limit_batches is not None:
-        limit_batches -= TRAIN_PIPELINE_STAGES - 1
-
-    # Because TrainPipelineSparseDist buffer batches internally, we load in
-    # TRAIN_PIPELINE_STAGES - 1 batches from the next_iterator into the buffers so that
-    # when train_val_test switches to the next phase, train_pipeline will start
-    # producing results for the TRAIN_PIPELINE_STAGES - 1 buffered batches (as opposed
-    # to the last TRAIN_PIPELINE_STAGES - 1 batches from iterator).
-    combined_iterator = itertools.chain(
-        iterator
-        if limit_batches is None
-        else itertools.islice(iterator, limit_batches),
-        itertools.islice(next_iterator, TRAIN_PIPELINE_STAGES - 1),
-    )
-    auroc = metrics.AUROC(compute_on_step=False).to(device)
-    accuracy = metrics.Accuracy(compute_on_step=False).to(device)
-
-    # Infinite iterator instead of while-loop to leverage tqdm progress bar.
-    for _ in tqdm(iter(int, 1), desc=f"Evaluating {stage} set"):
-        try:
-            _loss, logits, labels = train_pipeline.progress(combined_iterator)
-            preds = torch.sigmoid(logits)
-            auroc(preds, labels)
-            accuracy(preds, labels)
-        except StopIteration:
-            break
-    auroc_result = auroc.compute().item()
-    accuracy_result = accuracy.compute().item()
-    if dist.get_rank() == 0:
-        print(f"AUROC over {stage} set: {auroc_result}.")
-        print(f"Accuracy over {stage} set: {accuracy_result}.")
-    return auroc_result, accuracy_result
-
-
-def _train(
-    train_pipeline: TrainPipelineSparseDist,
-    iterator: Iterator[Batch],
-    next_iterator: Iterator[Batch],
-    within_epoch_val_dataloader: DataLoader,
-    epoch: int,
-    epochs: int,
-    change_lr: bool,
-    lr_change_point: float,
-    lr_after_change_point: float,
-    validation_freq_within_epoch: Optional[int],
-    limit_train_batches: Optional[int],
-    limit_val_batches: Optional[int],
-) -> None:
-    """
-    Trains model for 1 epoch. Helper function for train_val_test.
-
-    Args:
-        args (argparse.Namespace): parsed command line args.
-        train_pipeline (TrainPipelineSparseDist): pipelined model.
-        iterator (Iterator[Batch]): Iterator used for training batches.
-        next_iterator (Iterator[Batch]): Iterator used for validation batches
-            in between epochs. Used to queue up the next TRAIN_PIPELINE_STAGES - 1
-            batches before train_val_test switches to validation mode. This is done
-            so that when validation starts, the first output train_pipeline generates
-            an output for is the 1st validation batch (as opposed to a buffered train
-            batch).
-        within_epoch_val_dataloader (DataLoader): Dataloader to create iterators for
-            validation within an epoch. This is only used if
-            validation_freq_within_epoch is specified.
-        epoch (int): Which epoch the model is being trained on.
-        epochs (int): Number of epochs to train.
-        change_lr (bool): Whether learning rate should be changed part way through
-            training.
-        lr_change_point (float): The point through training at which learning rate
-            should change to the value set by lr_after_change_point.
-            Applied only if change_lr is set to True.
-        lr_after_change_point (float): Learning rate after change point in first epoch.
-            Applied only if change_lr is set to True.
-        validation_freq_within_epoch (Optional[int]): Frequency at which validation
-            will be run within an epoch.
-        limit_train_batches (Optional[int]): Number of train batches.
-        limit_val_batches (Optional[int]): Number of validation batches.
-
-
-
-    Returns:
-        None.
-    """
-    train_pipeline._model.train()
-
-    # For the first epoch, train_pipeline has no buffered batches, but for all other
-    # epochs, train_pipeline will have TRAIN_PIPELINE_STAGES - 1 from iterator already
-    # present in its buffer.
-    if limit_train_batches is not None and epoch > 0:
-        limit_train_batches -= TRAIN_PIPELINE_STAGES - 1
-
-    # Because TrainPipelineSparseDist buffer batches internally, we load in
-    # TRAIN_PIPELINE_STAGES - 1 batches from the next_iterator into the buffers so that
-    # when train_val_test switches to the next phase, train_pipeline will start
-    # producing results for the TRAIN_PIPELINE_STAGES - 1 buffered batches (as opposed
-    # to the last TRAIN_PIPELINE_STAGES - 1 batches from iterator).
-    combined_iterator = itertools.chain(
-        iterator
-        if limit_train_batches is None
-        else itertools.islice(iterator, limit_train_batches),
-        itertools.islice(next_iterator, TRAIN_PIPELINE_STAGES - 1),
-    )
-    samples_per_trainer = TOTAL_TRAINING_SAMPLES / dist.get_world_size() * epochs
-
-    # Infinite iterator instead of while-loop to leverage tqdm progress bar.
-    for it in tqdm(itertools.count(), desc=f"Epoch {epoch}"):
-        try:
-            train_pipeline.progress(combined_iterator)
-            if change_lr and (
-                (it * (epoch + 1) / samples_per_trainer) > lr_change_point
-            ):  # progress made through the epoch
-                print(f"Changing learning rate to: {lr_after_change_point}")
-                optimizer = train_pipeline._optimizer
-                lr = lr_after_change_point
-                for g in optimizer.param_groups:
-                    g["lr"] = lr
-
-            if validation_freq_within_epoch and it % validation_freq_within_epoch == 0:
-                _evaluate(
-                    limit_val_batches,
-                    train_pipeline,
-                    iter(within_epoch_val_dataloader),
-                    iterator,
-                    "val",
-                )
-                train_pipeline._model.train()
-        except StopIteration:
-            break
-
-
-@dataclass
-class TrainValTestResults:
-    val_accuracies: List[float] = field(default_factory=list)
-    val_aurocs: List[float] = field(default_factory=list)
-    test_accuracy: Optional[float] = None
-    test_auroc: Optional[float] = None
-
-
-def train_val_test(
-    args: argparse.Namespace,
-    train_pipeline: TrainPipelineSparseDist,
-    train_dataloader: DataLoader,
-    val_dataloader: DataLoader,
-    test_dataloader: DataLoader,
-) -> TrainValTestResults:
-    """
-    Train/validation/test loop. Contains customized logic to ensure each dataloader's
-    batches are used for the correct designated purpose (train, val, test). This logic
-    is necessary because TrainPipelineSparseDist buffers batches internally (so we
-    avoid batches designated for one purpose like training getting buffered and used for
-    another purpose like validation).
-
-    Args:
-        args (argparse.Namespace): parsed command line args.
-        train_pipeline (TrainPipelineSparseDist): pipelined model.
-        train_dataloader (DataLoader): DataLoader used for training.
-        val_dataloader (DataLoader): DataLoader used for validation.
-        test_dataloader (DataLoader): DataLoader used for testing.
-
-    Returns:
-        TrainValTestResults.
-    """
-
-    train_val_test_results = TrainValTestResults()
-
-    train_iterator = iter(train_dataloader)
-    test_iterator = iter(test_dataloader)
-    for epoch in range(args.epochs):
-        val_iterator = iter(val_dataloader)
-        _train(
-            train_pipeline,
-            train_iterator,
-            val_iterator,
-            val_dataloader,
-            epoch,
-            args.epochs,
-            args.change_lr,
-            args.lr_change_point,
-            args.lr_after_change_point,
-            args.validation_freq_within_epoch,
-            args.limit_train_batches,
-            args.limit_val_batches,
-        )
-        train_iterator = iter(train_dataloader)
-        val_next_iterator = (
-            test_iterator if epoch == args.epochs - 1 else train_iterator
-        )
-        val_accuracy, val_auroc = _evaluate(
-            args.limit_val_batches,
-            train_pipeline,
-            val_iterator,
-            val_next_iterator,
-            "val",
-        )
-
-        train_val_test_results.val_accuracies.append(val_accuracy)
-        train_val_test_results.val_aurocs.append(val_auroc)
-
-    test_accuracy, test_auroc = _evaluate(
-        args.limit_test_batches,
-        train_pipeline,
-        test_iterator,
-        iter(test_dataloader),
-        "test",
-    )
-    train_val_test_results.test_accuracy = test_accuracy
-    train_val_test_results.test_auroc = test_auroc
-
-    return train_val_test_results
-
-
-def main(argv: List[str]) -> None:
-    """
-    Trains, validates, and tests a Deep Learning Recommendation Model (DLRM)
-    (https://arxiv.org/abs/1906.00091). The DLRM model contains both data parallel
-    components (e.g. multi-layer perceptrons & interaction arch) and model parallel
-    components (e.g. embedding tables). The DLRM model is pipelined so that dataloading,
-    data-parallel to model-parallel comms, and forward/backward are overlapped. Can be
-    run with either a random dataloader or an in-memory Criteo 1 TB click logs dataset
-    (https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).
-
-    Args:
-        argv (List[str]): command line args.
-
-    Returns:
-        None.
-    """
-    args = parse_args(argv)
-
-    rank = int(os.environ["LOCAL_RANK"])
-    if torch.cuda.is_available():
-        device: torch.device = torch.device(f"cuda:{rank}")
-        backend = "nccl"
-        torch.cuda.set_device(device)
-    else:
-        device: torch.device = torch.device("cpu")
-        backend = "gloo"
-
-    if not torch.distributed.is_initialized():
-        dist.init_process_group(backend=backend)
-
-    if args.num_embeddings_per_feature is not None:
-        args.num_embeddings_per_feature = list(
-            map(int, args.num_embeddings_per_feature.split(","))
-        )
-        args.num_embeddings = None
-
-    # TODO add CriteoIterDataPipe support and add random_dataloader arg
-    train_dataloader = get_dataloader(args, backend, "train")
-    val_dataloader = get_dataloader(args, backend, "val")
-    test_dataloader = get_dataloader(args, backend, "test")
-
-    # Sets default limits for random dataloader iterations when left unspecified.
-    if args.in_memory_binary_criteo_path is None:
-        for stage in STAGES:
-            attr = f"limit_{stage}_batches"
-            if getattr(args, attr) is None:
-                setattr(args, attr, 10)
-
-    eb_configs = [
-        EmbeddingBagConfig(
-            name=f"t_{feature_name}",
-            embedding_dim=args.embedding_dim,
-            num_embeddings=none_throws(args.num_embeddings_per_feature)[feature_idx]
-            if args.num_embeddings is None
-            else args.num_embeddings,
-            feature_names=[feature_name],
-        )
-        for feature_idx, feature_name in enumerate(DEFAULT_CAT_NAMES)
-    ]
-    sharded_module_kwargs = {}
-    if args.over_arch_layer_sizes is not None:
-        sharded_module_kwargs["over_arch_layer_sizes"] = list(
-            map(int, args.over_arch_layer_sizes.split(","))
-        )
-
-    train_model = DLRMTrain(
-        embedding_bag_collection=EmbeddingBagCollection(
-            tables=eb_configs, device=torch.device("meta")
-        ),
-        dense_in_features=len(DEFAULT_INT_NAMES),
-        dense_arch_layer_sizes=list(map(int, args.dense_arch_layer_sizes.split(","))),
-        over_arch_layer_sizes=list(map(int, args.over_arch_layer_sizes.split(","))),
-        dense_device=device,
-    )
-    fused_params = {
-        "learning_rate": args.learning_rate,
-        "optimizer": OptimType.EXACT_ROWWISE_ADAGRAD if args.adagrad else OptimType.EXACT_SGD,
-    }
-    sharders = [
-        EmbeddingBagCollectionSharder(fused_params=fused_params),
-    ]
-
-    model = DistributedModelParallel(
-        module=train_model,
-        device=device,
-        sharders=cast(List[ModuleSharder[nn.Module]], sharders),
-    )
-
-    def optimizer_with_params():
-        if args.adagrad:
-            return lambda params: torch.optim.Adagrad(params, lr=args.learning_rate)
-        else:
-            return lambda params: torch.optim.SGD(params, lr=args.learning_rate)
-
-    dense_optimizer = KeyedOptimizerWrapper(
-        dict(model.named_parameters()),
-        optimizer_with_params(),
-    )
-    optimizer = CombinedOptimizer([model.fused_optimizer, dense_optimizer])
-
-    train_pipeline = TrainPipelineSparseDist(
-        model,
-        optimizer,
-        device,
-    )
-    train_val_test(
-        args, train_pipeline, train_dataloader, val_dataloader, test_dataloader
-    )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/torchrec_dlrm/dlrm_packager.py b/torchrec_dlrm/dlrm_packager.py
deleted file mode 100644
index 680c4aa..0000000
--- a/torchrec_dlrm/dlrm_packager.py
+++ /dev/null
@@ -1,136 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import argparse
-import sys
-from typing import List
-
-from modules.dlrm_predict import (
-    DLRMPredictFactory,
-    DLRMModelConfig,
-)
-from torch.package import PackageExporter
-from torchrec.datasets.criteo import DEFAULT_INT_NAMES, DEFAULT_CAT_NAMES
-from torchrec.inference.model_packager import PredictFactoryPackager
-
-# OSS Only
-
-
-class DLRMPredictFactoryPackager(PredictFactoryPackager):
-    @classmethod
-    def set_extern_modules(cls, pe: PackageExporter) -> None:
-        pe.extern(
-            [
-                "io",
-                "_imp",
-                "_ctypes",
-                "_string",
-                "numpy.**",
-                "pandas.**",
-                "pyarrow.**",
-                "six.moves",
-                "sys",
-            ]
-        )
-
-    @classmethod
-    def set_mocked_modules(cls, pe: PackageExporter) -> None:
-        pe.mock(
-            [
-                "markupsafe._speedups",
-            ]
-        )
-
-
-def parse_args(argv: List[str]) -> argparse.Namespace:
-    parser = argparse.ArgumentParser(description="torchrec dlrm model packager")
-    parser.add_argument(
-        "--num_embeddings",
-        type=int,
-        default=100_000,
-        help="max_ind_size. The number of embeddings in each embedding table. Defaults"
-        " to 100_000 if num_embeddings_per_feature is not supplied.",
-    )
-    parser.add_argument(
-        "--num_embeddings_per_feature",
-        type=str,
-        default="45833188,36746,17245,7413,20243,3,7114,1441,62,29275261,1572176,345138,"
-        "10,2209,11267,128,4,974,14,48937457,11316796,40094537,452104,12606,104,35",
-        help="Comma separated max_ind_size per sparse feature. The number of embeddings"
-        " in each embedding table. 26 values are expected for the Criteo dataset.",
-    )
-    parser.add_argument(
-        "--sparse_feature_names",
-        type=str,
-        default=",".join(DEFAULT_CAT_NAMES),
-        help="Comma separated names of the sparse features.",
-    )
-    parser.add_argument(
-        "--dense_arch_layer_sizes",
-        type=str,
-        default="512,256,64",
-        help="Comma separated layer sizes for dense arch.",
-    )
-    parser.add_argument(
-        "--over_arch_layer_sizes",
-        type=str,
-        default="512,512,256,1",
-        help="Comma separated layer sizes for over arch.",
-    )
-    parser.add_argument(
-        "--embedding_dim",
-        type=int,
-        default=64,
-        help="Size of each embedding.",
-    )
-    parser.add_argument(
-        "--num_dense_features",
-        type=int,
-        default=len(DEFAULT_INT_NAMES),
-        help="Number of dense features.",
-    )
-    parser.add_argument(
-        "--output_path",
-        type=str,
-        help="Output path of model package.",
-    )
-    return parser.parse_args(argv)
-
-
-def main(argv: List[str]) -> None:
-    """
-    Use torch.package to package the torchrec DLRM Model.
-
-    Args:
-        argv (List[str]): command line args.
-
-    Returns:
-        None.
-    """
-
-    args = parse_args(argv)
-
-    model_config = DLRMModelConfig(
-        dense_arch_layer_sizes=list(map(int, args.dense_arch_layer_sizes.split(","))),
-        dense_in_features=args.num_dense_features,
-        embedding_dim=args.embedding_dim,
-        id_list_features_keys=args.sparse_feature_names.split(","),
-        num_embeddings_per_feature=list(
-            map(int, args.num_embeddings_per_feature.split(","))
-        ),
-        num_embeddings=args.num_embeddings,
-        over_arch_layer_sizes=list(map(int, args.over_arch_layer_sizes.split(","))),
-    )
-
-    DLRMPredictFactoryPackager.save_predict_factory(
-        DLRMPredictFactory, {"config.pkl": model_config}, args.output_path
-    )
-    print(f"Package is saved to {args.output_path}")
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/torchrec_dlrm/modules/__init__.py b/torchrec_dlrm/modules/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/torchrec_dlrm/modules/dlrm_predict.py b/torchrec_dlrm/modules/dlrm_predict.py
deleted file mode 100644
index 6bfb2f9..0000000
--- a/torchrec_dlrm/modules/dlrm_predict.py
+++ /dev/null
@@ -1,156 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-from dataclasses import dataclass
-from typing import List, Dict, Optional
-
-import torch
-import torchrec.distributed as trec_dist
-from torchrec.datasets.criteo import (  # noqa
-    DEFAULT_INT_NAMES,
-    DEFAULT_CAT_NAMES,
-    CAT_FEATURE_COUNT,
-)
-from torchrec.inference.model_packager import load_pickle_config
-from torchrec.inference.modules import (
-    PredictFactory,
-    PredictModule,
-    quantize_embeddings,
-)
-from torchrec.models.dlrm import DLRM
-from torchrec.modules.embedding_configs import EmbeddingBagConfig
-from torchrec.modules.embedding_modules import EmbeddingBagCollection
-from torchrec.sparse.jagged_tensor import KeyedJaggedTensor
-
-logger: logging.Logger = logging.getLogger(__name__)
-
-# OSS Only
-
-
-@dataclass
-class DLRMModelConfig:
-    dense_arch_layer_sizes: List[int]
-    dense_in_features: int
-    embedding_dim: int
-    id_list_features_keys: List[str]
-    num_embeddings_per_feature: List[int]
-    num_embeddings: int
-    over_arch_layer_sizes: List[int]
-
-
-class DLRMPredictModule(PredictModule):
-    """
-    nn.Module to wrap DLRM model to use for inference.
-
-    Args:
-        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags
-            used to define SparseArch.
-        dense_in_features (int): the dimensionality of the dense input features.
-        dense_arch_layer_sizes (List[int]): the layer sizes for the DenseArch.
-        over_arch_layer_sizes (List[int]): the layer sizes for the OverArch. NOTE: The
-            output dimension of the InteractionArch should not be manually specified
-            here.
-        id_list_features_keys (List[str]): the names of the sparse features. Used to
-            construct a batch for inference.
-        dense_device: (Optional[torch.device]).
-    """
-
-    def __init__(
-        self,
-        embedding_bag_collection: EmbeddingBagCollection,
-        dense_in_features: int,
-        dense_arch_layer_sizes: List[int],
-        over_arch_layer_sizes: List[int],
-        id_list_features_keys: List[str],
-        dense_device: Optional[torch.device] = None,
-    ) -> None:
-        module = DLRM(
-            embedding_bag_collection=embedding_bag_collection,
-            dense_in_features=dense_in_features,
-            dense_arch_layer_sizes=dense_arch_layer_sizes,
-            over_arch_layer_sizes=over_arch_layer_sizes,
-            dense_device=dense_device,
-        )
-        super().__init__(module, dense_device)
-
-        self.id_list_features_keys: List[str] = id_list_features_keys
-
-    def predict_forward(
-        self, batch: Dict[str, torch.Tensor]
-    ) -> Dict[str, torch.Tensor]:
-        """
-        Args:
-            batch (Dict[str, torch.Tensor]): currently expects input dense features
-                to be mapped to the key "float_features" and input sparse features
-                to be mapped to the key "id_list_features".
-
-        Returns:
-            Dict[str, torch.Tensor]: output of inference.
-        """
-        try:
-            predictions = self.predict_module(
-                batch["float_features"],
-                KeyedJaggedTensor(
-                    keys=self.id_list_features_keys,
-                    lengths=batch["id_list_features.lengths"],
-                    values=batch["id_list_features.values"],
-                ),
-            )
-        except Exception as e:
-            logger.info(e)
-            raise e
-        return {"default": predictions.to(torch.device("cpu")).float()}
-
-
-class DLRMPredictFactory(PredictFactory):
-    def __init__(self) -> None:
-        self.model_config: DLRMModelConfig = load_pickle_config(
-            "config.pkl", DLRMModelConfig
-        )
-
-    def create_predict_module(self, rank: int, world_size: int) -> torch.nn.Module:
-        logging.basicConfig(level=logging.INFO)
-        device = torch.device("cuda", rank)
-        torch.cuda.set_device(device)
-        trec_dist.DistributedModelParallel.SHARE_SHARDED = True
-
-        eb_configs = [
-            EmbeddingBagConfig(
-                name=f"t_{feature_name}",
-                embedding_dim=self.model_config.embedding_dim,
-                num_embeddings=self.model_config.num_embeddings_per_feature[feature_idx]
-                if self.model_config.num_embeddings is None
-                else self.model_config.num_embeddings,
-                feature_names=[feature_name],
-            )
-            for feature_idx, feature_name in enumerate(
-                self.model_config.id_list_features_keys
-            )
-        ]
-        ebc = EmbeddingBagCollection(tables=eb_configs, device=torch.device("meta"))
-
-        module = DLRMPredictModule(
-            embedding_bag_collection=ebc,
-            dense_in_features=self.model_config.dense_in_features,
-            dense_arch_layer_sizes=self.model_config.dense_arch_layer_sizes,
-            over_arch_layer_sizes=self.model_config.over_arch_layer_sizes,
-            dense_device=device,
-        )
-        module = quantize_embeddings(module, dtype=torch.qint8, inplace=True)
-        return trec_dist.DistributedModelParallel(
-            module=module,
-            device=device,
-            env=trec_dist.ShardingEnv.from_local(world_size, rank),
-            init_data_parallel=False,
-        )
-
-    def batching_metadata(self) -> Dict[str, str]:
-        return {
-            "float_features": "dense",
-            "id_list_features": "sparse",
-        }
diff --git a/torchrec_dlrm/modules/dlrm_train.py b/torchrec_dlrm/modules/dlrm_train.py
deleted file mode 100644
index b54055d..0000000
--- a/torchrec_dlrm/modules/dlrm_train.py
+++ /dev/null
@@ -1,83 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-from typing import Tuple, Optional, List
-
-import torch
-from torch import nn
-from torchrec.datasets.utils import Batch
-from torchrec.models.dlrm import DLRM
-from torchrec.modules.embedding_modules import EmbeddingBagCollection
-
-
-class DLRMTrain(nn.Module):
-    """
-    nn.Module to wrap DLRM model to use with train_pipeline.
-
-    DLRM Recsys model from "Deep Learning Recommendation Model for Personalization and
-    Recommendation Systems" (https://arxiv.org/abs/1906.00091). Processes sparse
-    features by learning pooled embeddings for each feature. Learns the relationship
-    between dense features and sparse features by projecting dense features into the
-    same embedding space. Also, learns the pairwise relationships between sparse
-    features.
-
-    The module assumes all sparse features have the same embedding dimension
-    (i.e, each EmbeddingBagConfig uses the same embedding_dim)
-
-    Args:
-        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags
-            used to define SparseArch.
-        dense_in_features (int): the dimensionality of the dense input features.
-        dense_arch_layer_sizes (list[int]): the layer sizes for the DenseArch.
-        over_arch_layer_sizes (list[int]): the layer sizes for the OverArch. NOTE: The
-            output dimension of the InteractionArch should not be manually specified
-            here.
-        dense_device: (Optional[torch.device]).
-
-    Call Args:
-        batch: batch used with criteo and random data from torchrec.datasets
-
-    Returns:
-        Tuple[loss, Tuple[loss, logits, labels]]
-
-    Example::
-
-        ebc = EmbeddingBagCollection(config=ebc_config)
-        model = DLRMTrain(
-           embedding_bag_collection=ebc,
-           dense_in_features=100,
-           dense_arch_layer_sizes=[20],
-           over_arch_layer_sizes=[5, 1],
-        )
-    """
-
-    def __init__(
-        self,
-        embedding_bag_collection: EmbeddingBagCollection,
-        dense_in_features: int,
-        dense_arch_layer_sizes: List[int],
-        over_arch_layer_sizes: List[int],
-        dense_device: Optional[torch.device] = None,
-    ) -> None:
-        super().__init__()
-        self.model = DLRM(
-            embedding_bag_collection=embedding_bag_collection,
-            dense_in_features=dense_in_features,
-            dense_arch_layer_sizes=dense_arch_layer_sizes,
-            over_arch_layer_sizes=over_arch_layer_sizes,
-            dense_device=dense_device,
-        )
-        self.loss_fn: nn.Module = nn.BCEWithLogitsLoss()
-
-    def forward(
-        self, batch: Batch
-    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:
-        logits = self.model(batch.dense_features, batch.sparse_features)
-        logits = logits.squeeze()
-        loss = self.loss_fn(logits, batch.labels.float())
-
-        return loss, (loss.detach(), logits.detach(), batch.labels.detach())
diff --git a/torchrec_dlrm/tests/test_dlrm_main.py b/torchrec_dlrm/tests/test_dlrm_main.py
deleted file mode 100644
index b06ff3a..0000000
--- a/torchrec_dlrm/tests/test_dlrm_main.py
+++ /dev/null
@@ -1,100 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the BSD-style license found in the
-# LICENSE file in the root directory of this source tree.
-
-import os
-import tempfile
-import unittest
-import uuid
-
-from torch.distributed.launcher.api import elastic_launch, LaunchConfig
-from torchrec import test_utils
-from torchrec.datasets.test_utils.criteo_test_utils import CriteoTest
-
-from ..dlrm_main import main
-
-
-class MainTest(unittest.TestCase):
-    @classmethod
-    def _run_trainer_random(cls) -> None:
-        main(
-            [
-                "--limit_train_batches",
-                "10",
-                "--limit_val_batches",
-                "8",
-                "--limit_test_batches",
-                "6",
-                "--over_arch_layer_sizes",
-                "8,1",
-                "--dense_arch_layer_sizes",
-                "8,8",
-                "--embedding_dim",
-                "8",
-                "--num_embeddings",
-                "8",
-            ]
-        )
-
-    @test_utils.skip_if_asan
-    def test_main_function(self) -> None:
-        with tempfile.TemporaryDirectory() as tmpdir:
-            lc = LaunchConfig(
-                min_nodes=1,
-                max_nodes=1,
-                nproc_per_node=2,
-                run_id=str(uuid.uuid4()),
-                rdzv_backend="c10d",
-                rdzv_endpoint=os.path.join(tmpdir, "rdzv"),
-                rdzv_configs={"store_type": "file"},
-                start_method="spawn",
-                monitor_interval=1,
-                max_restarts=0,
-            )
-
-            elastic_launch(config=lc, entrypoint=self._run_trainer_random)()
-
-    @classmethod
-    def _run_trainer_criteo_in_memory(cls) -> None:
-        with CriteoTest._create_dataset_npys(
-            num_rows=50, filenames=[f"day_{i}" for i in range(24)]
-        ) as files:
-            main(
-                [
-                    "--over_arch_layer_sizes",
-                    "8,1",
-                    "--dense_arch_layer_sizes",
-                    "8,8",
-                    "--embedding_dim",
-                    "8",
-                    "--num_embeddings",
-                    "64",
-                    "--batch_size",
-                    "2",
-                    "--in_memory_binary_criteo_path",
-                    os.path.dirname(files[0]),
-                    "--epochs",
-                    "2",
-                ]
-            )
-
-    @test_utils.skip_if_asan
-    def test_main_function_criteo_in_memory(self) -> None:
-        with tempfile.TemporaryDirectory() as tmpdir:
-            lc = LaunchConfig(
-                min_nodes=1,
-                max_nodes=1,
-                nproc_per_node=2,
-                run_id=str(uuid.uuid4()),
-                rdzv_backend="c10d",
-                rdzv_endpoint=os.path.join(tmpdir, "rdzv"),
-                rdzv_configs={"store_type": "file"},
-                start_method="spawn",
-                monitor_interval=1,
-                max_restarts=0,
-            )
-
-            elastic_launch(config=lc, entrypoint=self._run_trainer_criteo_in_memory)()
