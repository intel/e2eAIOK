diff --git a/modelzoo/dien/train/ai-matrix/infer.sh b/modelzoo/dien/train/ai-matrix/infer.sh
index e973f6a..c8c55bd 100755
--- a/modelzoo/dien/train/ai-matrix/infer.sh
+++ b/modelzoo/dien/train/ai-matrix/infer.sh
@@ -1,29 +1,33 @@
 #!/bin/bash
 
-TOTAL_RECOMMDS=606720
+export KMP_AFFINITY=granularity=fine,verbose,compact,1,0
+export OMP_NUM_THREADS=1
 
-if [ -d results ]; then
-    mv results results_$(date +%Y%m%d%H%M%S)
-fi
-mkdir results
+/opt/intel/oneapi/intelpython/latest/envs/tensorflow/bin/python script/train.py --mode=test --advanced --slice_id=0 --batch_size=128 --num-inter-threads=4 --num-intra-threads=32 --train_path /home/vmagent/app/dataset/amazon_reviews/train/local_train_splitByUser --test_path /home/vmagent/app/dataset/amazon_reviews/valid/local_test_splitByUser --meta_path /home/vmagent/app/dataset/amazon_reviews/meta.yaml
+exit
 
-batchs='256 512 1024'
+batchs='128'
+
 
 for batch in $batchs
 do
-	echo "----------------------------------------------------------------"
-	echo "Running inference with batch size of $batch"
-	echo "----------------------------------------------------------------"
-	start=`date +%s%N`
-	python script/train.py --mode=test --batch_size=$batch |& tee results/result_infer_${batch}.txt
-	end=`date +%s%N`
-	total_time=$(((end-start)/1000000))
-    #total_time=`bc <<< "scale = 3; ($end-$start)/1000000000"`
-    total_images=$TOTAL_RECOMMDS
-    system_performance=$((1000*$total_images/$total_time))
-    echo "Total recommendations: $total_images" >> results/result_infer_${batch}.txt
-    echo "System time in miliseconds is: $total_time" >> results/result_infer_${batch}.txt
-    echo "System performance in recommendations/second is: $system_performance" >> results/result_infer_${batch}.txt
+    echo "----------------------------------------------------------------"
+    echo "Running inference with batch size of $batch"
+    echo "----------------------------------------------------------------"
+    start=`date +%s%N`
+    NUM_INSTANCES=64
+    NUM_INSTANCES_BEGIN=0
+    NUM_INSTANCES_END=${NUM_INSTANCES}
+    NUM_INSTANCES_MINUS_ONE=$((${NUM_INSTANCES}-1))
+    OUT_SHIFT=0
+    CORES_PER_INST=1
+    bash run_infer.sh ${NUM_INSTANCES_BEGIN} ${NUM_INSTANCES_END} ${OUT_SHIFT}
+    sleep 20
+
+    end=`date +%s%N`
+    total_time=$(((end-start)/1000000))
+    sleep 3
+    echo "Inference System time in miliseconds is: $total_time" | tee -a results/result_infer_${batch}.txt
 done
 
-python process_results.py --infer
+#python process_results.py --infer
diff --git a/modelzoo/dien/train/ai-matrix/run_infer.sh b/modelzoo/dien/train/ai-matrix/run_infer.sh
new file mode 100755
index 0000000..0e2bc3d
--- /dev/null
+++ b/modelzoo/dien/train/ai-matrix/run_infer.sh
@@ -0,0 +1,10 @@
+NUM_INSTANCES=$1
+NUM_INSTANCES_END=$2
+OUT_SHIFT=$3
+batch=128
+CORES_PER_INST=1
+mkdir -p results
+for (( i=${NUM_INSTANCES} ; i < ${NUM_INSTANCES_END}; i++ ))
+do
+    numactl -C$((${i} - ${NUM_INSTANCES})) --localalloc python script/train.py --mode=test --advanced --slice_id=${i} --batch_size=$batch --num-inter-threads=1 --num-intra-threads=1 2>results/result_infer_${batch}_$((${i}+${OUT_SHIFT}))_err.txt >results/result_infer_${batch}_$((${i}+${OUT_SHIFT})).txt &
+done
diff --git a/modelzoo/dien/train/ai-matrix/script/adv_data_iterator.py b/modelzoo/dien/train/ai-matrix/script/adv_data_iterator.py
new file mode 100644
index 0000000..bd27e5a
--- /dev/null
+++ b/modelzoo/dien/train/ai-matrix/script/adv_data_iterator.py
@@ -0,0 +1,145 @@
+import numpy
+import json
+import pickle as pkl
+import random
+
+import gzip
+
+import shuffle
+
+def unicode_to_utf8(d):
+    return dict((key, value) for (key,value) in d.items())
+
+def load_dict(filename):
+    try:
+        with open(filename, 'rb') as f:
+            return unicode_to_utf8(json.load(f))
+    except:
+        with open(filename, 'rb') as f:
+            data = pkl.load(f)
+            if isinstance(data, dict):
+                return unicode_to_utf8(pkl.load(f))
+            return data
+
+def fopen(filename, mode='r'):
+    if filename.endswith('.gz'):
+        return gzip.open(filename, mode)
+    return open(filename, mode)
+
+
+class AdvDataIterator:
+
+    def __init__(self, source,
+                 uid_voc,
+                 mid_voc,
+                 cat_voc,
+                 batch_size=128,
+                 maxlen=100,
+                 skip_empty=False,
+                 shuffle_each_epoch=False,
+                 sort_by_length=True,
+                 max_batch_size=20,
+                 minlen=None):
+        if shuffle_each_epoch:
+            self.source_orig = source
+            self.source = shuffle.main(self.source_orig, temporary=True)
+        else:
+            self.source = fopen(source, 'r')
+        self.source_dicts = []
+        for source_dict in [uid_voc, mid_voc, cat_voc]:
+            self.source_dicts.append(load_dict(source_dict))
+
+        self.batch_size = batch_size
+        self.maxlen = maxlen
+        self.minlen = minlen
+        self.skip_empty = skip_empty
+
+        self.n_uid = self.source_dicts[0]
+        self.n_mid = self.source_dicts[1]
+        self.n_cat = self.source_dicts[2]
+
+        self.shuffle = shuffle_each_epoch
+        self.sort_by_length = sort_by_length
+
+        self.source_buffer = []
+        self.k = batch_size * max_batch_size
+
+        self.end_of_data = False
+
+    def get_n(self):
+        return self.n_uid, self.n_mid, self.n_cat
+
+    def __iter__(self):
+        return self
+
+    def reset(self):
+        if self.shuffle:
+            self.source= shuffle.main(self.source_orig, temporary=True)
+        else:
+            self.source.seek(0)
+
+    def next(self):
+        return self.__next__()
+
+    def __next__(self):
+        if self.end_of_data:
+            self.end_of_data = False
+            self.reset()
+            raise StopIteration
+
+        source = []
+        target = []
+
+        # load next batch and sort batch by mid hist len
+        if len(self.source_buffer) == 0:
+            for k_ in range(self.k):
+                ss = self.source.readline()
+                if ss == "":
+                    break
+                self.source_buffer.append(ss.strip("\n").split("\t"))
+
+            # sort by  history behavior length
+            if self.sort_by_length:
+                his_length = numpy.array([len(s[4].split("")) for s in self.source_buffer])
+                tidx = his_length.argsort()
+
+                _sbuf = [self.source_buffer[i] for i in tidx]
+                self.source_buffer = _sbuf
+            else:
+                self.source_buffer.reverse()
+
+        # if there is no next batch
+        if len(self.source_buffer) == 0:
+            self.end_of_data = False
+            self.reset()
+            raise StopIteration
+
+        try:
+
+            # actual work here
+            while True:
+
+                # read from source file and map to word index
+                try:
+                    ss = self.source_buffer.pop()
+                except IndexError:
+                    # end of file
+                    break
+
+                # split ss hist colums
+                source.append(
+                    [ss[1], ss[2], ss[3], ss[4].split(""), ss[5].split(""),
+                    [x.split("|") for x in ss[6].split("")],
+                    [x.split("|") for x in ss[7].split("")]])
+                target.append([float(ss[0]), 1-float(ss[0])])
+
+                if len(source) >= self.batch_size or len(target) >= self.batch_size:
+                    break
+        except IOError:
+            self.end_of_data = True
+
+        # all sentence pairs in maxibatch filtered out because of length
+        if len(source) == 0 or len(target) == 0:
+            source, target = self.next()
+
+        return source, target
diff --git a/modelzoo/dien/train/ai-matrix/script/model.py b/modelzoo/dien/train/ai-matrix/script/model.py
index c22edf6..c8fa877 100644
--- a/modelzoo/dien/train/ai-matrix/script/model.py
+++ b/modelzoo/dien/train/ai-matrix/script/model.py
@@ -1,5 +1,6 @@
 import tensorflow as tf
 from tensorflow.python.ops.rnn_cell import GRUCell
+# from tensorflow.compat.v1.nn.rnn_cell import GRUCell
 from tensorflow.python.ops.rnn_cell import LSTMCell
 from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn
 #from tensorflow.python.ops.rnn import dynamic_rnn
@@ -8,6 +9,14 @@ from utils import *
 from Dice import dice
 import numpy as np
 
+from tensorflow.python.client import timeline
+from tensorflow.python.platform import gfile
+
+import horovod.tensorflow as hvd
+hvd.init()
+print('local_rank=%d, rank=%d, size=%d' % (hvd.local_rank(), hvd.rank(), hvd.size()))
+print('one_ccl is_enabled: ', hvd.ccl_built())
+
 class Model(object):
     def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, 
      data_type='FP32', use_negsampling = False, synthetic_input = False, batch_size = 32,
@@ -111,6 +120,9 @@ class Model(object):
                     self.cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cat_his_batch_ph)
                     if self.use_negsampling:
                         self.noclk_cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.noclk_cat_batch_ph)
+                    self.uid_shape = tf.shape(self.uid_batch_ph)
+                    self.mid_shape = tf.shape(self.mid_batch_ph)
+                    self.cat_shape = tf.shape(self.cat_batch_ph)
             else:
                 print('embedding on ' + device)
                 self.uid_embeddings_var = tf.compat.v1.get_variable("uid_embedding_var", [n_uid, EMBEDDING_DIM], dtype=self.model_dtype)
@@ -145,6 +157,13 @@ class Model(object):
             self.noclk_his_eb_sum_1 = tf.reduce_sum(self.noclk_his_eb, 2)
             self.noclk_his_eb_sum = tf.reduce_sum(self.noclk_his_eb_sum_1, 1)
 
+    def _sparse_to_dense_grads(self, grads_and_vars):
+        # for g, v in grads_and_vars:
+        #     print("g", g)
+        #     print("v", v)
+        #     print("g_after", tf.convert_to_tensor(g))
+        return [(tf.convert_to_tensor(g), v) for g, v in grads_and_vars]
+
     def build_fcn_net(self, inp, use_dice = False):
         def dtype_getter(getter, name, dtype=None, *args, **kwargs):
             var = getter(name, dtype=self.model_dtype, *args, **kwargs)
@@ -174,13 +193,23 @@ class Model(object):
                 if self.use_negsampling:
                     self.loss += self.aux_loss
                 tf.compat.v1.summary.scalar('loss', self.loss)
-                self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)
+                # self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)
+                # self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.loss)
+                # self.optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=self.lr, momentum=0.9).minimize(self.loss)
+
+                # convert sparse optimizer to dense optimizer
+                adam_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr * hvd.size())
+                if hvd.size() > 1:
+                    adam_optimizer = hvd.DistributedOptimizer(adam_optimizer)
+                gradients = adam_optimizer.compute_gradients(self.loss)
+                gradients = self._sparse_to_dense_grads(gradients)
+                self.optimizer = adam_optimizer.apply_gradients(gradients)
 
                 # Accuracy metric
                 self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(self.y_hat), self.target_ph), self.model_dtype))
                 tf.compat.v1.summary.scalar('accuracy', self.accuracy)
 
-            self.merged = tf.compat.v1.summary.merge_all()
+            #self.merged =  tf.compat.v1.summary.merge_all()
 
     def auxiliary_loss(self, h_states, click_seq, noclick_seq, mask, stag = None):
         def dtype_getter(getter, name, dtype=None, *args, **kwargs):
@@ -217,21 +246,49 @@ class Model(object):
         loss, accuracy, _ = sess.run([self.loss, self.accuracy, self.optimizer])
         return loss, accuracy, 0
 
-    def train(self, sess, inps):
+    def train(self, sess, inps, timeline_flag=False, options=None,run_metadata=None, step=None):
         if self.use_negsampling:
-            loss, accuracy, aux_loss, _ = sess.run([self.loss, self.accuracy, self.aux_loss, self.optimizer], feed_dict={
-                self.uid_batch_ph: inps[0],
-                self.mid_batch_ph: inps[1],
-                self.cat_batch_ph: inps[2],
-                self.mid_his_batch_ph: inps[3],
-                self.cat_his_batch_ph: inps[4],
-                self.mask: inps[5],
-                self.target_ph: inps[6],
-                self.seq_len_ph: inps[7],
-                self.lr: inps[8],
-                self.noclk_mid_batch_ph: inps[9],
-                self.noclk_cat_batch_ph: inps[10],
-            })
+            if timeline_flag:
+                loss, accuracy, aux_loss, _ = sess.run([self.loss, self.accuracy, self.aux_loss, self.optimizer],
+                    options=options,run_metadata=run_metadata,
+                    feed_dict={
+                    self.uid_batch_ph: inps[0],
+                    self.mid_batch_ph: inps[1],
+                    self.cat_batch_ph: inps[2],
+                    self.mid_his_batch_ph: inps[3],
+                    self.cat_his_batch_ph: inps[4],
+                    self.mask: inps[5],
+                    self.target_ph: inps[6],
+                    self.seq_len_ph: inps[7],
+                    self.lr: inps[8],
+                    self.noclk_mid_batch_ph: inps[9],
+                    self.noclk_cat_batch_ph: inps[10],
+                })
+
+                fetched_timeline = timeline.Timeline(run_metadata.step_stats)
+                chrome_trace = fetched_timeline.generate_chrome_trace_format()
+
+                with open('./timeline/dien_24core_1inst_train_timeline_im0525_8260_fp32_step{}_nosparse_adam_disable_noaddn_batch128_newunsortedsum_inter1_intra24_omp24_0615.json'.format(step), 'w') as f:
+                # with open('./timeline/dien_24core_1inst_train_timeline_im0525_8260_fp32_step{}_nosparse_adam_disable_noaddn_batch128_inter1_intra24_omp24_0615.json'.format(step), 'w') as f:
+                    f.write(chrome_trace)
+            else:
+                loss, accuracy, aux_loss, _, uid_shape, mid_shape, cat_shape, cat_res = sess.run([self.loss, self.accuracy, self.aux_loss, self.optimizer, self.uid_shape, self.mid_shape, self.cat_shape, self.cat_batch_ph], feed_dict={
+                    self.uid_batch_ph: inps[0],
+                    self.mid_batch_ph: inps[1],
+                    self.cat_batch_ph: inps[2],
+                    self.mid_his_batch_ph: inps[3],
+                    self.cat_his_batch_ph: inps[4],
+                    self.mask: inps[5],
+                    self.target_ph: inps[6],
+                    self.seq_len_ph: inps[7],
+                    self.lr: inps[8],
+                    self.noclk_mid_batch_ph: inps[9],
+                    self.noclk_cat_batch_ph: inps[10],
+                })
+                # print("uid shape", uid_shape)
+                # print("mid shape", mid_shape)
+                # print("cat shape", cat_shape)
+                # print("cat_res", cat_res)
             return loss, accuracy, aux_loss
         else:
             loss, accuracy, _ = sess.run([self.loss, self.accuracy, self.optimizer], feed_dict={
@@ -247,20 +304,43 @@ class Model(object):
             })
             return loss, accuracy, 0
 
-    def calculate(self, sess, inps):
+    def calculate(self, sess, inps, timeline=False, options=None,run_metadata=None):
         if self.use_negsampling:
-            probs, loss, accuracy, aux_loss = sess.run([self.y_hat, self.loss, self.accuracy, self.aux_loss], feed_dict={
-                self.uid_batch_ph: inps[0],
-                self.mid_batch_ph: inps[1],
-                self.cat_batch_ph: inps[2],
-                self.mid_his_batch_ph: inps[3],
-                self.cat_his_batch_ph: inps[4],
-                self.mask: inps[5],
-                self.target_ph: inps[6],
-                self.seq_len_ph: inps[7],
-                self.noclk_mid_batch_ph: inps[8],
-                self.noclk_cat_batch_ph: inps[9],
-            })
+            if timeline:
+                probs, loss, accuracy, aux_loss = sess.run(
+                    [self.y_hat, self.loss, self.accuracy, self.aux_loss], options=options,run_metadata=run_metadata,
+                    feed_dict={
+                    self.uid_batch_ph: inps[0],
+                    self.mid_batch_ph: inps[1],
+                    self.cat_batch_ph: inps[2],
+                    self.mid_his_batch_ph: inps[3],
+                    self.cat_his_batch_ph: inps[4],
+                    self.mask: inps[5],
+                    self.target_ph: inps[6],
+                    self.seq_len_ph: inps[7],
+                    self.noclk_mid_batch_ph: inps[8],
+                    self.noclk_cat_batch_ph: inps[9],
+                })
+            else:
+                probs, loss, accuracy, aux_loss, item_his_eb_shape, din_all_shape, d_layer_1_all_shape = sess.run(
+                    [self.y_hat, self.loss, self.accuracy, self.aux_loss, self.item_his_eb_shape, self.din_all_shape, self.d_layer_1_all_shape],
+                    feed_dict={
+                    self.uid_batch_ph: inps[0],
+                    self.mid_batch_ph: inps[1],
+                    self.cat_batch_ph: inps[2],
+                    self.mid_his_batch_ph: inps[3],
+                    self.cat_his_batch_ph: inps[4],
+                    self.mask: inps[5],
+                    self.target_ph: inps[6],
+                    self.seq_len_ph: inps[7],
+                    self.noclk_mid_batch_ph: inps[8],
+                    self.noclk_cat_batch_ph: inps[9],
+                })
+
+                # print("item_his_eb_shape", item_his_eb_shape)
+                # print("din_all_shape",din_all_shape)
+                # print("d_layer_1_all_shape",d_layer_1_all_shape)
+
             return probs, loss, accuracy, aux_loss
         else:
             probs, loss, accuracy = sess.run([self.y_hat, self.loss, self.accuracy], feed_dict={
@@ -370,7 +450,7 @@ class Model_WideDeep(Model):
             # Accuracy metric
             self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(self.y_hat), self.target_ph), tf.float32))
             tf.compat.v1.summary.scalar('accuracy', self.accuracy)
-        self.merged = tf.compat.v1.summary.merge_all()
+        #self.merged =  tf.compat.v1.summary.merge_all()
 
 
 class Model_DIN_V2_Gru_QA_attGru(Model):
@@ -454,9 +534,12 @@ class Model_DIN_V2_Gru_Vec_attGru_Neg(Model):
         with tf.compat.v1.variable_scope("dien", custom_getter=dtype_getter, dtype=self.model_dtype):
             # RNN layer(-s)
             with tf.name_scope('rnn_1'):
+                print('-----------------------------------')
+                self.item_his_eb_shape = tf.shape(self.item_his_eb)
+                #TODO(yunfei):use tf2 grucell
                 rnn_outputs, _ = dynamic_rnn(tf.compat.v1.nn.rnn_cell.GRUCell(HIDDEN_SIZE), inputs=self.item_his_eb,
                                              sequence_length=self.seq_len_ph, dtype=self.model_dtype,
-                                             scope="gru1")
+                                             scope="gru1",parallel_iterations=32)
                 tf.compat.v1.summary.histogram('GRU_outputs', rnn_outputs)
 
             aux_loss_1 = self.auxiliary_loss(rnn_outputs[:, :-1, :], self.item_his_eb[:, 1:, :],
@@ -466,7 +549,7 @@ class Model_DIN_V2_Gru_Vec_attGru_Neg(Model):
 
             # Attention layer
             with tf.name_scope('Attention_layer_1'):
-                att_outputs, alphas = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask,
+                att_outputs, alphas, din_all_shape, d_layer_1_all_shape = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask,
                                                         softmax_stag=1, stag='1_1', mode='LIST', return_alphas=True)
                 tf.compat.v1.summary.histogram('alpha_outputs', alphas)
 
@@ -474,11 +557,15 @@ class Model_DIN_V2_Gru_Vec_attGru_Neg(Model):
                 rnn_outputs2, final_state2 = dynamic_rnn(VecAttGRUCell(HIDDEN_SIZE), inputs=rnn_outputs,
                                                          att_scores = tf.expand_dims(alphas, -1),
                                                          sequence_length=self.seq_len_ph, dtype=self.model_dtype,
-                                                         scope="gru2")
+                                                         scope="gru2",parallel_iterations=32)
                 tf.compat.v1.summary.histogram('GRU2_Final_State', final_state2)
+                rnn_outputs2_shape = tf.shape(rnn_outputs2)
+                final_state2_shape = tf.shape(final_state2)
 
             inp = tf.concat([self.uid_batch_embedded, self.item_eb, self.item_his_eb_sum, self.item_eb * self.item_his_eb_sum, final_state2], 1)
             self.build_fcn_net(inp, use_dice=True)
+            self.din_all_shape = din_all_shape
+            self.d_layer_1_all_shape = d_layer_1_all_shape
 
 
 class Model_DIN_V2_Gru_Vec_attGru(Model):
diff --git a/modelzoo/dien/train/ai-matrix/script/rnn.py b/modelzoo/dien/train/ai-matrix/script/rnn.py
index dc61b74..8c3c616 100644
--- a/modelzoo/dien/train/ai-matrix/script/rnn.py
+++ b/modelzoo/dien/train/ai-matrix/script/rnn.py
@@ -668,6 +668,12 @@ def _dynamic_rnn_loop(cell,
     ValueError: If the input depth cannot be inferred via shape inference
       from the inputs.
   """
+  # tf.compat.v1.disable_v2_behavior()
+  # tf.compat.v1.disable_control_flow_v2()
+  # tf.compat.v1.disable_eager_execution()
+  # tf.compat.v1.disable_resource_variables()
+  # tf.compat.v1.disable_tensor_equality()
+  # tf.compat.v1.disable_v2_tensorshape()
   state = initial_state
   assert isinstance(parallel_iterations, int), "parallel_iterations must be int"
 
@@ -848,8 +854,8 @@ def raw_rnn(cell, loop_fn,
         time=time + 1, cell_output=output, cell_state=cell_state,
         loop_state=loop_state)
     # Emit zeros and copy forward state for minibatch entries that are finished.
-    state = tf.where(finished, state, next_state)
-    emit = tf.where(finished, tf.zeros_like(emit), emit)
+    state = array_ops.where(finished, state, next_state)
+    emit = array_ops.where(finished, tf.zeros_like(emit), emit)
     emit_ta = emit_ta.write(time, emit)
     # If any new minibatch entries are marked as finished, mark these.
     finished = tf.logical_or(finished, next_finished)
diff --git a/modelzoo/dien/train/ai-matrix/script/train.py b/modelzoo/dien/train/ai-matrix/script/train.py
index 2cd436a..ab03c02 100644
--- a/modelzoo/dien/train/ai-matrix/script/train.py
+++ b/modelzoo/dien/train/ai-matrix/script/train.py
@@ -1,5 +1,5 @@
+import pickle
 import numpy
-from data_iterator import DataIterator
 import tensorflow as tf
 from model import *
 import time
@@ -8,26 +8,66 @@ import sys
 from utils import *
 
 import argparse
+
+from tensorflow.python.client import timeline
+from tensorflow.python.platform import gfile
+
+import os
 parser = argparse.ArgumentParser()
-parser.add_argument("--mode", type=str, default='train', help="mode, train or test")
+parser.add_argument("--learning_rate", type=float, dest="lr", default=0.001)
+parser.add_argument("--batch_size", type=int, default=128, help="batch size")
+parser.add_argument("--embedding_device", type=str, default='cpu',
+                    help="synthetic input embedding layer reside on gpu or cpu")
+parser.add_argument("--data_type", type=str, default='FP32',
+                    help="data type: FP32 or FP16")
+parser.add_argument("--num_accelerators", type=int, default=1,
+                    help="number of accelerators used for training")
+parser.add_argument("--num-intra-threads", type=int,
+                    dest="num_intra_threads", default=None, help="num-intra-threads")
+parser.add_argument("--num-inter-threads", type=int,
+                    dest="num_inter_threads", default=None, help="num-inter-threads")
+
+parser.add_argument("--slice_id", type=int, nargs='?', const=True, default=0,
+                help="used to slided inference")
+parser.add_argument("--advanced", type=bool, nargs='?', const=True, default=False,
+                    help="if we use previous categorified data")
+parser.add_argument("--mode", type=str, default='train',
+                    help="mode, train or test")
 parser.add_argument("--model", type=str, default='DIEN', help="model")
 parser.add_argument("--seed", type=int, default=3, help="seed value")
-parser.add_argument("--batch_size", type=int, default=128, help="batch size")
-parser.add_argument("--data_type", type=str, default='FP32', help="data type: FP32 or FP16")
-parser.add_argument("--num_accelerators", type=int, default=1, help="number of accelerators used for training")
-parser.add_argument("--embedding_device", type=str, default='gpu', help="synthetic input embedding layer reside on gpu or cpu")
+
+parser.add_argument("--train_path", type=str, dest="train_path", default=None)
+parser.add_argument("--test_path", type=str, dest="test_path", default=None)
+parser.add_argument("--meta_path", type=str, dest="meta_path", default=None)
+parser.add_argument("--saved_path", type=str, dest="saved_path", default=None)
+
 args = parser.parse_args()
 
+if args.embedding_device == 'cpu':
+    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
+    os.environ["HOROVOD_CPU_OPERATIONS"] = "CCL"
+    os.environ["HOROVOD_CCL_CACHE"] = "1"
+    #os.environ["OMP_NUM_THREADS"] = "22"
+
+    import horovod.tensorflow as hvd
+else:
+    os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID" # see issue #152
+    os.environ["CUDA_VISIBLE_DEVICES"]="0"
+
 EMBEDDING_DIM = 18
 HIDDEN_SIZE = 18 * 2
 ATTENTION_SIZE = 18 * 2
-best_auc = 0.0
+BEST_AUC = 0.0
+TARGET_AUC = 0.82
+CURRENT_AUC = 0.0
+LOWER_THAN_CURRENT_CNT = 0
 
-TOTAL_TRAIN_SIZE = 512000
-#TOTAL_TRAIN_SIZE = 16000
+#TOTAL_TRAIN_SIZE = 512000
+TOTAL_TRAIN_SIZE = 5120000
+#TOTAL_TRAIN_SIZE = 51200
 
 
-def prepare_data(input, target, maxlen = None, return_neg = False):
+def prepare_data(input, target, maxlen=None, return_neg=False):
     # x: a list of sentences
     lengths_x = [len(s[4]) for s in input]
     seqs_mid = [inp[3] for inp in input]
@@ -61,16 +101,17 @@ def prepare_data(input, target, maxlen = None, return_neg = False):
 
         if len(lengths_x) < 1:
             return None, None, None, None
-    
+
     n_samples = len(seqs_mid)
     maxlen_x = numpy.max(lengths_x)
     neg_samples = len(noclk_seqs_mid[0][0])
 
-
     mid_his = numpy.zeros((n_samples, maxlen_x)).astype('int64')
     cat_his = numpy.zeros((n_samples, maxlen_x)).astype('int64')
-    noclk_mid_his = numpy.zeros((n_samples, maxlen_x, neg_samples)).astype('int64')
-    noclk_cat_his = numpy.zeros((n_samples, maxlen_x, neg_samples)).astype('int64')
+    noclk_mid_his = numpy.zeros(
+        (n_samples, maxlen_x, neg_samples)).astype('int64')
+    noclk_cat_his = numpy.zeros(
+        (n_samples, maxlen_x, neg_samples)).astype('int64')
     if args.data_type == 'FP32':
         data_type = 'float32'
     elif args.data_type == 'FP16':
@@ -95,325 +136,494 @@ def prepare_data(input, target, maxlen = None, return_neg = False):
     else:
         return uids, mids, cats, mid_his, cat_his, mid_mask, numpy.array(target), numpy.array(lengths_x)
 
-def eval(sess, test_data, model, model_path):
+
+def eval(sess, test_data, model, model_path, test_prepared = None):
     loss_sum = 0.
     accuracy_sum = 0.
     aux_loss_sum = 0.
     nums = 0
     stored_arr = []
     eval_time = 0
-    for src, tgt in test_data:
-        nums += 1
-        sys.stdout.flush()
-        uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src, tgt, return_neg=True)
-        # print("begin evaluation")
-        start_time = time.time()
-        prob, loss, acc, aux_loss = model.calculate(sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats])
-        end_time = time.time()
-        # print("evaluation time of one batch: %.3f" % (end_time - start_time))
-        # print("end evaluation")
-        eval_time += end_time - start_time
-        loss_sum += loss
-        aux_loss_sum = aux_loss
-        accuracy_sum += acc
-        prob_1 = prob[:, 0].tolist()
-        target_1 = target[:, 0].tolist()
-        for p ,t in zip(prob_1, target_1):
-            stored_arr.append([p, t])
-        # print("nums: ", nums)
-        # break
+    prepare_time = 0
+
+    sample_freq = 70000
+    options = tf.compat.v1.RunOptions(
+        trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
+    run_metadata = tf.compat.v1.RunMetadata()
+
+    start_prepare_time = time.time()
+    prepared_data = []
+    if test_prepared:
+        prepared_data = test_prepared
+        for data in test_prepared:
+            nums += 1
+            sys.stdout.flush()
+            uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = data
+            end_prepare_time = time.time()
+            # print("begin evaluation")
+            start_time = time.time()
+
+            if nums % sample_freq == 0:
+                prob, loss, acc, aux_loss = model.calculate(sess,
+                                                            [uids, mids, cats, mid_his, cat_his, mid_mask,
+                                                                target, sl, noclk_mids, noclk_cats],
+                                                            timeline=True, options=options, run_metadata=run_metadata)
+            else:
+                prob, loss, acc, aux_loss = model.calculate(sess,
+                                                            [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats])
+            end_time = time.time()
+            eval_time += (end_time - start_time)
+            prepare_time += (end_prepare_time - start_prepare_time)
+            loss_sum += loss
+            aux_loss_sum = aux_loss
+            accuracy_sum += acc
+            prob_1 = prob[:, 0].tolist()
+            target_1 = target[:, 0].tolist()
+            for p, t in zip(prob_1, target_1):
+                stored_arr.append([p, t])
+            start_prepare_time = time.time()
+    else:
+        for src, tgt in test_data:
+            nums += 1
+            sys.stdout.flush()
+            data = prepare_data(src, tgt, return_neg=True)
+            prepared_data.append(data)
+            uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = data
+            end_prepare_time = time.time()
+            # print("begin evaluation")
+            start_time = time.time()
+
+            if nums % sample_freq == 0:
+                prob, loss, acc, aux_loss = model.calculate(sess,
+                                                            [uids, mids, cats, mid_his, cat_his, mid_mask,
+                                                                target, sl, noclk_mids, noclk_cats],
+                                                            timeline=True, options=options, run_metadata=run_metadata)
+            else:
+                prob, loss, acc, aux_loss = model.calculate(sess,
+                                                            [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats])
+            end_time = time.time()
+            # print("evaluation time of one batch: %.3f" % (end_time - start_time))
+            # print("end evaluation")
+            eval_time += (end_time - start_time)
+            prepare_time += (end_prepare_time - start_prepare_time)
+            loss_sum += loss
+            aux_loss_sum = aux_loss
+            accuracy_sum += acc
+            prob_1 = prob[:, 0].tolist()
+            target_1 = target[:, 0].tolist()
+            for p, t in zip(prob_1, target_1):
+                stored_arr.append([p, t])
+            # print("nums: ", nums)
+            # break
+            start_prepare_time = time.time()
+
     test_auc = calc_auc(stored_arr)
     accuracy_sum = accuracy_sum / nums
     loss_sum = loss_sum / nums
     aux_loss_sum / nums
-    global best_auc
-    if best_auc < test_auc:
-        best_auc = test_auc
+    global BEST_AUC
+    if BEST_AUC < test_auc:
+        BEST_AUC = test_auc
         if args.mode == 'train':
             model.save(sess, model_path)
-    return test_auc, loss_sum, accuracy_sum, aux_loss_sum, eval_time, nums
-def train_synthetic(   
-        batch_size = 128,
-        maxlen = 100,
-        model_type = 'DNN',
-        data_type = 'FP32',
-        seed = 2,
-        n_uid = 543060,
-        n_mid = 100000 * 300,
-        n_cat = 1601,
-        embedding_device = 'gpu'      
-):
-    print("batch_size: ", batch_size)
-    print("model: ", model_type)
-    model_path = "dnn_save_path/ckpt_noshuff" + model_type + str(seed)
-    best_model_path = "dnn_best_model/ckpt_noshuff" + model_type + str(seed)
-    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
-    synthetic_input = True
-    
-    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options,log_device_placement=False)) as sess:
-        # parameters needs to put in config file
-       
-        if model_type == 'DNN':
-            model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type = data_type, 
-            synthetic_input = synthetic_input, batch_size = batch_size, max_length = maxlen, device = embedding_device)
-        elif model_type == 'PNN':
-            model = Model_PNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'Wide':
-            model = Model_WideDeep(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIN':
-            model = Model_DIN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIN-V2-gru-att-gru':
-            model = Model_DIN_V2_Gru_att_Gru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIN-V2-gru-gru-att':
-            model = Model_DIN_V2_Gru_Gru_att(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIN-V2-gru-qa-attGru':
-            model = Model_DIN_V2_Gru_QA_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIN-V2-gru-vec-attGru':
-            model = Model_DIN_V2_Gru_Vec_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
-        elif model_type == 'DIEN':
-            model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type, 
-            synthetic_input = synthetic_input, batch_size = batch_size, max_length = maxlen, device = embedding_device)
-        else:
-            print ("Invalid model_type : %s", model_type)
-            return
-        
-        sess.run(tf.compat.v1.global_variables_initializer())
-        sess.run(tf.compat.v1.local_variables_initializer())
-        sys.stdout.flush()
-        
-        iter = 0
-        train_size = 0
-        approximate_accelerator_time = 0
 
-        for itr in range(1):
-            for i in range(500):   
-                start_time = time.time()
-                _, _, _ = model.train_synthetic_input(sess)
-                end_time = time.time()
-                # print("training time of one batch: %.3f" % (end_time - start_time))
-                one_iter_time = end_time - start_time   
-                approximate_accelerator_time += one_iter_time                 
-                iter += 1
-                sys.stdout.flush()
-                if (iter % 100) == 0:
-                    print('iter: %d ----> speed: %.4f  QPS' % 
-                                        (iter, 1.0 * batch_size /one_iter_time ))    
-         
-        print("Total recommendations: %d" % (iter * batch_size))
-        print("Approximate accelerator time in seconds is %.3f" % approximate_accelerator_time)
-        print("Approximate accelerator performance in recommendations/second is %.3f" % (float(iter * batch_size)/float(approximate_accelerator_time)))
-
-     
+    global CURRENT_AUC
+    global LOWER_THAN_CURRENT_CNT
+    if test_auc > CURRENT_AUC:
+        CURRENT_AUC = test_auc
+        LOWER_THAN_CURRENT_CNT = 0
+    else:
+        print("current auc is %.4f and test auc is %.4f" % (CURRENT_AUC, test_auc))
+        LOWER_THAN_CURRENT_CNT += 1
+    return test_auc, loss_sum, accuracy_sum, aux_loss_sum, eval_time, prepare_time, nums, prepared_data
+
+
 def train(
-        train_file = "local_train_splitByUser",
-        test_file = "local_test_splitByUser",
-        uid_voc = "uid_voc.pkl",
-        mid_voc = "mid_voc.pkl",
-        cat_voc = "cat_voc.pkl",
-        batch_size = 128,
-        maxlen = 100,
-        test_iter = 100,
-        save_iter = 100,
-        model_type = 'DNN',
-        data_type = 'FP32',
-	    seed = 2,
+        train_file="local_train_splitByUser",
+        test_file="local_test_splitByUser",
+        uid_voc="uid_voc.pkl",
+        mid_voc="mid_voc.pkl",
+        cat_voc="cat_voc.pkl",
+        batch_size=128,
+        maxlen=100,
+        test_iter=500,
+        save_iter=500,
+        model_type='DNN',
+        data_type='FP32',
+    seed=2,
 ):
+    lr = 0.001 if not args.lr else args.lr
+    embedding_device = args.embedding_device
     print("batch_size: ", batch_size)
     print("model: ", model_type)
-    model_path = "dnn_save_path/ckpt_noshuff" + model_type + str(seed)
-    best_model_path = "dnn_best_model/ckpt_noshuff" + model_type + str(seed)
-    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
-    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)) as sess:
-        train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen, shuffle_each_epoch=False)
-        test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)
+    print("embedding_device", embedding_device)
+
+    if embedding_device == 'gpu':
+        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
+
+    # create model saved path
+    print(f"best model will be saved to {args.saved_path}/dnn_best_model")
+    os.makedirs(args.saved_path + "/dnn_best_model", exist_ok=True)
+    best_model_path = args.saved_path + "/dnn_best_model/ckpt_noshuff" + model_type + str(seed)
+    print(best_model_path)
+    previous_best_trained_model_path = None
+    global CURRENT_AUC
+    if os.path.exists(best_model_path + ".meta"):
+        # best retrained model exsist, start from there
+        previous_best_trained_model_path = best_model_path
+        import yaml
+        result_metrics_path = os.path.join(args.saved_path, "result.yaml")
+        # result = {"AUC": CURRENT_AUC, "training_time": train_elapse_time, "best_trained_model": f"{best_model_path}"}
+        with open(result_metrics_path) as f:
+            results = yaml.load(f, Loader=yaml.FullLoader)
+        CURRENT_AUC = results['AUC']
+        print(f"load previous best trained model, auc is {CURRENT_AUC}, path is {previous_best_trained_model_path}")
+
+    session_config = tf.compat.v1.ConfigProto()
+    if args.num_intra_threads and args.num_inter_threads:
+        session_config.intra_op_parallelism_threads = args.num_intra_threads
+        session_config.inter_op_parallelism_threads = args.num_inter_threads
+
+    session_start_time = time.time()
+    with tf.compat.v1.Session(config=session_config) as sess:
+        train_data = DataIterator(
+            train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen, shuffle_each_epoch=False)
+        test_data = DataIterator(
+            test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)
         n_uid, n_mid, n_cat = train_data.get_n()
-        print("Number of uid = %i, mid = %i, cat = %i" % (n_uid, n_mid, n_cat)) #Number of uid = 543060, mid = 367983, cat = 1601 for Amazon dataset
+        # Number of uid = 543060, mid = 367983, cat = 1601 for Amazon dataset
+        print("Number of uid = %i, mid = %i, cat = %i" % (n_uid, n_mid, n_cat))
         if model_type == 'DNN':
-            model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type = data_type, 
-            batch_size = batch_size, max_length = maxlen)
+            model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type=data_type,
+                              batch_size=batch_size, max_length=maxlen)
         elif model_type == 'PNN':
-            model = Model_PNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_PNN(n_uid, n_mid, n_cat,
+                              EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'Wide':
-            model = Model_WideDeep(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_WideDeep(n_uid, n_mid, n_cat,
+                                   EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN':
-            model = Model_DIN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN(n_uid, n_mid, n_cat,
+                              EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-att-gru':
-            model = Model_DIN_V2_Gru_att_Gru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_att_Gru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-gru-att':
-            model = Model_DIN_V2_Gru_Gru_att(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_Gru_att(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-qa-attGru':
-            model = Model_DIN_V2_Gru_QA_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_QA_attGru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-vec-attGru':
-            model = Model_DIN_V2_Gru_Vec_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_Vec_attGru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIEN':
-            model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type, 
-            batch_size = batch_size, max_length = maxlen)
+            model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type,
+                                                    batch_size=batch_size, max_length=maxlen, device=embedding_device)
         else:
-            print ("Invalid model_type : %s", model_type)
+            print("Invalid model_type : %s", model_type)
             return
-        # for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):
-        #     print("global variable dtype: ", var.dtype)
-        #     if var.dtype == 'float32_ref':
-        #         print("global variable: ", var)
-        # model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+
         sess.run(tf.compat.v1.global_variables_initializer())
         sess.run(tf.compat.v1.local_variables_initializer())
+
+        if hvd.size() > 1:
+            sess.run(hvd.broadcast_global_variables(0))
         sys.stdout.flush()
-        #print('test_auc: %.4f ---- test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f ---- eval_time: %.3f ---- num_iters: %d' % eval(sess, test_data, model, best_model_path))
         sys.stdout.flush()
 
+        if previous_best_trained_model_path:
+            model.restore(sess, previous_best_trained_model_path)
+
+        ##### Train start #####
+        # epoch is 1, train iterations stop on 1. max_num_records or 2.hit target_auc
         iter = 0
-        lr = 0.001
         train_size = 0
-        approximate_accelerator_time = 0
 
+        session_end_time = time.time()
+        session_init_elapse_time = session_end_time - session_start_time
+
+        data_load_elapse_time = 0
+        save_elapse_time = 0
+        test_elapse_time = 0
+        train_elapse_time = 0
+        test_prepare_time = 0
+        test_prepared = None
         for itr in range(1):
             loss_sum = 0.0
             accuracy_sum = 0.
             aux_loss_sum = 0.
+
+            sample_freq = 20000
+            options = tf.compat.v1.RunOptions(
+                trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
+            run_metadata = tf.compat.v1.RunMetadata()
+            nums = 0
+            elapsed_time_records = []
+
+            total_data = []
+
+            data_load_start_time = time.time()
+
+            print("Start to load Data from disk")
+            # 1. load all data
             for src, tgt in train_data:
-                
-                uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src, tgt, maxlen, return_neg=True)
+                nums += 1
+                uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(
+                    src, tgt, maxlen, return_neg=True)
+                data_load_end_time = time.time()
+                data_load_elapse_time += (data_load_end_time -
+                                          data_load_start_time)
+
+                total_data.append(
+                    [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats])
+                data_load_start_time = time.time()
+
+            print(f"Loading Data from disk is completed with {data_load_elapse_time} secs, start to train")
+            nums = 0
+            # 2. train with all data
+            for i in range(len(total_data)):
+                nums += 1
+                uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = tuple(
+                    total_data[i])
+
                 start_time = time.time()
-                loss, acc, aux_loss = model.train(sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, lr, noclk_mids, noclk_cats])
+                try:
+                    if nums == sample_freq:
+                        loss, acc, aux_loss = model.train(sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, lr, noclk_mids, noclk_cats],
+                                                          timeline_flag=True, options=options, run_metadata=run_metadata, step=nums)
+                    else:
+                        loss, acc, aux_loss = model.train(
+                            sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, lr, noclk_mids, noclk_cats])
+                    loss_sum += loss
+                    accuracy_sum += acc
+                    aux_loss_sum += aux_loss
+                except:
+                    pass
                 end_time = time.time()
-                # print("training time of one batch: %.3f" % (end_time - start_time))
-                approximate_accelerator_time += end_time - start_time
-                loss_sum += loss
-                accuracy_sum += acc
-                aux_loss_sum += aux_loss
+                train_elapse_time += (end_time - start_time)
+                elapsed_time_records.append(end_time - start_time)
+
                 iter += 1
                 train_size += batch_size
                 sys.stdout.flush()
                 if (iter % test_iter) == 0:
-                    # print("train_size: %d" % train_size)
-                    # print("approximate_accelerator_time: %.3f" % approximate_accelerator_time)
-                    print('iter: %d ----> train_loss: %.4f ---- train_accuracy: %.4f ---- train_aux_loss: %.4f' % \
-                                        (iter, loss_sum / test_iter, accuracy_sum / test_iter, aux_loss_sum / test_iter))
-                    print(' test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f ---- eval_time: %.3f ---- num_iters: %d' % eval(sess, test_data, model, best_model_path))
+                    train_time = sum(elapsed_time_records[(iter - test_iter):])
+                    print('iter: %d ----> train_loss: %.4f ---- train_accuracy: %.4f ---- train_aux_loss: %.4f ---- train_time: %.3f' %
+                          (iter, loss_sum / test_iter, accuracy_sum / test_iter, aux_loss_sum / test_iter, train_time))
+                    test_auc, loss_sum, accuracy_sum, aux_loss_sum, eval_time, prepare_time, nums, test_prepared = eval(
+                        sess, test_data, model, best_model_path, test_prepared)
+                    print(' test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f ---- eval_time: %.3f ---- num_iters: %d' %
+                            (test_auc, loss_sum, accuracy_sum, aux_loss_sum, eval_time, nums))
+                    test_elapse_time += eval_time
+                    test_prepare_time += prepare_time
                     loss_sum = 0.0
                     accuracy_sum = 0.0
                     aux_loss_sum = 0.0
-                if (iter % save_iter) == 0:
-                    print('save model iter: %d' %(iter))
-                    model.save(sess, model_path+"--"+str(iter))
+                    print(f"current auc is {CURRENT_AUC}, target auc is {TARGET_AUC}")
+
                 if train_size >= TOTAL_TRAIN_SIZE:
+                    print(f"accumulated trained num_records {train_size} is larger than TOTAL_TRAIN_SIZE {TOTAL_TRAIN_SIZE}")
+                    break
+
+                if CURRENT_AUC >= TARGET_AUC:
+                    print(f"current AUC {CURRENT_AUC} is greater than target {TARGET_AUC}, stop training")
+                    break
+                if LOWER_THAN_CURRENT_CNT >= 6:
                     break
+
+            print("iteration: ", nums)
+
             lr *= 0.5
             if train_size >= TOTAL_TRAIN_SIZE:
                 break
+
+        # train complet
+        import yaml
+        result_metrics_path = os.path.join(args.saved_path, "result.yaml")
+        result = {"AUC": CURRENT_AUC, "training_time": train_elapse_time, "best_trained_model": f"{args.saved_path}"}
+        with open(result_metrics_path, "w") as f:
+            results = yaml.dump(result, f)
+
         print("iter: %d" % iter)
         print("Total recommendations: %d" % TOTAL_TRAIN_SIZE)
-        print("Approximate accelerator time in seconds is %.3f" % approximate_accelerator_time)
-        print("Approximate accelerator performance in recommendations/second is %.3f" % (float(TOTAL_TRAIN_SIZE)/float(approximate_accelerator_time)))
+        print("process time breakdown in seconds are session_init %.3f, train_prepare %.3f, train %.3f, test_prepare %.3f, test %.3f, model save %.3f" %
+              (session_init_elapse_time, data_load_elapse_time, train_elapse_time, test_prepare_time, test_elapse_time, save_elapse_time))
 
 def test(
-        train_file = "local_train_splitByUser",
-        test_file = "local_test_splitByUser",
-        uid_voc = "uid_voc.pkl",
-        mid_voc = "mid_voc.pkl",
-        cat_voc = "cat_voc.pkl",
-        batch_size = 128,
-        maxlen = 100,
-        model_type = 'DNN',
-        data_type = 'FP32',
-	    seed = 2
+        train_file="local_train_splitByUser",
+        test_file="local_test_splitByUser",
+        uid_voc="uid_voc.pkl",
+        mid_voc="mid_voc.pkl",
+        cat_voc="cat_voc.pkl",
+        batch_size=128,
+        maxlen=100,
+        model_type='DNN',
+        data_type='FP32',
+    seed=2
 ):
     print("batch_size: ", batch_size)
     print("model: ", model_type)
     model_path = "dnn_best_model_trained/ckpt_noshuff" + model_type + str(seed)
+    print(model_path)
+
     gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
-    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)) as sess:
-        train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)
-        test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)
-        n_uid, n_mid, n_cat = train_data.get_n()
+
+    # with tf.io.gfile.GFile("/home2/yunfeima/tmp/dien/frozen_graph.pb", "rb") as f:
+    #     graph_def = tf.compat.v1.GraphDef()
+    #     graph_def.ParseFromString(f.read())
+    # with tf.Graph().as_default() as graph:
+    #     tf.import_graph_def(graph_def, name='')
+    #     # pass
+
+    # with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
+    # with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(
+    #         intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)) as sess:
+    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)
+    with tf.compat.v1.Session(config=sess_config) as sess:
+        test_file = f"{test_file}"
+        test_data = DataIterator(
+            test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)
+        n_uid, n_mid, n_cat = test_data.get_n()
         if model_type == 'DNN':
-            model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DNN(n_uid, n_mid, n_cat,
+                              EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'PNN':
-            model = Model_PNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_PNN(n_uid, n_mid, n_cat,
+                              EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'Wide':
-            model = Model_WideDeep(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_WideDeep(n_uid, n_mid, n_cat,
+                                   EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN':
-            model = Model_DIN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN(n_uid, n_mid, n_cat,
+                              EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-att-gru':
-            model = Model_DIN_V2_Gru_att_Gru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_att_Gru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-gru-att':
-            model = Model_DIN_V2_Gru_Gru_att(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_Gru_att(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-qa-attGru':
-            model = Model_DIN_V2_Gru_QA_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_QA_attGru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIN-V2-gru-vec-attGru':
-            model = Model_DIN_V2_Gru_Vec_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
+            model = Model_DIN_V2_Gru_Vec_attGru(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)
         elif model_type == 'DIEN':
-            model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type)
+            model = Model_DIN_V2_Gru_Vec_attGru_Neg(
+                n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, data_type, device='cpu')
         else:
-            print ("Invalid model_type : %s", model_type)
+            print("Invalid model_type : %s", model_type)
             return
         # for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):
         #     print("global variable: ", var)
         if data_type == 'FP32':
             model.restore(sess, model_path)
+
+            # output_node_names = ["dien/fcn/add_6",
+            #                     "dien/fcn/Metrics/add",
+            #                     "dien/fcn/Metrics/Mean_1",
+            #                     "dien/aux_loss/Mean"]
+            # output_node_names = ["dien/fcn/add_6",
+            #                     "dien/fcn/Metrics/Mean_1"]
+
+            # print(output_node_names)
+            # graph_def = tf.compat.v1.get_default_graph().as_graph_def()
+            # frozen_graph_def = tf.graph_util.convert_variables_to_constants(
+            #     sess,
+            #     sess.graph_def,
+            #     output_node_names)
+            # tf.io.write_graph(frozen_graph_def, '/home2/yunfeima/tmp/dien', 'constant_sub_node_fixed_reshape.pb',as_text=False)
+            # exit(0)
+
         if data_type == 'FP16':
-            fp32_variables = [var_name for var_name, _ in tf.contrib.framework.list_variables(model_path)]
+            fp32_variables = [var_name for var_name,
+                              _ in tf.contrib.framework.list_variables(model_path)]
             #print("fp32_variables: ", fp32_variables)
             sess.run(tf.compat.v1.global_variables_initializer())
             sess.run(tf.compat.v1.local_variables_initializer())
             for variable in tf.global_variables():
                 #print("variable: ", variable)
                 if variable.op.name in fp32_variables:
-                    var = tf.contrib.framework.load_variable(model_path, variable.op.name)
+                    var = tf.contrib.framework.load_variable(
+                        model_path, variable.op.name)
                     # print("var: ", var)
                     # print("var.dtype: ", var.dtype)
                     if(variable.dtype == 'float16_ref'):
-                        tf.add_to_collection('assignOps', variable.assign(tf.cast(var, tf.float16)))
+                        tf.add_to_collection(
+                            'assignOps', variable.assign(tf.cast(var, tf.float16)))
                         # print("var value: ", sess.run(tf.cast(var, tf.float16)))
                     else:
                         tf.add_to_collection('assignOps', variable.assign(var))
                 else:
-                    raise ValueError("Variable %s is missing from checkpoint!" % variable.op.name)
+                    raise ValueError(
+                        "Variable %s is missing from checkpoint!" % variable.op.name)
             sess.run(tf.get_collection('assignOps'))
             # for variable in sess.run(tf.get_collection('assignOps')):
             #     print("after load checkpoint: ", variable)
         # for variable in tf.global_variables():
         #     print("after load checkpoint: ", sess.run(variable))
         approximate_accelerator_time = 0
-        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, num_iters = eval(sess, test_data, model, model_path)
-        approximate_accelerator_time += eval_time
-        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f' % (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time))
-        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, num_iters = eval(sess, test_data, model, model_path)
-        approximate_accelerator_time += eval_time
-        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f' % (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time))
-        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, num_iters = eval(sess, test_data, model, model_path)
+        test_elapse_time = 0
+        prepare_elapse_time = 0
+        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, prepare_time, num_iters, test_prepared = eval(
+            sess, test_data, model, model_path)
         approximate_accelerator_time += eval_time
-        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f' % (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time))
-        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, num_iters = eval(sess, test_data, model, model_path)
-        approximate_accelerator_time += eval_time
-        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f' % (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time))
-        test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, num_iters = eval(sess, test_data, model, model_path)
-        approximate_accelerator_time += eval_time
-        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f' % (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time))
+        test_elapse_time += eval_time
+        prepare_elapse_time += prepare_time
+        print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.9f ---- test_aux_loss: %.4f ---- eval_time: %.3f ---- prepare_time: %.3f' %
+              (test_auc, test_loss, test_accuracy, test_aux_loss, eval_time, prepare_time))
         print("Total recommendations: %d" % (num_iters*batch_size))
-        print("Approximate accelerator time in seconds is %.3f" % approximate_accelerator_time)
-        print("Approximate accelerator performance in recommendations/second is %.3f" % (float(5*num_iters*batch_size)/float(approximate_accelerator_time)))
+        print("Approximate accelerator time in seconds is %.3f" %
+              approximate_accelerator_time)
+        print("Approximate accelerator performance in recommendations/second is %.3f" %
+              (float(num_iters*batch_size)/float(approximate_accelerator_time)))
+        print("Process time breakdown, prepare data took %.3f and test took %.3f, avg is prepare %.3f, test %.3f" % (
+            prepare_elapse_time, test_elapse_time, prepare_elapse_time/5, test_elapse_time/5,))
+
 
 if __name__ == '__main__':
-    if tf.__version__[0] == '2':
-        tf.compat.v1.disable_v2_behavior()
+    tf.compat.v1.disable_v2_behavior()
+    tf.compat.v1.disable_control_flow_v2()
+    tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_resource_variables()
+    tf.compat.v1.disable_tensor_equality()
+    tf.compat.v1.disable_v2_tensorshape()
     SEED = args.seed
     if tf.__version__[0] == '1':
-        tf.compat.v1.set_random_seed(SEED)  
+        tf.compat.v1.set_random_seed(SEED)
     elif tf.__version__[0] == '2':
         tf.random.set_seed(SEED)
     numpy.random.seed(SEED)
     random.seed(SEED)
+    SLICEID = args.slice_id
+    if args.advanced:
+        print("Advanced train")
+        from adv_data_iterator import AdvDataIterator as DataIterator
+    else:
+        print("Original train")
+        from data_iterator import DataIterator
+    # load meta yaml
+    meta = None
+    if os.path.exists(args.meta_path):
+        import yaml
+        with open(args.meta_path) as f:
+            m = yaml.load(f, Loader=yaml.FullLoader)
+        meta = {"uid_voc": m['uid_voc'], "mid_voc": m['mid_voc'], "cat_voc": m['cat_voc']}
+    if not meta:
+        meta = {}
+    if args.train_path:
+        meta["train_file"] = args.train_path
+    if args.test_path:
+        meta["test_file"] = args.test_path
+    meta["model_type"] = args.model
+    meta["seed"] = SEED
+    meta["batch_size"] = args.batch_size
+    meta["data_type"] = args.data_type
+    print(meta)
     if args.mode == 'train':
-        train(model_type=args.model, seed=SEED, batch_size=args.batch_size, data_type=args.data_type)
+        train(**meta)
     elif args.mode == 'test':
-        test(model_type=args.model, seed=SEED, batch_size=args.batch_size, data_type=args.data_type)
-    elif args.mode == 'synthetic':
-        train_synthetic(model_type=args.model, seed=SEED, batch_size=args.batch_size, 
-        data_type=args.data_type, embedding_device = args.embedding_device
-        ) 
+        test(**meta)
     else:
         print('do nothing...')
-
-
diff --git a/modelzoo/dien/train/ai-matrix/script/utils.py b/modelzoo/dien/train/ai-matrix/script/utils.py
index 51150d6..937c126 100644
--- a/modelzoo/dien/train/ai-matrix/script/utils.py
+++ b/modelzoo/dien/train/ai-matrix/script/utils.py
@@ -1,12 +1,11 @@
 import tensorflow as tf
 from tensorflow.python.ops.rnn_cell import *
 #from tensorflow.python.ops.rnn_cell_impl import  _Linear
-from tensorflow import keras
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import init_ops
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import variable_scope as vs
-from keras import backend as K
+from tensorflow.keras import backend as K
 
 from tensorflow.python.util import nest
 from tensorflow.python.ops import nn_ops
@@ -384,8 +383,18 @@ def din_fcn_attention(query, facts, attention_size, mask, stag='null', mode='SUM
     queries = tf.tile(query, [1, tf.shape(facts)[1]])
     queries = tf.reshape(queries, tf.shape(facts))
     din_all = tf.concat([queries, facts, queries-facts, queries*facts], axis=-1)
+
+    din_all_shape = tf.shape(din_all)
+    # din_all = tf.reshape(din_all, [-1, 144])
+    # din_all = tf.reshape(din_all, [din_all_shape[0] * din_all_shape[1], din_all_shape[2]])
+
     d_layer_1_all = tf.compat.v1.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att' + stag)
+    d_layer_1_all_shape = tf.shape(d_layer_1_all)
+    # d_layer_1_all = tf.reshape(d_layer_1_all, [-1, 80])
+
     d_layer_2_all = tf.compat.v1.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att' + stag)
+    # d_layer_2_all = tf.reshape(d_layer_2_all, [-1, 40])
+
     d_layer_3_all = tf.compat.v1.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att' + stag)
     d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(facts)[1]])
     scores = d_layer_3_all
@@ -412,7 +421,8 @@ def din_fcn_attention(query, facts, attention_size, mask, stag='null', mode='SUM
         output = facts * tf.expand_dims(scores, -1)
         output = tf.reshape(output, tf.shape(facts))
     if return_alphas:
-        return output, scores
+        return output, scores, din_all_shape, d_layer_1_all_shape
+        # return output, scores
     return output
 
 def self_attention(facts, ATTENTION_SIZE, mask, stag='null'):
diff --git a/modelzoo/dien/train/ai-matrix/start_service.sh b/modelzoo/dien/train/ai-matrix/start_service.sh
new file mode 100755
index 0000000..73003ef
--- /dev/null
+++ b/modelzoo/dien/train/ai-matrix/start_service.sh
@@ -0,0 +1,5 @@
+source /etc/profile.d/spark-env.sh
+/etc/init.d/ssh start
+$HADOOP_HOME/sbin/start-dfs.sh
+$SPARK_HOME/sbin/start-master.sh
+$SPARK_HOME/sbin/start-worker.sh spark://`hostname`:7077
