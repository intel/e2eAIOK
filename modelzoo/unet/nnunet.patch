diff -urN nnUNet/.git nnUNetNew/.git
--- nnUNet/.git	2023-02-17 12:15:29.542698700 +0000
+++ nnUNetNew/.git	1970-01-01 00:00:00.000000000 +0000
@@ -1 +0,0 @@
-gitdir: ../../../.git/modules/modelzoo/third_party/nnUNet
diff -urN nnUNet/.github/ISSUE_TEMPLATE/all-issues.md nnUNetNew/.github/ISSUE_TEMPLATE/all-issues.md
--- nnUNet/.github/ISSUE_TEMPLATE/all-issues.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/.github/ISSUE_TEMPLATE/all-issues.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,21 +0,0 @@
----
-name: All Issues
-about: Template for all types of issues
-title: ''
-labels: ''
-assignees: ''
-
----
-
-Please read the following resources before posting issues:
-
-Common questions:
-https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/common_questions.md
-
-Common Problems and their solutions:
-https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/common_problems_and_solutions.md
-
-Expected epoch times and tips on how to identify bottlenecks:
-https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/expected_epoch_times.md
-
-
diff -urN nnUNet/.gitignore nnUNetNew/.gitignore
--- nnUNet/.gitignore	2023-02-17 12:15:29.542698700 +0000
+++ nnUNetNew/.gitignore	1970-01-01 00:00:00.000000000 +0000
@@ -1,115 +0,0 @@
-# Byte-compiled / optimized / DLL files
-__pycache__/
-*.py[cod]
-*$py.class
-
-# C extensions
-*.so
-
-# Distribution / packaging
-.Python
-env/
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-*.egg-info/
-.installed.cfg
-*.egg
-
-# PyInstaller
-#  Usually these files are written by a python script from a template
-#  before PyInstaller builds the exe, so as to inject date/other infos into it.
-*.manifest
-*.spec
-
-# Installer logs
-pip-log.txt
-pip-delete-this-directory.txt
-
-# Unit test / coverage reports
-htmlcov/
-.tox/
-.coverage
-.coverage.*
-.cache
-nosetests.xml
-coverage.xml
-*,cover
-.hypothesis/
-
-# Translations
-*.mo
-*.pot
-
-# Django stuff:
-*.log
-local_settings.py
-
-# Flask stuff:
-instance/
-.webassets-cache
-
-# Scrapy stuff:
-.scrapy
-
-# Sphinx documentation
-docs/_build/
-
-# PyBuilder
-target/
-
-# IPython Notebook
-.ipynb_checkpoints
-
-# pyenv
-.python-version
-
-# celery beat schedule file
-celerybeat-schedule
-
-# dotenv
-.env
-
-# virtualenv
-venv/
-ENV/
-
-# Spyder project settings
-.spyderproject
-
-# Rope project settings
-.ropeproject
-
-*.memmap
-*.png
-*.zip
-*.npz
-*.npy
-*.jpg
-*.jpeg
-.idea
-*.txt
-.idea/*
-*.png
-*.nii.gz
-*.nii
-*.tif
-*.bmp
-*.pkl
-*.xml
-*.pkl
-*.pdf
-*.png
-*.jpg
-*.jpeg
-
-*.model
-
diff -urN nnUNet/documentation/celltrackingchallenge/MIC-DKFZ.ipynb nnUNetNew/documentation/celltrackingchallenge/MIC-DKFZ.ipynb
--- nnUNet/documentation/celltrackingchallenge/MIC-DKFZ.ipynb	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/documentation/celltrackingchallenge/MIC-DKFZ.ipynb	1970-01-01 00:00:00.000000000 +0000
@@ -1 +0,0 @@
-{"cells":[{"cell_type":"markdown","metadata":{"id":"D0uBJLr23WpA"},"source":["This notebook provides an example for training a nnUnet on the Cell Tracking Challenge dataset Fluo-N2DH-SIM"]},{"cell_type":"markdown","metadata":{"id":"74xHUq4v3pNE"},"source":["First of all, clone the repository, install and import all necessary packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836,"status":"ok","timestamp":1659343632884,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"94H69Of68ux2","outputId":"67ad2c6a-0c71-4f43-878d-de92c5e9d344"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'nnUNet'...\n","remote: Enumerating objects: 5963, done.\u001b[K\n","remote: Counting objects: 100% (77/77), done.\u001b[K\n","remote: Compressing objects: 100% (36/36), done.\u001b[K\n","remote: Total 5963 (delta 40), reused 62 (delta 34), pack-reused 5886\u001b[K\n","Receiving objects: 100% (5963/5963), 1.46 MiB | 16.38 MiB/s, done.\n","Resolving deltas: 100% (4703/4703), done.\n"]}],"source":["!git clone https://github.com/MIC-DKFZ/nnUNet.git\n","!python -c 'import torch;print(torch.backends.cudnn.version())'\n","!python -c 'import torch;print(torch.__version__)'\n","\n","!pip install nnunet\n","!pip install --upgrade git+https://github.com/FabianIsensee/hiddenlayer.git@more_plotted_details#egg=hiddenlayer\n","!pip install imagecodecs"]},{"cell_type":"markdown","metadata":{"id":"HFOKut8c4AWq"},"source":["Now connect to your Google Drive "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91562,"status":"ok","timestamp":1659084038428,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"dMyE53_y-lNl","outputId":"5023d7ef-18df-481b-dc57-009ab72e17d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"KUXBf0MRBYQx"},"source":["Start by creating the following paths in your googledrive:\n","1. ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base```\n","2. ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed```\n","3. ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_trained_models```\n","\n","Data preparation:\n","1. Obtain the data *Fluo-N2DH-SIM* from the Cell Tracking Cahllenge webpage: http://celltrackingchallenge.net/2d-datasets/\n","2. Place the data for training in the folder ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/``` and rename it to ```Fluo-N2DH-SIM+_train```\n","3. Place the data for the challenge in the folder ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/``` and rename it to ```Fluo-N2DH-SIM+_test```\n","4. Download the corresponding [Task089_Fluo-N2DH-SIM.py](https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py) file from the nnUnet repository to convert the data: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/celltrackingchallenge/instructions.md. \n","5. Place the ```Task089_Fluo-N2DH-SIM.py``` into the folder ```CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/```\n","\n","The conversion of the data to a fromat that is readable by nnU-Net will be done in the next cell.\n","\n","nnUNet_raw_data_base, nnUNet_preprocessed and RESULTS_FOLDER must also be defined in user specified previously created folders."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":634996,"status":"ok","timestamp":1659349926648,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"hGjEXSwv9pM8","outputId":"bbcaf71d-df27-4f97-e3c5-190054db067e"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n","Run the following command to install 'ipykernel' into the Python environment. \n","Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["%%bash\n","cd \"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ\"\n","export nnUNet_raw_data_base=\"nnUNet_raw_data_base\"\n","export nnUNet_preprocessed=\"nnUNet_preprocessed\"\n","export RESULTS_FOLDER=\"nnUNet_trained_models\"\n","\n","python Task089_Fluo-N2DH-SIM.py  --source_train \"Fluo-N2DH-SIM+_train\" --source_test \"Fluo-N2DH-SIM+_test\""]},{"cell_type":"markdown","metadata":{"id":"2ceSppf_4nRx"},"source":["Verify the dataset integrity (update the paths)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":638983,"status":"ok","timestamp":1659352961230,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"ppg_v3s9-RNl","outputId":"0949ef62-9b5a-4a0f-fe2a-cbc92269e8c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Please cite the following paper when using nnUNet:\n","\n","Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n","\n","\n","If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n","\n","Verifying training set\n","checking case 01_t000\n","checking case 01_t001\n","checking case 01_t002\n","checking case 01_t003\n","checking case 01_t004\n","checking case 01_t005\n","checking case 01_t006\n","checking case 01_t007\n","checking case 01_t008\n","checking case 01_t009\n","checking case 01_t010\n","checking case 01_t011\n","checking case 01_t012\n","checking case 01_t013\n","checking case 01_t014\n","checking case 01_t015\n","checking case 01_t016\n","checking case 01_t017\n","checking case 01_t018\n","checking case 01_t019\n","checking case 01_t020\n","checking case 01_t021\n","checking case 01_t022\n","checking case 01_t023\n","checking case 01_t024\n","checking case 01_t025\n","checking case 01_t026\n","checking case 01_t027\n","checking case 01_t028\n","checking case 01_t029\n","checking case 01_t030\n","checking case 01_t031\n","checking case 01_t032\n","checking case 01_t033\n","checking case 01_t034\n","checking case 01_t035\n","checking case 01_t036\n","checking case 01_t037\n","checking case 01_t038\n","checking case 01_t039\n","checking case 01_t040\n","checking case 01_t041\n","checking case 01_t042\n","checking case 01_t043\n","checking case 01_t044\n","checking case 01_t045\n","checking case 01_t046\n","checking case 01_t047\n","checking case 01_t048\n","checking case 01_t049\n","checking case 01_t050\n","checking case 01_t051\n","checking case 01_t052\n","checking case 01_t053\n","checking case 01_t054\n","checking case 01_t055\n","checking case 01_t056\n","checking case 01_t057\n","checking case 01_t058\n","checking case 01_t059\n","checking case 01_t060\n","checking case 01_t061\n","checking case 01_t062\n","checking case 01_t063\n","checking case 01_t064\n","checking case 02_t000\n","checking case 02_t001\n","checking case 02_t002\n","checking case 02_t003\n","checking case 02_t004\n","checking case 02_t005\n","checking case 02_t006\n","checking case 02_t007\n","checking case 02_t008\n","checking case 02_t009\n","checking case 02_t010\n","checking case 02_t011\n","checking case 02_t012\n","checking case 02_t013\n","checking case 02_t014\n","checking case 02_t015\n","checking case 02_t016\n","checking case 02_t017\n","checking case 02_t018\n","checking case 02_t019\n","checking case 02_t020\n","checking case 02_t021\n","checking case 02_t022\n","checking case 02_t023\n","checking case 02_t024\n","checking case 02_t025\n","checking case 02_t026\n","checking case 02_t027\n","checking case 02_t028\n","checking case 02_t029\n","checking case 02_t030\n","checking case 02_t031\n","checking case 02_t032\n","checking case 02_t033\n","checking case 02_t034\n","checking case 02_t035\n","checking case 02_t036\n","checking case 02_t037\n","checking case 02_t038\n","checking case 02_t039\n","checking case 02_t040\n","checking case 02_t041\n","checking case 02_t042\n","checking case 02_t043\n","checking case 02_t044\n","checking case 02_t045\n","checking case 02_t046\n","checking case 02_t047\n","checking case 02_t048\n","checking case 02_t049\n","checking case 02_t050\n","checking case 02_t051\n","checking case 02_t052\n","checking case 02_t053\n","checking case 02_t054\n","checking case 02_t055\n","checking case 02_t056\n","checking case 02_t057\n","checking case 02_t058\n","checking case 02_t059\n","checking case 02_t060\n","checking case 02_t061\n","checking case 02_t062\n","checking case 02_t063\n","checking case 02_t064\n","checking case 02_t065\n","checking case 02_t066\n","checking case 02_t067\n","checking case 02_t068\n","checking case 02_t069\n","checking case 02_t070\n","checking case 02_t071\n","checking case 02_t072\n","checking case 02_t073\n","checking case 02_t074\n","checking case 02_t075\n","checking case 02_t076\n","checking case 02_t077\n","checking case 02_t078\n","checking case 02_t079\n","checking case 02_t080\n","checking case 02_t081\n","checking case 02_t082\n","checking case 02_t083\n","checking case 02_t084\n","checking case 02_t085\n","checking case 02_t086\n","checking case 02_t087\n","checking case 02_t088\n","checking case 02_t089\n","checking case 02_t090\n","checking case 02_t091\n","checking case 02_t092\n","checking case 02_t093\n","checking case 02_t094\n","checking case 02_t095\n","checking case 02_t096\n","checking case 02_t097\n","checking case 02_t098\n","checking case 02_t099\n","checking case 02_t100\n","checking case 02_t101\n","checking case 02_t102\n","checking case 02_t103\n","checking case 02_t104\n","checking case 02_t105\n","checking case 02_t106\n","checking case 02_t107\n","checking case 02_t108\n","checking case 02_t109\n","checking case 02_t110\n","checking case 02_t111\n","checking case 02_t112\n","checking case 02_t113\n","checking case 02_t114\n","checking case 02_t115\n","checking case 02_t116\n","checking case 02_t117\n","checking case 02_t118\n","checking case 02_t119\n","checking case 02_t120\n","checking case 02_t121\n","checking case 02_t122\n","checking case 02_t123\n","checking case 02_t124\n","checking case 02_t125\n","checking case 02_t126\n","checking case 02_t127\n","checking case 02_t128\n","checking case 02_t129\n","checking case 02_t130\n","checking case 02_t131\n","checking case 02_t132\n","checking case 02_t133\n","checking case 02_t134\n","checking case 02_t135\n","checking case 02_t136\n","checking case 02_t137\n","checking case 02_t138\n","checking case 02_t139\n","checking case 02_t140\n","checking case 02_t141\n","checking case 02_t142\n","checking case 02_t143\n","checking case 02_t144\n","checking case 02_t145\n","checking case 02_t146\n","checking case 02_t147\n","checking case 02_t148\n","checking case 02_t149\n","Verifying label values\n","Expected label values are [0, 1, 2]\n","Labels OK\n","Verifying test set\n","Dataset OK\n","01_t014\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t015\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t016\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t017\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t018\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t019\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t020\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t040\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t041\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t042\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t043\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t044\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t045\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t046\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t096\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t097\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t098\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t099\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t100\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t101\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t102\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t000\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t001\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t002\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t003\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t004\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t005\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t006\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t056\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t057\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t058\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t059\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t060\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t061\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t062\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t047\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t048\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t049\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t050\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t051\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t052\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t053\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t103\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t104\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t105\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t106\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t107\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t108\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t109\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t007\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t008\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t009\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t010\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t011\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t012\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t013\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t063\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t064\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t000\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t001\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t002\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t003\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t004\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t054\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t055\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t056\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t057\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t058\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t059\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t060\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t110\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t111\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t112\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t113\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t114\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t115\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t116\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t028\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t029\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t030\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t031\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t032\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t033\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t034\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t026\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t027\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t028\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t029\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t030\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t031\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t032\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t082\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t083\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t084\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t085\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t086\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t087\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t088\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t145\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t146\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t147\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t148\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t149\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t035\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t036\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t037\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t038\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t039\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t040\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t041\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t012\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t013\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t014\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t015\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t016\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t017\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t018\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t061\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t062\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t063\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t064\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t065\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t066\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t067\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t117\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t118\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t119\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t120\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t121\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t122\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t123\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t021\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t022\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t023\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t024\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t025\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t026\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t027\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t005\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t006\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t007\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t008\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t009\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t010\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t011\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t075\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t076\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t077\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t078\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t079\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t080\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t081\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t124\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t125\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t126\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t127\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t128\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t129\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t130\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t042\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t043\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t044\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t045\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t046\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t047\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t048\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t033\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t034\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t035\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t036\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t037\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t038\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t039\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t089\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t090\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t091\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t092\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t093\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t094\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t095\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t131\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t132\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t133\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t134\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t135\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t136\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t137\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t049\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t050\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t051\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t052\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t053\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t054\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","01_t055\n","before crop: (5, 1, 690, 628) after crop: (5, 1, 690, 628) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t019\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t020\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t021\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t022\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t023\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t024\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t025\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t068\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t069\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t070\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t071\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t072\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t073\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t074\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t138\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t139\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t140\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t141\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t142\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t143\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","02_t144\n","before crop: (5, 1, 773, 739) after crop: (5, 1, 773, 739) spacing: [9.99e+02 1.25e-01 1.25e-01] \n","\n","\n","\n","\n"," Task089_Fluo-N2DH-SIM_thickborder_time\n","number of threads:  (8, 8) \n","\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","Are we using the nonzero mask for normalizaion? OrderedDict([(0, False), (1, False), (2, False), (3, False), (4, False)])\n","the median shape of the dataset is  [  1. 773. 739.]\n","the max shape in the dataset is  [  1. 773. 739.]\n","the min shape in the dataset is  [  1. 690. 628.]\n","we don't want feature maps smaller than  4  in the bottleneck\n","the transposed median shape of the dataset is  [  1. 773. 739.]\n","generating configuration for 3d_fullres\n","{0: {'batch_size': 5, 'num_pool_per_axis': [0, 7, 7], 'patch_size': array([  1, 896, 768]), 'median_patient_size_in_voxels': array([  1, 773, 739]), 'current_spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'original_spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [3, 3, 3]]}}\n","transpose forward [0, 1, 2]\n","transpose backward [0, 1, 2]\n","Initializing to run preprocessing\n","npz folder: /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base/nnUNet_cropped_data/Task089_Fluo-N2DH-SIM_thickborder_time\n","output_folder: /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t000.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t001.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t002.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t003.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t004.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t005.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t006.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t056.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t057.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t058.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t059.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t060.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t061.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t062.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9285\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t047.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t049.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t050.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t051.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t052.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t053.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t054.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t055.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t063.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t064.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7832\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t000.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7862\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t001.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7841\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t002.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7862\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t003.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7790\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t004.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t054.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t021.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t022.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t023.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t024.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t025.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t026.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t027.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7720\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t005.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7765\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t006.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7671\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t007.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 7554\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t008.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9567\n","2 7369\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t009.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9755\n","2 7419\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t010.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9409\n","2 7304\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t011.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t061.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t042.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t043.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t044.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t045.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t046.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t047.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t048.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7648\n","2 8645\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t033.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7530\n","2 8615\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t034.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7519\n","2 8645\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t035.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7654\n","2 8752\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t036.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7833\n","2 8958\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t037.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8118\n","2 9146\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t038.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7552\n","2 9015\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t039.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t068.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t035.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t036.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t037.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t038.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t039.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t040.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t041.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8333\n","2 7553\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t019.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8121\n","2 7478\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t020.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8104\n","2 7400\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t021.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7808\n","2 7258\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t022.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7934\n","2 7227\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t023.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7348\n","2 7456\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t024.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7034\n","2 7784\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t025.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t075.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t028.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t029.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t030.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t031.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t032.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t033.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t034.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7142\n","2 8043\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t026.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7295\n","2 8242\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t027.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8135\n","2 8477\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t028.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7232\n","2 8671\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t029.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7585\n","2 8531\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t030.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8047\n","2 8640\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t031.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7511\n","2 8452\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t032.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t089.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t007.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t008.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t009.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t010.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t011.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t012.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t013.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8988\n","2 7073\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t012.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9276\n","2 7386\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t013.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8743\n","2 7258\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t014.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8166\n","2 7449\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t015.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7838\n","2 7347\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t016.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7801\n","2 7216\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t017.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8298\n","2 7531\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t018.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t082.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t014.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t015.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t016.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t017.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t018.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t019.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/01_t020.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 7936\n","2 9385\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t040.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8461\n","2 9666\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t041.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8712\n","2 9752\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t042.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 8788\n","2 9780\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t043.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9087\n","2 9757\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t044.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9217\n","2 9951\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t045.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9444\n","2 9822\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t046.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t096.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 9526\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t048.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t049.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t050.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t051.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t052.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t053.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t103.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t104.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t105.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t106.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t107.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t108.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t109.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t055.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t056.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t057.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t058.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t059.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t060.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t110.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t111.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t112.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t113.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t114.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t115.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t116.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t090.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t091.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t092.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t093.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t094.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t095.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t083.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t084.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t085.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t086.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t087.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t088.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t145.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t146.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t147.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t148.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t149.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t062.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t063.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t064.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t065.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t066.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t067.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t117.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t118.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t119.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t120.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t121.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t122.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t123.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t076.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t077.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t078.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t079.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t080.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t081.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t124.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t125.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t126.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t127.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t128.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t129.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t130.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t069.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t070.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t071.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t072.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t073.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t074.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t131.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t132.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t133.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t134.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t135.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t136.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t137.npz\n"," \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t097.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t098.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t099.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t100.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t101.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t102.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t138.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t139.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t140.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t141.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t142.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t143.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_stage0/02_t144.npz\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","not using nonzero mask for normalization\n","Are we using the nonzero maks for normalizaion? OrderedDict([(0, False), (1, False), (2, False), (3, False), (4, False)])\n","the median shape of the dataset is  [  1. 773. 739.]\n","the max shape in the dataset is  [  1. 773. 739.]\n","the min shape in the dataset is  [  1. 690. 628.]\n","we don't want feature maps smaller than  4  in the bottleneck\n","the transposed median shape of the dataset is  [  1. 773. 739.]\n","[{'batch_size': 4, 'num_pool_per_axis': [7, 7], 'patch_size': array([896, 768]), 'median_patient_size_in_voxels': array([  1, 773, 739]), 'current_spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'original_spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n","Initializing to run preprocessing\n","npz folder: /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base/nnUNet_cropped_data/Task089_Fluo-N2DH-SIM_thickborder_time\n","output_folder: /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t000.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t001.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t002.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t003.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t004.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t005.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t006.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t056.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t057.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t058.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t059.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t060.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t061.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t062.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t021.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t022.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t023.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t024.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t025.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t026.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t027.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t063.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t064.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7832\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t000.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7862\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t001.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7841\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t002.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7862\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t003.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7790\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t004.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t049.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t050.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t051.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t052.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t053.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t054.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t055.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8988\n","2 7073\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t012.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9276\n","2 7386\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t013.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8743\n","2 7258\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t014.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8166\n","2 7449\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t015.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7838\n","2 7347\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t016.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7801\n","2 7216\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t017.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8298\n","2 7531\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t018.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t007.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t008.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t009.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t010.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t011.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t012.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t013.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7720\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t005.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7765\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t006.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7671\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t007.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 7554\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t008.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9567\n","2 7369\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t009.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9755\n","2 7419\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t010.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9409\n","2 7304\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t011.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t028.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t029.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t030.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t031.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t032.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t033.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t034.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8333\n","2 7553\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t019.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8121\n","2 7478\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t020.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8104\n","2 7400\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t021.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7808\n","2 7258\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t022.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7934\n","2 7227\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t023.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7348\n","2 7456\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t024.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7034\n","2 7784\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t025.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t035.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t036.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t037.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t038.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t039.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t040.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t041.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7648\n","2 8645\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t033.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7530\n","2 8615\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t034.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7519\n","2 8645\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t035.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7654\n","2 8752\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t036.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7833\n","2 8958\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t037.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8118\n","2 9146\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t038.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7552\n","2 9015\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t039.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t042.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t043.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t044.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t045.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t046.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t047.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t048.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7936\n","2 9385\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t040.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8461\n","2 9666\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t041.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8712\n","2 9752\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t042.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8788\n","2 9780\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t043.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9087\n","2 9757\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t044.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9217\n","2 9951\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t045.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9444\n","2 9822\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t046.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)}no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t014.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t015.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t016.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t017.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t018.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t019.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 690, 628)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 690, 628)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/01_t020.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7142\n","2 8043\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t026.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7295\n","2 8242\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t027.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8135\n","2 8477\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t028.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7232\n","2 8671\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t029.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7585\n","2 8531\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t030.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 8047\n","2 8640\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t031.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 7511\n","2 8452\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t032.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9285\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t047.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 9526\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t048.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t049.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t050.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t051.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t052.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t053.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t103.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t104.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t105.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t106.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t107.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t108.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t109.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t054.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t055.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t056.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t057.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t058.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t059.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t060.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t110.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t111.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t112.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t113.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t114.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t115.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t116.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t089.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t090.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t091.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t092.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t093.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t094.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t095.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t145.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t146.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t147.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t148.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t149.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t061.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t062.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t063.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t064.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t065.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t066.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t067.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t117.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t118.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t119.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t120.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t121.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t122.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t123.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t075.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t076.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t077.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t078.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t079.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t080.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t081.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t131.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t132.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t133.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t134.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t135.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t136.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t137.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t096.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t097.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t098.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t099.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t100.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t101.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t102.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t068.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t069.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t070.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t071.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t072.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t073.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t074.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t124.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t125.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t126.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t127.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t128.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t129.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t130.npz\n"," \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t082.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t083.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t084.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t085.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t086.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t087.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t088.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t138.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t139.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t140.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t141.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t142.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t143.npz\n","no resampling necessary\n","no resampling necessary\n","before: {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'spacing_transposed': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is transposed)': (5, 1, 773, 739)} \n","after:  {'spacing': array([9.99e+02, 1.25e-01, 1.25e-01]), 'data.shape (data is resampled)': (5, 1, 773, 739)} \n","\n","normalization...\n","normalization done\n","1 10000\n","2 10000\n","saving:  /content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed/Task089_Fluo-N2DH-SIM_thickborder_time/nnUNetData_plans_v2.1_2D_stage0/02_t144.npz\n"]}],"source":["%%bash\n","export nnUNet_raw_data_base=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base\"\n","export nnUNet_preprocessed=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed\"\n","export RESULTS_FOLDER=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_trained_models\"\n","\n","nnUNet_plan_and_preprocess -t 089 --verify_dataset_integrity"]},{"cell_type":"markdown","metadata":{"id":"yMKNWXvEn4xc"},"source":["Train the network (update the paths).\n","\n","Due to the available computing resources in GoogleCloab, training nnU-Net is not recommended (04.08.2022)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYIrVKMRzidv"},"outputs":[],"source":["%%bash\n","export nnUNet_raw_data_base=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base\"\n","export nnUNet_preprocessed=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed\"\n","export RESULTS_FOLDER=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_trained_models\"\n","\n","nnUNet_train 2d nnUNetTrainerV2 Task089_Fluo-N2DH-SIM_thickborder_time 1"]},{"cell_type":"markdown","metadata":{"id":"R1nNm_ozzo3W"},"source":["Since this training is very time-consuming in Colab  and it is possible that the session gets closed, this is an alternative to use one of the pretrained models for inference (instead of the model that was just trained here). \n","\n","List all the available pretrained models and download one of them\n","\n","(update the paths)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28554,"status":"ok","timestamp":1659355616661,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"l6hM4EJRzOGl","outputId":"32c6698a-d54c-47a3-c090-d29aeeb9a612"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Please cite the following paper when using nnUNet:\n","\n","Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n","\n","\n","If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n","\n","The following pretrained models are available:\n","\n","\n","Task001_BrainTumour\n","Brain Tumor Segmentation. \n","Segmentation targets are edema, enhancing tumor and necrosis, \n","Input modalities are 0: FLAIR, 1: T1, 2: T1 with contrast agent, 3: T2. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task002_Heart\n","Left Atrium Segmentation. \n","Segmentation target is the left atrium, \n","Input modalities are 0: MRI. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task003_Liver\n","Liver and Liver Tumor Segmentation. \n","Segmentation targets are liver and tumors, \n","Input modalities are 0: abdominal CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task004_Hippocampus\n","Hippocampus Segmentation. \n","Segmentation targets posterior and anterior parts of the hippocampus, \n","Input modalities are 0: MRI. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task005_Prostate\n","Prostate Segmentation. \n","Segmentation targets are peripheral and central zone, \n","Input modalities are 0: T2, 1: ADC. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task006_Lung\n","Lung Nodule Segmentation. \n","Segmentation target are lung nodules, \n","Input modalities are 0: abdominal CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task007_Pancreas\n","Pancreas Segmentation. \n","Segmentation targets are pancras and pancreas tumor, \n","Input modalities are 0: abdominal CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task008_HepaticVessel\n","Hepatic Vessel Segmentation. \n","Segmentation targets are hepatic vesels and liver tumors, \n","Input modalities are 0: abdominal CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task009_Spleen\n","Spleen Segmentation. \n","Segmentation target is the spleen, \n","Input modalities are 0: abdominal CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task010_Colon\n","Colon Cancer Segmentation. \n","Segmentation target are colon caner primaries, \n","Input modalities are 0: CT scan. \n","Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\n","\n","Task017_AbdominalOrganSegmentation\n","Multi-Atlas Labeling Beyond the Cranial Vault - Abdomen. \n","Segmentation targets are thirteen different abdominal organs, \n","Input modalities are 0: abdominal CT scan. \n","Also see https://www.synapse.org/#!Synapse:syn3193805/wiki/217754\n","\n","Task024_Promise\n","Prostate MR Image Segmentation 2012. \n","Segmentation target is the prostate, \n","Input modalities are 0: T2. \n","Also see https://promise12.grand-challenge.org/\n","\n","Task027_ACDC\n","Automatic Cardiac Diagnosis Challenge. \n","Segmentation targets are right ventricle, left ventricular cavity and left myocardium, \n","Input modalities are 0: cine MRI. \n","Also see https://acdc.creatis.insa-lyon.fr/\n","\n","Task029_LiTS\n","Liver and Liver Tumor Segmentation Challenge. \n","Segmentation targets are liver and liver tumors, \n","Input modalities are 0: abdominal CT scan. \n","Also see https://competitions.codalab.org/competitions/17094\n","\n","Task035_ISBILesionSegmentation\n","Longitudinal multiple sclerosis lesion segmentation Challenge. \n","Segmentation target is MS lesions, \n","input modalities are 0: FLAIR, 1: MPRAGE, 2: proton density, 3: T2. \n","Also see https://smart-stats-tools.org/lesion-challenge\n","\n","Task038_CHAOS_Task_3_5_Variant2\n","CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge (Task 3 & 5). \n","Segmentation targets are left and right kidney, liver, spleen, \n","Input modalities are 0: T1 in-phase, T1 out-phase, T2 (can be any of those)\n","Also see https://chaos.grand-challenge.org/\n","\n","Task048_KiTS_clean\n","Kidney and Kidney Tumor Segmentation Challenge. Segmentation targets kidney and kidney tumors, Input modalities are 0: abdominal CT scan. Also see https://kits19.grand-challenge.org/\n","\n","Task055_SegTHOR\n","SegTHOR: Segmentation of THoracic Organs at Risk in CT images. \n","Segmentation targets are aorta, esophagus, heart and trachea, \n","Input modalities are 0: CT scan. \n","Also see https://competitions.codalab.org/competitions/21145\n","\n","Task061_CREMI\n","MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images (Synaptic Cleft segmentation task). \n","Segmentation target is synaptic clefts, \n","Input modalities are 0: serial section transmission electron microscopy of neural tissue. \n","Also see https://cremi.org/\n","\n","Task075_Fluo_C3DH_A549_ManAndSim\n","Fluo-C3DH-A549-SIM and Fluo-C3DH-A549 datasets of the cell tracking challenge. Segmentation target are C3DH cells in fluorescence microscopy images.\n","Input modalities are 0: fluorescence_microscopy\n","Also see http://celltrackingchallenge.net/\n","\n","Task076_Fluo_N3DH_SIM\n","Fluo-N3DH-SIM dataset of the cell tracking challenge. Segmentation target are N3DH cells and cell borders in fluorescence microscopy images.\n","Input modalities are 0: fluorescence_microscopy\n","Also see http://celltrackingchallenge.net/\n","Note that the segmentation output of the models are cell center and cell border. These outputs mus tbe converted to an instance segmentation for the challenge. \n","See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task076_Fluo_N3DH_SIM.py\n","\n","Task082_BraTS2020\n","Brain tumor segmentation challenge 2020 (BraTS)\n","Segmentation targets are 0: background, 1: edema, 2: enhancing tumor, 3: necrosis\n","Input modalities are 0: T1, 1: T1ce, 2: T2, 3: FLAIR (MRI images)\n","Also see https://www.med.upenn.edu/cbica/brats2020/\n","\n","Task089_Fluo-N2DH-SIM_thickborder_time\n","Fluo-N2DH-SIM dataset of the cell tracking challenge. Segmentation target are nuclei of N2DH cells and cell borders in fluorescence microscopy images.\n","Input modalities are 0: t minus 4, 0: t minus 3, 0: t minus 2, 0: t minus 1, 0: frame of interest\n","Note that the input channels are different time steps from a time series acquisition\n","Note that the segmentation output of the models are cell center and cell border. These outputs mus tbe converted to an instance segmentation for the challenge. \n","See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py\n","Also see http://celltrackingchallenge.net/\n","\n","Task114_heart_MNMs\n","Cardiac MRI short axis images from the M&Ms challenge 2020.\n","Input modalities are 0: MRI \n","See also https://www.ub.edu/mnms/ \n","Note: Labels of the M&Ms Challenge are not in the same order as for the ACDC challenge. \n","See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task114_heart_mnms.py\n","\n","Task115_COVIDSegChallenge\n","Covid lesion segmentation in CT images. Data originates from COVID-19-20 challenge.\n","Predicted labels are 0: background, 1: covid lesion\n","Input modalities are 0: CT \n","See also https://covid-segmentation.grand-challenge.org/\n","\n","Task135_KiTS2021\n","Kidney and kidney tumor segmentation in CT images. Data originates from KiTS2021 challenge.\n","Predicted labels are 0: background, 1: kidney, 2: tumor, 3: cyst \n","Input modalities are 0: CT \n","See also https://kits21.kits-challenge.org/\n","\n","\n","Please cite the following paper when using nnUNet:\n","\n","Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n","\n","\n","If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n","\n","\n","######################################################\n","!!!!!!!!!!!!!!!!!!!!!!!!WARNING!!!!!!!!!!!!!!!!!!!!!!!\n","######################################################\n","Using the pretrained model weights is subject to the license of the dataset they were trained on. Some allow commercial use, others don't. It is your responsibility to make sure you use them appropriately! Use nnUNet_print_pretrained_model_info(task_name) to see a summary of the dataset and where to find its license!\n","######################################################\n","\n","Downloading pretrained model from url: https://zenodo.org/record/4003545/files/Task089_Fluo-N2DH-SIM_thickborder_time.zip?download=1\n","Download finished. Extracting...\n","Done\n","\n","\n","Please cite the following paper when using nnUNet:\n","\n","Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n","\n","\n","If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n","\n","Fluo-N2DH-SIM dataset of the cell tracking challenge. Segmentation target are nuclei of N2DH cells and cell borders in fluorescence microscopy images.\n","Input modalities are 0: t minus 4, 0: t minus 3, 0: t minus 2, 0: t minus 1, 0: frame of interest\n","Note that the input channels are different time steps from a time series acquisition\n","Note that the segmentation output of the models are cell center and cell border. These outputs mus tbe converted to an instance segmentation for the challenge. \n","See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py\n","Also see http://celltrackingchallenge.net/\n"]}],"source":["%%bash\n","export nnUNet_raw_data_base=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base\"\n","export nnUNet_preprocessed=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed\"\n","export RESULTS_FOLDER=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_trained_models\"\n","\n","nnUNet_print_available_pretrained_models\n","nnUNet_download_pretrained_model Task089_Fluo-N2DH-SIM_thickborder_time\n","nnUNet_print_pretrained_model_info Task089_Fluo-N2DH-SIM_thickborder_time"]},{"cell_type":"markdown","metadata":{"id":"Rpp2CQqkpHvQ"},"source":["Predict with the chosen model (update the paths)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9560,"status":"ok","timestamp":1659365058041,"user":{"displayName":"PABLO DELGADO RODRIGUEZ","userId":"11999848093282927146"},"user_tz":-120},"id":"Ht-QXttsC3tA","outputId":"63d7a418-7739-4e2c-ad6d-2b192625b678"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Please cite the following paper when using nnUNet:\n","\n","Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n","\n","\n","If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n","\n"]},{"name":"stderr","output_type":"stream","text":["usage: nnUNet_predict [-h] -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME\n","                      [-tr TRAINER_CLASS_NAME]\n","                      [-ctr CASCADE_TRAINER_CLASS_NAME] [-m MODEL]\n","                      [-p PLANS_IDENTIFIER] [-f FOLDS [FOLDS ...]] [-z]\n","                      [-l LOWRES_SEGMENTATIONS] [--part_id PART_ID]\n","                      [--num_parts NUM_PARTS]\n","                      [--num_threads_preprocessing NUM_THREADS_PREPROCESSING]\n","                      [--num_threads_nifti_save NUM_THREADS_NIFTI_SAVE]\n","                      [--disable_tta] [--overwrite_existing] [--mode MODE]\n","                      [--all_in_gpu ALL_IN_GPU] [--step_size STEP_SIZE]\n","                      [-chk CHK] [--disable_mixed_precision]\n","nnUNet_predict: error: the following arguments are required: -t/--task_name\n"]}],"source":["%%bash\n","export nnUNet_raw_data_base=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_raw_data_base\"\n","export nnUNet_preprocessed=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_preprocessed\"\n","export RESULTS_FOLDER=\"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/nnUNet_trained_models\"\n","\n","nnUNet_predict -i $nnUNet_raw_data_base/nnUNet_raw_data/Task089_Fluo-N2DH-SIM_thickborder_time/imagesTs/ -o \"/content/drive/MyDrive/CTC_trainable_solutions/Data_Pablo/MIC-DKFZ/output_2D\" -t 89 --save_npz -m 2d -f all\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPVySCYTaQDTB+X8AJpjDTk","collapsed_sections":[],"mount_file_id":"1yN6b_Oe2Niafn6SAQaKlcM14zXZiuOeS","name":"MIC-DKFZ.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
diff -urN nnUNet/documentation/celltrackingchallenge/instructions.md nnUNetNew/documentation/celltrackingchallenge/instructions.md
--- nnUNet/documentation/celltrackingchallenge/instructions.md	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/documentation/celltrackingchallenge/instructions.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,92 +0,0 @@
-# Fluo-C3DH-A549 and Fluo-C3DH-A549-SIM
-These datasets are usable by nnU-Net as is, so all we need to do is convert them into nifti format. 
-
-When participating in the competition we just merged the two datasets into one nnU-Net task and trained for them 
-together. This could have been a good idea or a horrible one. This was not evaluated (time was short and we were young).
-
-## Dataset conversion
-Open [Task075_Fluo_C3DH_A549_ManAndSim.py](../../nnunet/dataset_conversion/Task075_Fluo_C3DH_A549_ManAndSim.py), 
-adapt the paths in it and execute it with python.
-
-Note that this script will create a custom cross-validation split so that we stratify properly. Again we're not 
-sure whether this is necessary. Just roll with it.
-
-Now run planning and preprocessing with 
-`nnUNet_plan_and_preprocess -t 75 -pl2d None` (`-pl2d None` disables the 2d configuration which is not needed here!)
-
-## Training
-You can now execute nnU-Net training:
-```bash
-nnUNet_train 3d_fullres nnUNetTrainerV2 75 0
-nnUNet_train 3d_fullres nnUNetTrainerV2 75 1
-nnUNet_train 3d_fullres nnUNetTrainerV2 75 2
-nnUNet_train 3d_fullres nnUNetTrainerV2 75 3
-```
-
-## Inference
-
-Use these 4 models as an ensemble for test set prediction (`nnUNet_predict [...] -f 0 1 2 3`). You can use the 
-imagesTs folder that was created as part of the dataset converion. After that you need to 
-convert the predicted nifti images back to tiff. 
-
-Best would probably be to just use our [inference code](http://celltrackingchallenge.net/participants/DKFZ-GE/) 
-(with the pretrained weights). Or at least use it as an inspiration.
-
-# Fluo-N2DH-SIM
-This dataset is an instance segmentation problem. But nnU-Net can only do semantic segmentation. Dang. That's it. Bye.
-
-...
-
-Nah just kidding. We use an age old trick (which we didnt invent) and convert the instance segmentation task into a 
-semantic segmentation task. Semantic classes are 'cell center' and 'cell border'. This will allow us to convert back
-to instance segmentation when we are done.
-
-Ah and this dataset is also a time series and because it may be hard to distinguish cell instances in 2D without time 
-information we hack some of that good stuff in there. Essentially we stack the previous images of the frame of interest 
-in the color channel. So the input to the model is [t-4, t-3, t-2, t-1, frame_of_interest]. Stacking in color channels
-is probably not very effective but remember that we are just applying nnU-Net here and cannot change the method 
-(it's supposed to be out-of-the-box ;-) ).
-
-## Dataset conversion
-
-Open the file [Task089_Fluo-N2DH-SIM.py](../../nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py) and 
-modify the paths. Then execute it with python. This will convert the raw dataset.
-
-Then run `nnUNet_plan_and_preprocess -t 89 -pl3d None` (`-pl3d None` because this is a 2D dataset and we don't need 
-the 3d configurations).
-
-## Training
-You can now execute nnU-Net training:
-```bash
-nnUNet_train 2d nnUNetTrainerV2 89 all
-```
-
-This just trains a single model on all available training cases. No ensembling here (maybe we should have!?).
-
-## Inference
-You can use the images in `imagesTs` but honestly just use the code we provide for inference 
-[here](http://celltrackingchallenge.net/participants/DKFZ-GE/). If you choose to run inference all by yourself, 
-remember to specify the 'all' fold correctly (`nnUNet_predict [...] -f all`).
-
-# Fluo-N3DH-SIM+
-Instance segmentation just like Fluo-N2DH-SIM so we convert the instances into a two-call semantic segmentation 
-problem (cell border & center) so that we can solve it with nnU-Net. No messing with time information because 3D is 
-[noice](https://thumbs.gfycat.com/CoordinatedEnergeticChipmunk-size_restricted.gif)! 
-
-## Dataset conversion
-Open the file [Task076_Fluo_N3DH_SIM.py](../../nnunet/dataset_conversion/Task076_Fluo_N3DH_SIM.py) and 
-modify the paths. Then execute it with python. This will convert the raw dataset.
-
-Then run `nnUNet_plan_and_preprocess -t 76 -pl2d None` (`-pl2d None` because this is a 3D dataset and we don't need the 
-2d configurations).
-
-## Training
-You can now execute nnU-Net training:
-```bash
-nnUNet_train 3d_fullres nnUNetTrainerV2 76 all
-```
-
-## Inference
-You can use the images in `imagesTs` but honestly just use the code we provide for inference 
-[here](http://celltrackingchallenge.net/participants/DKFZ-GE/). If you choose to run inference all by yourself, 
-remember to specify the 'all' fold correctly (`nnUNet_predict [...] -f all`).
diff -urN nnUNet/documentation/common_problems_and_solutions.md nnUNetNew/documentation/common_problems_and_solutions.md
--- nnUNet/documentation/common_problems_and_solutions.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/common_problems_and_solutions.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,117 +0,0 @@
-# Common Issues and their Solutions
-
-- [Common Issues and their Solutions](#common-issues-and-their-solutions)
-  * [RuntimeError: Expected scalar type half but found float](#runtimeerror--expected-scalar-type-half-but-found-float)
-  * [nnU-Net gets 'stuck' during preprocessing, training or inference](#nnu-net-gets--stuck--during-preprocessing--training-or-inference)
-  * [nnU-Net training: RuntimeError: CUDA out of memory](#nnu-net-training--runtimeerror--cuda-out-of-memory)
-  * [nnU-Net training in Docker container: RuntimeError: unable to write to file </torch_781_2606105346>](#nnu-net-training-in-docker-container--runtimeerror--unable-to-write-to-file---torch-781-2606105346-)
-  * [Downloading pretrained models: unzip: cannot find zipfile directory in one of /home/isensee/.nnunetdownload_16031094034174126](#downloading-pretrained-models--unzip--cannot-find-zipfile-directory-in-one-of--home-isensee-nnunetdownload-16031094034174126)
-  * [Downloading pre-trained models: `unzip: 'unzip' is not recognized as an internal or external command` OR `Command 'unzip' not found`](#downloading-pre-trained-models---unzip---unzip--is-not-recognized-as-an-internal-or-external-command--or--command--unzip--not-found-)
-  * [nnU-Net training (2D U-Net): High (and increasing) system RAM usage, OOM](#nnu-net-training--2d-u-net---high--and-increasing--system-ram-usage--oom)
-  * [nnU-Net training of cascade: Error `seg from prev stage missing`](#nnu-net-training-of-cascade--error--seg-from-prev-stage-missing-)
-  * [nnU-Net training: `RuntimeError: CUDA error: device-side assert triggered`](#nnu-net-training---runtimeerror--cuda-error--device-side-assert-triggered-)
-  * [nnU-Net training: Error: mmap length is greater than file size and EOFError](#nnu-net-training--error--mmap-length-is-greater-than-file-size-and-eoferror)
-  * [running nnU-Net on Azure instances](#running-nnu-net-on-azure-instances)
-
-## RuntimeError: Expected scalar type half but found float
-
-This can happen when running inference (or training) with mixed precision enabled on older GPU hardware. It points 
-to some operation not being implemented in half precision for the type of GPU you are using. There are flags to enforce
- the use of fp32 for both nnUNet_predict and nnUNet_train. If you run into this error, using these flags will probably 
- solve it. See `nnUNet_predict -h` and `nnUNet_train -h` for what the flags are.
-
-## nnU-Net gets 'stuck' during preprocessing, training or inference
-nnU-Net uses python multiprocessing to leverage multiple CPU cores during preprocessing, background workers for data 
-augmentation in training, preprocessing of cases during inference as well as resampling and exporting the final 
-predictions during validation and inference. Unfortunately, python (or maybe it is just me as a programmer) is not 
-very good at communicating errors that happen in background workers, causing the main process to indefinitely wait for 
-them to return indefinitely.
-
-Whenever nnU-Net appears to be stuck, this is what you should do:
-
-1) There is almost always an error message which will give you an indication of what the problem is. This error message 
-is often not at the bottom of the text output, but further up. If you run nnU-Net on a GPU cluster (like we do) the 
-error message may be WAYYYY off in the log file, sometimes at the very start of the training/inference. Locate the 
-error message (if necessary copy the stdout to a text editor and search for 'error')
-
-2) If there is no error message, this could mean that your OS silently killed a background worker because it was about 
-to go out of memory. In this case, please rerun whatever command you have been running and closely monitor your system 
-RAM (not GPU memory!) usage. If your RAM is full or close to full, you need to take action:
-   - reduce the number of background workers: use `-tl` and `-tf` in `nnUNet_plan_and_preprocess` (you may have to 
-   go as low as 1!). Reduce the number of workers used by `nnUNet_predict` by reducing `--num_threads_preprocessing` and 
-   `--num_threads_nifti_save`.
-   - If even `-tf 1` during preprocessing is not low enough, consider adding a swap partition located on an SSD.
-   - upgrade your RAM! (32 GB should get the job done)
-
-
-## nnU-Net training: RuntimeError: CUDA out of memory
-
-This section is dealing with error messages such as this:
-
-```
-RuntimeError: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 10.76 GiB total capacity; 2.82 GiB already allocated; 4.18 GiB free; 4.33 GiB reserved in total by PyTorch)
-```
-
-This message appears when the GPU memory is insufficient. For most datasets, nnU-Net uses about 8GB of video memory. 
-To ensure that you can run all trainings, we recommend to use a GPU with at least 11GB (this will have some headroom).
-If you are running other programs on the GPU you intend to train on (for example the GUI of your operating system), 
-the amount of VRAM available to nnU-Net is less than whatever is on your GPU. Please close all unnecessary programs or 
-invest in a second GPU. We for example like to use a low cost GPU (GTX 1050 or slower) for the display outputs while 
-having the 2080ti (or equivelant) handle the training.
-
-At the start of each training, cuDNN will run some benchmarks in order to figure out the fastest convolution algorithm 
-for the current network architecture (we use `torch.backends.cudnn.benchmark=True`). VRAM consumption will jump all over
-the place while these benchmarks run and can briefly exceed the 8GB nnU-Net typically requires. If you keep running into
- `RuntimeError: CUDA out of memory` problems you may want to consider disabling that. You can do so by setting the 
- `--deterministic` flag when using `nnUNet_train`. Setting this flag can slow down your training, so it is recommended 
- to only use it if necessary.
- 
-## nnU-Net training in Docker container: RuntimeError: unable to write to file </torch_781_2606105346>
-
-Nvidia NGC (https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) is a great place to find Docker containers with 
-the most recent software (pytorch, cuDNN, etc.) in them. When starting Docker containers with command provided on the 
-Nvidia website, the docker will crash with errors like this when running nnU-Net: `RuntimeError: unable to write to 
-file </torch_781_2606105346>`. Please start the docker with the `--ipc=host` flag to solve this.
-
-## Downloading pretrained models: unzip: cannot find zipfile directory in one of /home/isensee/.nnunetdownload_16031094034174126
-
-Sometimes downloading the large zip files containing our pretrained models can fail and cause the error above. Please 
-make sure to use the most recent nnU-Net version (we constantly try to improve the downloading). If that does not fix it
-you can always download the zip file from our zenodo (https://zenodo.org/record/4003545) and use the 
-`nnUNet_install_pretrained_model_from_zip` command to install the model.
-
-## Downloading pre-trained models: `unzip: 'unzip' is not recognized as an internal or external command` OR `Command 'unzip' not found`
-
-On Windows systems and on a bare WSL2 system, the `unzip` command may not be present.
-Either install it, unzip the pre-trained model from zenodo download, or update to a newer version of nnUNet that uses the Python build in
-(https://docs.python.org/3/library/zipfile.html)
-
-## nnU-Net training (2D U-Net): High (and increasing) system RAM usage, OOM
-
-There was a issue with mixed precision causing a system RAM memory leak. This is fixed when using cuDNN 8.0.2 or newer, 
-but the current pytorch master comes with cuDNN 7.6.5. If you encounter this problem, please consider using Nvidias NGC 
-pytorch container for training (the pytorch it comes with has a recent cuDNN version). You can also install the new 
-cuDNN version on your system and compile pytorch yourself (instructions on the pytorch website!). This is what we do at DKFZ.
-
-
-## nnU-Net training of cascade: Error `seg from prev stage missing` 
-You need to run all five folds of `3d_lowres`. Segmentations of the previous stage can only be generated from the 
-validation set, otherwise we would overfit.
-
-## nnU-Net training: `RuntimeError: CUDA error: device-side assert triggered`
-This error often goes along with something like `void THCudaTensor_scatterFillKernel(TensorInfo<Real, IndexType>, 
-TensorInfo<long, IndexType>, Real, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = -1]: 
-block: [4770,0,0], thread: [374,0,0] Assertion indexValue >= 0 && indexValue < tensor.sizes[dim] failed.`.
-
-This means that your dataset contains unexpected values in the segmentations. nnU-Net expects all labels to be 
-consecutive integers. So if your dataset has 4 classes (background and three foregound labels), then the labels 
-must be 0, 1, 2, 3 (where 0 must be background!). There cannot be any other values in the ground truth segmentations.
-
-If you run `nnUNet_plan_and_preprocess` with the `--verify_dataset_integrity` option, this should never happen because 
-it will check for wrong values in the label images.
-
-## nnU-Net training: Error: mmap length is greater than file size and EOFError
-Please delete all .npy files in the nnUNet_preprocessed folder of the test you were trying to train. Then try again.
-
-## running nnU-Net on Azure instances
-see https://github.com/MIC-DKFZ/nnUNet/issues/437, thank you @Alaska47
\ No newline at end of file
diff -urN nnUNet/documentation/common_questions.md nnUNetNew/documentation/common_questions.md
--- nnUNet/documentation/common_questions.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/common_questions.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,214 +0,0 @@
-# FAQ
-
-- [FAQ](#faq)
-  - [Where can I find the segmentation metrics of my experiments?](#where-can-i-find-the-segmentation-metrics-of-my-experiments)
-  - [What postprocessing is selected?](#what-postprocessing-is-selected)
-  - [Evaluating test set results](#evaluating-test-set-results)
-  - [Creating and managing data splits](#creating-and-managing-data-splits)
-  - [How can I swap component XXX (for example the loss) of nnU-Net?](#how-can-i-swap-component-xxx-for-example-the-loss-of-nnu-net)
-  - [How does nnU-Net handle multi-modal images?](#how-does-nnu-net-handle-multi-modal-images)
-  - [Why does nnU-Net not use all my GPU memory?](#why-does-nnu-net-not-use-all-my-gpu-memory)
-  - [Do I need to always run all U-Net configurations?](#do-i-need-to-always-run-all-u-net-configurations)
-  - [Sharing Models](#sharing-models)
-  - [Can I run nnU-Net on smaller GPUs?](#can-i-run-nnu-net-on-smaller-gpus)
-  - [Why is no 3d_lowres model created?](#why-is-no-3d_lowres-model-created)
-
-## Where can I find the segmentation metrics of my experiments?
-**Results for the validation sets of each fold** are stored in the respective output folder after the training is completed. For example, this could be. 
-`${RESULTS_FOLDER}/nnUNet/3d_fullres/Task003_Liver/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0`. After training there will
- be a `validation_raw` subfolder and a `validation_raw_postprocessed` subfolder. In each of these folders is going to 
- be a `summary.json` file with the segmentation metrics. There are metrics for each individual validation case and then 
- at the bottom there is also a mean across all cases. 
- 
-**Cross-validation metrics** can only be computed after all five folds were run. You first need to run
-`nnUNet_determine_postprocessing` first (see `nnUNet_determine_postprocessing -h` for help). This will collect the 
-predictions from the validation sets of the five folds, compute metrics on them and then determine the postprocessing. 
-Once this is all done, there will be new folders located in the output directory (for example 
-`${RESULTS_FOLDER}/nnUNet/3d_fullres/Task003_Liver/nnUNetTrainerV2__nnUNetPlansv2.1/`): `cv_niftis_raw` (raw predictions 
-from the cross-validation) and `cv_niftis_postprocessed` (postprocessed predictions). In each of these folders is 
-going to be a `summary.json` file with the metrics (see above).
-
-Note that the postprocessing determined on each individual fold is completely ignored by nnU-Net because it needs to 
-find a single postprocessing configuration for the whole cross-validation. The postprocessed results in each fold are 
-just for development purposes!
-
-**Test set results** see [here](#evaluating-test-set-results).
-
-**Ensemble performance** will be accessible here `${RESULTS_FOLDER}/nnUNet/ensembles/TASKNAME` after you ran 
-`nnUNet_find_best_configuration`. There are summary.csv for a quick overview and then there is also going to be 
-detailed results in the form of `summary.json` in the respective subfolders.
-
-## What postprocessing is selected?
-After you run `nnUNet_determine_postprocessing` (see `nnUNet_determine_postprocessing -h` for help) there will be a 
-`postprocessing.json` file located in the output directory of your training (for example 
-`${RESULTS_FOLDER}/nnUNet/3d_fullres/Task003_Liver/nnUNetTrainerV2__nnUNetPlansv2.1/`). If you open this with a text 
-editor, there is a key "for_which_classes", followed by some list. For LiTS (classes 0: bg, 1: liver, 2: tumor) 
-this can for example be:
-```python
-    "for_which_classes": [
-        [
-            1,
-            2
-        ],
-        1
-```
-This means that nnU-Net will first remove all but the largest components for the merged object consisting of classes 
-1 and 2 (essentially the liver including the tumors) and then in a second step also remove all but the largest 
-connected component for the liver class.
-
-Note that you do not have to run `nnUNet_determine_postprocessing` if you use `nnUNet_find_best_configuration`. 
-`nnUNet_find_best_configuration` will do that for you.
-
-Ensemble results and postprocessing will be stored in `${RESULTS_FOLDER}/nnUNet/ensembles` 
-(this will all be generated by `nnUNet_find_best_configuration`).
-
-## Evaluating test set results
-This feature was only added recently. Please run `pip install --upgrade nnunet` or reinstall nnunet from the master.
-
-You can now use `nnUNet_evaluate_folder` to compute metrics on predicted test cases. For example:
-
-```
-nnUNet_evaluate_folder -ref FOLDER_WITH_GT -pred FOLDER_WITH_PREDICTIONS -l 1 2 3 4
-```
-
-This example is for a dataset that has 4 foreground classes (labels 1, 2, 3, 4). `FOLDER_WITH_GT` and 
-`FOLDER_WITH_PREDICTIONS` must contain files with the same names containing the reference and predicted segmentations 
-of each case, respectivelty. The files must be nifti (end with .nii.gz).
-
-## Creating and managing data splits
-
-At the start of each training, nnU-Net will check whether the splits_final.pkl file is present in the directory where 
-the preprocessed data of the requested dataset is located. If the file is not present, nnU-Net will create its own 
-split: a five-fold cross-validation using all the available training cases. nnU-Net needs this five-fold 
-cross-validation to be able to determine the postprocessing and to run model/ensemble selection.
-
-There are however situations in which you may want to create your own split, for example
-- in datasets like ACDC where several training cases are connected (there are two time steps for each patient) you 
-may need to manually create splits to ensure proper stratification.
-- cases are annotated by multiple annotators and you would like to use the annotations as separate training examples
-- if you are running experiments with a domain transfer, you might want to train only on cases from domain A and 
-validate on domain B
-- ...
-
-Creating your own data split is simple: the splits_final.pkl file contains the following data structure (assume there are five training cases A, B, C, D, and E):
-```python
-splits = [
-    {'train': ['A', 'B', 'C', 'D'], 'val': ['E']},
-    {'train': ['A', 'B', 'C', 'E'], 'val': ['D']},
-    {'train': ['A', 'B', 'D', 'E'], 'val': ['C']},
-    {'train': ['A', 'C', 'D', 'E'], 'val': ['B']},
-    {'train': ['B', 'C', 'D', 'E'], 'val': ['A']}
-]
-```
-
-Use load_pickle and save_pickle from batchgenerators.utilities.file_and_folder_operations for loading/storing the splits.
-
-Splits is a list of length NUMBER_OF_FOLDS. Each entry in the list is a dict, with 'train' and 'val' as keys and lists 
-of the corresponding case names (without the _0000 etc!) as values.
-
-nnU-Net's five-fold cross validation will always create a list of len(splits)=5. But you can do whatever you want. Note 
-that if you define only 4 splits (fold 0-3) and then set fold=4 when training (that would be the fifth split), 
-nnU-Net will print a warning and proceed to use a random 80:20 data split. 
-
-## How can I swap component XXX (for example the loss) of nnU-Net?
-
-All changes in nnU-Net are handled the same way:
-
-1) create a new nnU-Net trainer class. Place the file somewhere in the nnunet.training.network_training folder 
-(any subfolder will do. If you create a new subfolder, make sure to include an empty `__init__.py` file!)
-
-2) make your new trainer class derive from the trainer you would like to change (most likely this is going to be nnUNetTrainerV2)
-
-3) identify the function that you need to overwrite. You may have to go up the inheritance hierarchy to find it!
-
-4) overwrite that function in your custom trainer, make sure whatever you do is compatible with the rest of nnU-Net
-
-What these changes need to look like specifically is hard to say without knowing what you are exactly trying to do. 
-Before you open a new issue on GitHub, please have a look around the `nnunet.training.network_training` folder first! 
-There are tons of examples modifying various parts of the pipeline.
-
-Also see [here](extending_nnunet.md)
-
-## How does nnU-Net handle multi-modal images?
-
-Multi-modal images are treated as color channels. BraTS, which comes with T1, T1c, T2 and Flair images for each 
-training case will thus for example have 4 input channels.
-
-## Why does nnU-Net not use all my GPU memory?
-
-nnU-net and all its parameters are optimized for a training setting that uses about 8GB of VRAM for a network training. 
-Using more VRAM will not speed up the training. Using more VRAM has also not (yet) been beneficial for model 
-performance consistently enough to make that the default. If you really want to train with more VRAM, you can do one of these things:
-
-1) Manually edit the plans files to increase the batch size. A larger batch size gives better (less noisy) gradients 
-and may improve your model performance if the dataset is large. Note that nnU-Net always runs for 1000 epochs with 250 
-iterations each (250000 iterations). The training time thus scales approximately linearly with the batch size 
-(batch size 4 is going to need twice as long for training than batch size 2!)
-
-2) Manually edit the plans files to increase the patch size. This one is tricky and should only been attempted if you 
-know what you are doing! Again, training times will be increased if you do this! 3) is a better way of increasing the 
-patch size.
-
-3) Run `nnUNet_plan_and_preprocess` with a larger GPU memory budget. This will make nnU-Net plan for larger patch sizes 
-during experiment planning. Doing this can change the patch size, network topology, the batch size as well as the 
-presence of the U-Net cascade. To run with a different memory budget, you need to specify a different experiment planner, for example
-`nnUNet_plan_and_preprocess -t TASK_ID -pl2d None -pl3d ExperimentPlanner3D_v21_32GB` (note that `-pl2d None` will 
-disable 2D U-Net configuration. There is currently no planner for larger 2D U-Nets). We have planners for 8 GB (default), 
-11GB and 32GB available. If you need a planner for a different GPU size, you should be able to quickly hack together 
-your own using the code of the 11GB or 32GB planner (same goes for a 2D planner). Note that we have experimented with 
-these planners and not found an increase in segmentation performance as a result of using them. Training times are 
-again longer than with the default.
-
-## Do I need to always run all U-Net configurations?
-The model training pipeline above is for challenge participations. Depending on your task you may not want to train all 
-U-Net models and you may also not want to run a cross-validation all the time.
-Here are some recommendations about what U-Net model to train:
-- It is safe to say that on average, the 3D U-Net model (3d_fullres) was most robust. If you just want to use nnU-Net because you 
-need segmentations, I recommend you start with this.
-- If you are not happy with the results from the 3D U-Net then you can try the following:
-  - if your cases are very large so that the patch size of the 3d U-Net only covers a very small fraction of an image then 
-  it is possible that the 3d U-Net cannot capture sufficient contextual information in order to be effective. If this 
-  is the case, you should consider running the 3d U-Net cascade (3d_lowres followed by 3d_cascade_fullres)
-  - If your data is very anisotropic then a 2D U-Net may actually be a better choice (Promise12, ACDC, Task05_Prostate 
-  from the decathlon are examples for anisotropic data)
-
-You do not have to run five-fold cross-validation all the time. If you want to test single model performance, use
- *all* for `FOLD` instead of a number. Note that this will then not give you an estimate of your performance on the 
- training set. You will also no tbe able to automatically identify which ensembling should be used and nnU-Net will 
- not be able to configure a postprocessing. 
- 
-CAREFUL: DO NOT use fold=all when you intend to run the cascade! You must run the cross-validation in 3d_lowres so 
-that you get proper (=not overfitted) low resolution predictions.
- 
-## Sharing Models
-You can share trained models by simply sending the corresponding output folder from `RESULTS_FOLDER/nnUNet` to 
-whoever you want share them with. The recipient can then use nnU-Net for inference with this model.
-
-You can now also use `nnUNet_export_model_to_zip` to export a trained model (or models) to a zip file. The recipient 
-can then use `nnUNet_install_pretrained_model_from_zip` to install the model from this zip file. 
-
-## Can I run nnU-Net on smaller GPUs?
-nnU-Net is guaranteed to run on GPUs with 11GB of memory. Many configurations may also run on 8 GB.
-If you have an 11GB and there is still an `Out of Memory` error, please read 'nnU-Net training: RuntimeError: CUDA out of memory' [here](common_problems_and_solutions.md).
- 
-If you wish to configure nnU-Net to use a different amount of GPU memory, simply adapt the reference value for the GPU memory estimation 
-accordingly (with some slack because the whole thing is not an exact science!). For example, in 
-[experiment_planner_baseline_3DUNet_v21_11GB.py](nnunet/experiment_planning/experiment_planner_baseline_3DUNet_v21_11GB.py) 
-we provide an example that attempts to maximise the usage of GPU memory on 11GB as opposed to the default which leaves 
-much more headroom). This is simply achieved by this line:
-
-```python
-ref = Generic_UNet.use_this_for_batch_size_computation_3D * 11 / 8
-```
-
-with 8 being what is currently used (approximately) and 11 being the target. Should you get CUDA out of memory 
-issues, simply reduce the reference value. You should do this adaptation as part of a separate ExperimentPlanner class. 
-Please read the instructions [here](documentation/extending_nnunet.md).
-
-
-## Why is no 3d_lowres model created?
-3d_lowres is created only if the patch size in 3d_fullres less than 1/8 of the voxels of the median shape of the data 
-in 3d_fullres (for example Liver is about 512x512x512 and the patch size is 128x128x128, so that's 1/64 and thus 
-3d_lowres is created). You can enforce the creation of 3d_lowres models for smaller datasets by changing the value of
-`HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0` (located in experiment_planning.configuration).
-    
diff -urN nnUNet/documentation/data_format_inference.md nnUNetNew/documentation/data_format_inference.md
--- nnUNet/documentation/data_format_inference.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/data_format_inference.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,34 +0,0 @@
-# Data format for Inference 
-
-The data format for inference must match the one used for the raw data (specifically, the images must be in exactly 
-the same format as in the imagesTr folder). As before, the filenames must start with a
-unique identifier, followed by a 4-digit modality identifier. Here is an example for two different datasets:
-
-1) Task005_Prostate:
-
-    This task has 2 modalities, so the files in the input folder must look like this:
-
-        input_folder
-        ├── prostate_03_0000.nii.gz
-        ├── prostate_03_0001.nii.gz
-        ├── prostate_05_0000.nii.gz
-        ├── prostate_05_0001.nii.gz
-        ├── prostate_08_0000.nii.gz
-        ├── prostate_08_0001.nii.gz
-        ├── ...
-
-    _0000 is always the T2 image and _0001 is always the ADC image (as specified by 'modality' in the dataset.json)
-
-2) Task002_Heart:
-
-        imagesTs
-        ├── la_001_0000.nii.gz
-        ├── la_002_0000.nii.gz
-        ├── la_006_0000.nii.gz
-        ├── ...
-    
-    Task002 only has one modality, so each case only has one _0000.nii.gz file.
-  
-
-The segmentations in the output folder will be named INDENTIFIER.nii.gz (omitting the modality identifier).
-   
\ No newline at end of file
diff -urN nnUNet/documentation/dataset_conversion.md nnUNetNew/documentation/dataset_conversion.md
--- nnUNet/documentation/dataset_conversion.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/dataset_conversion.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,226 +0,0 @@
-# Dataset conversion instructions
-nnU-Net requires the raw data to be brought into a specific format so that it know how to read and interpret it. This 
-format closely, but not entirely, follows the format used by the 
-[Medical Segmentation Decathlon](http://medicaldecathlon.com/) (MSD).
-
-The entry point to nnU-Net is the nnUNet_raw_data_base folder (which the user needs to specify when installing nnU-Net!). 
-Each segmentation dataset is stored as a separate 'Task'. Tasks are associated with a task ID, a three digit integer 
-(this is different from the MSD!) and 
-a task name (which you can freely choose): Task005_Prostate has 'Prostate' as task name and the task id is 5. Tasks are stored in the 
-nnUNet_raw_data_base/nnUNet_raw_data folder like this:
-
-    nnUNet_raw_data_base/nnUNet_raw_data/
-    ├── Task001_BrainTumour
-    ├── Task002_Heart
-    ├── Task003_Liver
-    ├── Task004_Hippocampus
-    ├── Task005_Prostate
-    ├── ...
-
-Within each task folder, the following structure is expected:
-
-    Task001_BrainTumour/
-    ├── dataset.json
-    ├── imagesTr
-    ├── (imagesTs)
-    └── labelsTr
-    
-**Please make your custom task ids start at 500 to ensure that there will be no conflicts with downloaded pretrained models!!! (IDs cannot exceed 999)**
-
-imagesTr contains the images belonging to the training cases. nnU-Net will run pipeline configuration, training with 
-cross-validation, as well as finding postprocesing and the best ensemble on this data. imagesTs (optional) contains the 
-images that belong to the 
-test cases , labelsTr the images with the ground truth segmentation maps for the training cases. dataset.json contains 
-metadata of the dataset.
-
-Each training case is associated with an identifier = a unique name for that case. This identifier is used by nnU-Net to 
-recognize which label file belongs to which image. **All images (including labels) must be 3D nifti files (.nii.gz)!**
- 
-The image files can have any scalar pixel type. The label files must contain segmentation maps that contain consecutive integers, 
-starting with 0: 0, 1, 2, 3, ... num_labels. 0 is considered background. Each class then has its own associated integer 
-value.
-Images may have multiple modalities. This is especially often the case for medical images. Modalities are very much 
-like color channels in photos (three color channels: red, green blue), but can be much more diverse: CT, different types 
-or MRI, and many other. Imaging modalities are identified by nnU-Net by their suffix: a four-digit integer at the end 
-of the filename. Imaging files must therefore follow the following naming convention: case_identifier_XXXX.nii.gz. 
-Hereby, XXXX is the modality identifier. What modalities these identifiers belong to is specified in the dataset.json 
-file (see below). Label files are saved as case_identifier.nii.gz
-
-This naming scheme results in the following folder structure. It is the responsibility of the user to bring their 
-data into this format!
-
-Here is an example for the first Task of the MSD: BrainTumour. Each image has four modalities: FLAIR (0000), 
-T1w (0001), T1gd (0002) and T2w (0003). Note that the imagesTs folder is optional and does not have to be present.
-
-    nnUNet_raw_data_base/nnUNet_raw_data/Task001_BrainTumour/
-    ├── dataset.json
-    ├── imagesTr
-    │   ├── BRATS_001_0000.nii.gz
-    │   ├── BRATS_001_0001.nii.gz
-    │   ├── BRATS_001_0002.nii.gz
-    │   ├── BRATS_001_0003.nii.gz
-    │   ├── BRATS_002_0000.nii.gz
-    │   ├── BRATS_002_0001.nii.gz
-    │   ├── BRATS_002_0002.nii.gz
-    │   ├── BRATS_002_0003.nii.gz
-    │   ├── BRATS_003_0000.nii.gz
-    │   ├── BRATS_003_0001.nii.gz
-    │   ├── BRATS_003_0002.nii.gz
-    │   ├── BRATS_003_0003.nii.gz
-    │   ├── BRATS_004_0000.nii.gz
-    │   ├── BRATS_004_0001.nii.gz
-    │   ├── BRATS_004_0002.nii.gz
-    │   ├── BRATS_004_0003.nii.gz
-    │   ├── ...
-    ├── imagesTs
-    │   ├── BRATS_485_0000.nii.gz
-    │   ├── BRATS_485_0001.nii.gz
-    │   ├── BRATS_485_0002.nii.gz
-    │   ├── BRATS_485_0003.nii.gz
-    │   ├── BRATS_486_0000.nii.gz
-    │   ├── BRATS_486_0001.nii.gz
-    │   ├── BRATS_486_0002.nii.gz
-    │   ├── BRATS_486_0003.nii.gz
-    │   ├── BRATS_487_0000.nii.gz
-    │   ├── BRATS_487_0001.nii.gz
-    │   ├── BRATS_487_0002.nii.gz
-    │   ├── BRATS_487_0003.nii.gz
-    │   ├── BRATS_488_0000.nii.gz
-    │   ├── BRATS_488_0001.nii.gz
-    │   ├── BRATS_488_0002.nii.gz
-    │   ├── BRATS_488_0003.nii.gz
-    │   ├── BRATS_489_0000.nii.gz
-    │   ├── BRATS_489_0001.nii.gz
-    │   ├── BRATS_489_0002.nii.gz
-    │   ├── BRATS_489_0003.nii.gz
-    │   ├── ...
-    └── labelsTr
-        ├── BRATS_001.nii.gz
-        ├── BRATS_002.nii.gz
-        ├── BRATS_003.nii.gz
-        ├── BRATS_004.nii.gz
-        ├── ...
-
-Here is another example of the second task of the MSD, which has only one modality:
-
-    nnUNet_raw_data_base/nnUNet_raw_data/Task002_Heart/
-    ├── dataset.json
-    ├── imagesTr
-    │   ├── la_003_0000.nii.gz
-    │   ├── la_004_0000.nii.gz
-    │   ├── ...
-    ├── imagesTs
-    │   ├── la_001_0000.nii.gz
-    │   ├── la_002_0000.nii.gz
-    │   ├── ...
-    └── labelsTr
-        ├── la_003.nii.gz
-        ├── la_004.nii.gz
-        ├── ...
-
-For each training case, all images must have the same geometry to ensure that their pixel arrays are aligned. Also 
-make sure that all your data is co-registered!
-
-The dataset.json file used by nnU-Net is identical to the ones used by the MSD. For your custom tasks you need to create 
-them as well and thereby exactly follow the same structure. [This](https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2)
-is where you can download the MSD data for reference. 
-
-**NEW:** There now is a utility with which you can generate the dataset.json automatically. You can find it 
-[here](../nnunet/dataset_conversion/utils.py) (look for the function `generate_dataset_json`). 
-See [Task120](../nnunet/dataset_conversion/Task120_Massachusetts_RoadSegm.py) for an example on how to use it. And read 
-its documentation!
-
-Here is the content of the dataset.json from the Prostate task:
-
-    { 
-     "name": "PROSTATE", 
-     "description": "Prostate transitional zone and peripheral zone segmentation",
-     "reference": "Radboud University, Nijmegen Medical Centre",
-     "licence":"CC-BY-SA 4.0",
-     "relase":"1.0 04/05/2018",
-     "tensorImageSize": "4D",
-     "modality": { 
-       "0": "T2", 
-       "1": "ADC"
-     }, 
-     "labels": { 
-       "0": "background", 
-       "1": "PZ", 
-       "2": "TZ"
-     }, 
-     "numTraining": 32, 
-     "numTest": 16,
-     "training":[{"image":"./imagesTr/prostate_16.nii.gz","label":"./labelsTr/prostate_16.nii.gz"},{"image":"./imagesTr/prostate_04.nii.gz","label":"./labelsTr/prostate_04.nii.gz"},...], 
-     "test": ["./imagesTs/prostate_08.nii.gz","./imagesTs/prostate_22.nii.gz","./imagesTs/prostate_30.nii.gz",...]
-     }
-
-Note that we truncated the "training" and "test" lists for clarity. You need to specify all the cases in there. If you 
-don't have test images (imagesTs does not exist) you can leave "test" blank: `"test": []`.
-
-Please also have a look at the python files located [here](../nnunet/dataset_conversion). They show how we created our 
-custom dataset.jsons for a range of public datasets.
-
-## How to use decathlon datasets
-The previous release of nnU-Net allowed users to either start with 4D or 3D niftis. This resulted in some confusion, 
-however, because some users would not know where they should save their data. We therefore dropped support for the 4D 
-niftis used by the MSD. Instead, we provide a utility that converts the MSD datasets into the format specified above:
-
-```bash
-nnUNet_convert_decathlon_task -i FOLDER_TO_TASK_AS_DOWNLOADED_FROM_MSD -p NUM_PROCESSES
-```
-
-FOLDER_TO_TASK_AS_DOWNLOADED_FROM_MSD needs to point to the downloaded task folder (such as Task05_Prostate, note the 
-2-digit task id!). The converted Task will be saved under the same name in nnUNet_raw_data_base/nnUNet_raw_data 
-(but with a 3 digit identifier). You can overwrite the task id of the converted task by using the `-output_task_id` option.
-
-
-## How to use 2D data with nnU-Net
-nnU-Net was originally built for 3D images. It is also strongest when applied to 3D segmentation problems because a 
-large proportion of its design choices were built with 3D in mind. Also note that many 2D segmentation problems, 
-especially in the non-biomedical domain, may benefit from pretrained network architectures which nnU-Net does not
-support.
-Still, there is certainly a need for an out of the box segmentation solution for 2D segmentation problems. And 
-also on 2D segmentation tasks nnU-Net cam perform extremely well! We have, for example, won a 2D task in the cell 
-tracking challenge with nnU-Net (see our Nature Methods paper) and we have also successfully applied nnU-Net to 
-histopathological segmentation problems. 
-
-Working with 2D data in nnU-Net requires a small workaround in the creation of the dataset. 
-Essentially, all images must be converted to pseudo 3D images (so an image with shape (X, Y).
-When working with 2D images it is important to follow the correct axis ordering. When loading the images with SimpleITK, 
-the resulting numpy array shape should be (1, x, y). We recommend you save your images with SimpleITK so that the
-correct shape is guaranteed. If you prefer to save your images with nibabel, please save them as (y, x, 1) 
-(SimpleITK reverts the ordering of the axes when reading). 
-To check whether your 2D images have the correct shape you can run the following snippet:
-```
-import SimpleITK as sitk
-print(sitk.GetArrayFromImage(sitk.ReadImage(FILENAME)).shape)
-```
-
-The resulting image must be saved in nifti format. Hereby it is important to set the spacing of the 
-first axis (the one with shape 1) to a value larger than the others. If you are working with niftis anyways, then 
-doing this should be easy for you.
-This example here is intended for demonstrating how nnU-Net can be used with 
-'regular' 2D images. We selected the massachusetts road segmentation dataset for this because it can be obtained 
-easily, it comes with a good amount of training cases but is still not too large to be difficult to handle.
-    
-See [here](../nnunet/dataset_conversion/Task120_Massachusetts_RoadSegm.py) for an example. 
-This script contains a lot of comments and useful information. Also have a look 
-[here](../nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py).
-
-
-## How to update an existing dataset
-When updating a dataset you not only need to change the data located in `nnUNet_raw_data_base/nnUNet_raw_data`. Make 
-sure to also delete the whole (!) corresponding dataset in `nnUNet_raw_data_base/nnUNet_cropped_data`. nnU-Net will not 
-repeat the cropping (and thus will not update your dataset) if the old files are still in nnUNet_cropped_data!
-
-The best way of updating an existing dataset is (**choose one**):
-- delete all data and models belonging to the old version of the dataset (nnUNet_preprocessed, corresponding results 
-  in RESULTS_FOLDER/nnUNet, nnUNet_cropped_data, nnUNet_raw_data), then update
-- (recommended) create the updated dataset from scratch using a new task ID **and** name
-
-
-## How to convert other image formats to nifti
-Please have a look at the following tasks:
-- [Task120](../nnunet/dataset_conversion/Task120_Massachusetts_RoadSegm.py): 2D png images
-- [Task075](../nnunet/dataset_conversion/Task075_Fluo_C3DH_A549_ManAndSim.py) and [Task076](../nnunet/dataset_conversion/Task076_Fluo_N3DH_SIM.py): 3D tiff
-- [Task089](../nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py) 2D tiff
diff -urN nnUNet/documentation/expected_epoch_times.md nnUNetNew/documentation/expected_epoch_times.md
--- nnUNet/documentation/expected_epoch_times.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/expected_epoch_times.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,188 +0,0 @@
-# Introduction
-Trainings can take some time. A well-running training setup is essential to get the most of nnU-Net. nnU-Net does not 
-require any fancy hardware, just a well-balanced system. We recommend at least 32 GB of RAM, 6 CPU cores (12 threads), 
-SSD storage (this can be SATA and does not have to be PCIe. DO NOT use an external SSD connected via USB!) and a 
-2080 ti GPU. If your system has multiple GPUs, the other components need to scale linearly with the number of GPUs.
-
-# Benchmark Details
-To ensure your system is running as intended, we provide some benchmark numbers against which you can compare. Here 
-are the details about benchmarking:
-
-- We benchmark **2d**, **3d_fullres** and a modified 3d_fullres that uses 3x the default batch size (called **3d_fullres large** here) 
-- The datasets **Task002_Heart**, **Task005_Prostate** and **Task003_Liver** of the Medical Segmentation Decathlon are used 
-(they provide a good spectrum of dataset properties)
-- we use the nnUNetTrainerV2_5epochs trainer. This will run only for 5 epochs and it will skip validation. 
-From the 5 epochs, we select the fastest one as the epoch time. 
-- We will also be running the nnUNetTrainerV2_5epochs_dummyLoad trainer on the 3d_fullres config 
-(called **3d_fullres dummy**). This trainer does not use the dataloader and instead uses random dummy inputs, 
-bypassing all data augmentation (CPU) and I/O bottlenecks. 
-- All trainings are done with mixed precision. This is why Pascal GPUs (Titan Xp) are so slow (they do not have 
-tensor cores) 
-
-# How to run the benchmark
-First go into the folder where the preprocessed data and plans file of the task you would like to use are located. 
-For me this is `/home/fabian/data/nnUNet_preprocessed/Task002_Heart`
-
-Then run the following python snippet. This will create our custom **3d_fullres_large** configuration. Note that this 
-large configuration will only run on GPUs with 16GB or more! We included it in the test because some GPUs 
-(V100, A100) can shine when they get more work to do per iteration.
-```python
-from batchgenerators.utilities.file_and_folder_operations import *
-plans = load_pickle('nnUNetPlansv2.1_plans_3D.pkl')
-stage = max(plans['plans_per_stage'].keys())
-plans['plans_per_stage'][stage]['batch_size'] *= 3
-save_pickle(plans, 'nnUNetPlansv2.1_bs3x_plans_3D.pkl')
-```
-
-Now you can run the benchmarks. Each should only take a couple of minutes
-```bash
-nnUNet_train 2d nnUNetTrainerV2_5epochs TASKID 0
-nnUNet_train 3d_fullres nnUNetTrainerV2_5epochs TASKID 0
-nnUNet_train 3d_fullres nnUNetTrainerV2_5epochs_dummyLoad TASKID 0
-nnUNet_train 3d_fullres nnUNetTrainerV2_5epochs TASKID 0 -p nnUNetPlansv2.1_bs3x # optional, only for GPUs with more than 16GB of VRAM
-```
-
-The time we are interested in is the epoch time. You can find it in the text output (stdout) or the log file 
-located in your `RESULTS_FOLDER`. Note that the trainers used here run for 5 epochs. Select the fastest time from your 
-output as your benchmark time.
-
-# Results
-
-The following table shows the results we are getting on our servers/workstations. We are using pytorch 1.11.0 that we 
-compiled ourselves using the instructions found [here](https://github.com/pytorch/pytorch#from-source). The cuDNN 
-version we used is 8.3.2.44. You should be seeing similar numbers when you 
-run the benchmark on your server/workstation. Note that fluctuations of a couple of seconds are normal!
-
-IMPORTANT: Compiling pytorch from source is currently mandatory for best performance! Pytorch 1.8 does not have 
-working tensorcore acceleration for 3D convolutions when installed with pip or conda!
-
-IMPORTANT: A100 and V100 are very fast with the newer cuDNN versions and need more CPU workers to prevent bottlenecks,
-set the environment variable `nnUNet_n_proc_DA=XX` to increase the number of data augmentation workers. 
-Recommended: 20 for V100, 32 for A100. Datasets with many input modalities (BraTS: 4) require A LOT of CPU and 
-should be used with even larger values for `nnUNet_n_proc_DA`.
-
-## Pytorch 1.11.0 compiled with cuDNN 8.3.2.44
-
-|                                   | A100 40GB PCIe 250W | A100 40GB (DGX A100) 400W | V100 32GB SXM3 (DGX2) 350W | V100 32GB PCIe 250W | Quadro RTX6000 24GB 260W | Titan RTX 24GB 280W | RTX 2080 ti 11GB 250W |
-|-----------------------------------|---------------------|---------------------------|----------------------------|---------------------|--------------------------|---------------------|-----------------------|
-| Task002_Heart 2d                  | 36.75               | 41.48                     | 57.4                       | 61.32               | 70.96                    | 70.39               | 86.1                  |
-| Task002_Heart 3d_fullres          | 47.16               | 46.22                     | 81.92                      | 84.82               | 109.5                    | 107.44              | 123.27                |
-| Task002_Heart 3d_fullres dummy    | 46.52               | 43.6                      | 66.84                      | 78.75               | 99.88                    | 97.39               | 116.36                |
-| Task002_Heart 3d_fullres large    | 121.55              | 111.64                    | 221.03                     | 242.56              | 284.73                   | 302.02              | OOM                   |
-|                                   |                     |                           |                            |                     |                          |                     |                       |
-| Task003_Liver 2d                  | 35.66               | 39.26                     | 65.34                      | 65.76               | 79.72                    | 70.44               | 86.37                 |
-| Task003_Liver 3d_fullres          | 41.49               | 39.67                     | 74.21                      | 76.79               | 77.63                    | 86.75               | 94.16                 |
-| Task003_Liver 3d_fullres dummy    | 40.63               | 37.71                     | 62.37                      | 70.55               | 76.37                    | 74.66               | 86.8                  |
-| Task003_Liver 3d_fullres large    | 102.48              | 97.85                     | 202.04                     | 209.4               | 226.45                   | 254.77              | OOM                   |
-|                                   |                     |                           |                            |                     |                          |                     |                       |
-| Task005_Prostate 2d               | 36.41               | 37.07                     | 64.58                      | 65.88               | 70.47                    | 77.95               | 88.54                 |
-| Task005_Prostate 3d_fullres       | 42.95               | 41.92                     | 90.92                      | 95.54               | 85.69                    | 90.66               | 109.78                |
-| Task005_Prostate 3d_fullres dummy | 41.78               | 39.56                     | 78.22                      | 88.83               | 83.69                    | 81.31               | 101.75                |
-| Task005_Prostate 3d_fullres large | 106.98              | 102.75                    | 239.32                     | 259.14              | 255.64                   | 270.8               | OOM                   |
-
-
-## OLD: Pytorch 1.7.1 compiled with cuDNN 8.1.0.77
-(Columns are different. The table above includes the A100 PCIe and lacks the Titan Xp GPUs!)
-
-|                                   | A100 40GB (DGX A100) 400W | V100 32GB SXM3 (DGX2) 350W | V100 32GB PCIe 250W | Quadro RTX6000 24GB 260W | Titan RTX 24GB 280W | RTX 2080 ti 11GB 250W | Titan Xp 12GB 250W |
-|-----------------------------------|---------------------------|----------------------------|---------------------|--------------------------|---------------------|-----------------------|--------------------|
-| Task002_Heart 2d                  | 40.06                     | 66.03                      | 76.19               | 78.01                    | 79.78               | 98.49                 | 177.87             |
-| Task002_Heart 3d_fullres          | 51.17                     | 85.96                      | 99.29               | 110.47                   | 112.34              | 148.36                | 504.93             |
-| Task002_Heart 3d_fullres dummy    | 48.53                     | 79                         | 89.66               | 105.16                   | 105.56              | 138.4                 | 501.64             |
-| Task002_Heart 3d_fullres large    | 118.5                     | 220.45                     | 251.25              | 322.28                   | 300.96              | OOM                   | OOM                |
-|                                   |                           |                            |                     |                          |                     |                       |                    |
-| Task003_Liver 2d                  | 39.71                     | 60.69                      | 69.65               | 72.29                    | 76.17               | 92.54                 | 183.73             |
-| Task003_Liver 3d_fullres          | 44.48                     | 75.53                      | 87.19               | 85.18                    | 86.17               | 106.76                | 290.87             |
-| Task003_Liver 3d_fullres dummy    | 41.1                      | 70.96                      | 80.1                | 79.43                    | 79.43               | 101.54                | 289.03             |
-| Task003_Liver 3d_fullres large    | 115.33                    | 213.27                     | 250.09              | 261.54                   | 266.66              | OOM                   | OOM                |
-|                                   |                           |                            |                     |                          |                     |                       |                    |
-| Task005_Prostate 2d               | 42.21                     | 68.88                      | 80.46               | 83.62                    | 81.59               | 102.81                | 183.68             |
-| Task005_Prostate 3d_fullres       | 47.19                     | 76.33                      | 85.4                | 100                      | 102.05              | 132.82                | 415.45             |
-| Task005_Prostate 3d_fullres dummy | 43.87                     | 70.58                      | 81.32               | 97.48                    | 98.99               | 124.73                | 410.12             |
-| Task005_Prostate 3d_fullres large | 117.31                    | 209.12                     | 234.28              | 277.14                   | 284.35              | OOM                   | OOM                |
-
-# Troubleshooting
-Your epoch times are substantially slower than ours? That's not good! This section will help you figure out what is 
-wrong. Note that each system is unique and we cannot help you find bottlenecks beyond providing the information 
-presented in this section!
-
-## First step: Make sure you have the right software!
-In order to get maximum performance, you need to have pytorch that ships with or was compiled with a recent cuDNN v
-ersion (8002 or newer is a must!). 
-You can check your cudnn versionlike this:
-```bash
-python -c 'import torch;print(torch.backends.cudnn.version())'
-```
-If the output is `8002` or higher, then you are good to go. If not you may have to take action: either update your 
-pytorch version or maybe even compile it yourself.
-Compiling yourself will almost always give the maximum performance. Please follow the 
-[instructions on the pytorch website](https://github.com/pytorch/pytorch#from-source). You'll need the cuDNN tar file 
-which you can download from the [Nvidia homepage](https://developer.nvidia.com/cudnn).
-
-## Identifying the bottleneck
-If the software is up to date and you are still experiencing problems, this is how you can figure out what is going on:
-
-While a training is running, run `htop` and `watch -n 0.1 nvidia-smi` (depending on your region you may have to use 
-`0,1` instead). If you have physical access to the machine, also have a look at the LED indicating I/O activity.
-
-Here is what you can read from that:
-- `nvidia-smi` shows the GPU activity. `watch -n 0.1` makes this command refresh every 0.1s. This will allow you to 
-see your GPU in action. A well running training will have your GPU pegged at 90-100% with no drops in GPU utilization. 
-Your power should also be close to the maximum (for example `237W / 250 W`) at all times. 
-- `htop` gives you an overview of the CPU usage. nnU-Net uses 12 processes for data augmentation + one main process. 
-This means that up to 13 processes should be running simultaneously.
-- the I/O LED indicates that your system is reading/writing data from/to your hard drive/SSD. Whenever this is 
-blinking your system is doing something with your HDD/SSD.
-
-### GPU bottleneck
-If `nvidia-smi` is constantly showing 80-100% GPU utilization and the reported power draw is near the maximum, your 
-GPU is the bottleneck. This is great! That means that your other components are not slowing it down. Your epochs times 
-should be the same as ours reported above. If they are not then you need to investigate your software stack (see cuDNN stuff above).
-
-What can you do about it?
-1) There is nothing holding you back. Everything is fine!
-2) If you need faster training, consider upgrading your GPU. Performance numbers are above, feel free to use them for guidance.
-3) Think about whether you need more (slower) GPUs or less (faster) GPUs. Make sure to include Server/Workstation 
-costs into your calculations. Sometimes it is better to go with more cheaper but slower GPUs run run multiple trainings 
-in parallel.
-
-### CPU bottleneck
-You can recognize a CPU bottleneck as follows:
-1) htop is consistently showing 10+ processes that are associated with your nnU-Net training
-2) nvidia-smi is reporting jumps of GPU activity with zeroes in between
-
-What can you do about it?
-1) Depending on your single core performance, some datasets may require more than the default 12 processes for data 
-augmentation. The CPU requirements for DA increase roughly linearly with the number of input modalities. Most datasets 
-will train fine with much less than 12 (6 or even just 4). But datasets with for example 4 modalities may require more. 
-If you have more than 12 CPU threads available, set the environment variable `nnUNet_n_proc_DA` to a number higher than 12.
-2) If your CPU has less than 12 threads in total, running 12 threads can overburden it. Try lowering `nnUNet_n_proc_DA` 
-to the number of threads you have available.
-3) (sounds stupid, but this is the only other way) upgrade your CPU. I have seen Servers with 8 CPU cores (16 threads)
- and 8 GPUs in them. That is not well balanced. CPUs are cheap compared to GPUs. On a 'workstation' (single or dual GPU) 
- you can get something like a Ryzen 3900X or 3950X. On a server you could consider Xeon 6226R or 6258R on the Intel 
- side or the EPYC 7302P, 7402P, 7502P or 7702P on the AMD side. Make sure to scale the number of cores according to your 
- number of GPUs and use case. Feel free to also use our nnU-net recommendations from above.
- 
-### I/O bottleneck
-On a workstation, I/O bottlenecks can be identified by looking at the LED indicating I/O activity. This is what an 
-I/O bottleneck looks like:
-- nvidia-smi is reporting jumps of GPU activity with zeroes in between
-- htop is not showing many active CPU processes
-- I/O LED is blinking rapidly or turned on constantly
-
-Detecting I/O bottlenecks is difficult on servers where you may not have physical access. Tools like `iotop` are 
-difficult to read and can only be run with sudo. However, the presence of an I/O LED is not strictly necessary. If
-- nvidia-smi is reporting jumps of GPU activity with zeroes in between
-- htop is not showing many active CPU processes
-
-then the only possible issue to my knowledge is in fact an I/O bottleneck. 
-
-Here is what you can do about an I/O bottleneck:
-1) Make sure you are actually using an SSD to store the preprocessed data (`nnUNet_preprocessed`). Do not use an 
-SSD connected via USB! Never use a HDD. Do not use a network drive that was not specifically designed to handle fast I/O 
-(Note that you can use a network drive if it was designed for this purpose. At the DKFZ we use a
-[flashblade](https://www.purestorage.com/products/file-and-object/flashblade.html) connected via ethernet and that works 
-great)
-2) A SATA SSD is only enough to feed 1-2 GPUs. If you have more GPUs installed you may have to upgrade to an nvme 
-drive (make sure to get PCIe interface!).
diff -urN nnUNet/documentation/extending_nnunet.md nnUNetNew/documentation/extending_nnunet.md
--- nnUNet/documentation/extending_nnunet.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/extending_nnunet.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,119 +0,0 @@
-
-# Extending/Changing nnU-Net
-
-To use nnU-Net as a framework and make changes to its components, please make sure to install it with the `git clone` 
-and `pip install -e .` commands so that a local copy of the code is created.
-Changing components of nnU-Net needs to be done in different places, depending on whether these components belong to 
-the inferred, blueprint or empirical parameters. We cover some of the most common use cases below. They should give 
-you a good indication of where to start.
-
-Generally it is recommended to look into the code where the thing you would like to change is currently implemented 
-and then derive a strategy on how to change it. If you have any questions, feel free to open an issue on GitHub and 
-we will help you as much as we can.
-
-## Changes to blueprint parameters
-This section gives guidance on how to implement changes to loss function, training schedule, learning rates, optimizer, 
-some architecture parameters, data augmentation etc. All these parameters are part of the **nnU-Net trainer class**, 
-which we have already seen in the sections above. The default trainer class for 2D, 3D low resolution and 3D full 
-resolution U-Net is nnUNetTrainerV2, the default for the 3D full resolution U-Net from the cascade is 
-nnUNetTrainerV2CascadeFullRes. Trainer classes in nnU-Net inherit from each other, nnUNetTrainerV2CascadeFullRes for 
-example has nnUNetTrainerV2 as parent class and only overrides cascade-specific code.
-
-Due to the inheritance of trainer classes, changes can be integrated into nnU-Net quite easily and with minimal effort. 
-Simply create a new trainer class (with some custom name), change the functionality you need to change and then specify 
-this class (via its name) during training - done.
-
-This process requires the new class to be located in a subfolder of nnunet.training.network_training! Do not save it 
-somewhere else or nnU-Net will not be able to find it! Also don't use the same name twice! nnU-Net always picks the 
-first trainer that matches the requested name.
-
-Don't worry about overwriting results of another trainer class. nnU-Net always generates output folders that are named 
-after the trainer class used to generate the results. 
-
-Due to the variety of possible changes to the blueprint parameters of nnU-Net, we here only present a summary of where 
-to look for what kind of modification. During method development we have already created a large number of nnU-Net 
-blueprint variations which should give a good indication of where to start:
-
-| Type of modification    | Examples                                                                                                                                                                                                                                                                                                                                              |
-|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| loss function           | nnunet.training.network_training.loss_function.*                                                                                                                                                                                                                                                                                                      |
-| data augmentation       | nnunet.training.network_training.data_augmentation.*                                                                                                                                                                                                                                                                                                  |
-| Optimizer, lr, momentum | nnunet.training.network_training.optimizer_and_lr.*                                                                                                                                                                                                                                                                                                   |
-| (Batch)Normalization    | nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_BN.py<br>nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_FRN.py<br>nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_GN.py<br>nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_NoNormalization_lr1en3.py |
-| Nonlinearity            | nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_ReLU.py<br>nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_Mish.py                                                                                                                                                                                    |
-| Architecture            | nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_3ConvPerStage.py<br>nnunet.training.network_training.architectural_variants.nnUNetTrainerV2_ResencUNet                                                                                                                                                                        |
-| ...                     | (see nnunet.training.network_training and subfolders)                                                                                                                                                                                                                                                                                                 |
-
-## Changes to Inferred Parameters
-The inferred parameters are determined based on the dataset fingerprint, a low dimensional representation of the properties 
-of the training cases. It captures, for example, the image shapes, voxel spacings and intensity information from 
-the training cases. The datset fingerprint is created by the DatasetAnalyzer (which is located in nnunet.preprocessing) 
-while running `nnUNet_plan_and_preprocess`. 
-
-`nnUNet_plan_and_preprocess` uses so called ExperimentPlanners for running the adaptation process. Default ExperimentPlanner 
-classes are ExperimentPlanner2D_v21 for the 2D U-Net and ExperimentPlanner3D_v21 for the 3D full resolution U-Net and the 
-U-Net cascade. Just like nnUNetTrainers, the ExperimentPlanners inherit from each other, resulting in minimal programming 
-effort to incorporate changes. Just like with the trainers, simply give your custom ExperimentPlanners a unique name and 
-save them in some subfolder of nnunet.experiment_planning. You can then specify your class names when running 
-`nnUNet_plan_and_preprocess` and nnU-Net will find them automatically. When inheriting from ExperimentPlanners, you **MUST** 
-overwrite the class variables `self.data_identifier` and `self.plans_fname` (just like for example 
-[here](../nnunet/experiment_planning/alternative_experiment_planning/normalization/experiment_planner_3DUNet_CT2.py)). 
-If you omit this step the planner will overwrite the plans file and the preprocessed data of the planner it inherits from.
-
-To train with your custom configuration, simply specify the correct plans identifier with `-p` when you call the 
-`nnUNet_train` command. The plans file also contains the data_identifier specified in your ExperimentPlanner, so the 
-trainer class will automatically know what data should be used.
-
-Possible adaptations to the inferred parameters could include a different way of prioritizing batch size vs patch size 
-(currently, nnU-Net prioritizies patch size), a different handling of the spacing information for architecture template 
-instantiation, changing the definition of target spacing, or using different strategies for finding the 3d low 
-resolution U-Net configuration.
-
-The folders located in nnunet.experiment_planning contain several example ExperimentPlanner that modify various aspects 
-of the inferred parameters. You can use them as inspiration for your own.
-
-If you wish to run a different preprocessing, you most likely will have to implement your own Preprocessor class. 
-The preprocessor class that is used by some ExperimentPlanner is specified in its preprocessor_name class variable. The 
-default is `self.preprocessor_name = "GenericPreprocessor"` for 3D and `PreprocessorFor2D` for 2D (the 2D preprocessor 
-ignores the target spacing for the first axis to ensure that images are only resampled in the axes that will make up the training samples). 
-GenericPreprocessor (and all custom Preprocessors you implement) must be located in nnunet.preprocessing. The 
-preprocessor_name is saved in the plans file (by ExperimentPlanner), so that the 
-nnUNetTrainer knows which preprocessor must be used during inference to match the preprocessing of the training data. 
-
-Modifications to the preprocessing pipeline could be the addition of bias field correction to MRI images, a different CT
-preprocessing scheme or a different way of resampling segmentations and image data for anisotropic cases. 
-An example is provided [here](../nnunet/preprocessing/preprocessing.py).
-
-When implementing a custom preprocessor, you should also create a custom ExperimentPlanner that uses it (via self.preprocessor_name). 
-This experiment planner must also use a matching data_identifier and plans_fname to ensure no other data is overwritten.
-
-## Use a different network architecture
-Changing the network architecture in nnU-Net is easy, but not self-explanatory. Any new segmentation network you implement 
-needs to understand what nnU-Net requests from it (wrt how many downsampling operations are done, whether deep supervision 
-is used, what the convolutional kernel sizes are supposed to be). It needs to be able to dynamiccaly change its topology, 
-just like our implementation of the [Generic_UNet](../nnunet/network_architecture/generic_UNet.py). Furthermore, it must be
-able to generate a value that can be used to estimate memory consumption. What we have implemented for Generic_UNet effectively
-counts the number of voxels found in all feature maps that are present in a given configuration. Although this estimation 
-disregards the number of parameters we have found it to work quite well. Unless you implement an architecture with 
-unreasonably high number of parameters, the large majority of the VRAM used during training will be occupied by feature 
-maps, so parameters can be (mostly) disregarded. For implementing your own network, it is key to understand that the 
-number we are computing here cannot be interpreted directly as memory consumption (other factors than the feature maps 
-of the convolutions also play a role, such as instance normalization. This is furthermore very hard to predict because 
-there are also several different algorithms for running the convolutions, each with its own memory requirement. We train 
-models with cudnn.benchmark=True, so it is impossible to predict which algorithm is used). 
-So instead, to approch this problem in the most straightforward way, we manually identify the largest configuration we 
-can fit in the GPU of choice (manually define the dowmsampling, patch size etc) and use this value (-10% or so to be save) 
-as **reference** in the ExperimentPlanner that uses this architecture. 
-
-To illustrate this process, we have implemented a U-Net with a residual encoder 
-(see FabiansUNet in [generic_modular_residual_UNet.py](../nnunet/network_architecture/generic_modular_residual_UNet.py)). 
-This UNet has a class variable called use_this_for_3D_configuration. This value was found with the code located in 
-find_3d_configuration (same python file). The corresponding ExperimentPlanner 
-[ExperimentPlanner3DFabiansResUNet_v21](../nnunet/experiment_planning/alternative_experiment_planning/experiment_planner_residual_3DUNet_v21.py)
-compares this value to values generated for the currently configured network topology (which are also computed by 
-FabiansUNet.compute_approx_vram_consumption) to ensure that the GPU memory target is met.
-
-## Tutorials
-We have created tutorials on how to [manually edit plans files](tutorials/edit_plans_files.md), 
-[change the target spacing](tutorials/custom_spacing.md) and 
-[changing the normalization scheme for preprocessing](tutorials/custom_preprocessing.md).
diff -urN nnUNet/documentation/inference_example_Prostate.md nnUNetNew/documentation/inference_example_Prostate.md
--- nnUNet/documentation/inference_example_Prostate.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/inference_example_Prostate.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,78 +0,0 @@
-# Example: inference with pretrained nnU-Net models
-
-This is a step-by-step example of how to run inference with pretrained nnU-Net models on the Prostate dataset of the 
-Medical Segemtnation Decathlon.
-
-1) Install nnU-Net by following the instructions [here](../readme.md#installation). Make sure to set all relevant paths, 
-also see [here](setting_up_paths.md). This step is necessary so that nnU-Net knows where to store trained models.
-2) Download the Prostate dataset of the Medical Segmentation Decathlon from 
-[here](https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2). Then extract the archive to a 
-destination of your choice.
-3) We selected the Prostate dataset for this example because we have a utility script that converts the test data into 
-the correct format. 
-
-    Decathlon data come as 4D niftis. This is not compatible with nnU-Net (see dataset format specified 
-    [here](dataset_conversion.md)). Convert the Prostate dataset into the correct format with
-
-    ```bash
-    nnUNet_convert_decathlon_task -i /xxx/Task05_Prostate
-    ```
-    
-    Note that `Task05_Prostate` must be the folder that has the three 'imagesTr', 'labelsTr', 'imagesTs' subfolders!
-    The converted dataset can be found in `$nnUNet_raw_data_base/nnUNet_raw_data` ($nnUNet_raw_data_base is the folder for 
-    raw data that you specified during installation)
-4) Download the pretrained model using this command:
-    ```bash
-    nnUNet_download_pretrained_model Task005_Prostate
-    ```
-5) The prostate dataset requires two image modalities as input. This is very much liKE RGB images have three color channels. 
-nnU-Net recognizes modalities by the file ending: a single test case of the prostate dataset therefore consists of two files 
-`case_0000.nii.gz` and `case_0001.nii.gz`. Each of these files is a 3D image. The file ending with 0000.nii.gz must 
-always contain the T2 image and 0001.nii.gz the ADC image. Whenever you are using pretrained models, you can use
-    ```bash
-    nnUNet_print_pretrained_model_info Task005_Prostate
-    ```
-   to obtain information on which modality needs to get which number. The output for Prostate is the following:
-    
-        Prostate Segmentation. 
-        Segmentation targets are peripheral and central zone, 
-        input modalities are 0: T2, 1: ADC. 
-        Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/
-6) The script we ran in 3) automatically converted the test data for us and stored them in
-`$nnUNet_raw_data_base/nnUNet_raw_data/Task005_Prostate/imagesTs`. Note that you need to do this conversion youself when 
-using other than Medcial Segmentation Decathlon datasets. No worries. Doing this is easy (often as simple as appending 
-a _0000 to the file name if only one input modality is required). Instructions can be found here [here](data_format_inference.md).
-7) You can now predict the Prostate test cases with the pretrained model. We exemplarily use the 3D full resoltion U-Net here:
-    ```bash
-    nnUNet_predict -i $nnUNet_raw_data_base/nnUNet_raw_data/Task005_Prostate/imagesTs/ -o OUTPUT_DIRECTORY -t 5 -m 3d_fullres
-    ``` 
-    Note that `-t 5` specifies the task with id 5 (which corresponds to the Prostate dataset). You can also give the full 
-    task name `Task005_Prostate`. `OUTPUT_DIRECTORY` is where the resulting segmentations are saved.
-    
-    Predictions should be quite fast and you should be done within a couple of minutes. If you would like to speed it 
-    up (at the expense of a slightly lower segmentation quality) you can disable test time data augmentation by 
-    setting the `--disable_tta` flag (8x speedup). If this is still too slow for you, you can consider using only a 
-    single model instead of the ensemble by specifying `-f 0`. This will use only the model trained on fold 0 of the 
-    cross-validation for another 5x speedup.
-8) If you want to use an ensemble of different U-Net configurations for inference, you need to run the following commands:
-
-    Prediction with 3d full resolution U-Net (this command is a little different than the one above). 
-    ```bash
-    nnUNet_predict -i $nnUNet_raw_data_base/nnUNet_raw_data/Task005_Prostate/imagesTs/ -o OUTPUT_DIRECTORY_3D -t 5 --save_npz -m 3d_fullres
-    ```
-    
-    Prediction with 2D U-Net
-    ```bash
-    nnUNet_predict -i $nnUNet_raw_data_base/nnUNet_raw_data/Task005_Prostate/imagesTs/ -o OUTPUT_DIRECTORY_2D -t 5 --save_npz -m 2d
-    ```
-    `--save_npz` will tell nnU-Net to also store the softmax probabilities for ensembling. 
-    
-    You can then merge the predictions with
-    ```bash
-    nnUNet_ensemble -f OUTPUT_DIRECTORY_3D OUTPUT_DIRECTORY_2D -o OUTPUT_FOLDER_ENSEMBLE -pp POSTPROCESSING_FILE
-    ```
-   This will merge the predictions from `OUTPUT_DIRECTORY_2D` and `OUTPUT_DIRECTORY_3D`. `-pp POSTPROCESSING_FILE` 
-   (optional!) is a file that gives nnU-Net information on how to postprocess the ensemble. These files were also 
-   downloaded as part of the pretrained model weights and are located at `RESULTS_FOLDER/nnUNet/ensembles/
-   Task005_Prostate/ensemble_2d__nnUNetTrainerV2__nnUNetPlansv2.1--3d_fullres__nnUNetTrainerV2__nnUNetPlansv2.1/postprocessing.json`. 
-   We will make the postprocessing files more accessible in a future (soon!) release.
diff -urN nnUNet/documentation/setting_up_paths.md nnUNetNew/documentation/setting_up_paths.md
--- nnUNet/documentation/setting_up_paths.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/setting_up_paths.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,84 +0,0 @@
-# Setting up Paths
-
-nnU-Net relies on environment variables to know where raw data, preprocessed data and trained model weights are stored. 
-To use the full functionality of nnU-Net, the following three environment variables must be set:
-
-1) nnUNet_raw_data_base: This is where nnU-Net finds the raw data and stored the cropped data. The folder located at 
-nnUNet_raw_data_base must have at least the subfolder nnUNet_raw_data, which in turn contains one subfolder for each Task. 
-It is the responsibility of the user to bring the raw data into the appropriate format - nnU-Net will then take care of 
-the rest ;-) For more information on the required raw data format, see [here](dataset_conversion.md).
-
-    Example tree structure:
-    ```
-    nnUNet_raw_data_base/nnUNet_raw_data/Task002_Heart
-    ├── dataset.json
-    ├── imagesTr
-    │   ├── la_003_0000.nii.gz
-    │   ├── la_004_0000.nii.gz
-    │   ├── ...
-    ├── imagesTs
-    │   ├── la_001_0000.nii.gz
-    │   ├── la_002_0000.nii.gz
-    │   ├── ...
-    └── labelsTr
-        ├── la_003.nii.gz
-        ├── la_004.nii.gz
-        ├── ...
-    nnUNet_raw_data_base/nnUNet_raw_data/Task005_Prostate/
-    ├── dataset.json
-    ├── imagesTr
-    │   ├── prostate_00_0000.nii.gz
-    │   ├── prostate_00_0001.nii.gz
-    │   ├── ...
-    ├── imagesTs
-    │   ├── prostate_03_0000.nii.gz
-    │   ├── prostate_03_0001.nii.gz
-    │   ├── ...
-    └── labelsTr
-        ├── prostate_00.nii.gz
-        ├── prostate_01.nii.gz
-        ├── ...
-    ```
-
-2) nnUNet_preprocessed: This is the folder where the preprocessed data will be saved. The data will also be read from 
-this folder during training. Therefore it is important that it is located on a drive with low access latency and high 
-throughput (a regular sata or nvme SSD is sufficient).
-
-3) RESULTS_FOLDER: This specifies where nnU-Net will save the model weights. If pretrained models are downloaded, this 
-is where it will save them.
-
-### How to set environment variables
-(nnU-Net was developed for Ubuntu/Linux. The following guide is intended for this operating system and will not work on 
-others. We do not provide support for other operating systems!)
-
-There are several ways you can do this. The most common one is to set the paths in your .bashrc file, which is located 
-in your home directory. For me, this file is located at /home/fabian/.bashrc. You can open it with any text editor of 
-choice. If you do not see the file, that may be because it is hidden by default. You can run `ls -al /home/fabian` to 
-ensure that you see it. In rare cases it may not be present and you can simply create it with `touch /home/fabian/.bashrc`.
-
-Once the file is open in a text editor, add the following lines to the bottom:
-```
-export nnUNet_raw_data_base="/media/fabian/nnUNet_raw_data_base"
-export nnUNet_preprocessed="/media/fabian/nnUNet_preprocessed"
-export RESULTS_FOLDER="/media/fabian/nnUNet_trained_models"
-```
-
-(of course adapt the paths to your system and remember that nnUNet_preprocessed should be located on an SSD!)
-
-Then save and exit. To be save, make sure to reload the .bashrc by running `source /home/fabian/.bashrc`. Reloading 
-needs only be done on terminal sessions that were already open before you saved the changes. Any new terminal you open 
-after will have these paths set. You can verify that the paths are set up properly by typing `echo $RESULTS_FOLDER` 
-etc and it should print out the correct folder.
-
-### An alternative way of setting these paths
-The method above sets the paths permanently (until you delete the lines from your .bashrc) on your system. If you wish 
-to set them only temporarily, you can run the export commands in your terminal:
-
-```
-export nnUNet_raw_data_base="/media/fabian/nnUNet_raw_data_base"
-export nnUNet_preprocessed="/media/fabian/nnUNet_preprocessed"
-export RESULTS_FOLDER="/media/fabian/nnUNet_trained_models"
-```
-
-This will set the paths for the current terminal session only (the variables will be lost if you close the terminal 
-and need to be reset every time).
diff -urN nnUNet/documentation/training_example_Hippocampus.md nnUNetNew/documentation/training_example_Hippocampus.md
--- nnUNet/documentation/training_example_Hippocampus.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/training_example_Hippocampus.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,40 +0,0 @@
-# Example: 3D U-Net training on the Hippocampus dataset
- 
-This is a step-by-step example on how to run a 3D full resolution Training with the Hippocampus dataset from the 
-Medical Segmentation Decathlon.
-
-1) Install nnU-Net by following the instructions [here](../readme.md#installation). Make sure to set all relevant paths, 
-also see [here](setting_up_paths.md). This step is necessary so that nnU-Net knows where to store raw data, 
-preprocessed data and trained models.
-2) Download the Hippocampus dataset of the Medical Segmentation Decathlon from 
-[here](https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2). Then extract the archive to a 
-destination of your choice.
-3) Decathlon data come as 4D niftis. This is not compatible with nnU-Net (see dataset format specified 
-[here](dataset_conversion.md)). Convert the Hippocampus dataset into the correct format with
-
-    ```bash
-    nnUNet_convert_decathlon_task -i /xxx/Task04_Hippocampus
-    ```
-    
-    Note that `Task04_Hippocampus` must be the folder that has the three 'imagesTr', 'labelsTr', 'imagesTs' subfolders!
-    The converted dataset can be found in $nnUNet_raw_data_base/nnUNet_raw_data ($nnUNet_raw_data_base is the folder for 
-    raw data that you specified during installation)
-4) You can now run nnU-Nets pipeline configuration (and the preprocessing) with the following line:
-    ```bash
-    nnUNet_plan_and_preprocess -t 4
-    ```
-   Where 4 refers to the task ID of the Hippocampus dataset.
-5) Now you can already start network training. This is how you train a 3d full resoltion U-Net on the Hippocampus dataset:
-    ```bash
-    nnUNet_train 3d_fullres nnUNetTrainerV2 4 0
-    ```
-   nnU-Net per default requires all trainings as 5-fold cross validation. The command above will run only the training for the 
-   first fold (fold 0). 4 is the task identifier of the hippocampus dataset. Training one fold should take about 9 
-   hours on a modern GPU.
-   
-This tutorial is only intended to demonstrate how easy it is to get nnU-Net running. You do not need to finish the 
-network training - pretrained models for the hippocampus task are available (see [here](../readme.md#run-inference)).
-
-The only prerequisite for running nnU-Net on your custom dataset is to bring it into a structured, nnU-Net compatible 
-format. nnU-Net will take care of the rest. See [here](dataset_conversion.md) for instructions on how to convert 
-datasets into nnU-Net compatible format.
diff -urN nnUNet/documentation/tutorials/custom_preprocessing.md nnUNetNew/documentation/tutorials/custom_preprocessing.md
--- nnUNet/documentation/tutorials/custom_preprocessing.md	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/documentation/tutorials/custom_preprocessing.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,60 +0,0 @@
-When you would like to interfere with the way resampling during preprocessing is handled or you would like to implement 
-a custom normalization scheme, you need to create a new custom preprocessor class and an ExperimentPlanner to go along 
-with it. While this may appear cumbersome, the great thing about this approach is that the same code will be used for 
-inference as well thus guaranteeing that images are preprocessed properly (i.e. the way the model expects).
-
-In this tutorial we will implement a custom normalization scheme for the Task120 Massachusetts Road Segmentation. Make 
-sure to download the dataset and run the code in [Task120_Massachusetts_RoadSegm.py](../../nnunet/dataset_conversion/Task120_Massachusetts_RoadSegm.py) prior to this tutorial.
-
-The images in the dataset are RGB with a value range of [0, 255]. nnU-nets defaultnormalization scheme will normalize 
-each color channel independently to have mean 0 and standard deviation 1. This works reasonably well, but may result 
-in a shift of the color channels relative to each other and thus disturb the models performance. To address that, the new
-normalization will rescale the value range from [0, 255] to [0, 1] by simply dividing the intensities of each image by 
-255. Thus, there will be no longer a shift between the color channels.
-
-The new preprocessor class is located in [preprocessor_scale_RGB_to_0_1.py](../../nnunet/preprocessing/custom_preprocessors/preprocessor_scale_RGB_to_0_1.py). 
-To acutally use it, we need to tell the ExperimentPlanner its name. For this purpose, it is best to create a new 
-ExperimentPlanner class. I created one and placed it in [experiment_planner_2DUNet_v21_RGB_scaleto_0_1.py](../../nnunet/experiment_planning/alternative_experiment_planning/normalization/experiment_planner_2DUNet_v21_RGB_scaleto_0_1.py).
-
-Now go have a look at these two classes. Details are in the comments there.
-
-To run the new preprocessor, you need to specify its accompanying ExperimentPlanner when running 
-`nnUNet_plan_and_preprocess`:
-
-```bash
-nnUNet_plan_and_preprocess -t 120 -pl3d None -pl2d ExperimentPlanner2D_v21_RGB_scaleTo_0_1
-```
-
-After that you can run the training:
-
-```bash
-nnUNet_train 2d nnUNetTrainerV2 120 FOLD -p nnUNet_RGB_scaleTo_0_1
-```
-
-Note that `nnUNet_RGB_scaleTo_0_1` is the plans identifier defined in our custom ExperimentPlanner. Specify it for all 
-nnUNet_* commands whenever you want to use the models resulting from this training.
-
-Now let all 5 folds run for the original nnU-Net as well as the one that uses the newly defined normalization scheme. 
-To compare the results, you can make use of nnUNet_determine_postprocessing to get the necessary metrics, for example:
-
-```bash
-nnUNet_determine_postprocessing -t 120 -tr nnUNetTrainerV2 -p nnUNet_RGB_scaleTo_0_1
-```
-
-This will create a `cv_niftis_raw` and `cv_niftis_postprocessed` subfolder in the training output directory. In each
- of these folders is a summary.json file that you can open with a regular text editor. In this file, there are metrics 
- for each training example in the dataset representing the outcome of the 5-fold cross-validation. At the very bottom 
- of the file, the metrics are aggregated through averaging (field "mean") and this is what you should be using to 
- compare the experiments. I recommend using the non-postprocessed summary.json (located in `cv_niftis_raw`) for this 
- because determining the postprocessing may actually overfit to the training dataset. Here are the results I obtained:
- 
-Vanilla nnU-Net:    0.7720\
-new normalization scheme: 0.7711
-
-(no improvement but hey it was worth a try!)
-
-Remember to always place custom ExperimentPlanner in nnunet.experiment_planning (any file or submodule) and 
-preprocessors in nnunet.preprocessing (any file or submodule). Make sure to use unique names!
-
-The example classes from this tutorial only work with 2D. You need to generate a separate set of planner and preprocessor
-for 3D data (cumbersome, I know. Needs to be improved in the future).
\ No newline at end of file
diff -urN nnUNet/documentation/tutorials/custom_spacing.md nnUNetNew/documentation/tutorials/custom_spacing.md
--- nnUNet/documentation/tutorials/custom_spacing.md	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/documentation/tutorials/custom_spacing.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,33 +0,0 @@
-Sometimes you want to set custom target spacings. This is done by creating a custom ExperimentPlanner.
-Let's run this with the Task002_Heart example from the Medical Segmentation Decathlon. This dataset is not too large 
-and working with it is therefore a breeze!
-
-This example requires you to have downloaded the dataset and converted it to nnU-Net format with 
-nnUNet_convert_decathlon_task
-
-We need to run the nnUNet_plan_and_preprocess command with a custom 3d experiment planner to achieve this. I have 
-created an appropriate trainer and placed it in [experiment_planner_baseline_3DUNet_v21_customTargetSpacing_2x2x2.py](../../nnunet/experiment_planning/alternative_experiment_planning/target_spacing/experiment_planner_baseline_3DUNet_v21_customTargetSpacing_2x2x2.py)
-
-This will set a hard coded target spacing of 2x2x2mm for the 3d_fullres configuration (3d_lowres is unchanged). 
-Go have a look at this ExperimentPlanner now.
-
-To run nnUNet_plan_and_preprocess with the new ExperimentPlanner, simply specify it:
-
-`nnUNet_plan_and_preprocess -t 2 -pl2d None -pl3d ExperimentPlanner3D_v21_customTargetSpacing_2x2x2`
-
-Note how we are disabling 2D preprocessing with `-pl2d None`. The ExperimentPlanner I created is only for 3D. 
-You will need to generate a separate one for 3D.
-
-Once this is completed your task will have been preprocessed with the desired target spacing. You can use it by 
-specifying the new custom plans file that is linked to it (see 
-`ExperimentPlanner3D_v21_customTargetSpacing_2x2x2` source code) when running any nnUNet_* command, for example:
-
-`nnUNet_train 3d_fullres nnUNetTrainerV2 2 FOLD -p nnUNetPlansv2.1_trgSp_2x2x2`
-
-(make sure to omit the `_plans_3D.pkl` suffix!)
-
-**TODO**: how to compare with the default run?
-
-IMPORTANT: When creating custom ExperimentPlanner, make sure to always place them under a unique class name somewhere
-in the nnunet.experiment_planning module. If you create subfolders, make sure they contain an __init__py file 
-(can be empty). If you fail to do so nnU-Net will not be able to locate your ExperimentPlanner and crash!  
\ No newline at end of file
diff -urN nnUNet/documentation/tutorials/edit_plans_files.md nnUNetNew/documentation/tutorials/edit_plans_files.md
--- nnUNet/documentation/tutorials/edit_plans_files.md	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/documentation/tutorials/edit_plans_files.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,141 +0,0 @@
-Changing the plans files grants you a lot of flexibility: You can depart from nnU-Net's default configuration and play 
-with different U-Net topologies, batch sizes and patch sizes. It is a powerful tool!
-To better understand the components describing the network topology in our plans files, please read section 6.2 
-in the [supplementary information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-020-01008-z/MediaObjects/41592_2020_1008_MOESM1_ESM.pdf) 
-(page 13) of our paper!
-    
-The goal of this tutorial is to demonstrate how to read and modify plans files and how to use them in your 
-experiments. The file used here works with Task120 and requires you to have downloaded the dataset, run 
-nnunet.dataset_conversion.Task120_Massachusetts_RoadSegm.py and then run nnUNet_plan_and_preprocess for it.
-
-Note that this task is 2D only, but the same principles we use here can be easily extended to 3D and other tasks as well.
-
-The output of `nnUNet_plan_and_preprocess` for this task looks like this:
-
-    [{'batch_size': 2, 
-    'num_pool_per_axis': [8, 8], 
-    'patch_size': array([1280, 1024]), 
-    'median_patient_size_in_voxels': array([   1, 1500, 1500]), 
-    'current_spacing': array([999.,   1.,   1.]), 
-    'original_spacing': array([999.,   1.,   1.]), 
-    'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 
-    'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 
-    'do_dummy_2D_data_aug': False}]
-
-This is also essentially what is saved in the plans file under the key 'plans_per_stage'
-
-For this task, nnU-Net intends to use a patch size of 1280x1024 and a U-Net architecture with 8 pooling 
-operations per axis. Due to GPU memory constraints, the batch size is just 2.
-
-Knowing the dataset we could hypothesize that a different approach might produce better results: The decision 
-of whether a pixel belongs to 'road' or not does not depend on the large contextual information that the large 
-patch size (and U-Net architecture) offer and could instead be made with more local information. Training with
-a batch size of just 2 in a dataset with 800 training cases means that each batch contains only limited variability.
-So one possible conclusion could be that smaller patches but larger batch sizes might result in a better 
-segmentation outcome. Let's investigate (using the same GPU memory constraint, determined manually with trial 
-and error!):
-
-Variant 1: patch size 512x512, batch size 12
-The following snippet makes the necessary adaptations to the plans file
-
-```python
-from batchgenerators.utilities.file_and_folder_operations import *
-import numpy as np
-from nnunet.paths import preprocessing_output_dir
-task_name = 'Task120_MassRoadsSeg'
-
-# if it breaks upon loading the plans file, make sure to run the Task120 dataset conversion and
-# nnUNet_plan_and_preprocess first!
-plans_fname = join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_plans_2D.pkl')
-plans = load_pickle(plans_fname)
-plans['plans_per_stage'][0]['batch_size'] = 12
-plans['plans_per_stage'][0]['patch_size'] = np.array((512, 512))
-plans['plans_per_stage'][0]['num_pool_per_axis'] = [7, 7]
-# because we changed the num_pool_per_axis, we need to change conv_kernel_sizes and pool_op_kernel_sizes as well!
-plans['plans_per_stage'][0]['pool_op_kernel_sizes'] = [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]]
-plans['plans_per_stage'][0]['conv_kernel_sizes'] = [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]
-# for a network with num_pool_per_axis [7,7] the correct length of pool kernel sizes is 7 and the length of conv
-# kernel sizes is 8! Note that you can also change these numbers if you believe it makes sense. A pool kernel size
-# of 1 will result in no pooling along that axis, a kernel size of 3 will reduce the size of the feature map
-# representations by factor 3 instead of 2.
-
-# save the plans under a new plans name. Note that the new plans file must end with _plans_2D.pkl!
-save_pickle(plans, join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_ps512_bs12_plans_2D.pkl'))
-```
-
-
-Variant 2: patch size 256x256, batch size 60
-
-```python
-from batchgenerators.utilities.file_and_folder_operations import *
-import numpy as np
-from nnunet.paths import preprocessing_output_dir
-task_name = 'Task120_MassRoadsSeg'
-plans_fname = join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_plans_2D.pkl')
-plans = load_pickle(plans_fname)
-plans['plans_per_stage'][0]['batch_size'] = 60
-plans['plans_per_stage'][0]['patch_size'] = np.array((256, 256))
-plans['plans_per_stage'][0]['num_pool_per_axis'] = [6, 6]
-plans['plans_per_stage'][0]['pool_op_kernel_sizes'] = [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]]
-plans['plans_per_stage'][0]['conv_kernel_sizes'] = [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]
-save_pickle(plans, join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_ps256_bs60_plans_2D.pkl'))
-```
-
-You can now use these custom plans files to train the networks and compare the results! Remeber that all nnUNet_* 
-commands have the -h argument to display their arguments. nnUNet_train supports custom plans via the -p argument. 
-Custom plans must be the prefix, so here this is everything except '_plans_2D.pkl':
-
-Variant 1:
-```bash
-nnUNet_train 2d nnUNetTrainerV2 120 FOLD -p nnUNetPlansv2.1_ps512_bs12
-```
-
-Variant 2:
-```bash
-nnUNet_train 2d nnUNetTrainerV2 120 FOLD -p nnUNetPlansv2.1_ps256_bs60
-```
-
-
-Let all 5 folds run for each plans file (original and the two variants). To compare the results, you can make use of
-nnUNet_determine_postprocessing to get the necessary metrics, for example:
-
-```bash
-nnUNet_determine_postprocessing -t 120 -tr nnUNetTrainerV2 -p nnUNetPlansv2.1_ps512_bs12 -m 2d
-```
-
-This will create a `cv_niftis_raw` and `cv_niftis_postprocessed` subfolder in the training output directory. In each
- of these folders is a summary.json file that you can open with a regular text editor. In this file, there are metrics 
- for each training example in the dataset representing the outcome of the 5-fold cross-validation. At the very bottom 
- of the file, the metrics are aggregated through averaging (field "mean") and this is what you should be using to 
- compare the experiments. I recommend using the non-postprocessed summary.json (located in `cv_niftis_raw`) for this 
- because determining the postprocessing may actually overfit to the training dataset. Here are the results I obtained:
- 
-Vanilla nnU-Net:    0.7720\
-Variant 1: 0.7724\
-Variant 2: 0.7734
-
-The results are remarkable similar and I would not necessarily conclude that such a small improvement in Dice is a 
-significant outcome. Nonetheless it was worth a try :-)
-
-Despite the results shown here I would like to emphasize that modifying the plans file can be an extremely powerful 
-tool to improve the performance of nnU-Net on some datasets. You never know until you try it.
-
-**ADDITIONAL INFORMATION (READ THIS!)**
-
-  - when working with 3d plans ('nnUNetPlansv2.1_plans_3D.pkl') the 3d_lowres and 3d_fullres stage will be encoded 
-    in the same plans file. If len(plans['plans_per_stage']) == 2, then [0] is the 3d_lowres and [1] is the 
-    3d_fullres variant. If len(plans['plans_per_stage']) == 1 then [0] will be 3d_fullres and 3d_cascade_fullres 
-    (they use the same plans).
-    
-  - 'pool_op_kernel_sizes' together with determines 'patch_size' determines the size of the feature map 
-    representations at the bottleneck. For Variant 1 & 2 presented here, the size of the feature map representation is
-    
-    `print(plans['plans_per_stage'][0]['patch_size'] / np.prod(plans['plans_per_stage'][0]['pool_op_kernel_sizes'], 0))`
-    
-    > [4., 4.]
-    
-    If you see a non-integer number here, your model will crash! Make sure these are always integers!
-    nnU-Net will never create smaller bottlenecks than 4!
-
-  - do not change the 'current_spacing' in the plans file! This will not work properly. To change the target spacing, 
-  have a look at the [custom spacing](custom_spacing.md) tutorial.
\ No newline at end of file
diff -urN nnUNet/documentation/using_nnUNet_as_baseline.md nnUNetNew/documentation/using_nnUNet_as_baseline.md
--- nnUNet/documentation/using_nnUNet_as_baseline.md	2023-02-17 12:15:29.543698700 +0000
+++ nnUNetNew/documentation/using_nnUNet_as_baseline.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,4 +0,0 @@
-(The U-Net is the current punching bag of methods development. nnU-Net is going to be that looking forward. That is 
-cool (great, in fact!), but it should be done correctly. Here are tips on how to benchmark against nnU-Net)
-
-This is work in progress
\ No newline at end of file
diff -urN nnUNet/nnunet/__init__.py nnUNetNew/nnunet/__init__.py
--- nnUNet/nnunet/__init__.py	2023-02-17 12:15:29.544698700 +0000
+++ nnUNetNew/nnunet/__init__.py	2023-02-17 12:13:25.641936963 +0000
@@ -1,7 +1,3 @@
 from __future__ import absolute_import
-print("\n\nPlease cite the following paper when using nnUNet:\n\nIsensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "
-      "\"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" "
-      "Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n\n")
-print("If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n")
 
 from . import *
\ No newline at end of file
diff -urN nnUNet/nnunet/dataset_conversion/Task029_LiverTumorSegmentationChallenge.py nnUNetNew/nnunet/dataset_conversion/Task029_LiverTumorSegmentationChallenge.py
--- nnUNet/nnunet/dataset_conversion/Task029_LiverTumorSegmentationChallenge.py	2023-02-17 12:15:29.545698700 +0000
+++ nnUNetNew/nnunet/dataset_conversion/Task029_LiverTumorSegmentationChallenge.py	2023-02-17 12:13:25.641936963 +0000
@@ -52,11 +52,11 @@
 
 
 if __name__ == "__main__":
-    train_dir = "/media/fabian/DeepLearningData/tmp/LITS-Challenge-Train-Data"
-    test_dir = "/media/fabian/My Book/datasets/LiTS/test_data"
+    train_dir = "/data/lits17"
+    test_dir = "/data/lits17"
 
 
-    output_folder = "/media/fabian/My Book/MedicalDecathlon/MedicalDecathlon_raw_splitted/Task029_LITS"
+    output_folder = "/data/nnUNet_raw_data_base/nnUNet_raw_data/Task029_LITS"
     img_dir = join(output_folder, "imagesTr")
     lab_dir = join(output_folder, "labelsTr")
     img_dir_te = join(output_folder, "imagesTs")
diff -urN nnUNet/nnunet/dataset_conversion/Task040_KiTS.py nnUNetNew/nnunet/dataset_conversion/Task040_KiTS.py
--- nnUNet/nnunet/dataset_conversion/Task040_KiTS.py	2023-02-17 12:15:29.545698700 +0000
+++ nnUNetNew/nnunet/dataset_conversion/Task040_KiTS.py	2023-02-17 12:13:25.642936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -198,8 +199,8 @@
 
 
 if __name__ == "__main__":
-    base = "/media/fabian/My Book/datasets/KiTS2019_Challenge/kits19/data"
-    out = "/media/fabian/My Book/MedicalDecathlon/nnUNet_raw_splitted/Task040_KiTS"
+    base = "/data/kits19/data"
+    out = "/data/nnUNet_raw_data_base/nnUNet_raw_data/Task041_KiTS"
     cases = subdirs(base, join=False)
 
     maybe_mkdir_p(out)
@@ -230,11 +231,13 @@
         "1": "Kidney",
         "2": "Tumor"
     }
-    json_dict['numTraining'] = len(cases)
-    json_dict['numTest'] = 0
+    json_dict['numTraining'] = 210
+    json_dict['numTest'] = 90
     json_dict['training'] = [{'image': "./imagesTr/%s.nii.gz" % i, "label": "./labelsTr/%s.nii.gz" % i} for i in
-                             cases]
-    json_dict['test'] = []
+                             cases[:210]]
+    json_dict['test'] = ["./imagesTs/%s.nii.gz" % i for i in cases[210:]]
 
     save_json(json_dict, os.path.join(out, "dataset.json"))
 
+
+# python nnunet/dataset_conversion/Task040_KiTS.py
\ No newline at end of file
diff -urN nnUNet/nnunet/dataset_conversion/amos_convert_label.py nnUNetNew/nnunet/dataset_conversion/amos_convert_label.py
--- nnUNet/nnunet/dataset_conversion/amos_convert_label.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/dataset_conversion/amos_convert_label.py	2023-02-17 12:13:25.642936963 +0000
@@ -0,0 +1,124 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+from logging import root
+import os
+from glob import glob
+import numpy as np
+import nibabel as nib
+import json
+from batchgenerators.utilities.file_and_folder_operations import *
+
+
+def convert_label(input_nifti_file_path, out_nifti_file_path):
+    print(output_label_nifti_file_path)
+    os.system(f'mkdir -p {output_label_nifti_file_path}')
+
+    for patient in glob(f'{input_nifti_file_path}/*'):
+        patient_name = os.path.basename(os.path.normpath(patient))
+        print(patient_name)
+
+        nifti_file = nib.load(patient)
+
+        data = np.asarray(nifti_file.dataobj)
+
+        for i in range(3, 16):
+            data[np.where(data == i)] = i-1
+
+        data[np.where(data == 1)] = 20
+        data[np.where(data == 2)] = 1
+        data[np.where(data == 20)] = 2
+
+        new_nifti = nib.Nifti1Image(data, nifti_file.affine, nifti_file.header)
+        nib.save(new_nifti, os.path.join(out_nifti_file_path, patient_name))
+
+
+def convert_json(input_path, output_path):
+    with open(input_path) as f:
+        d = json.load(f)
+        # d["labels"] = {
+        #     "0": "background", 
+        #     "1": "kidney", 
+        #     "2": "spleen", 
+        #     "3": "gall bladder", 
+        #     "4": "esophagus", 
+        #     "5": "liver", 
+        #     "6": "stomach", 
+        #     "7": "arota", 
+        #     "8": "postcava", 
+        #     "9": "pancreas", 
+        #     "10": "right adrenal gland", 
+        #     "11": "left adrenal gland", 
+        #     "12": "duodenum", 
+        #     "13": "bladder", 
+        #     "14": "prostate/uterus"
+        # }
+        d["labels"] = {
+            "0": "background", 
+            "1": "spleen", 
+            "2": "right kidney", 
+            "3": "left kidney", 
+            "4": "gall bladder", 
+            "5": "esophagus", 
+            "6": "liver", 
+            "7": "stomach", 
+            "8": "arota", 
+            "9": "postcava", 
+            "10": "pancreas", 
+            "11": "right adrenal gland", 
+            "12": "left adrenal gland", 
+            "13": "duodenum", 
+            "14": "bladder", 
+            "15": "prostate/uterus"
+        }
+        d["numTest"] = 0
+        d["test"] = []
+
+    save_json(d, output_path)
+
+
+def copy_label(input_path, output_path):
+    os.system(f'mkdir -p {output_path}')
+    os.system(f'cp -r {input_path} {output_path}')
+    
+
+def convert_sample(input_path, output_path):
+    os.system(f'mkdir -p {output_path}')
+    os.system(f'cp -r {input_path} {output_path}')
+    os.system(f"cd {output_path}/imagesTr && rename 's/\.nii/_0000\.nii/' *")
+    
+
+if __name__ == '__main__':
+    root_path = '/home/vmagent/app/dataset/nnUNet_raw_data_base/nnUNet_raw_data'
+    source_task = 'Task505_AMOS'
+    target_task = 'Task508_AMOS_kidney'
+
+    input_label_nifti_file_path = f'{root_path}/{source_task}/labelsTr'
+    output_label_nifti_file_path = f'{root_path}/{target_task}/labelsTr'
+    # convert_label(input_label_nifti_file_path, output_label_nifti_file_path)
+
+    input_label_nifti_file_path = f'{root_path}/{source_task}/labelsTr'
+    output_label_nifti_file_path = f'{root_path}/{target_task}'
+    copy_label(input_label_nifti_file_path, output_label_nifti_file_path)
+
+    input_data_nifti_file_path = f'{root_path}/{source_task}/imagesTr'
+    output_data_nifti_file_path = f'{root_path}/{target_task}'
+    convert_sample(input_data_nifti_file_path, output_data_nifti_file_path)
+
+    input_json_path = f'{root_path}/{source_task}/task1_dataset.json'
+    output_json_path = f'{root_path}/{target_task}/dataset.json'
+    convert_json(input_json_path, output_json_path)
+
+
+# python nnunet/dataset_conversion/amos_convert_label.py
\ No newline at end of file
diff -urN nnUNet/nnunet/dataset_conversion/kits_convert_label.py nnUNetNew/nnunet/dataset_conversion/kits_convert_label.py
--- nnUNet/nnunet/dataset_conversion/kits_convert_label.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/dataset_conversion/kits_convert_label.py	2023-02-17 12:13:25.642936963 +0000
@@ -0,0 +1,215 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+from logging import root
+import os
+import sys
+from glob import glob
+import numpy as np
+import nibabel as nib
+import json
+from batchgenerators.utilities.file_and_folder_operations import *
+import argparse
+from nnunet.paths import nnUNet_raw_data, preprocessing_output_dir
+from copy import deepcopy
+
+
+def convert_data(input_nifti_file_path, out_nifti_file_path):
+    os.system(f'mkdir -p {out_nifti_file_path}')
+
+    generated = [
+        os.path.basename(os.path.normpath(patient))
+        for patient in 
+            glob(f'{out_nifti_file_path}/case_*')
+    ]
+
+    for patient in glob(f'{input_nifti_file_path}/case_*'):
+        patient_name = os.path.basename(os.path.normpath(patient))
+        print(patient_name)
+        if patient_name in generated:
+            continue
+
+        nifti_file = nib.load(patient)
+        nifti_file = nib.as_closest_canonical(nifti_file)
+        print(nib.aff2axcodes(nifti_file.affine))
+
+        nib.save(nifti_file, os.path.join(out_nifti_file_path, patient_name))
+
+
+def convert_label(input_nifti_file_path, out_nifti_file_path, axis_reorder=True):
+    os.system(f'mkdir -p {out_nifti_file_path}')
+    print(out_nifti_file_path)
+
+    generated = [
+        os.path.basename(os.path.normpath(patient))
+        for patient in 
+            glob(f'{out_nifti_file_path}/case_*')
+    ]
+
+    for patient in glob(f'{input_nifti_file_path}/*'):
+        patient_name = os.path.basename(os.path.normpath(patient))
+        print(patient_name)
+        if patient_name in generated:
+            continue
+
+        nifti_file = nib.load(patient)
+        if axis_reorder:
+            nifti_file = nib.as_closest_canonical(nifti_file)
+
+        # method 1
+        # data = nifti_file.get_fdata()
+        # data[np.where(data == 2)] = 1
+        # nib.save(nifti_file, os.path.join(out_nifti_file_path, patient_name))
+        # del nifti_file, data
+
+        # method 2
+        data = np.asarray(nifti_file.dataobj)
+        data[np.where(data == 2)] = 1
+        new_nifti = nib.Nifti1Image(data, nifti_file.affine, nifti_file.header)
+        nib.save(new_nifti, os.path.join(out_nifti_file_path, patient_name))
+
+
+def convert_json(input_path, output_path):
+    with open(input_path) as f:
+        d = json.load(f)
+        d["labels"] = {
+            "0": "background",
+            "1": "Kidney"
+        }
+        d["numTest"] = 0
+        d["test"] = []
+
+    save_json(d, output_path)
+
+
+def convert_predict_data():
+    input_nifti_file_path = '/data/tmp'
+    out_nifti_file_path = '/data/tmp2'
+
+    os.system(f'mkdir -p {out_nifti_file_path}')
+    print(out_nifti_file_path)
+
+    for patient in glob(f'{input_nifti_file_path}/*.gz'):
+        patient_name = os.path.basename(os.path.normpath(patient))
+        print(patient_name)
+
+        nifti_file = nib.load(patient)
+
+        # method 2
+        data = np.asarray(nifti_file.dataobj)
+        data[np.where(data == 1)] = 20
+        data[np.where(data == 2)] = 1
+        data[np.where(data == 20)] = 2
+        new_nifti = nib.Nifti1Image(data, nifti_file.affine, nifti_file.header)
+        nib.save(new_nifti, os.path.join(out_nifti_file_path, patient_name))
+
+
+def change_intensity_distribution(source_task_name, target_task_name):
+
+    def print_intensity(plans):
+        print(f"mean: {plans['dataset_properties']['intensityproperties'][0]['mean']}")
+        print(f"std: {plans['dataset_properties']['intensityproperties'][0]['sd']}")
+        print(f"percentile_99_5: {plans['dataset_properties']['intensityproperties'][0]['percentile_99_5']}")
+        print(f"percentile_00_5: {plans['dataset_properties']['intensityproperties'][0]['percentile_00_5']}")
+
+    source_plans_fname = join(preprocessing_output_dir, source_task_name, source_plan_name)
+    source_plans = load_pickle(source_plans_fname)
+
+    target_plans_fname = join(preprocessing_output_dir, target_task_name, target_plan_name)
+    target_plans = load_pickle(target_plans_fname)
+
+    print('before changing...')
+    print_intensity(source_plans)
+
+    source_plans['dataset_properties']['intensityproperties'][0]['mean']            = target_plans['dataset_properties']['intensityproperties'][0]['mean']
+    source_plans['dataset_properties']['intensityproperties'][0]['sd']              = target_plans['dataset_properties']['intensityproperties'][0]['sd']
+    source_plans['dataset_properties']['intensityproperties'][0]['percentile_99_5'] = target_plans['dataset_properties']['intensityproperties'][0]['percentile_99_5']
+    source_plans['dataset_properties']['intensityproperties'][0]['percentile_00_5'] = target_plans['dataset_properties']['intensityproperties'][0]['percentile_00_5']
+
+    print('after changing...')
+    print_intensity(source_plans)
+    
+    save_pickle(source_plans, source_plans_fname)
+
+
+def change_split_plan(file_path):
+    plans = load_pickle(file_path)
+    plans = plans[:5]
+
+    labeled_num = 34
+    small_plan = deepcopy(plans[1])
+    small_plan['train'] = small_plan['train'][-labeled_num:]
+    plans.append(small_plan)
+
+    print(small_plan)
+    save_pickle(plans, file_path)
+
+
+def move_fold_k_to_test(split_file, input_folder, output_folder, k=1):
+    os.system(f'mkdir -p {output_folder}')
+    plans = load_pickle(split_file)
+    for case in plans[k]['val']:
+        case_name = f'{case}_0000.nii.gz'
+        print(case_name)
+        os.system(f'mv {input_folder}/{case_name} {output_folder}/{case_name}')
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser()
+    parser.add_argument("task")
+    args = parser.parse_args()
+
+    if args.task == 'basic':
+        root_path = f'{nnUNet_raw_data}/nnUNet_raw_data'
+        source_task = 'Task041_KiTS'
+        target_task = 'Task507_KiTS_kidney'
+
+        input_label_nifti_file_path = f'{root_path}/{source_task}/labelsTr'
+        output_label_nifti_file_path = f'{root_path}/{target_task}/labelsTr'
+        convert_label(input_label_nifti_file_path, output_label_nifti_file_path)
+
+        input_data_nifti_file_path = f'{root_path}/{source_task}/imagesTr'
+        output_data_nifti_file_path = f'{root_path}/{target_task}/imagesTr'
+        convert_data(input_data_nifti_file_path, output_data_nifti_file_path)
+
+        input_json_path = f'{root_path}/{source_task}/dataset.json'
+        output_json_path = f'{root_path}/{target_task}/dataset.json'
+        convert_json(input_json_path, output_json_path)
+    
+    elif args.task == 'intensity':
+        source_task_name = 'Task507_KiTS_kidney'
+        target_task_name = 'Task508_AMOS_kidney'
+
+        source_plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+        target_plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+
+        change_intensity_distribution(source_task_name, target_task_name)
+
+    elif args.task == 'split':
+        source_task_name = 'Task507_KiTS_kidney'
+        split_plan_file_path = f'{preprocessing_output_dir}/{source_task_name}/splits_final.pkl'
+        
+        change_split_plan(split_plan_file_path)
+    
+    elif args.task == 'move_test':
+        source_task_name = 'Task507_KiTS_kidney'
+        split_plan_file_path = f'{preprocessing_output_dir}/{source_task_name}/splits_final.pkl'
+        
+        input_folder = f'{nnUNet_raw_data}/{source_task_name}/imagesTr'
+        output_folder = f'{nnUNet_raw_data}/{source_task_name}/testTr'
+
+        move_fold_k_to_test(split_plan_file_path, input_folder, output_folder, k=1)
+
+
+
+# python nnunet/dataset_conversion/kits_convert_label.py move_test
\ No newline at end of file
diff -urN nnUNet/nnunet/dataset_conversion/kits_tumor_size.py nnUNetNew/nnunet/dataset_conversion/kits_tumor_size.py
--- nnUNet/nnunet/dataset_conversion/kits_tumor_size.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/dataset_conversion/kits_tumor_size.py	2023-02-17 12:13:25.642936963 +0000
@@ -0,0 +1,104 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+import pandas as pd
+import json
+from nnunet.paths import nnUNet_raw_data
+from batchgenerators.utilities.file_and_folder_operations import *
+import shutil
+import os
+
+
+def get_size_map(file_path):
+    with open(file_path, 'r') as f:
+        case_list = json.load(f)
+    case_id = []
+    radiographic_size = []
+    for case in case_list:
+        case_id.append(case['case_id'])
+        radiographic_size.append(case['radiographic_size'])
+    d = {
+        "case_id": case_id,
+        "r_size": radiographic_size
+    }
+    df = pd.DataFrame(d)
+    print(df['r_size'].describe())
+    case_small = df[df['r_size'] < 4]['case_id'].to_list()
+    case_big = df[df['r_size'] >= 4]['case_id'].to_list()
+    return case_small, case_big
+
+
+def save_raw_data(image_base, label_base, task_id, task_name, case_ids):
+    
+    foldername = "Task%03.0d_%s" % (task_id, task_name)
+    out_base = join(nnUNet_raw_data, foldername)
+    imagestr = join(out_base, "imagesTr")
+    imagests = join(out_base, "imagesTs")
+    labelstr = join(out_base, "labelsTr")
+    maybe_mkdir_p(imagestr)
+    maybe_mkdir_p(imagests)
+    maybe_mkdir_p(labelstr)
+
+    for p in nifti_files(image_base, join=False):
+        cur_case_id = p.rsplit('_', 1)[0]
+        if cur_case_id not in case_ids:
+            continue
+        label_file = join(label_base, f'{cur_case_id}.nii.gz')
+        image_file = join(image_base, p)
+        shutil.copy(image_file, join(imagestr, cur_case_id + "_0000.nii.gz"))
+        shutil.copy(label_file, join(labelstr, cur_case_id + ".nii.gz"))
+
+    json_dict = {}
+    json_dict['name'] = "KiTS"
+    json_dict['description'] = "kidney and kidney tumor segmentation"
+    json_dict['tensorImageSize'] = "4D"
+    json_dict['reference'] = "KiTS data for nnunet"
+    json_dict['licence'] = ""
+    json_dict['release'] = "0.0"
+    json_dict['modality'] = {
+        "0": "CT",
+    }
+    json_dict['labels'] = {
+        "0": "background",
+        "1": "Kidney",
+        "2": "Tumor"
+    }
+
+    json_dict['numTraining'] = len(case_ids)
+    json_dict['numTest'] = 0
+    json_dict['training'] = [{'image': "./imagesTr/%s.nii.gz" % i, "label": "./labelsTr/%s.nii.gz" % i} for i in
+                             case_ids]
+    json_dict['test'] = []
+
+    save_json(json_dict, os.path.join(out_base, "dataset.json"))
+
+
+if __name__ == "__main__":
+    file_path = '/data/kits19/data/kits.json'
+    os.system(f'du -sh ~/code/kits19/data/kits.json')
+    case_small, case_big = get_size_map(file_path)
+
+
+    image_base = f'{nnUNet_raw_data}/Task040_KiTS/imagesTr'
+    label_base = f'{nnUNet_raw_data}/Task040_KiTS/labelsTr'
+
+    task_id = 502
+    task_name = "KiTS_tumor_small"
+    save_raw_data(image_base, label_base, task_id, task_name, case_small)
+
+    task_id = 503
+    task_name = "KiTS_tumor_big"
+    save_raw_data(image_base, label_base, task_id, task_name, case_big)
+    
+
diff -urN nnUNet/nnunet/evaluation/evaluator.py nnUNetNew/nnunet/evaluation/evaluator.py
--- nnUNet/nnunet/evaluation/evaluator.py	2023-02-17 12:15:29.547698700 +0000
+++ nnUNetNew/nnunet/evaluation/evaluator.py	2023-02-17 12:13:25.643936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -396,8 +397,10 @@
         json_dict["id"] = hashlib.md5(json.dumps(json_dict).encode("utf-8")).hexdigest()[:12]
         save_json(json_dict, json_output_file)
 
+    mean_score = [v["Dice"] for k,v in all_scores["mean"].items() if k != "0"]
+    mean_score = sum(mean_score) / len(mean_score) 
 
-    return all_scores
+    return mean_score
 
 
 def aggregate_scores_for_experiment(score_file,
@@ -443,7 +446,7 @@
     return json_dict
 
 
-def evaluate_folder(folder_with_gts: str, folder_with_predictions: str, labels: tuple, **metric_kwargs):
+def evaluate_folder(folder_with_gts: str, folder_with_predictions: str, labels: tuple, eval_on_common: bool, **metric_kwargs):
     """
     writes a summary.json to folder_with_predictions
     :param folder_with_gts: folder where the ground truth segmentations are saved. Must be nifti files.
@@ -453,6 +456,9 @@
     """
     files_gt = subfiles(folder_with_gts, suffix=".nii.gz", join=False)
     files_pred = subfiles(folder_with_predictions, suffix=".nii.gz", join=False)
+    if eval_on_common:
+        files_gt = [i for i in files_gt if i in files_pred]
+        files_pred = [i for i in files_pred if i in files_gt]
     assert all([i in files_pred for i in files_gt]), "files missing in folder_with_predictions"
     assert all([i in files_gt for i in files_pred]), "files missing in folder_with_gts"
     test_ref_pairs = [(join(folder_with_predictions, i), join(folder_with_gts, i)) for i in files_pred]
@@ -479,5 +485,7 @@
                                                                        "evaluate the background label (0) but in "
                                                                        "this case that would not give any useful "
                                                                        "information.")
+    parser.add_argument("--common", required=False, default=False, action="store_true",
+                        help="find the intersection between pred and ref, and then evaluate")
     args = parser.parse_args()
-    return evaluate_folder(args.ref, args.pred, args.l)
+    return evaluate_folder(args.ref, args.pred, args.l, args.common)
diff -urN nnUNet/nnunet/experiment_planning/alternative_experiment_planning/target_spacing/experiment_planner_baseline_3DUNet_v21_customTargetSpacing_kits19.py nnUNetNew/nnunet/experiment_planning/alternative_experiment_planning/target_spacing/experiment_planner_baseline_3DUNet_v21_customTargetSpacing_kits19.py
--- nnUNet/nnunet/experiment_planning/alternative_experiment_planning/target_spacing/experiment_planner_baseline_3DUNet_v21_customTargetSpacing_kits19.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/experiment_planning/alternative_experiment_planning/target_spacing/experiment_planner_baseline_3DUNet_v21_customTargetSpacing_kits19.py	2023-02-17 12:13:25.643936963 +0000
@@ -0,0 +1,34 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+import numpy as np
+from nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21 import ExperimentPlanner3D_v21
+from nnunet.paths import *
+
+
+class ExperimentPlanner3D_v21_customTargetSpacing_kits19(ExperimentPlanner3D_v21):
+    def __init__(self, folder_with_cropped_data, preprocessed_output_folder):
+        super(ExperimentPlanner3D_v21, self).__init__(folder_with_cropped_data, preprocessed_output_folder)
+        # we change the data identifier and plans_fname. This will make this experiment planner save the preprocessed
+        # data in a different folder so that they can co-exist with the default (ExperimentPlanner3D_v21). We also
+        # create a custom plans file that will be linked to this data
+        self.data_identifier = "nnUNetData_plans_v2.1_trgSp_kits19"
+        self.plans_fname = join(self.preprocessed_output_folder,
+                                "nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl")
+
+    def get_target_spacing(self):
+        # simply return the desired spacing as np.array
+        return np.array([3.22, 1.62, 1.62]) # make sure this is float!!!! Not int!
+
diff -urN nnUNet/nnunet/experiment_planning/change_batch_size.py nnUNetNew/nnunet/experiment_planning/change_batch_size.py
--- nnUNet/nnunet/experiment_planning/change_batch_size.py	2023-02-17 12:15:29.548698700 +0000
+++ nnUNetNew/nnunet/experiment_planning/change_batch_size.py	2023-02-17 12:13:25.643936963 +0000
@@ -1,3 +1,17 @@
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
 from batchgenerators.utilities.file_and_folder_operations import *
 import numpy as np
 
diff -urN nnUNet/nnunet/experiment_planning/change_patch_size.py nnUNetNew/nnunet/experiment_planning/change_patch_size.py
--- nnUNet/nnunet/experiment_planning/change_patch_size.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/experiment_planning/change_patch_size.py	2023-02-17 12:13:25.643936963 +0000
@@ -0,0 +1,75 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+from batchgenerators.utilities.file_and_folder_operations import *
+import numpy as np
+from nnunet.paths import preprocessing_output_dir, network_training_output_dir_base
+import sys
+
+# task_name = 'Task040_KiTS'
+# plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+
+# task_name = 'Task502_KiTS_tumor_small'
+# plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+
+# task_name = 'Task503_KiTS_tumor_big'
+# plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+
+# task_name = 'Task504_KiTS_kidney'
+# plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+# plan_name = 'plans.pkl'
+
+task_name = 'Task506_AMOS_kidney'
+plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+# # plan_name = 'model_final_checkpoint.model.pkl'
+
+# task_name = 'Task507_KiTS_kidney'
+# plan_name = 'nnUNetPlansv2.1_trgSp_kits19_plans_3D.pkl'
+
+
+if __name__ == '__main__':
+    # data_prop_fname = join(preprocessing_output_dir, task_name, 'dataset_properties.pkl')
+    # print(load_pickle(data_prop_fname))
+    # sys.exit(0)
+
+    plans_fname = join(preprocessing_output_dir, task_name, plan_name)
+    plans = load_pickle(plans_fname)
+
+    # 修改 plan.pkl
+    # # plans['num_classes'] = 14
+    # plans['conv_per_stage'] = 2
+    # for i in range(len(plans['plans_per_stage'])):
+    #     plans['plans_per_stage'][i]['patch_size'] = np.array((80, 128, 128))
+    #     # plans['plans_per_stage'][i]['batch_size'] = 3
+    # save_pickle(plans, plans_fname)
+
+    # # 修改 model.pkl
+    # plans['plans']['transpose_forward'] = [2, 0, 1]
+    # plans['plans']['transpose_backward'] = [1, 2, 0]
+    # save_pickle(plans, plans_fname)
+    
+
+    # print(plans)
+    for i in range(len(plans['plans_per_stage'])):
+        print(f"plans['plans_per_stage']{i}: {plans['plans_per_stage'][i]}")
+        print(f"patch_size: {plans['plans_per_stage'][i]['patch_size']}")
+        print(f"batch_size: {plans['plans_per_stage'][i]['batch_size']}")
+    print(f"num_classes: {plans['num_classes']}")
+    print(f"conv_per_stage: {plans['conv_per_stage']}")
+    print(f"mean: {plans['dataset_properties']['intensityproperties'][0]['mean']}")
+    print(f"std: {plans['dataset_properties']['intensityproperties'][0]['sd']}")
+    print(f"percentile_99_5: {plans['dataset_properties']['intensityproperties'][0]['percentile_99_5']}")
+    print(f"percentile_00_5: {plans['dataset_properties']['intensityproperties'][0]['percentile_00_5']}")
+    print(f"transpose_forward: {plans['transpose_forward']}")
+    print(f"transpose_backward: {plans['transpose_backward']}")
diff -urN nnUNet/nnunet/experiment_planning/experiment_planner_baseline_3DUNet.py nnUNetNew/nnunet/experiment_planning/experiment_planner_baseline_3DUNet.py
--- nnUNet/nnunet/experiment_planning/experiment_planner_baseline_3DUNet.py	2023-02-17 12:15:29.549698700 +0000
+++ nnUNetNew/nnunet/experiment_planning/experiment_planner_baseline_3DUNet.py	2023-02-17 12:13:25.644936963 +0000
@@ -426,6 +426,8 @@
             shutil.rmtree(join(self.preprocessed_output_folder, "gt_segmentations"))
         shutil.copytree(join(self.folder_with_cropped_data, "gt_segmentations"),
                         join(self.preprocessed_output_folder, "gt_segmentations"))
+        self.load_my_plans()
+        
         normalization_schemes = self.plans['normalization_schemes']
         use_nonzero_mask_for_normalization = self.plans['use_mask_for_norm']
         intensityproperties = self.plans['dataset_properties']['intensityproperties']
diff -urN nnUNet/nnunet/experiment_planning/nnUNet_plan_and_preprocess.py nnUNetNew/nnunet/experiment_planning/nnUNet_plan_and_preprocess.py
--- nnUNet/nnunet/experiment_planning/nnUNet_plan_and_preprocess.py	2023-02-17 12:15:29.549698700 +0000
+++ nnUNetNew/nnunet/experiment_planning/nnUNet_plan_and_preprocess.py	2023-02-17 12:13:25.644936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -22,7 +23,7 @@
 from nnunet.utilities.task_name_id_conversion import convert_id_to_task_name
 from nnunet.preprocessing.sanity_checks import verify_dataset_integrity
 from nnunet.training.model_restore import recursive_find_python_class
-
+import sys
 
 def main():
     import argparse
@@ -39,9 +40,12 @@
     parser.add_argument("-pl2d", "--planner2d", type=str, default="ExperimentPlanner2D_v21",
                         help="Name of the ExperimentPlanner class for the 2D U-Net. Default is ExperimentPlanner2D_v21. "
                              "Can be 'None', in which case this U-Net will not be configured")
+    parser.add_argument("-no_plan", action="store_true",
+                        help="Set this flag if you dont want to run the planning. If this is set then this script "
+                             "will not run the experiment planning and not create the plans file")
     parser.add_argument("-no_pp", action="store_true",
                         help="Set this flag if you dont want to run the preprocessing. If this is set then this script "
-                             "will only run the experiment planning and create the plans file")
+                             "will not run the experiment preprocessing")
     parser.add_argument("-tl", type=int, required=False, default=8,
                         help="Number of processes used for preprocessing the low resolution data for the 3D low "
                              "resolution U-Net. This can be larger than -tf. Don't overdo it or you will run out of "
@@ -76,6 +80,7 @@
 
     args = parser.parse_args()
     task_ids = args.task_ids
+    dont_run_planning = args.no_plan
     dont_run_preprocessing = args.no_pp
     tl = args.tl
     tf = args.tf
@@ -103,6 +108,7 @@
 
         if args.verify_dataset_integrity:
             verify_dataset_integrity(join(nnUNet_raw_data, task_name))
+            sys.exit(0)
 
         crop(task_name, False, tf)
 
@@ -156,12 +162,14 @@
                                          args.overwrite_plans_identifier)
             else:
                 exp_planner = planner_3d(cropped_out_dir, preprocessing_output_dir_this_task)
-            exp_planner.plan_experiment()
+            if not dont_run_planning:
+                exp_planner.plan_experiment()
             if not dont_run_preprocessing:  # double negative, yooo
                 exp_planner.run_preprocessing(threads)
         if planner_2d is not None:
             exp_planner = planner_2d(cropped_out_dir, preprocessing_output_dir_this_task)
-            exp_planner.plan_experiment()
+            if not dont_run_planning:
+                exp_planner.plan_experiment()
             if not dont_run_preprocessing:  # double negative, yooo
                 exp_planner.run_preprocessing(threads)
 
diff -urN nnUNet/nnunet/inference/predict.py nnUNetNew/nnunet/inference/predict.py
--- nnUNet/nnunet/inference/predict.py	2023-02-17 12:15:29.551698700 +0000
+++ nnUNetNew/nnunet/inference/predict.py	2023-02-17 12:13:25.644936963 +0000
@@ -16,7 +16,7 @@
 import argparse
 from copy import deepcopy
 from typing import Tuple, Union, List
-
+import os
 import numpy as np
 from batchgenerators.augmentations.utils import resize_segmentation
 from nnunet.inference.segmentation_export import save_segmentation_nifti_from_softmax, save_segmentation_nifti
@@ -212,18 +212,28 @@
             d = data
 
         print("predicting", output_filename)
+        trainer.was_initialized = False
         trainer.load_checkpoint_ram(params[0], False)
-        softmax = trainer.predict_preprocessed_data_return_seg_and_softmax(
+        trainer.optional_model_optimize(True, model_index=0)
+        trainer.network.eval()
+        softmax = trainer.postprocess_pred(
+            trainer.predict_preprocessed_data_return_seg_and_softmax(
             d, do_mirroring=do_tta, mirror_axes=trainer.data_aug_params['mirror_axes'], use_sliding_window=True,
             step_size=step_size, use_gaussian=True, all_in_gpu=all_in_gpu,
             mixed_precision=mixed_precision)[1]
+        )
 
-        for p in params[1:]:
+        for index, p in enumerate(params[1:]):
+            trainer.was_initialized = False
             trainer.load_checkpoint_ram(p, False)
-            softmax += trainer.predict_preprocessed_data_return_seg_and_softmax(
+            trainer.optional_model_optimize(True, model_index=index)
+            trainer.network.eval()
+            softmax += trainer.postprocess_pred(
+                trainer.predict_preprocessed_data_return_seg_and_softmax(
                 d, do_mirroring=do_tta, mirror_axes=trainer.data_aug_params['mirror_axes'], use_sliding_window=True,
                 step_size=step_size, use_gaussian=True, all_in_gpu=all_in_gpu,
                 mixed_precision=mixed_precision)[1]
+            )
 
         if len(params) > 1:
             softmax /= len(params)
diff -urN nnUNet/nnunet/inference/predict_simple.py nnUNetNew/nnunet/inference/predict_simple.py
--- nnUNet/nnunet/inference/predict_simple.py	2023-02-17 12:15:29.551698700 +0000
+++ nnUNetNew/nnunet/inference/predict_simple.py	2023-02-17 12:13:25.645936963 +0000
@@ -20,6 +20,7 @@
 from nnunet.paths import default_plans_identifier, network_training_output_dir, default_cascade_trainer, default_trainer
 from batchgenerators.utilities.file_and_folder_operations import join, isdir
 from nnunet.utilities.task_name_id_conversion import convert_id_to_task_name
+from nnunet.utilities.log import print_to_console
 
 
 def main():
@@ -211,7 +212,7 @@
 
     model_folder_name = join(network_training_output_dir, model, task_name, trainer + "__" +
                               args.plans_identifier)
-    print("using model stored in ", model_folder_name)
+    print_to_console("using model stored in ", model_folder_name)
     assert isdir(model_folder_name), "model output folder not found. Expected: %s" % model_folder_name
 
     predict_from_folder(model_folder_name, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,
@@ -219,6 +220,7 @@
                         overwrite_existing=overwrite_existing, mode=mode, overwrite_all_in_gpu=all_in_gpu,
                         mixed_precision=not args.disable_mixed_precision,
                         step_size=step_size, checkpoint_name=args.chk)
+    print_to_console("finish inference and exporting!")
 
 
 if __name__ == "__main__":
diff -urN nnUNet/nnunet/network_architecture/DA/CAC_UNet.py nnUNetNew/nnunet/network_architecture/DA/CAC_UNet.py
--- nnUNet/nnunet/network_architecture/DA/CAC_UNet.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/network_architecture/DA/CAC_UNet.py	2023-02-17 12:13:25.645936963 +0000
@@ -0,0 +1,68 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+import torch
+import torch.nn as nn
+import numpy as np
+
+
+class CAC_UNet(nn.Module):
+    def __init__(self, backbone, adapter, source_loss_weight, target_loss_weight):
+        super().__init__()
+        self.backbone = backbone
+        self.adapter = adapter
+        self.source_loss_weight = source_loss_weight
+        self.target_loss_weight = target_loss_weight 
+        self.backbone_loss = backbone.loss
+        self.inference_network = None
+
+    def forward(self, input, inference=True):
+        if inference and self.inference_network:
+            return self.inference_network(input)
+        else:
+            return self.backbone(input)
+
+    def compute_loss(self, input_sample, label, source_pred_to_target_pred=lambda x: x):
+        source_data, data = input_sample
+        source_label, target = label
+
+        source_output, *source_feat = self.backbone(source_data)
+        source_loss = self.backbone_loss(source_output, source_label)
+        
+        target_loss = None
+        output, *target_feat = self.backbone(data)
+        output = [source_pred_to_target_pred(item) for item in output]
+        if self.target_loss_weight > 0:
+            target_loss = self.backbone_loss(output, target)
+        
+        adv_loss = self.adapter(*(
+            (source_output, *source_feat),
+            (output, *target_feat),
+            source_label
+        ))
+
+        # calc total loss
+        total_loss = source_loss * self.source_loss_weight
+        if self.target_loss_weight > 0:
+            total_loss += target_loss * self.target_loss_weight
+        if adv_loss:
+            total_loss += adv_loss
+
+        return (total_loss, adv_loss, source_loss, target_loss)
+
+    def get_parameters(self, model_finetune=True):
+        parameters = self.backbone.get_parameters(model_finetune)
+        parameters += self.adapter.get_parameters()
+        return parameters
diff -urN nnUNet/nnunet/network_architecture/DA/CAC_UNet_bkp.py nnUNetNew/nnunet/network_architecture/DA/CAC_UNet_bkp.py
--- nnUNet/nnunet/network_architecture/DA/CAC_UNet_bkp.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/network_architecture/DA/CAC_UNet_bkp.py	2023-02-17 12:13:25.645936963 +0000
@@ -0,0 +1,70 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+import torch
+import torch.nn as nn
+import numpy as np
+
+
+class CAC_UNet(nn.Module):
+    def __init__(self, backbone, adapter, source_loss_weight, target_loss_weight):
+        super().__init__()
+        self.backbone = backbone
+        self.adapter = adapter
+        self.source_loss_weight = source_loss_weight
+        self.target_loss_weight = target_loss_weight 
+        self.inference_network = None
+
+    def forward(self, input, inference=True):
+        if inference and self.inference_network:
+            return self.inference_network(input)
+        else:
+            return self.backbone(input)
+
+    def compute_loss(self, input_sample, label, source_pred_to_target_pred=lambda x: x):
+        source_data, data = input_sample
+        source_label, target = label
+
+        source_output, *source_feat = self.backbone(source_data)
+        source_loss = self.backbone.loss(source_output, source_label)
+        
+        target_loss = None
+        output, *target_feat = self.backbone(data)
+        output = [source_pred_to_target_pred(item) for item in output]
+        if self.target_loss_weight > 0:
+            target_loss = self.backbone.loss(output, target)
+        
+        adv_loss = self.adapter(*(
+            (source_output, *source_feat),
+            (output, *target_feat),
+            source_label
+        ))
+
+        # calc total loss
+        total_loss = source_loss * self.source_loss_weight
+        if self.target_loss_weight > 0:
+            total_loss += target_loss * self.target_loss_weight
+        if adv_loss:
+            total_loss += adv_loss
+
+        return (total_loss, adv_loss, source_loss, target_loss)
+
+    def get_parameters(self, model_finetune=True):
+        parameters = self.backbone.get_parameters(model_finetune)
+        parameters += self.adapter.get_parameters()
+        return parameters
+
+    def get_metrics(self):
+        return self.adapter.get_metrics()
diff -urN nnUNet/nnunet/network_architecture/DA/DA_Loss.py nnUNetNew/nnunet/network_architecture/DA/DA_Loss.py
--- nnUNet/nnunet/network_architecture/DA/DA_Loss.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/network_architecture/DA/DA_Loss.py	2023-02-17 12:13:25.645936963 +0000
@@ -0,0 +1,196 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+from typing import Optional, List
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from nnunet.network_architecture.DA.discriminator import SegDiscriminator, FCDiscriminator
+
+from tllib.modules.grl import GradientReverseLayer, WarmStartGradientReverseLayer
+from tllib.utils.metric import binary_accuracy, accuracy
+
+
+class SegDomainAdversarialLoss(nn.Module):
+    def __init__(self, domain_discriminator: nn.Module, reduction: Optional[str] = 'mean',
+                 grl: Optional[nn.Module] = WarmStartGradientReverseLayer(alpha=1., lo=0., hi=1., max_iters=1000, auto_step=True), 
+                 sigmoid=True):
+        super(SegDomainAdversarialLoss, self).__init__()
+
+        # define gradient reverselayer type
+        # grl = GradientReverseLayer()
+        # grl = WarmStartGradientReverseLayer(alpha=1., lo=0., hi=1., max_iters=1000, auto_step=True)
+        self.grl = grl
+        
+        self.domain_discriminator = domain_discriminator
+        self.sigmoid = sigmoid
+        self.reduction = reduction
+        self.bce = lambda input, target, weight: \
+            F.binary_cross_entropy(input, target, weight=weight, reduction=reduction)
+        self.domain_discriminator_accuracy = None
+
+    def forward(self, f_s_list: List[torch.Tensor], f_t_list: List[torch.Tensor],
+                w_s: Optional[torch.Tensor] = None, w_t: Optional[torch.Tensor] = None) -> torch.Tensor:
+        f = [
+            self.grl(torch.cat((f_s, f_t), dim=0))
+            for f_s, f_t in zip(f_s_list, f_t_list)
+        ]
+        d = self.domain_discriminator(f)
+        d_s, d_t = d.chunk(2, dim=0)
+        d_label_s = torch.ones((d_s.size(0), 1)).to(d_s.device)
+        d_label_t = torch.zeros((d_t.size(0), 1)).to(d_t.device)
+        self.domain_discriminator_accuracy = 0.5 * (
+                    binary_accuracy(d_s, d_label_s) + binary_accuracy(d_t, d_label_t))
+
+        if w_s is None:
+            w_s = torch.ones_like(d_label_s)
+        if w_t is None:
+            w_t = torch.ones_like(d_label_t)
+        return 0.5 * (
+            F.binary_cross_entropy_with_logits(d_s, d_label_s, weight=w_s.view_as(d_s), reduction=self.reduction) +
+            F.binary_cross_entropy_with_logits(d_t, d_label_t, weight=w_t.view_as(d_t), reduction=self.reduction)
+        )
+
+
+class SegOutDomainAdversarialLoss(nn.Module):
+    def __init__(self, domain_discriminator: nn.Module, reduction: Optional[str] = 'mean',
+                 grl: Optional[nn.Module] = WarmStartGradientReverseLayer(alpha=1., lo=0., hi=1., max_iters=1000, auto_step=True), 
+                 sigmoid=True):
+        super().__init__()
+        self.grl = grl
+        self.domain_discriminator = domain_discriminator
+        self.sigmoid = sigmoid
+        self.reduction = reduction
+        self.bce = lambda input, target, weight: \
+            F.binary_cross_entropy(input, target, weight=weight, reduction=reduction)
+        self.domain_discriminator_accuracy = None
+
+    def forward(self, f_s: torch.Tensor, f_t: torch.Tensor,
+                w_s: Optional[torch.Tensor] = None, w_t: Optional[torch.Tensor] = None) -> torch.Tensor:
+        f = self.grl(torch.cat((f_s, f_t), dim=0))
+        d = self.domain_discriminator(f)
+        if self.sigmoid:
+            d_s, d_t = d.chunk(2, dim=0)
+            d_label_s = torch.ones((d_s.size(0), 1)).to(d_s.device)
+            d_label_t = torch.zeros((d_t.size(0), 1)).to(d_t.device)
+            self.domain_discriminator_accuracy = 0.5 * (
+                        binary_accuracy(d_s, d_label_s) + binary_accuracy(d_t, d_label_t))
+
+            if w_s is None:
+                w_s = torch.ones_like(d_label_s)
+            if w_t is None:
+                w_t = torch.ones_like(d_label_t)
+            return 0.5 * (
+                F.binary_cross_entropy(d_s, d_label_s, weight=w_s.view_as(d_s), reduction=self.reduction) +
+                F.binary_cross_entropy(d_t, d_label_t, weight=w_t.view_as(d_t), reduction=self.reduction)
+            )
+        else:
+            d_label = torch.cat((
+                torch.ones((f_s.size(0),)).to(f_s.device),
+                torch.zeros((f_t.size(0),)).to(f_t.device),
+            )).long()
+            if w_s is None:
+                w_s = torch.ones((f_s.size(0),)).to(f_s.device)
+            if w_t is None:
+                w_t = torch.ones((f_t.size(0),)).to(f_t.device)
+            self.domain_discriminator_accuracy = accuracy(d, d_label)
+            loss = F.cross_entropy(d, d_label, reduction='none') * torch.cat([w_s, w_t], dim=0)
+            if self.reduction == "mean":
+                return loss.mean()
+            elif self.reduction == "sum":
+                return loss.sum()
+            elif self.reduction == "none":
+                return loss
+            else:
+                raise NotImplementedError(self.reduction)
+
+
+class CACDomainAdversarialLoss(nn.Module):
+    def __init__(self, **kwargs):
+        super().__init__()
+        self.loss_weights = kwargs.pop('loss_weight')
+
+        self.encoder_domain_adv = self.decoder_domain_adv = self.seg_domain_adv = None
+
+        if self.loss_weights[0] > 0:
+            encoder_domain_discri = SegDiscriminator(**kwargs)
+            self.encoder_domain_adv = SegDomainAdversarialLoss(encoder_domain_discri)
+            
+        if self.loss_weights[1] > 0:
+            decoder_domain_discri = SegDiscriminator(**kwargs)
+            self.decoder_domain_adv = SegDomainAdversarialLoss(decoder_domain_discri)
+            
+        if self.loss_weights[2] > 0:
+            seg_domain_discri = FCDiscriminator()
+            self.seg_domain_adv = SegOutDomainAdversarialLoss(seg_domain_discri)
+
+    def get_parameters(self):
+        parameters = []
+        if self.encoder_domain_adv:
+            parameters += [{
+                'params': self.encoder_domain_adv.parameters(),
+            }]
+        if self.decoder_domain_adv:
+            parameters += [{
+                'params': self.decoder_domain_adv.parameters(),
+            }]
+        if self.seg_domain_adv:
+            parameters += [{
+                'params': self.seg_domain_adv.parameters(),
+            }]
+        return parameters
+    
+    def get_metrics(self):
+        metric = {}
+        if self.encoder_domain_adv:
+            metric['train/encoder_adv_acc'] = self.encoder_domain_adv.domain_discriminator_accuracy.item() / 100.
+
+        if self.decoder_domain_adv:
+            metric['train/decoder_adv_acc'] = self.decoder_domain_adv.domain_discriminator_accuracy.item() / 100.
+
+        if self.seg_domain_adv:
+            metric['train/seg_adv_acc'] = self.seg_domain_adv.domain_discriminator_accuracy.item() / 100.
+        return metric
+
+    def forward(self, *data):
+        source_data, target_data, source_label = data
+        source_output, encoder_f_s, decoder_f_s = source_data
+        output, encoder_f_t, decoder_f_t = target_data
+
+        loss_list = []
+        if self.encoder_domain_adv:
+            gan_encoder = self.loss_weights[0] * self.encoder_domain_adv(encoder_f_s, encoder_f_t)
+            loss_list.append(gan_encoder)
+        if self.decoder_domain_adv:
+            gan_decoder = self.loss_weights[1] * self.decoder_domain_adv(decoder_f_s, decoder_f_t)
+            loss_list.append(gan_decoder)
+            # from thop import profile
+            # macs, params = profile(self.decoder_domain_adv, inputs=(decoder_f_s, decoder_f_t, ))
+            # print(f'flops: {macs}, params: {params}')
+            # sys.exit(0)
+        if self.seg_domain_adv:
+            gan_seg = self.loss_weights[2] * self.seg_domain_adv(
+                source_label[0],
+                torch.argmax(source_output[0], dim=1, keepdim=True)
+            )
+            loss_list.append(gan_seg)
+
+        if not loss_list:
+            return None
+        l = loss_list[0]
+        for item in loss_list[1:]:
+            l += item
+        return l
+
diff -urN nnUNet/nnunet/network_architecture/DA/discriminator.py nnUNetNew/nnunet/network_architecture/DA/discriminator.py
--- nnUNet/nnunet/network_architecture/DA/discriminator.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/network_architecture/DA/discriminator.py	2023-02-17 12:13:25.646936963 +0000
@@ -0,0 +1,205 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+from torch import nn
+import torch
+import numpy as np
+from nnunet.network_architecture.initialization import InitWeights_He
+from nnunet.network_architecture.generic_UNet import (ConvDropoutNormNonlin, StackedConvLayers)
+import torch.nn.functional
+
+
+
+class SegDiscriminator(nn.Module):
+    BASE_NUM_FILTERS = 50
+
+    def __init__(self, input_channels, threeD=True, 
+                 pool_op_kernel_sizes=None,
+                 ):
+
+        super(SegDiscriminator, self).__init__()
+
+        if threeD:
+            conv_op = nn.Conv3d
+            dropout_op = nn.Dropout3d
+            norm_op = nn.InstanceNorm3d
+        else:
+            conv_op = nn.Conv2d
+            dropout_op = nn.Dropout2d
+            norm_op = nn.InstanceNorm2d
+         
+        nonlin=nn.LeakyReLU
+        weightInitializer=InitWeights_He(1e-2)
+        basic_block=ConvDropoutNormNonlin
+        num_conv_per_stage=2
+
+        nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}
+        # dropout_op_kwargs = {'p': 0.5, 'inplace': True}
+        dropout_op_kwargs = {'p': 0, 'inplace': True}
+        # norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}
+        norm_op_kwargs = {'eps': 1e-5, 'affine': True}
+        conv_kwargs = {'stride': 1, 'dilation': 1, 'bias': True}
+
+        self.input_channels = input_channels
+        self.conv_kwargs = conv_kwargs
+        self.nonlin = nonlin
+        self.nonlin_kwargs = nonlin_kwargs
+        self.dropout_op_kwargs = dropout_op_kwargs
+        self.norm_op_kwargs = norm_op_kwargs
+        self.weightInitializer = weightInitializer
+        self.conv_op = conv_op
+        self.norm_op = norm_op
+        self.dropout_op = dropout_op
+
+        if conv_op == nn.Conv2d:
+            if pool_op_kernel_sizes is None:
+                pool_op_kernel_sizes = [(2, 2)] * len(input_channels)
+            self.avgpool = nn.AdaptiveAvgPool2d(1)
+            conv_kernel_sizes = (3, 3)
+            conv_pad_sizes = [1, 1]
+        elif conv_op == nn.Conv3d:
+            if pool_op_kernel_sizes is None:
+                pool_op_kernel_sizes = [(2, 2, 2)] * len(input_channels)
+            self.avgpool = nn.AdaptiveAvgPool3d(1)
+            conv_kernel_sizes = (3, 3, 3)
+            conv_pad_sizes = [1, 1, 1]
+        else:
+            raise ValueError("unknown convolution dimensionality, conv op: %s" % str(conv_op))
+
+        self.conv_kwargs['kernel_size'] = conv_kernel_sizes
+        self.conv_kwargs['padding'] = conv_pad_sizes
+
+        self.conv_blocks_context = []
+        for d in range(len(input_channels)):
+
+            if d == 0:
+                input_features = input_channels[d]
+                output_features = self.BASE_NUM_FILTERS
+            else:
+                input_features = d * self.BASE_NUM_FILTERS + input_channels[d]
+                output_features = (d+1) * self.BASE_NUM_FILTERS
+            
+            first_stride = pool_op_kernel_sizes[d]
+
+            # add convolutions
+            self.conv_blocks_context.append(StackedConvLayers(input_features, output_features, num_conv_per_stage,
+                                                              self.conv_op, self.conv_kwargs, self.norm_op,
+                                                              self.norm_op_kwargs, self.dropout_op,
+                                                              self.dropout_op_kwargs, self.nonlin, self.nonlin_kwargs,
+                                                              first_stride, basic_block=basic_block))
+
+        # register all modules properly
+        self.conv_blocks_context = nn.ModuleList(self.conv_blocks_context)
+        if self.weightInitializer is not None:
+            self.apply(self.weightInitializer)
+        
+    def forward(self, features):
+        if not isinstance(features, list) and not isinstance(features, tuple):
+            features = [features]
+
+        input = features[0]
+        x = self.conv_blocks_context[0](input)
+
+        for i in range(1, len(self.input_channels)):
+            x = torch.cat((x, features[i]), dim=1)
+            x = self.conv_blocks_context[i](x)
+   
+        x = self.avgpool(x)
+        x = x.view(x.size(0) * x.size(1), -1)
+
+        return x
+        
+
+class FCDiscriminator(nn.Module):
+    BASE_NUM_FILTERS = 50
+    CONV_NUM = 4
+
+    def __init__(self):
+        super().__init__()
+
+        input_channel=1
+        num_conv_per_stage=2
+        pool_op_kernel_size = (2, 2)
+        first_stride = pool_op_kernel_size
+        conv_kernel_sizes = (3, 3)
+        conv_pad_sizes = [1, 1]
+        weightInitializer=InitWeights_He(1e-2)
+        basic_block=ConvDropoutNormNonlin
+        avgpool = nn.AdaptiveAvgPool2d(1)
+
+        conv_op = nn.Conv2d
+        norm_op = nn.BatchNorm2d
+        dropout_op = nn.Dropout2d
+        nonlin=nn.LeakyReLU
+
+        nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}
+        # dropout_op_kwargs = {'p': 0.5, 'inplace': True}
+        dropout_op_kwargs = {'p': 0, 'inplace': True}
+        # norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}
+        norm_op_kwargs = {'eps': 1e-5, 'affine': True}
+        conv_kwargs = {'stride': 1, 'dilation': 1, 'bias': True}
+
+        self.conv_kwargs = conv_kwargs
+        self.nonlin = nonlin
+        self.nonlin_kwargs = nonlin_kwargs
+        self.dropout_op_kwargs = dropout_op_kwargs
+        self.norm_op_kwargs = norm_op_kwargs
+        self.weightInitializer = weightInitializer
+        self.conv_op = conv_op
+        self.norm_op = norm_op
+        self.dropout_op = dropout_op
+        self.avgpool = avgpool
+        self.conv_kwargs['kernel_size'] = conv_kernel_sizes
+        self.conv_kwargs['padding'] = conv_pad_sizes
+
+        self.conv_blocks_context = []
+        for d in range(self.CONV_NUM):
+
+            if d == 0:
+                input_features = input_channel
+            else:
+                input_features = d * self.BASE_NUM_FILTERS
+            output_features = (d+1) * self.BASE_NUM_FILTERS
+
+            # add convolutions
+            self.conv_blocks_context.append(StackedConvLayers(input_features, output_features, num_conv_per_stage,
+                                                              self.conv_op, self.conv_kwargs, self.norm_op,
+                                                              self.norm_op_kwargs, self.dropout_op,
+                                                              self.dropout_op_kwargs, self.nonlin, self.nonlin_kwargs,
+                                                              first_stride, basic_block=basic_block))
+
+        # register all modules properly
+        self.conv_blocks_context = nn.ModuleList(self.conv_blocks_context)
+        if self.weightInitializer is not None:
+            self.apply(self.weightInitializer)
+
+        self.final_layer = nn.Sequential(
+            nn.Linear(output_features, 1),
+            nn.Sigmoid()
+        )
+        
+    def forward(self, x):
+        x = torch.flatten(x, start_dim=0, end_dim=2)
+        x = torch.unsqueeze(x, 1)
+
+        for i in range(self.CONV_NUM):
+            x = self.conv_blocks_context[i](x)
+   
+        x = self.avgpool(x)
+        x = x.view(x.size(0), -1)
+        x = self.final_layer(x)
+
+        return x
diff -urN nnUNet/nnunet/network_architecture/DA/generic_UNet_DA.py nnUNetNew/nnunet/network_architecture/DA/generic_UNet_DA.py
--- nnUNet/nnunet/network_architecture/DA/generic_UNet_DA.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/network_architecture/DA/generic_UNet_DA.py	2023-02-17 12:13:25.646936963 +0000
@@ -0,0 +1,129 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+from copy import deepcopy
+from nnunet.utilities.nd_softmax import softmax_helper
+from torch import nn
+import torch
+import numpy as np
+from nnunet.network_architecture.initialization import InitWeights_He
+from nnunet.network_architecture.generic_UNet import Generic_UNet, ConvDropoutNormNonlin
+from nnunet.training.loss_functions.deep_supervision import MultipleOutputLoss2
+from nnunet.training.loss_functions.dice_loss import DC_and_CE_loss
+import torch.nn.functional
+
+
+class Generic_UNet_DA(Generic_UNet):
+
+    def __init__(self, threeD, input_channels, base_num_features, num_classes, 
+                 num_conv_per_stage=2,  
+                 pool_op_kernel_sizes=None,
+                 conv_kernel_sizes=None,
+                ):
+        
+        if threeD:
+            conv_op = nn.Conv3d
+            dropout_op = nn.Dropout3d
+            norm_op = nn.InstanceNorm3d
+
+        else:
+            conv_op = nn.Conv2d
+            dropout_op = nn.Dropout2d
+            norm_op = nn.InstanceNorm2d
+        
+        self.threeD = threeD
+        num_pool = len(pool_op_kernel_sizes)
+        feat_map_mul_on_downscale=2
+        nonlin = nn.LeakyReLU
+        norm_op_kwargs = {'eps': 1e-5, 'affine': True}
+        dropout_op_kwargs = {'p': 0, 'inplace': True}
+        nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}
+        deep_supervision=True
+        dropout_in_localization=False
+        upscale_logits=False
+        convolutional_pooling=True
+        convolutional_upsampling=True
+        weightInitializer=InitWeights_He(1e-2)
+        final_nonlin = lambda x: x
+        max_num_features=None
+        basic_block=ConvDropoutNormNonlin
+        seg_output_use_bias=False
+
+        super().__init__(input_channels, base_num_features, num_classes, 
+                num_pool, num_conv_per_stage=num_conv_per_stage,
+                feat_map_mul_on_downscale=feat_map_mul_on_downscale, conv_op=conv_op,
+                norm_op=norm_op, norm_op_kwargs=norm_op_kwargs,
+                dropout_op=dropout_op, dropout_op_kwargs=dropout_op_kwargs,
+                nonlin=nonlin, nonlin_kwargs=nonlin_kwargs, deep_supervision=deep_supervision, 
+                dropout_in_localization=dropout_in_localization,
+                final_nonlin=final_nonlin, weightInitializer=weightInitializer, 
+                pool_op_kernel_sizes=pool_op_kernel_sizes,
+                conv_kernel_sizes=conv_kernel_sizes,
+                upscale_logits=upscale_logits, convolutional_pooling=convolutional_pooling, 
+                convolutional_upsampling=convolutional_upsampling,
+                max_num_features=max_num_features, basic_block=basic_block,
+                seg_output_use_bias=seg_output_use_bias)
+        self.set_loss()
+
+    def set_loss(self, batch_dice=True):
+        self.loss = DC_and_CE_loss({'batch_dice': batch_dice, 'smooth': 1e-5, 'do_bg': False}, {})
+
+        ################# Here we wrap the loss for deep supervision ############
+        # we need to know the number of outputs of the network
+        net_numpool = len(self.pool_op_kernel_sizes)
+
+        # we give each output a weight which decreases exponentially (division by 2) as the resolution decreases
+        # this gives higher resolution outputs more weight in the loss
+        weights = np.array([1 / (2 ** i) for i in range(net_numpool)])
+
+        # we don't use the lowest 2 outputs. Normalize weights so that they sum to 1
+        mask = np.array([True] + [True if i < net_numpool - 1 else False for i in range(1, net_numpool)])
+        weights[~mask] = 0
+        weights = weights / weights.sum()
+        self.ds_loss_weights = weights
+        # now wrap the loss
+        self.loss = MultipleOutputLoss2(self.loss, self.ds_loss_weights)
+        ################# END ###################
+
+    def forward(self, x):
+        skips = []
+        seg_outputs = []
+        decoder_feats = []
+        for d in range(len(self.conv_blocks_context) - 1):
+            x = self.conv_blocks_context[d](x)
+            skips.append(x)
+            if not self.convolutional_pooling:
+                x = self.td[d](x)
+
+        x = self.conv_blocks_context[-1](x)
+
+        for u in range(len(self.tu)):
+            x = self.tu[u](x)
+            x = torch.cat((x, skips[-(u + 1)]), dim=1)
+            x = self.conv_blocks_localization[u](x)
+            decoder_feats.append(x)
+            seg_outputs.append(self.final_nonlin(self.seg_outputs[u](x)))
+
+        if self._deep_supervision:
+            logits = tuple([seg_outputs[-1]] + [i(j) for i, j in
+                                              zip(list(self.upscale_logits_ops)[::-1], seg_outputs[:-1][::-1])])
+        else:
+            logits = tuple([seg_outputs[-1]])
+            
+        if self.training:
+            return logits, skips, decoder_feats[::-1]
+        return logits[0]
+
diff -urN nnUNet/nnunet/network_architecture/generic_UNet.py nnUNetNew/nnunet/network_architecture/generic_UNet.py
--- nnUNet/nnunet/network_architecture/generic_UNet.py	2023-02-17 12:15:29.552698700 +0000
+++ nnUNetNew/nnunet/network_architecture/generic_UNet.py	2023-02-17 12:13:25.646936963 +0000
@@ -15,6 +15,7 @@
 
 from copy import deepcopy
 from nnunet.utilities.nd_softmax import softmax_helper
+from nnunet.utilities.tensor_utilities import get_prefixed_named_param, get_unprefixed_named_param
 from torch import nn
 import torch
 import numpy as np
@@ -224,7 +225,6 @@
         self.num_classes = num_classes
         self.final_nonlin = final_nonlin
         self._deep_supervision = deep_supervision
-        self.do_ds = deep_supervision
 
         if conv_op == nn.Conv2d:
             upsample_mode = 'bilinear'
@@ -270,6 +270,7 @@
         output_features = base_num_features
         input_features = input_channels
 
+        self.encoder_channels = []
         for d in range(num_pool):
             # determine the first stride
             if d != 0 and self.convolutional_pooling:
@@ -285,6 +286,7 @@
                                                               self.norm_op_kwargs, self.dropout_op,
                                                               self.dropout_op_kwargs, self.nonlin, self.nonlin_kwargs,
                                                               first_stride, basic_block=basic_block))
+            self.encoder_channels.append(output_features)
             if not self.convolutional_pooling:
                 self.td.append(pool_op(pool_op_kernel_sizes[d]))
             input_features = output_features
@@ -401,11 +403,29 @@
             x = self.conv_blocks_localization[u](x)
             seg_outputs.append(self.final_nonlin(self.seg_outputs[u](x)))
 
-        if self._deep_supervision and self.do_ds:
-            return tuple([seg_outputs[-1]] + [i(j) for i, j in
+        if self._deep_supervision:
+            logits = tuple([seg_outputs[-1]] + [i(j) for i, j in
                                               zip(list(self.upscale_logits_ops)[::-1], seg_outputs[:-1][::-1])])
         else:
-            return seg_outputs[-1]
+            logits = tuple([seg_outputs[-1]])
+        
+        if self.training:
+            return logits
+        else:
+            return logits[0]
+
+    def get_parameters(self, model_finetune=True):
+        parameters = []
+        if model_finetune:
+            parameters += [{
+                'params': get_unprefixed_named_param(self, 'seg_outputs')
+            }]
+            parameters += [{
+                'params': get_prefixed_named_param(self, 'seg_outputs')
+            }]
+        else:
+            parameters = [{'params': self.parameters()}]
+        return parameters
 
     @staticmethod
     def compute_approx_vram_consumption(patch_size, num_pool_per_axis, base_num_features, max_num_features,
diff -urN nnUNet/nnunet/network_architecture/neural_network.py nnUNetNew/nnunet/network_architecture/neural_network.py
--- nnUNet/nnunet/network_architecture/neural_network.py	2023-02-17 12:15:29.552698700 +0000
+++ nnUNetNew/nnunet/network_architecture/neural_network.py	2023-02-17 12:13:25.646936963 +0000
@@ -17,11 +17,11 @@
 from batchgenerators.augmentations.utils import pad_nd_image
 from nnunet.utilities.random_stuff import no_op
 from nnunet.utilities.to_torch import to_cuda, maybe_to_torch
+from nnunet.utilities.nd_softmax import softmax_helper
 from torch import nn
 import torch
 from scipy.ndimage.filters import gaussian_filter
 from typing import Union, Tuple, List
-
 from torch.cuda.amp import autocast
 
 
@@ -62,7 +62,7 @@
         # depending on the loss, we do not hard code a nonlinearity into the architecture. To aggregate predictions
         # during inference, we need to apply the nonlinearity, however. So it is important to let the newtork know what
         # to apply in inference. For the most part this will be softmax
-        self.inference_apply_nonlin = lambda x: x  # softmax_helper
+        self.inference_apply_nonlin = softmax_helper
 
         # This is for saving a gaussian importance map for inference. It weights voxels higher that are closer to the
         # center. Prediction at the borders are often less accurate and are thus downweighted. Creating these Gaussians
diff -urN nnUNet/nnunet/run/default_configuration.py nnUNetNew/nnunet/run/default_configuration.py
--- nnUNet/nnunet/run/default_configuration.py	2023-02-17 12:15:29.553698700 +0000
+++ nnUNetNew/nnunet/run/default_configuration.py	2023-02-17 12:13:25.647936963 +0000
@@ -78,3 +78,25 @@
     print("\nI am using data from this folder: ", join(dataset_directory, plans['data_identifier']))
     print("###############################################")
     return plans_file, output_folder_name, dataset_directory, batch_dice, stage, trainer_class
+
+
+def get_source_configuration(network, task, plans_identifier=default_plans_identifier):
+    assert network in ['2d', '3d_lowres', '3d_fullres', '3d_cascade_fullres'], \
+        "network can only be one of the following: \'2d\', \'3d_lowres\', \'3d_fullres\', \'3d_cascade_fullres\'"
+
+    dataset_directory = join(preprocessing_output_dir, task)
+
+    if network == '2d':
+        plans_file = join(preprocessing_output_dir, task, plans_identifier + "_plans_2D.pkl")
+    else:
+        plans_file = join(preprocessing_output_dir, task, plans_identifier + "_plans_3D.pkl")
+
+    plans = load_pickle(plans_file)
+    
+    print("###############################################")
+    print("For that I will be using the following source data configuration:")
+    summarize_plans(plans_file)
+    print("\nI am using source data from this folder: ", join(dataset_directory, plans['data_identifier']))
+    print("###############################################")
+
+    return plans_file, dataset_directory
diff -urN nnUNet/nnunet/run/load_pretrained_weights.py nnUNetNew/nnunet/run/load_pretrained_weights.py
--- nnUNet/nnunet/run/load_pretrained_weights.py	2023-02-17 12:15:29.553698700 +0000
+++ nnUNetNew/nnunet/run/load_pretrained_weights.py	2023-02-17 12:13:25.647936963 +0000
@@ -18,7 +18,10 @@
     """
     THIS DOES NOT TRANSFER SEGMENTATION HEADS!
     """
-    saved_model = torch.load(fname)
+    if torch.cuda.is_available():
+        saved_model = torch.load(fname)
+    else:
+        saved_model = torch.load(fname, map_location=torch.device('cpu'))
     pretrained_dict = saved_model['state_dict']
 
     new_state_dict = {}
@@ -60,3 +63,10 @@
     else:
         raise RuntimeError("Pretrained weights are not compatible with the current network architecture")
 
+
+def print_model_param_size(model):
+    from thop import profile
+    input = torch.randn(1, 1, 80, 160, 160)
+    input = input.cuda()
+    macs, params = profile(model, inputs=(input, ))
+    print(f'flops: {macs}, params: {params}')
diff -urN nnUNet/nnunet/run/run_training.py nnUNetNew/nnunet/run/run_training.py
--- nnUNet/nnunet/run/run_training.py	2023-02-17 12:15:29.553698700 +0000
+++ nnUNetNew/nnunet/run/run_training.py	2023-02-17 12:13:25.647936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -13,6 +14,7 @@
 #    limitations under the License.
 
 
+import sys
 import argparse
 from batchgenerators.utilities.file_and_folder_operations import *
 from nnunet.run.default_configuration import get_default_configuration
@@ -23,6 +25,9 @@
 from nnunet.training.network_training.nnUNetTrainerCascadeFullRes import nnUNetTrainerCascadeFullRes
 from nnunet.training.network_training.nnUNetTrainerV2_CascadeFullRes import nnUNetTrainerV2CascadeFullRes
 from nnunet.utilities.task_name_id_conversion import convert_id_to_task_name
+import torch
+from nnunet.utilities.distributed import setup_dist, cleanup_dist, get_network
+
 
 
 def main():
@@ -31,7 +36,9 @@
     parser.add_argument("network_trainer")
     parser.add_argument("task", help="can be task name or task id")
     parser.add_argument("fold", help='0, 1, ..., 5 or \'all\'')
-    parser.add_argument("-val", "--validation_only", help="use this if you want to only run the validation",
+    parser.add_argument("-no_train", "--not_training", help="use this if you do not want to train the model",
+                        action="store_true")
+    parser.add_argument("-val", "--run_validate", help="use this if you want to run the validation",
                         action="store_true")
     parser.add_argument("-c", "--continue_training", help="use this if you want to continue a training",
                         action="store_true")
@@ -58,15 +65,15 @@
                         help="not used here, just for fun")
     parser.add_argument("--valbest", required=False, default=False, action="store_true",
                         help="hands off. This is not intended to be used")
-    parser.add_argument("--fp32", required=False, default=False, action="store_true",
-                        help="disable mixed precision training and run old school fp32")
+    parser.add_argument("--fp16", required=False, default=False, action="store_true",
+                        help="disable fp32 precision training and run mixed precision")
     parser.add_argument("--val_folder", required=False, default="validation_raw",
                         help="name of the validation folder. No need to use this for most people")
     parser.add_argument("--disable_saving", required=False, action='store_true',
                         help="If set nnU-Net will not save any parameter files (except a temporary checkpoint that "
                              "will be removed at the end of the training). Useful for development when you are "
                              "only interested in the results and want to save some disk space")
-    parser.add_argument("--disable_postprocessing_on_folds", required=False, action='store_true',
+    parser.add_argument("--enable_postprocessing_on_folds", required=False, action='store_true',
                         help="Running postprocessing on each fold only makes sense when developing with nnU-Net and "
                              "closely observing the model performance on specific configurations. You do not need it "
                              "when applying nnU-Net because the postprocessing for this will be determined only once "
@@ -89,17 +96,48 @@
                         help='path to nnU-Net checkpoint file to be used as pretrained model (use .model '
                              'file, for example model_final_checkpoint.model). Will only be used when actually training. '
                              'Optional. Beta. Use with caution.')
-
+    parser.add_argument('-chk',
+                        help='checkpoint name, default: model_final_checkpoint',
+                        required=False,
+                        default='model_final_checkpoint')
+    parser.add_argument('--backend',
+                        help='backend for distributed training',
+                        required=False,
+                        default='ccl')
+    parser.add_argument("--epochs", required=False, default=1000, type=int, 
+        help = "maximum training epochs")
+    parser.add_argument("--batch_size", required=False, default=0, type=int, 
+        help = "training batch size")
+    parser.add_argument("--initial_lr", required=False, default=1e-2, type=float, 
+        help = "initial learning rate")
+    parser.add_argument("--ipex", required=False, default=False, action="store_true",
+        help="enable Intel Extension for PyTorch while training in CPU")
     args = parser.parse_args()
 
+    # gloo, nccl, ccl
+    if torch.cuda.is_available():
+        backend = 'nccl'
+    else:
+        try:
+            import torch_ccl
+        except Exception as e:
+            print('No module named torch_ccl')
+        try:
+            import oneccl_bindings_for_pytorch
+        except Exception as e:
+            print('No module named oneccl_bindings_for_pytorch')
+        backend = args.backend
+    setup_dist(backend)
+
     task = args.task
     fold = args.fold
     network = args.network
     network_trainer = args.network_trainer
-    validation_only = args.validation_only
+    not_training = args.not_training
+    run_validate = args.run_validate
     plans_identifier = args.p
     find_lr = args.find_lr
-    disable_postprocessing_on_folds = args.disable_postprocessing_on_folds
+    enable_postprocessing_on_folds = args.enable_postprocessing_on_folds
 
     use_compressed_data = args.use_compressed_data
     decompress_data = not use_compressed_data
@@ -107,8 +145,7 @@
     deterministic = args.deterministic
     valbest = args.valbest
 
-    fp32 = args.fp32
-    run_mixed_precision = not fp32
+    run_mixed_precision = args.fp16
 
     val_folder = args.val_folder
     # interp_order = args.interp_order
@@ -148,10 +185,19 @@
         assert issubclass(trainer_class,
                           nnUNetTrainer), "network_trainer was found but is not derived from nnUNetTrainer"
 
+    model_finetune = False
+    if (args.pretrained_weights is not None):
+        model_finetune = True
+
     trainer = trainer_class(plans_file, fold, output_folder=output_folder_name, dataset_directory=dataset_directory,
                             batch_dice=batch_dice, stage=stage, unpack_data=decompress_data,
                             deterministic=deterministic,
-                            fp16=run_mixed_precision)
+                            fp16=run_mixed_precision, 
+                            epochs=args.epochs,
+                            batch_size=args.batch_size,
+                            initial_lr=args.initial_lr,
+                            model_finetune=model_finetune,
+                            enable_ipex=args.ipex)
     if args.disable_saving:
         trainer.save_final_checkpoint = False # whether or not to save the final checkpoint
         trainer.save_best_checkpoint = False  # whether or not to save the best checkpoint according to
@@ -160,40 +206,46 @@
         # the training chashes
         trainer.save_latest_only = True  # if false it will not store/overwrite _latest but separate files each
 
-    trainer.initialize(not validation_only)
+    trainer.initialize(not not_training)
 
     if find_lr:
         trainer.find_lr()
     else:
-        if not validation_only:
+        if not not_training:
             if args.continue_training:
                 # -c was set, continue a previous training and ignore pretrained weights
+                trainer.optional_model_optimize()
                 trainer.load_latest_checkpoint()
             elif (not args.continue_training) and (args.pretrained_weights is not None):
                 # we start a new training. If pretrained_weights are set, use them
-                load_pretrained_weights(trainer.network, args.pretrained_weights)
+                net = get_network(trainer.network)
+                load_pretrained_weights(net, args.pretrained_weights)
+                trainer.optional_model_optimize()
             else:
                 # new training without pretraine weights, do nothing
-                pass
+                trainer.optional_model_optimize()
 
             trainer.run_training()
-        else:
+        if run_validate:
             if valbest:
                 trainer.load_best_checkpoint(train=False)
             else:
-                trainer.load_final_checkpoint(train=False)
-
-        trainer.network.eval()
+                trainer.load_final_checkpoint(train=False, checkpoint_name=args.chk)
+            trainer.optional_model_optimize()
 
-        # predict validation
-        trainer.validate(save_softmax=args.npz, validation_folder_name=val_folder,
-                         run_postprocessing_on_folds=not disable_postprocessing_on_folds,
-                         overwrite=args.val_disable_overwrite)
+            # predict validation
+            trainer.validate(save_softmax=args.npz, validation_folder_name=val_folder,
+                            run_postprocessing_on_folds=enable_postprocessing_on_folds,
+                            overwrite=args.val_disable_overwrite)
 
         if network == '3d_lowres' and not args.disable_next_stage_pred:
             print("predicting segmentations for the next stage of the cascade")
             predict_next_stage(trainer, join(dataset_directory, trainer.plans['data_identifier'] + "_stage%d" % 1))
 
+    cleanup_dist()
+    sys.exit(0)
+
+
 
 if __name__ == "__main__":
     main()
diff -urN nnUNet/nnunet/run/run_training_da.py nnUNetNew/nnunet/run/run_training_da.py
--- nnUNet/nnunet/run/run_training_da.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/run/run_training_da.py	2023-02-17 12:13:25.648936963 +0000
@@ -0,0 +1,271 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+import argparse
+import sys
+from batchgenerators.utilities.file_and_folder_operations import *
+from nnunet.run.default_configuration import get_default_configuration, get_source_configuration
+from nnunet.paths import default_plans_identifier
+from nnunet.run.load_pretrained_weights import load_pretrained_weights, print_model_param_size
+from nnunet.training.cascade_stuff.predict_next_stage import predict_next_stage
+from nnunet.training.network_training.nnUNetTrainer import nnUNetTrainer
+from nnunet.utilities.task_name_id_conversion import convert_id_to_task_name
+from nnunet.training.network_training.nnUNetTrainerCascadeFullRes import nnUNetTrainerCascadeFullRes
+from nnunet.training.network_training.nnUNetTrainerV2_CascadeFullRes import nnUNetTrainerV2CascadeFullRes
+import torch
+from nnunet.utilities.distributed import setup_dist, cleanup_dist, get_network
+import torch.distributed as dist
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("network")
+    parser.add_argument("network_trainer")
+    parser.add_argument("source_task", help="can be task name or task id")
+    parser.add_argument("target_task", help="can be task name or task id")
+    parser.add_argument("fold", help='0, 1, ..., 5 or \'all\'')
+    parser.add_argument("-no_train", "--not_training", help="use this if you do not want to train the model",
+                        action="store_true")
+    parser.add_argument("-val", "--run_validate", help="use this if you want to run the validation",
+                        action="store_true")
+    parser.add_argument("-c", "--continue_training", help="use this if you want to continue a training",
+                        action="store_true")
+    parser.add_argument("-p", help="plans identifier. Only change this if you created a custom experiment planner",
+                        default=default_plans_identifier, required=False)
+    parser.add_argument("-sp", help="source plans identifier. Only change this if you created a custom experiment planner",
+                        default=default_plans_identifier, required=False)
+    parser.add_argument("--use_compressed_data", default=False, action="store_true",
+                        help="If you set use_compressed_data, the training cases will not be decompressed. Reading compressed data "
+                             "is much more CPU and RAM intensive and should only be used if you know what you are "
+                             "doing", required=False)
+    parser.add_argument("--deterministic",
+                        help="Makes training deterministic, but reduces training speed substantially. I (Fabian) think "
+                             "this is not necessary. Deterministic training will make you overfit to some random seed. "
+                             "Don't use that.",
+                        required=False, default=False, action="store_true")
+    parser.add_argument("--npz", required=False, default=False, action="store_true", help="if set then nnUNet will "
+                                                                                          "export npz files of "
+                                                                                          "predicted segmentations "
+                                                                                          "in the validation as well. "
+                                                                                          "This is needed to run the "
+                                                                                          "ensembling step so unless "
+                                                                                          "you are developing nnUNet "
+                                                                                          "you should enable this")
+    parser.add_argument("--valbest", required=False, default=False, action="store_true",
+                        help="hands off. This is not intended to be used")
+    parser.add_argument("--fp16", required=False, default=False, action="store_true",
+                        help="disable fp32 precision training and run mixed precision")
+    parser.add_argument("--val_folder", required=False, default="validation_raw",
+                        help="name of the validation folder. No need to use this for most people")
+    parser.add_argument("--disable_saving", required=False, action='store_true',
+                        help="If set nnU-Net will not save any parameter files (except a temporary checkpoint that "
+                             "will be removed at the end of the training). Useful for development when you are "
+                             "only interested in the results and want to save some disk space")
+    parser.add_argument("--enable_postprocessing_on_folds", required=False, action='store_true',
+                        help="Running postprocessing on each fold only makes sense when developing with nnU-Net and "
+                             "closely observing the model performance on specific configurations. You do not need it "
+                             "when applying nnU-Net because the postprocessing for this will be determined only once "
+                             "all five folds have been trained and nnUNet_find_best_configuration is called. Usually "
+                             "running postprocessing on each fold is computationally cheap, but some users have "
+                             "reported issues with very large images. If your images are large (>600x600x600 voxels) "
+                             "you should consider setting this flag.")
+    parser.add_argument('--val_disable_overwrite', action='store_false', default=True,
+                        help='Validation does not overwrite existing segmentations')
+    parser.add_argument('--disable_next_stage_pred', action='store_true', default=False,
+                        help='do not predict next stage')
+    parser.add_argument('-pretrained_weights', type=str, required=False, default=None,
+                        help='path to nnU-Net checkpoint file to be used as pretrained model (use .model '
+                             'file, for example model_final_checkpoint.model). Will only be used when actually training. '
+                             'Optional. Beta. Use with caution.')
+    parser.add_argument('-chk',
+                        help='checkpoint name, default: model_final_checkpoint',
+                        required=False,
+                        default='model_final_checkpoint')
+    parser.add_argument('--backend',
+                        help='backend for distributed training',
+                        required=False,
+                        default='ccl')
+    parser.add_argument("--epochs", required=False, default=1000, type=int, 
+        help = "maximum training epochs")
+    parser.add_argument("--batch_size", required=False, default=0, type=int, 
+        help = "training batch size")
+    parser.add_argument("--num_batch", required=False, default=250, type=int, 
+        help = "num batch per epoch")
+    parser.add_argument("--initial_lr", required=False, default=1e-2, type=float, 
+        help = "initial learning rate")
+    parser.add_argument("--loss_weights", nargs='+',
+                        help="the weight of each loss"
+                        "(source, target, encoder-gan, decoder-gan, seg-gan)", 
+                        type=float,
+                        default=[1.0, 0, 1.0, 0, 0])
+    parser.add_argument('-exp_name',
+                        help='experiement name',
+                        required=False,
+                        default='')
+    parser.add_argument("--ipex", required=False, default=False, action="store_true",
+        help="enable Intel Extension for PyTorch while training in CPU")
+    args = parser.parse_args()
+
+    # gloo, nccl, ccl
+    if torch.cuda.is_available():
+        backend = 'nccl'
+    else:
+        try:
+            import torch_ccl
+        except Exception as e:
+            print('No module named torch_ccl')
+        try:
+            import oneccl_bindings_for_pytorch
+        except Exception as e:
+            print('No module named oneccl_bindings_for_pytorch')
+        backend = args.backend
+    setup_dist(backend)
+
+    config_wandb(args)
+
+    source_task = args.source_task
+    task = args.target_task
+    fold = args.fold
+    network = args.network
+    network_trainer = args.network_trainer
+    not_training = args.not_training
+    run_validate = args.run_validate
+    plans_identifier = args.p
+    souce_plans_identifier = args.sp
+    enable_postprocessing_on_folds = args.enable_postprocessing_on_folds
+
+    use_compressed_data = args.use_compressed_data
+    decompress_data = not use_compressed_data
+
+    deterministic = args.deterministic
+    valbest = args.valbest
+
+    run_mixed_precision = args.fp16
+
+    val_folder = args.val_folder
+
+    if not task.startswith("Task"):
+        task_id = int(task)
+        task = convert_id_to_task_name(task_id)
+    if not source_task.startswith("Task"):
+        source_task_id = int(source_task)
+        source_task = convert_id_to_task_name(source_task_id)
+        source_plans_file, source_dataset_directory = get_source_configuration(network, source_task, souce_plans_identifier)
+
+    if fold == 'all':
+        pass
+    else:
+        fold = int(fold)
+
+    plans_file, output_folder_name, dataset_directory, batch_dice, stage, \
+    trainer_class = get_default_configuration(network, task, network_trainer, plans_identifier)
+
+    if trainer_class is None:
+        raise RuntimeError("Could not find trainer class in nnunet.training.network_training")
+
+    if network == "3d_cascade_fullres":
+        assert issubclass(trainer_class, (nnUNetTrainerCascadeFullRes, nnUNetTrainerV2CascadeFullRes)), \
+            "If running 3d_cascade_fullres then your " \
+            "trainer class must be derived from " \
+            "nnUNetTrainerCascadeFullRes"
+    else:
+        assert issubclass(trainer_class,
+                          nnUNetTrainer), "network_trainer was found but is not derived from nnUNetTrainer"
+
+    model_finetune = False
+    if (args.pretrained_weights is not None):
+        model_finetune = True
+    
+    trainer = trainer_class(plans_file, fold, output_folder=output_folder_name, dataset_directory=dataset_directory,
+                            batch_dice=batch_dice, stage=stage, unpack_data=decompress_data,
+                            deterministic=deterministic,
+                            fp16=run_mixed_precision,
+                            source_dataset_directory=source_dataset_directory,
+                            source_plans_file=source_plans_file,
+                            epochs=args.epochs,
+                            batch_size=args.batch_size,
+                            initial_lr=args.initial_lr,
+                            loss_weights=args.loss_weights,
+                            model_finetune=model_finetune,
+                            enable_ipex=args.ipex,
+                            num_batches_per_epoch=args.num_batch
+                            )
+    if args.disable_saving:
+        trainer.save_final_checkpoint = False # whether or not to save the final checkpoint
+        trainer.save_best_checkpoint = False  # whether or not to save the best checkpoint according to
+        # self.best_val_eval_criterion_MA
+        trainer.save_intermediate_checkpoints = True  # whether or not to save checkpoint_latest. We need that in case
+        # the training chashes
+        trainer.save_latest_only = True  # if false it will not store/overwrite _latest but separate files each
+
+    trainer.initialize(not not_training)
+
+    # # view model size
+    # print_model_param_size(trainer.network)
+    # sys.exit(0)
+
+    if not not_training:
+        if args.continue_training:
+            # -c was set, continue a previous training and ignore pretrained weights
+            trainer.optional_model_optimize()
+            trainer.load_latest_checkpoint()
+        elif (not args.continue_training) and (args.pretrained_weights is not None):
+            # we start a new training. If pretrained_weights are set, use them
+            net = get_network(trainer.network)
+            load_pretrained_weights(net.backbone, args.pretrained_weights)
+            trainer.optional_model_optimize()
+        else:
+            # new training without pretraine weights, do nothing
+            trainer.optional_model_optimize()
+
+        trainer.run_training()
+
+    mean_dice_score = 0
+    if run_validate:
+        if valbest:
+            trainer.load_best_checkpoint(train=False)
+        else:
+            trainer.load_final_checkpoint(train=False, checkpoint_name=args.chk)
+        trainer.optional_model_optimize()
+
+        # predict validation
+        mean_dice_score = trainer.validate(save_softmax=args.npz, validation_folder_name=val_folder,
+                            run_postprocessing_on_folds=enable_postprocessing_on_folds,
+                            overwrite=args.val_disable_overwrite)
+
+    if network == '3d_lowres' and not args.disable_next_stage_pred:
+        print("predicting segmentations for the next stage of the cascade")
+        predict_next_stage(trainer, join(dataset_directory, trainer.plans['data_identifier'] + "_stage%d" % 1))
+
+    cleanup_dist()
+    
+    return mean_dice_score
+
+
+def config_wandb(args):
+    import wandb
+    if dist.is_initialized() and dist.get_rank() > 0:
+        return
+    wandb_mode = 'online'
+    if not args.exp_name:
+        wandb_mode = 'disabled'
+    wandb.init(project='kits19', name=args.exp_name, config={}, mode=wandb_mode)
+    wandb.config.update(args)
+    wandb.define_metric("train/*", summary='mean')
+    wandb.define_metric("val/*", summary='mean')
+
+if __name__ == "__main__":
+    main()
diff -urN nnUNet/nnunet/training/data_augmentation/data_augmentation_moreDA.py nnUNetNew/nnunet/training/data_augmentation/data_augmentation_moreDA.py
--- nnUNet/nnunet/training/data_augmentation/data_augmentation_moreDA.py	2023-02-17 12:15:29.554698699 +0000
+++ nnUNetNew/nnunet/training/data_augmentation/data_augmentation_moreDA.py	2023-02-17 12:13:25.648936963 +0000
@@ -208,3 +208,131 @@
 
     return batchgenerator_train, batchgenerator_val
 
+
+def get_source_DA_augmentation(dataloader_train, patch_size, params=default_3D_augmentation_params,
+                            border_val_seg=-1,
+                            seeds_train=None, order_seg=1, order_data=3, deep_supervision_scales=None,
+                            soft_ds=False,
+                            classes=None, pin_memory=True, regions=None,
+                            use_nondetMultiThreadedAugmenter: bool = False):
+    assert params.get('mirror') is None, "old version of params, use new keyword do_mirror"
+
+    tr_transforms = []
+
+    if params.get("selected_data_channels") is not None:
+        tr_transforms.append(DataChannelSelectionTransform(params.get("selected_data_channels")))
+
+    if params.get("selected_seg_channels") is not None:
+        tr_transforms.append(SegChannelSelectionTransform(params.get("selected_seg_channels")))
+
+    # don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!
+    if params.get("dummy_2D") is not None and params.get("dummy_2D"):
+        ignore_axes = (0,)
+        tr_transforms.append(Convert3DTo2DTransform())
+        patch_size_spatial = patch_size[1:]
+    else:
+        patch_size_spatial = patch_size
+        ignore_axes = None
+
+    tr_transforms.append(SpatialTransform(
+        patch_size_spatial, patch_center_dist_from_border=None,
+        do_elastic_deform=params.get("do_elastic"), alpha=params.get("elastic_deform_alpha"),
+        sigma=params.get("elastic_deform_sigma"),
+        do_rotation=params.get("do_rotation"), angle_x=params.get("rotation_x"), angle_y=params.get("rotation_y"),
+        angle_z=params.get("rotation_z"), p_rot_per_axis=params.get("rotation_p_per_axis"),
+        do_scale=params.get("do_scaling"), scale=params.get("scale_range"),
+        border_mode_data=params.get("border_mode_data"), border_cval_data=0, order_data=order_data,
+        border_mode_seg="constant", border_cval_seg=border_val_seg,
+        order_seg=order_seg, random_crop=params.get("random_crop"), p_el_per_sample=params.get("p_eldef"),
+        p_scale_per_sample=params.get("p_scale"), p_rot_per_sample=params.get("p_rot"),
+        independent_scale_for_each_axis=params.get("independent_scale_factor_for_each_axis")
+    ))
+
+    if params.get("dummy_2D"):
+        tr_transforms.append(Convert2DTo3DTransform())
+
+    # we need to put the color augmentations after the dummy 2d part (if applicable). Otherwise the overloaded color
+    # channel gets in the way
+    tr_transforms.append(GaussianNoiseTransform(p_per_sample=0.1))
+    tr_transforms.append(GaussianBlurTransform((0.5, 1.), different_sigma_per_channel=True, p_per_sample=0.2,
+                                               p_per_channel=0.5))
+    tr_transforms.append(BrightnessMultiplicativeTransform(multiplier_range=(0.75, 1.25), p_per_sample=0.15))
+
+    if params.get("do_additive_brightness"):
+        tr_transforms.append(BrightnessTransform(params.get("additive_brightness_mu"),
+                                                 params.get("additive_brightness_sigma"),
+                                                 True, p_per_sample=params.get("additive_brightness_p_per_sample"),
+                                                 p_per_channel=params.get("additive_brightness_p_per_channel")))
+
+    tr_transforms.append(ContrastAugmentationTransform(p_per_sample=0.15))
+    tr_transforms.append(SimulateLowResolutionTransform(zoom_range=(0.5, 1), per_channel=True,
+                                                        p_per_channel=0.5,
+                                                        order_downsample=0, order_upsample=3, p_per_sample=0.25,
+                                                        ignore_axes=ignore_axes))
+    tr_transforms.append(
+        GammaTransform(params.get("gamma_range"), True, True, retain_stats=params.get("gamma_retain_stats"),
+                       p_per_sample=0.1))  # inverted gamma
+
+    if params.get("do_gamma"):
+        tr_transforms.append(
+            GammaTransform(params.get("gamma_range"), False, True, retain_stats=params.get("gamma_retain_stats"),
+                           p_per_sample=params["p_gamma"]))
+
+    if params.get("do_mirror") or params.get("mirror"):
+        tr_transforms.append(MirrorTransform(params.get("mirror_axes")))
+
+    if params.get("mask_was_used_for_normalization") is not None:
+        mask_was_used_for_normalization = params.get("mask_was_used_for_normalization")
+        tr_transforms.append(MaskTransform(mask_was_used_for_normalization, mask_idx_in_seg=0, set_outside_to=0))
+
+    tr_transforms.append(RemoveLabelTransform(-1, 0))
+
+    if params.get("move_last_seg_chanel_to_data") is not None and params.get("move_last_seg_chanel_to_data"):
+        tr_transforms.append(MoveSegAsOneHotToData(1, params.get("all_segmentation_labels"), 'seg', 'data'))
+        if params.get("cascade_do_cascade_augmentations") is not None and params.get(
+                "cascade_do_cascade_augmentations"):
+            if params.get("cascade_random_binary_transform_p") > 0:
+                tr_transforms.append(ApplyRandomBinaryOperatorTransform(
+                    channel_idx=list(range(-len(params.get("all_segmentation_labels")), 0)),
+                    p_per_sample=params.get("cascade_random_binary_transform_p"),
+                    key="data",
+                    strel_size=params.get("cascade_random_binary_transform_size"),
+                    p_per_label=params.get("cascade_random_binary_transform_p_per_label")))
+            if params.get("cascade_remove_conn_comp_p") > 0:
+                tr_transforms.append(
+                    RemoveRandomConnectedComponentFromOneHotEncodingTransform(
+                        channel_idx=list(range(-len(params.get("all_segmentation_labels")), 0)),
+                        key="data",
+                        p_per_sample=params.get("cascade_remove_conn_comp_p"),
+                        fill_with_other_class_p=params.get("cascade_remove_conn_comp_max_size_percent_threshold"),
+                        dont_do_if_covers_more_than_X_percent=params.get(
+                            "cascade_remove_conn_comp_fill_with_other_class_p")))
+
+    tr_transforms.append(RenameTransform('seg', 'target', True))
+
+    if regions is not None:
+        tr_transforms.append(ConvertSegmentationToRegionsTransform(regions, 'target', 'target'))
+
+    if deep_supervision_scales is not None:
+        if soft_ds:
+            assert classes is not None
+            tr_transforms.append(DownsampleSegForDSTransform3(deep_supervision_scales, 'target', 'target', classes))
+        else:
+            tr_transforms.append(DownsampleSegForDSTransform2(deep_supervision_scales, 0, input_key='target',
+                                                              output_key='target'))
+
+    tr_transforms.append(NumpyToTensor(['data', 'target'], 'float'))
+    tr_transforms = Compose(tr_transforms)
+
+    if use_nondetMultiThreadedAugmenter:
+        if NonDetMultiThreadedAugmenter is None:
+            raise RuntimeError('NonDetMultiThreadedAugmenter is not yet available')
+        batchgenerator_train = NonDetMultiThreadedAugmenter(dataloader_train, tr_transforms, params.get('num_threads'),
+                                                            params.get("num_cached_per_thread"), seeds=seeds_train,
+                                                            pin_memory=pin_memory)
+    else:
+        batchgenerator_train = MultiThreadedAugmenter(dataloader_train, tr_transforms, params.get('num_threads'),
+                                                      params.get("num_cached_per_thread"),
+                                                      seeds=seeds_train, pin_memory=pin_memory)
+
+    return batchgenerator_train
diff -urN nnUNet/nnunet/training/learning_rate/poly_lr.py nnUNetNew/nnunet/training/learning_rate/poly_lr.py
--- nnUNet/nnunet/training/learning_rate/poly_lr.py	2023-02-17 12:15:29.554698699 +0000
+++ nnUNetNew/nnunet/training/learning_rate/poly_lr.py	2023-02-17 12:13:25.648936963 +0000
@@ -14,4 +14,6 @@
 
 
 def poly_lr(epoch, max_epochs, initial_lr, exponent=0.9):
+    if epoch >= max_epochs:
+        return 0
     return initial_lr * (1 - epoch / max_epochs)**exponent
diff -urN nnUNet/nnunet/training/model_restore.py nnUNetNew/nnunet/training/model_restore.py
--- nnUNet/nnunet/training/model_restore.py	2023-02-17 12:15:29.553698700 +0000
+++ nnUNetNew/nnunet/training/model_restore.py	2023-02-17 12:13:25.648936963 +0000
@@ -94,7 +94,7 @@
     if fp16 is not None:
         trainer.fp16 = fp16
 
-    trainer.process_plans(info['plans'])
+    # trainer.process_plans(info['plans'])
     if checkpoint is not None:
         trainer.load_checkpoint(checkpoint, train)
     return trainer
@@ -118,6 +118,7 @@
     :param mixed_precision: if None then we take no action. If True/False we overwrite what the model has in its init
     :return:
     """
+    origin_folds = folds
     if isinstance(folds, str):
         folds = [join(folder, "all")]
         assert isdir(folds[0]), "no output folder for fold %s found" % folds
@@ -140,7 +141,7 @@
     trainer = restore_model(join(folds[0], "%s.model.pkl" % checkpoint_name), fp16=mixed_precision)
     trainer.output_folder = folder
     trainer.output_folder_base = folder
-    trainer.update_fold(0)
+    trainer.update_fold(origin_folds[0])
     trainer.initialize(False)
     all_best_model_files = [join(i, "%s.model" % checkpoint_name) for i in folds]
     print("using the following model files: ", all_best_model_files)
diff -urN nnUNet/nnunet/training/network_training/network_trainer.py nnUNetNew/nnunet/training/network_training/network_trainer.py
--- nnUNet/nnunet/training/network_training/network_trainer.py	2023-02-17 12:15:29.555698699 +0000
+++ nnUNetNew/nnunet/training/network_training/network_trainer.py	2023-02-17 12:13:25.649936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,17 +27,21 @@
 
 matplotlib.use("agg")
 from time import time, sleep
+from collections import defaultdict
 import torch
 import numpy as np
 from torch.optim import lr_scheduler
 import matplotlib.pyplot as plt
 import sys
+import os
 from collections import OrderedDict
 import torch.backends.cudnn as cudnn
 from abc import abstractmethod
 from datetime import datetime
 from tqdm import trange
 from nnunet.utilities.to_torch import maybe_to_torch, to_cuda
+from nnunet.utilities.distributed import get_network
+import torch.distributed as dist
 
 
 class NetworkTrainer(object):
@@ -97,7 +102,6 @@
         self.max_num_epochs = 1000
         self.num_batches_per_epoch = 250
         self.num_val_batches_per_epoch = 50
-        self.also_val_in_tr_mode = False
         self.lr_threshold = 1e-6  # the network will not terminate training if the lr is still above this threshold
 
         ################# LEAVE THESE ALONE ################################################
@@ -106,9 +110,8 @@
         self.best_val_eval_criterion_MA = None
         self.best_MA_tr_loss_for_patience = None
         self.best_epoch_based_on_MA_tr_loss = None
-        self.all_tr_losses = []
+        self.all_tr_losses = defaultdict(list)
         self.all_val_losses = []
-        self.all_val_losses_tr_mode = []
         self.all_val_eval_metrics = []  # does not have to be used
         self.epoch = 0
         self.log_file = None
@@ -189,6 +192,11 @@
         Should probably by improved
         :return:
         """
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+
+        # solid, dotted, dashdot, 'loosely dotted', 'loosely dashdotted', 'dashdotdotted', 
+        line_style_list = ['-', ':', '-.', (0, (1, 10)),  (0, (3, 10, 1, 10)), (0, (3, 5, 1, 5, 1, 5))]
         try:
             font = {'weight': 'normal',
                     'size': 18}
@@ -201,12 +209,14 @@
 
             x_values = list(range(self.epoch + 1))
 
-            ax.plot(x_values, self.all_tr_losses, color='b', ls='-', label="loss_tr")
+            for index, key in enumerate(sorted(self.all_tr_losses.keys())):
+                if 'adv_acc' in key:
+                    ax2.plot(x_values, self.all_tr_losses[key], color='g', ls=line_style_list[index], label=f'{key}, train=True')
+                else:
+                    ax.plot(x_values, self.all_tr_losses[key], color='b', ls=line_style_list[index], label=key)
 
             ax.plot(x_values, self.all_val_losses, color='r', ls='-', label="loss_val, train=False")
 
-            if len(self.all_val_losses_tr_mode) > 0:
-                ax.plot(x_values, self.all_val_losses_tr_mode, color='g', ls='-', label="loss_val, train=True")
             if len(self.all_val_eval_metrics) == len(x_values):
                 ax2.plot(x_values, self.all_val_eval_metrics, color='g', ls='--', label="evaluation metric")
 
@@ -223,6 +233,9 @@
 
     def print_to_log_file(self, *args, also_print_to_console=True, add_timestamp=True):
 
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+        
         timestamp = time()
         dt_object = datetime.fromtimestamp(timestamp)
 
@@ -255,6 +268,9 @@
         if also_print_to_console:
             print(*args)
 
+    def may_update_saved_ckpt(self, save_this):
+        return save_this
+
     def save_checkpoint(self, fname, save_optimizer=True):
         start_time = time()
         state_dict = self.network.state_dict()
@@ -278,12 +294,13 @@
             'state_dict': state_dict,
             'optimizer_state_dict': optimizer_state_dict,
             'lr_scheduler_state_dict': lr_sched_state_dct,
-            'plot_stuff': (self.all_tr_losses, self.all_val_losses, self.all_val_losses_tr_mode,
-                           self.all_val_eval_metrics),
+            'plot_stuff': (self.all_tr_losses, self.all_val_losses, self.all_val_eval_metrics),
             'best_stuff' : (self.best_epoch_based_on_MA_tr_loss, self.best_MA_tr_loss_for_patience, self.best_val_eval_criterion_MA)}
         if self.amp_grad_scaler is not None:
             save_this['amp_grad_scaler'] = self.amp_grad_scaler.state_dict()
 
+        save_this = self.may_update_saved_ckpt(save_this)
+
         torch.save(save_this, fname)
         self.print_to_log_file("done, saving took %.2f seconds" % (time() - start_time))
 
@@ -306,8 +323,8 @@
             return self.load_best_checkpoint(train)
         raise RuntimeError("No checkpoint found")
 
-    def load_final_checkpoint(self, train=False):
-        filename = join(self.output_folder, "model_final_checkpoint.model")
+    def load_final_checkpoint(self, train=False, checkpoint_name="model_final_checkpoint"):
+        filename = join(self.output_folder, f"{checkpoint_name}.model")
         if not isfile(filename):
             raise RuntimeError("Final checkpoint not found. Expected: %s. Please finish the training first." % filename)
         return self.load_checkpoint(filename, train=train)
@@ -352,8 +369,9 @@
         # match. Use heuristic to make it match
         for k, value in checkpoint['state_dict'].items():
             key = k
-            if key not in curr_state_dict_keys and key.startswith('module.'):
-                key = key[7:]
+            if key not in curr_state_dict_keys:
+                if dist.is_initialized() or key.startswith('module.'):
+                    key = key[7:]
             new_state_dict[key] = value
 
         if self.fp16:
@@ -362,7 +380,7 @@
                 if 'amp_grad_scaler' in checkpoint.keys():
                     self.amp_grad_scaler.load_state_dict(checkpoint['amp_grad_scaler'])
 
-        self.network.load_state_dict(new_state_dict)
+        get_network(self.network).load_state_dict(new_state_dict)
         self.epoch = checkpoint['epoch']
         if train:
             optimizer_state_dict = checkpoint['optimizer_state_dict']
@@ -376,8 +394,7 @@
             if issubclass(self.lr_scheduler.__class__, _LRScheduler):
                 self.lr_scheduler.step(self.epoch)
 
-        self.all_tr_losses, self.all_val_losses, self.all_val_losses_tr_mode, self.all_val_eval_metrics = checkpoint[
-            'plot_stuff']
+        self.all_tr_losses, self.all_val_losses, self.all_val_eval_metrics = checkpoint['plot_stuff']
 
         # load best loss (if present)
         if 'best_stuff' in checkpoint.keys():
@@ -387,14 +404,14 @@
         # after the training is done, the epoch is incremented one more time in my old code. This results in
         # self.epoch = 1001 for old trained models when the epoch is actually 1000. This causes issues because
         # len(self.all_tr_losses) = 1000 and the plot function will fail. We can easily detect and correct that here
-        if self.epoch != len(self.all_tr_losses):
+        if train and self.epoch != len(self.all_tr_losses['train/total_loss']):
             self.print_to_log_file("WARNING in loading checkpoint: self.epoch != len(self.all_tr_losses). This is "
                                    "due to an old bug and should only appear when you are loading old models. New "
                                    "models should have this fixed! self.epoch is now set to len(self.all_tr_losses)")
-            self.epoch = len(self.all_tr_losses)
-            self.all_tr_losses = self.all_tr_losses[:self.epoch]
+            self.epoch = len(self.all_tr_losses['train/total_loss'])
+            for key in self.all_tr_losses:
+                self.all_tr_losses[key] = self.all_tr_losses[key][:self.epoch]
             self.all_val_losses = self.all_val_losses[:self.epoch]
-            self.all_val_losses_tr_mode = self.all_val_losses_tr_mode[:self.epoch]
             self.all_val_eval_metrics = self.all_val_eval_metrics[:self.epoch]
 
         self._maybe_init_amp()
@@ -437,7 +454,7 @@
         while self.epoch < self.max_num_epochs:
             self.print_to_log_file("\nepoch: ", self.epoch)
             epoch_start_time = time()
-            train_losses_epoch = []
+            train_losses_epoch = defaultdict(list)
 
             # train one epoch
             self.network.train()
@@ -449,15 +466,18 @@
 
                         l = self.run_iteration(self.tr_gen, True)
 
-                        tbar.set_postfix(loss=l)
-                        train_losses_epoch.append(l)
+                        tbar.set_postfix(loss=l['train/total_loss'])
+                        for key in l:
+                            train_losses_epoch[key].append(l[key])
             else:
                 for _ in range(self.num_batches_per_epoch):
                     l = self.run_iteration(self.tr_gen, True)
-                    train_losses_epoch.append(l)
+                    for key in l:
+                        train_losses_epoch[key].append(l[key])
 
-            self.all_tr_losses.append(np.mean(train_losses_epoch))
-            self.print_to_log_file("train loss : %.4f" % self.all_tr_losses[-1])
+            for key in train_losses_epoch:
+                self.all_tr_losses[key].append(np.mean(train_losses_epoch[key]))
+            self.print_to_log_file("train loss : %.4f" % self.all_tr_losses['train/total_loss'][-1])
 
             with torch.no_grad():
                 # validation with train=False
@@ -469,16 +489,6 @@
                 self.all_val_losses.append(np.mean(val_losses))
                 self.print_to_log_file("validation loss: %.4f" % self.all_val_losses[-1])
 
-                if self.also_val_in_tr_mode:
-                    self.network.train()
-                    # validation with train=True
-                    val_losses = []
-                    for b in range(self.num_val_batches_per_epoch):
-                        l = self.run_iteration(self.val_gen, False)
-                        val_losses.append(l)
-                    self.all_val_losses_tr_mode.append(np.mean(val_losses))
-                    self.print_to_log_file("validation loss (train=True): %.4f" % self.all_val_losses_tr_mode[-1])
-
             self.update_train_loss_MA()  # needed for lr scheduler and stopping of training
 
             continue_training = self.on_epoch_end()
@@ -619,10 +629,10 @@
 
     def update_train_loss_MA(self):
         if self.train_loss_MA is None:
-            self.train_loss_MA = self.all_tr_losses[-1]
+            self.train_loss_MA = self.all_tr_losses['train/total_loss'][-1]
         else:
             self.train_loss_MA = self.train_loss_MA_alpha * self.train_loss_MA + (1 - self.train_loss_MA_alpha) * \
-                                 self.all_tr_losses[-1]
+                                 self.all_tr_losses['train/total_loss'][-1]
 
     def run_iteration(self, data_generator, do_backprop=True, run_online_evaluation=False):
         data_dict = next(data_generator)
@@ -684,6 +694,9 @@
     def validate(self, *args, **kwargs):
         pass
 
+    def postprocess_pred(self, softmax_pred):
+        return softmax_pred
+
     def find_lr(self, num_iters=1000, init_value=1e-6, final_value=10., beta=0.98):
         """
         stolen and adapted from here: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html
@@ -705,7 +718,7 @@
 
         for batch_num in range(1, num_iters + 1):
             # +1 because this one here is not designed to have negative loss...
-            loss = self.run_iteration(self.tr_gen, do_backprop=True, run_online_evaluation=False).data.item() + 1
+            loss = self.run_iteration(self.tr_gen, do_backprop=True, run_online_evaluation=False)['train/total_loss'].data.item() + 1
 
             # Compute the smoothed loss
             avg_loss = beta * avg_loss + (1 - beta) * loss
diff -urN nnUNet/nnunet/training/network_training/nnUNetTrainer.py nnUNetNew/nnunet/training/network_training/nnUNetTrainer.py
--- nnUNet/nnunet/training/network_training/nnUNetTrainer.py	2023-02-17 12:15:29.555698699 +0000
+++ nnUNetNew/nnunet/training/network_training/nnUNetTrainer.py	2023-02-17 12:13:25.650936963 +0000
@@ -25,7 +25,7 @@
 from batchgenerators.utilities.file_and_folder_operations import *
 from torch import nn
 from torch.optim import lr_scheduler
-
+import os
 import nnunet
 from nnunet.configuration import default_num_threads
 from nnunet.evaluation.evaluator import aggregate_scores
@@ -41,6 +41,7 @@
 from nnunet.training.network_training.network_trainer import NetworkTrainer
 from nnunet.utilities.nd_softmax import softmax_helper
 from nnunet.utilities.tensor_utilities import sum_tensor
+import torch.distributed as dist
 
 matplotlib.use("agg")
 
@@ -272,6 +273,9 @@
                                                            threshold_mode="abs")
 
     def plot_network_architecture(self):
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+
         try:
             from batchgenerators.utilities.file_and_folder_operations import join
             import hiddenlayer as hl
@@ -414,6 +418,19 @@
                                   pad_mode="constant", pad_sides=self.pad_all_sides, memmap_mode='r')
         return dl_tr, dl_val
 
+    def get_source_generators(self, folder_with_preprocessed_data):
+        source_dataset = load_dataset(folder_with_preprocessed_data)
+
+        if self.threeD:
+            dl_tr = DataLoader3D(source_dataset, self.basic_generator_patch_size, self.patch_size, self.batch_size,
+                                 False, oversample_foreground_percent=self.oversample_foreground_percent,
+                                 pad_mode="constant", pad_sides=self.pad_all_sides, memmap_mode='r')
+        else:
+            dl_tr = DataLoader2D(source_dataset, self.basic_generator_patch_size, self.patch_size, self.batch_size,
+                                 oversample_foreground_percent=self.oversample_foreground_percent,
+                                 pad_mode="constant", pad_sides=self.pad_all_sides, memmap_mode='r')
+        return dl_tr
+
     def preprocess_patient(self, input_files):
         """
         Used to predict new unseen data. Not used for the preprocessing of the training/test data
@@ -530,6 +547,8 @@
         """
         if debug=True then the temporary files generated for postprocessing determination will be kept
         """
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
 
         current_mode = self.network.training
         self.network.eval()
@@ -681,6 +700,9 @@
         self.network.train(current_mode)
 
     def run_online_evaluation(self, output, target):
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+            
         with torch.no_grad():
             num_classes = output.shape[1]
             output_softmax = softmax_helper(output)
@@ -705,6 +727,9 @@
             self.online_eval_fn.append(list(fn_hard))
 
     def finish_online_evaluation(self):
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+        
         self.online_eval_tp = np.sum(self.online_eval_tp, 0)
         self.online_eval_fp = np.sum(self.online_eval_fp, 0)
         self.online_eval_fn = np.sum(self.online_eval_fn, 0)
diff -urN nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py nnUNetNew/nnunet/training/network_training/nnUNetTrainerV2.py
--- nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py	2023-02-17 12:15:29.555698699 +0000
+++ nnUNetNew/nnunet/training/network_training/nnUNetTrainerV2.py	2023-02-17 12:13:25.651936963 +0000
@@ -1,3 +1,4 @@
+#    Copyright 2022, Intel Corporation.
 #    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
 #
 #    Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,7 +16,7 @@
 
 from collections import OrderedDict
 from typing import Tuple
-
+import sys
 import numpy as np
 import torch
 from nnunet.training.data_augmentation.data_augmentation_moreDA import get_moreDA_augmentation
@@ -29,11 +30,16 @@
 from nnunet.training.dataloading.dataset_loading import unpack_dataset
 from nnunet.training.network_training.nnUNetTrainer import nnUNetTrainer
 from nnunet.utilities.nd_softmax import softmax_helper
+from nnunet.utilities.distributed import get_network
 from sklearn.model_selection import KFold
 from torch import nn
-from torch.cuda.amp import autocast
 from nnunet.training.learning_rate.poly_lr import poly_lr
 from batchgenerators.utilities.file_and_folder_operations import *
+from torch.cuda.amp import autocast
+import torch.distributed as dist
+from torch.nn.parallel import DistributedDataParallel as DDP
+import math
+import os
 
 
 class nnUNetTrainerV2(nnUNetTrainer):
@@ -42,15 +48,29 @@
     """
 
     def __init__(self, plans_file, fold, output_folder=None, dataset_directory=None, batch_dice=True, stage=None,
-                 unpack_data=True, deterministic=True, fp16=False):
+                 unpack_data=True, deterministic=True, fp16=False, epochs=1000, batch_size=2, 
+                 initial_lr=1e-2, model_finetune=False, enable_ipex=False):
         super().__init__(plans_file, fold, output_folder, dataset_directory, batch_dice, stage, unpack_data,
                          deterministic, fp16)
-        self.max_num_epochs = 1000
-        self.initial_lr = 1e-2
+        self.init_args = (plans_file, fold, output_folder, dataset_directory, batch_dice, stage,
+            unpack_data, deterministic, fp16, epochs, batch_size, initial_lr, model_finetune, enable_ipex)
+        self.model_finetune = model_finetune
+        self.max_num_epochs = epochs
+        self.initial_lr = initial_lr
         self.deep_supervision_scales = None
         self.ds_loss_weights = None
-
+        self.save_every = 2
         self.pin_memory = True
+        self.enable_ipex = enable_ipex
+        self.num_batches_per_epoch = 250
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.num_batches_per_epoch = math.ceil(500 / dist.get_world_size())
+        self.load_plans_file()
+        self.process_plans(self.plans)
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.batch_size = 1
+        elif batch_size > 0:
+            self.batch_size = batch_size
 
     def initialize(self, training=True, force_load_plans=False):
         """
@@ -68,8 +88,6 @@
             if force_load_plans or (self.plans is None):
                 self.load_plans_file()
 
-            self.process_plans(self.plans)
-
             self.setup_DA_params()
 
             ################# Here we wrap the loss for deep supervision ############
@@ -121,7 +139,7 @@
             self.initialize_network()
             self.initialize_optimizer_and_scheduler()
 
-            assert isinstance(self.network, (SegmentationNetwork, nn.DataParallel))
+            assert isinstance(get_network(self.network), (SegmentationNetwork, nn.DataParallel, DDP))
         else:
             self.print_to_log_file('self.was_initialized is True, not running self.initialize again')
         self.was_initialized = True
@@ -163,10 +181,39 @@
 
     def initialize_optimizer_and_scheduler(self):
         assert self.network is not None, "self.initialize_network must be called first"
-        self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,
+        self.optimizer = torch.optim.SGD(self.network.get_parameters(self.model_finetune), self.initial_lr, weight_decay=self.weight_decay,
                                          momentum=0.99, nesterov=True)
         self.lr_scheduler = None
 
+    def optional_model_optimize(self, inference=False, model_index=0):
+        if (not torch.cuda.is_available()) and self.enable_ipex:
+            if not inference:
+                self.network, self.optimizer = ipex.optimize(self.network, optimizer=self.optimizer)
+            else:
+                if model_index not in self.inference_network_dict:                    
+                    self.network.eval()
+                    dummy_input = torch.rand((1, self.num_input_channels, *self.patch_size))
+
+                    # method 1: ipex
+                    inference_network = ipex.optimize(self.network.backbone)
+
+                    # method 2: int8
+                    # dynamic_qconfig = ipex.quantization.default_dynamic_qconfig
+                    # inference_network = prepare(self.network.backbone, dynamic_qconfig, example_inputs=dummy_input)
+                    # inference_network = convert(inference_network)
+
+                    # method 3: jit
+                    # inference_network = torch.jit.trace(inference_network, dummy_input)
+                    # inference_network = torch.jit.freeze(inference_network)
+
+                    self.inference_network_dict[model_index] = inference_network
+
+                self.network.inference_network = self.inference_network_dict[model_index]
+                
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.network = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.network)
+            self.network = DDP(self.network, find_unused_parameters=True)
+
     def run_online_evaluation(self, output, target):
         """
         due to deep supervision the return value and the reference are now lists of tensors. We only need the full
@@ -175,6 +222,9 @@
         :param target:
         :return:
         """
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+                
         target = target[0]
         output = output[0]
         return super().run_online_evaluation(output, target)
@@ -183,18 +233,15 @@
                  step_size: float = 0.5, save_softmax: bool = True, use_gaussian: bool = True, overwrite: bool = True,
                  validation_folder_name: str = 'validation_raw', debug: bool = False, all_in_gpu: bool = False,
                  segmentation_export_kwargs: dict = None, run_postprocessing_on_folds: bool = True):
-        """
-        We need to wrap this because we need to enforce self.network.do_ds = False for prediction
-        """
-        ds = self.network.do_ds
-        self.network.do_ds = False
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+        
         ret = super().validate(do_mirroring=do_mirroring, use_sliding_window=use_sliding_window, step_size=step_size,
                                save_softmax=save_softmax, use_gaussian=use_gaussian,
                                overwrite=overwrite, validation_folder_name=validation_folder_name, debug=debug,
                                all_in_gpu=all_in_gpu, segmentation_export_kwargs=segmentation_export_kwargs,
                                run_postprocessing_on_folds=run_postprocessing_on_folds)
 
-        self.network.do_ds = ds
         return ret
 
     def predict_preprocessed_data_return_seg_and_softmax(self, data: np.ndarray, do_mirroring: bool = True,
@@ -202,22 +249,27 @@
                                                          use_sliding_window: bool = True, step_size: float = 0.5,
                                                          use_gaussian: bool = True, pad_border_mode: str = 'constant',
                                                          pad_kwargs: dict = None, all_in_gpu: bool = False,
-                                                         verbose: bool = True, mixed_precision=True) -> Tuple[np.ndarray, np.ndarray]:
-        """
-        We need to wrap this because we need to enforce self.network.do_ds = False for prediction
-        """
-        ds = self.network.do_ds
-        self.network.do_ds = False
-        ret = super().predict_preprocessed_data_return_seg_and_softmax(data,
-                                                                       do_mirroring=do_mirroring,
-                                                                       mirror_axes=mirror_axes,
-                                                                       use_sliding_window=use_sliding_window,
-                                                                       step_size=step_size, use_gaussian=use_gaussian,
-                                                                       pad_border_mode=pad_border_mode,
-                                                                       pad_kwargs=pad_kwargs, all_in_gpu=all_in_gpu,
-                                                                       verbose=verbose,
-                                                                       mixed_precision=mixed_precision)
-        self.network.do_ds = ds
+                                                         verbose: bool = True, mixed_precision=True) -> Tuple[
+        np.ndarray, np.ndarray]:
+        if pad_border_mode == 'constant' and pad_kwargs is None:
+            pad_kwargs = {'constant_values': 0}
+
+        if do_mirroring and mirror_axes is None:
+            mirror_axes = self.data_aug_params['mirror_axes']
+
+        if do_mirroring:
+            assert self.data_aug_params["do_mirror"], "Cannot do mirroring as test time augmentation when training " \
+                                                      "was done without mirroring"
+
+        valid = list((SegmentationNetwork, nn.DataParallel, DDP))
+        assert isinstance(self.network, tuple(valid))
+        net = get_network(self.network)
+        ret = net.predict_3D(data, do_mirroring=do_mirroring, mirror_axes=mirror_axes,
+                             use_sliding_window=use_sliding_window, step_size=step_size,
+                             patch_size=self.patch_size, regions_class_order=self.regions_class_order,
+                             use_gaussian=use_gaussian, pad_border_mode=pad_border_mode,
+                             pad_kwargs=pad_kwargs, all_in_gpu=all_in_gpu, verbose=verbose,
+                             mixed_precision=mixed_precision)
         return ret
 
     def run_iteration(self, data_generator, do_backprop=True, run_online_evaluation=False):
@@ -240,37 +292,42 @@
             data = to_cuda(data)
             target = to_cuda(target)
 
+        if run_online_evaluation:
+            output = get_network(self.network)(data)
+            output = [output]
+            target_loss = self.loss(
+                output, [target[0]]
+            )
+            self.run_online_evaluation(output, target)
+            return target_loss.detach().cpu().numpy()
+        
         self.optimizer.zero_grad()
+        with autocast(enabled=self.fp16):
+            output = self.network(data)
+            del data
+            l = self.loss(output, target)
+            del target
 
-        if self.fp16:
-            with autocast():
-                output = self.network(data)
-                del data
-                l = self.loss(output, target)
-
-            if do_backprop:
+        if do_backprop:
+            if self.fp16:
                 self.amp_grad_scaler.scale(l).backward()
                 self.amp_grad_scaler.unscale_(self.optimizer)
                 torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)
                 self.amp_grad_scaler.step(self.optimizer)
                 self.amp_grad_scaler.update()
-        else:
-            output = self.network(data)
-            del data
-            l = self.loss(output, target)
-
-            if do_backprop:
+            else:
                 l.backward()
                 torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)
                 self.optimizer.step()
-
-        if run_online_evaluation:
-            self.run_online_evaluation(output, target)
-
-        del target
-
-        return l.detach().cpu().numpy()
-
+            # for name, params in self.network.named_parameters():
+            #     if params.grad is None: continue
+            #     print(f'rank: {dist.get_rank()}, name: {name}, grads: {torch.sum(params.grad)}, value: {torch.sum(params)}')
+            # sys.exit(0)
+        
+        return {
+            'train/total_loss': l.detach().cpu().numpy()
+        }
+        
     def do_split(self):
         """
         The default split is a 5 fold CV on all available training cases. nnU-Net will create a split (it is seeded,
@@ -402,8 +459,18 @@
             ep = self.epoch + 1
         else:
             ep = epoch
-        self.optimizer.param_groups[0]['lr'] = poly_lr(ep, self.max_num_epochs, self.initial_lr, 0.9)
-        self.print_to_log_file("lr:", np.round(self.optimizer.param_groups[0]['lr'], decimals=6))
+
+        self.lr_max_num_epochs = 200 if self.model_finetune else 1000
+        self.smaller_initial_lr = 0.2 * self.initial_lr if self.model_finetune else self.initial_lr
+        self.larger_initial_lr = self.initial_lr
+        
+        for index, group in enumerate(self.optimizer.param_groups):
+            if index == 0:
+                group['lr'] = poly_lr(ep, self.lr_max_num_epochs, self.smaller_initial_lr, 0.9)
+            else:
+                group['lr'] = poly_lr(ep, self.lr_max_num_epochs, self.larger_initial_lr, 0.9)
+            self.print_to_log_file(f"lr index {index}: {np.round(group['lr'], decimals=6)}")
+
 
     def on_epoch_end(self):
         """
@@ -418,7 +485,7 @@
         if self.epoch == 100:
             if self.all_val_eval_metrics[-1] == 0:
                 self.optimizer.param_groups[0]["momentum"] = 0.95
-                self.network.apply(InitWeights_He(1e-2))
+                get_network(self.network).apply(InitWeights_He(1e-2))
                 self.print_to_log_file("At epoch 100, the mean foreground Dice was 0. This can be caused by a too "
                                        "high momentum. High momentum (0.99) is good for datasets where it works, but "
                                        "sometimes causes issues such as this one. Momentum has now been reduced to "
@@ -435,8 +502,5 @@
         """
         self.maybe_update_lr(self.epoch)  # if we dont overwrite epoch then self.epoch+1 is used which is not what we
         # want at the start of the training
-        ds = self.network.do_ds
-        self.network.do_ds = True
-        ret = super().run_training()
-        self.network.do_ds = ds
-        return ret
+
+        super().run_training()
diff -urN nnUNet/nnunet/training/network_training/nnUNetTrainerV2_fp32.py nnUNetNew/nnunet/training/network_training/nnUNetTrainerV2_fp32.py
--- nnUNet/nnunet/training/network_training/nnUNetTrainerV2_fp32.py	2023-02-17 12:15:29.555698699 +0000
+++ nnUNetNew/nnunet/training/network_training/nnUNetTrainerV2_fp32.py	1970-01-01 00:00:00.000000000 +0000
@@ -1,27 +0,0 @@
-#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
-#
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
-#
-#        http://www.apache.org/licenses/LICENSE-2.0
-#
-#    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-
-
-from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2
-
-
-class nnUNetTrainerV2_fp32(nnUNetTrainerV2):
-    """
-    Info for Fabian: same as internal nnUNetTrainerV2_2
-    """
-
-    def __init__(self, plans_file, fold, output_folder=None, dataset_directory=None, batch_dice=True, stage=None,
-                 unpack_data=True, deterministic=True, fp16=False):
-        super().__init__(plans_file, fold, output_folder, dataset_directory, batch_dice, stage, unpack_data,
-                         deterministic, False)
diff -urN nnUNet/nnunet/training/network_training/nnUNetTrainer_DA_V2.py nnUNetNew/nnunet/training/network_training/nnUNetTrainer_DA_V2.py
--- nnUNet/nnunet/training/network_training/nnUNetTrainer_DA_V2.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/training/network_training/nnUNetTrainer_DA_V2.py	2023-02-17 12:14:34.748635065 +0000
@@ -0,0 +1,557 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+
+import numpy as np
+import torch
+from torch.cuda.amp import autocast
+from typing import Tuple
+from nnunet.training.data_augmentation.data_augmentation_moreDA import get_moreDA_augmentation, get_source_DA_augmentation
+from nnunet.utilities.to_torch import maybe_to_torch, to_cuda
+from nnunet.network_architecture.DA.generic_UNet_DA import Generic_UNet_DA
+from nnunet.network_architecture.DA.CAC_UNet import CAC_UNet
+from nnunet.network_architecture.neural_network import SegmentationNetwork
+from nnunet.training.dataloading.dataset_loading import unpack_dataset
+from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2
+from nnunet.utilities.distributed import get_network
+from nnunet.evaluation.evaluator import aggregate_scores
+from nnunet.inference.segmentation_export import save_segmentation_nifti_from_softmax
+from nnunet.configuration import default_num_threads
+from multiprocessing import Pool
+from nnunet.postprocessing.connected_components import determine_postprocessing
+from nnunet.network_architecture.DA.DA_Loss import CACDomainAdversarialLoss
+import shutil
+from time import sleep
+from torch import nn
+from batchgenerators.utilities.file_and_folder_operations import *
+import sys
+from nnunet.training.learning_rate.poly_lr import poly_lr
+import torch.distributed as dist
+from torch.nn.parallel import DistributedDataParallel as DDP
+import math
+import os
+
+
+class nnUNetTrainer_DA_V2(nnUNetTrainerV2):
+
+    def __init__(self, plans_file, fold, output_folder=None, dataset_directory=None, batch_dice=True, stage=None,
+                 unpack_data=True, deterministic=True, fp16=False, source_dataset_directory=None,
+                 source_plans_file=None, epochs=1000, batch_size=2, initial_lr=1e-2, 
+                 loss_weights=[1,0,1,0,0], model_finetune=False,
+                 enable_ipex=False, num_batches_per_epoch=250
+                 ):
+        super().__init__(plans_file, fold, output_folder, dataset_directory, batch_dice, stage, unpack_data,
+                         deterministic, fp16)
+        self.init_args = (plans_file, fold, output_folder, dataset_directory, batch_dice, stage, unpack_data,
+                          deterministic, fp16, source_dataset_directory, source_plans_file, epochs, batch_size, initial_lr,
+                          loss_weights, model_finetune,
+                          enable_ipex, num_batches_per_epoch)
+        self.model_finetune = model_finetune
+        self.max_num_epochs = epochs
+        self.initial_lr = initial_lr
+        self.deep_supervision_scales = None
+        self.ds_loss_weights = None
+        self.pin_memory = True
+        self.source_dataset_directory = source_dataset_directory
+        self.source_plans_file = source_plans_file
+        self.loss_weights = loss_weights
+        self.source_loss_weight, self.target_loss_weight = self.loss_weights[:2]
+        self.save_every = 2
+        self.enable_ipex = enable_ipex
+        self.num_batches_per_epoch = num_batches_per_epoch
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.num_batches_per_epoch = math.ceil(500 / dist.get_world_size())
+        self.source_plans = load_pickle(self.source_plans_file)
+        self.process_plans(self.source_plans)
+        target_plans = load_pickle(plans_file)
+        self.target_num_classes = target_plans['num_classes'] + 1
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.batch_size = 1
+        elif batch_size > 0:
+            self.batch_size = batch_size
+        self.inference_network_dict = {}
+
+    def initialize(self, training=True, force_load_plans=True):
+        """
+        - replaced get_default_augmentation with get_moreDA_augmentation
+        - enforce to only run this code once
+        - loss function wrapper for deep supervision
+
+        :param training:
+        :param force_load_plans:
+        :return:
+        """
+        if not self.was_initialized:
+            maybe_mkdir_p(self.output_folder)
+
+            if force_load_plans or (self.plans is None):
+                self.load_plans_file()
+
+            self.setup_DA_params()
+
+            self.folder_with_preprocessed_data = join(self.dataset_directory, self.plans['data_identifier'] +
+                                                      "_stage%d" % self.stage)
+            self.source_folder_with_preprocessed_data = join(self.source_dataset_directory, self.source_plans['data_identifier'] +
+                                                      "_stage%d" % self.stage)
+            if training:
+                self.dl_tr, self.dl_val = self.get_basic_generators()
+                self.source_dl = self.get_source_generators(self.source_folder_with_preprocessed_data)
+                if self.unpack_data:
+                    print("unpacking dataset")
+                    unpack_dataset(self.folder_with_preprocessed_data)
+                    unpack_dataset(self.source_folder_with_preprocessed_data)
+                    print("done")
+                else:
+                    print(
+                        "INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you "
+                        "will wait all winter for your model to finish!")
+
+                self.tr_gen, self.val_gen = get_moreDA_augmentation(
+                    self.dl_tr, self.dl_val,
+                    self.data_aug_params[
+                        'patch_size_for_spatialtransform'],
+                    self.data_aug_params,
+                    deep_supervision_scales=self.deep_supervision_scales,
+                    pin_memory=self.pin_memory,
+                    use_nondetMultiThreadedAugmenter=False
+                )
+
+                self.source_gen = get_source_DA_augmentation(
+                    self.source_dl,
+                    self.data_aug_params[
+                        'patch_size_for_spatialtransform'],
+                    self.data_aug_params,
+                    deep_supervision_scales=self.deep_supervision_scales,
+                    pin_memory=self.pin_memory,
+                    use_nondetMultiThreadedAugmenter=False
+                )
+
+                self.print_to_log_file("TRAINING KEYS:\n %s" % (str(self.dataset_tr.keys())),
+                                       also_print_to_console=False)
+                self.print_to_log_file("VALIDATION KEYS:\n %s" % (str(self.dataset_val.keys())),
+                                       also_print_to_console=False)
+            else:
+                pass
+
+            self.initialize_network()
+            self.initialize_optimizer_and_scheduler()
+
+            assert isinstance(get_network(self.network).backbone, (SegmentationNetwork, nn.DataParallel, DDP))
+        else:
+            self.print_to_log_file('self.was_initialized is True, not running self.initialize again')
+        self.was_initialized = True
+
+    def initialize_optimizer_and_scheduler(self):
+        assert self.network is not None, "self.initialize_network must be called first"
+        self.optimizer = torch.optim.SGD(self.network.get_parameters(self.model_finetune), self.initial_lr, weight_decay=self.weight_decay,
+                                         momentum=0.99, nesterov=True)
+        self.lr_scheduler = None
+
+    def optional_model_optimize(self, inference=False, model_index=0):
+        if (not torch.cuda.is_available()) and self.enable_ipex:
+            import intel_extension_for_pytorch as ipex
+            from intel_extension_for_pytorch.quantization import prepare, convert
+            if not inference:
+                self.network, self.optimizer = ipex.optimize(self.network, optimizer=self.optimizer)
+            else:
+                if model_index not in self.inference_network_dict:                    
+                    self.network.eval()
+                    dummy_input = torch.rand((1, self.num_input_channels, *self.patch_size))
+
+                    # method 1: ipex
+                    inference_network = ipex.optimize(self.network.backbone)
+
+                    # method 2: int8
+                    # dynamic_qconfig = ipex.quantization.default_dynamic_qconfig
+                    # inference_network = prepare(self.network.backbone, dynamic_qconfig, example_inputs=dummy_input)
+                    # inference_network = convert(inference_network)
+
+                    # method 3: jit
+                    # inference_network = torch.jit.trace(inference_network, dummy_input)
+                    # inference_network = torch.jit.freeze(inference_network)
+
+                    self.inference_network_dict[model_index] = inference_network
+
+                self.network.inference_network = self.inference_network_dict[model_index]
+
+
+        if dist.is_initialized() and dist.get_world_size() > 1:
+            self.network = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.network)
+            # self.network = DDP(self.network, find_unused_parameters=True)
+            self.network.backbone = DDP(self.network.backbone, find_unused_parameters=True)
+            self.network.adapter = DDP(self.network.adapter, find_unused_parameters=True)
+
+    def maybe_update_lr(self, epoch=None):
+        if epoch is None:
+            ep = self.epoch + 1
+        else:
+            ep = epoch
+
+        self.smaller_max_num_epochs = 100 if self.model_finetune else 1000
+        self.larger_max_num_epochs = 100 if self.model_finetune else 1000
+        self.smaller_initial_lr = 0.2 * self.initial_lr if self.model_finetune else self.initial_lr
+        self.larger_initial_lr = self.initial_lr
+        
+        for index, group in enumerate(self.optimizer.param_groups):
+            if index == 0:
+                group['lr'] = poly_lr(ep, self.smaller_max_num_epochs, self.smaller_initial_lr, 0.9)
+            else:
+                group['lr'] = poly_lr(ep, self.larger_max_num_epochs, self.larger_initial_lr, 0.9)
+            self.print_to_log_file(f"lr index {index}: {np.round(group['lr'], decimals=6)}")
+
+    def initialize_network(self):
+        """
+        - momentum 0.99
+        - SGD instead of Adam
+        - self.lr_scheduler = None because we do poly_lr
+        - deep supervision = True
+        - i am sure I forgot something here
+
+        Known issue: forgot to set neg_slope=0 in InitWeights_He; should not make a difference though
+        :return:
+        """
+        
+        backbone = Generic_UNet_DA(
+            self.threeD, self.num_input_channels, 
+            self.base_num_features, self.num_classes,         
+            self.conv_per_stage, self.net_num_pool_op_kernel_sizes, 
+            self.net_conv_kernel_sizes
+        )
+
+        adv_kwargs = {
+            'input_channels': backbone.encoder_channels,
+            'threeD': self.threeD,
+            'pool_op_kernel_sizes': self.net_num_pool_op_kernel_sizes,
+            'loss_weight': self.loss_weights[2:]
+        }
+        cac_domain_adv = CACDomainAdversarialLoss(**adv_kwargs)
+        
+        self.network = CAC_UNet(backbone, cac_domain_adv, self.source_loss_weight, self.target_loss_weight)
+        if torch.cuda.is_available():
+            self.network.cuda()
+
+    def run_iteration(self, data_generator, do_backprop=True, run_online_evaluation=False):
+        """
+        gradient clipping improves training stability
+
+        :param data_generator:
+        :param do_backprop:
+        :param run_online_evaluation:
+        :return:
+        """
+        data_dict = next(data_generator)
+        data = data_dict['data']
+        target = data_dict['target']
+
+        data = maybe_to_torch(data)
+        target = maybe_to_torch(target)
+
+        source_data_dict = next(self.source_gen)
+        source_data = source_data_dict['data']
+        source_label = source_data_dict['target']
+
+        source_data = maybe_to_torch(source_data)
+        source_label = maybe_to_torch(source_label)
+
+        if torch.cuda.is_available():
+            data = to_cuda(data)
+            target = to_cuda(target)
+            source_data = to_cuda(source_data)
+            source_label = to_cuda(source_label)
+        
+        if run_online_evaluation:
+            output = get_network(self.network).backbone(data)
+            output = [self.source_pred_to_target_pred(output)]
+            target_loss = get_network(self.network).backbone_loss(
+                output, [target[0]]
+            )
+            self.run_online_evaluation(output, target)
+            return target_loss.detach().cpu().numpy()
+
+        self.optimizer.zero_grad()
+        with autocast(enabled=self.fp16):
+            total_loss, adv_loss, source_loss, target_loss = self.network.compute_loss(
+                (source_data, data),
+                (source_label, target),
+                source_pred_to_target_pred=self.source_pred_to_target_pred
+            )
+
+        if do_backprop:
+            if self.fp16:
+                self.amp_grad_scaler.scale(total_loss).backward()
+                self.amp_grad_scaler.unscale_(self.optimizer)
+                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)
+                self.amp_grad_scaler.step(self.optimizer)
+                self.amp_grad_scaler.update()
+            else:
+                total_loss.backward()
+                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)
+                self.optimizer.step()
+            # for name, params in self.network.named_parameters():
+            #     if params.grad is None: continue
+            #     print(f'rank: {dist.get_rank()}, name: {name}, grads: {torch.sum(params.grad)}, value: {torch.sum(params)}')
+            # sys.exit(0)
+
+
+        all_metric = {
+            'train/total_loss': total_loss.detach().cpu().numpy(),
+            'train/source_loss': source_loss.detach().cpu().numpy()
+        }
+        if adv_loss:
+            all_metric['train/gan_loss'] = adv_loss.detach().cpu().numpy()
+        all_metric.update(get_network(self.network.adapter).get_metrics())
+
+        return all_metric
+
+    @staticmethod
+    def source_pred_to_target_pred(output, is_tensor=True, start_index=2, end_index=3):
+        # TODO: source class and target class index map need to refine
+        if is_tensor:
+            output_main = output[:, start_index:end_index+1, ...]
+            output_main = torch.max(output_main, 1, keepdim=True)[0]
+            output_auxiliary = torch.cat(
+                (
+                    output[:, :start_index, ...],
+                    output[:, end_index+1:, ...]
+                ),
+                dim = 1
+            )
+            output_auxiliary = torch.max(output_auxiliary, 1, keepdim=True)[0]
+            output = torch.cat((output_auxiliary, output_main), dim=1)
+        else:
+            output_main = output[start_index:end_index+1, ...]
+            output_main = np.max(output_main, axis=0, keepdims=True)
+            output_auxiliary = np.concatenate(
+                (
+                    output[:start_index, ...],
+                    output[end_index+1:, ...]
+                ),
+                axis = 0
+            )
+            output_auxiliary = np.max(output_auxiliary, axis=0, keepdims=True)
+            output = np.concatenate((output_auxiliary, output_main), axis=0)
+        return output
+
+    def postprocess_pred(self, softmax_pred):
+        return self.source_pred_to_target_pred(softmax_pred, is_tensor=False)
+
+    def validate(self, do_mirroring: bool = True, use_sliding_window: bool = True, step_size: float = 0.5,
+                 save_softmax: bool = True, use_gaussian: bool = True, overwrite: bool = True,
+                 validation_folder_name: str = 'validation_raw', debug: bool = False, all_in_gpu: bool = False,
+                 segmentation_export_kwargs: dict = None, run_postprocessing_on_folds: bool = True):
+        """
+        if debug=True then the temporary files generated for postprocessing determination will be kept
+        """
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+
+        self.print_to_log_file("start validating on given fold...")
+
+        current_mode = self.network.training
+        self.network.eval()
+
+        assert self.was_initialized, "must initialize, ideally with checkpoint (or train first)"
+        if self.dataset_val is None:
+            self.load_dataset()
+            self.do_split()
+
+        if segmentation_export_kwargs is None:
+            if 'segmentation_export_params' in self.plans.keys():
+                force_separate_z = self.plans['segmentation_export_params']['force_separate_z']
+                interpolation_order = self.plans['segmentation_export_params']['interpolation_order']
+                interpolation_order_z = self.plans['segmentation_export_params']['interpolation_order_z']
+            else:
+                force_separate_z = None
+                interpolation_order = 1
+                interpolation_order_z = 0
+        else:
+            force_separate_z = segmentation_export_kwargs['force_separate_z']
+            interpolation_order = segmentation_export_kwargs['interpolation_order']
+            interpolation_order_z = segmentation_export_kwargs['interpolation_order_z']
+
+        # predictions as they come from the network go here
+        output_folder = join(self.output_folder, validation_folder_name)
+        maybe_mkdir_p(output_folder)
+        # this is for debug purposes
+        my_input_args = {'do_mirroring': do_mirroring,
+                         'use_sliding_window': use_sliding_window,
+                         'step_size': step_size,
+                         'save_softmax': save_softmax,
+                         'use_gaussian': use_gaussian,
+                         'overwrite': overwrite,
+                         'validation_folder_name': validation_folder_name,
+                         'debug': debug,
+                         'all_in_gpu': all_in_gpu,
+                         'segmentation_export_kwargs': segmentation_export_kwargs,
+                         }
+        save_json(my_input_args, join(output_folder, "validation_args.json"))
+
+        if do_mirroring:
+            if not self.data_aug_params['do_mirror']:
+                raise RuntimeError("We did not train with mirroring so you cannot do inference with mirroring enabled")
+            mirror_axes = self.data_aug_params['mirror_axes']
+        else:
+            mirror_axes = ()
+
+        pred_gt_tuples = []
+
+        export_pool = Pool(default_num_threads)
+        results = []
+
+        for k in self.dataset_val.keys():
+            properties = load_pickle(self.dataset[k]['properties_file'])
+            fname = properties['list_of_data_files'][0].split("/")[-1][:-12]
+            if overwrite or (not isfile(join(output_folder, fname + ".nii.gz"))) or \
+                    (save_softmax and not isfile(join(output_folder, fname + ".npz"))):
+                data = np.load(self.dataset[k]['data_file'])['data']
+
+                print(k, data.shape)
+                data[-1][data[-1] == -1] = 0
+
+                softmax_pred = self.predict_preprocessed_data_return_seg_and_softmax(data[:-1],
+                                                                                     do_mirroring=do_mirroring,
+                                                                                     mirror_axes=mirror_axes,
+                                                                                     use_sliding_window=use_sliding_window,
+                                                                                     step_size=step_size,
+                                                                                     use_gaussian=use_gaussian,
+                                                                                     all_in_gpu=all_in_gpu,
+                                                                                     mixed_precision=self.fp16)[1]
+
+                softmax_pred = self.postprocess_pred(softmax_pred)
+                softmax_pred = softmax_pred.transpose([0] + [i + 1 for i in self.transpose_backward])
+
+                if save_softmax:
+                    softmax_fname = join(output_folder, fname + ".npz")
+                else:
+                    softmax_fname = None
+
+                """There is a problem with python process communication that prevents us from communicating objects
+                larger than 2 GB between processes (basically when the length of the pickle string that will be sent is
+                communicated by the multiprocessing.Pipe object then the placeholder (I think) does not allow for long
+                enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually
+                patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will
+                then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either
+                filename or np.ndarray and will handle this automatically"""
+                if np.prod(softmax_pred.shape) > (2e9 / 4 * 0.85):  # *0.85 just to be save
+                    np.save(join(output_folder, fname + ".npy"), softmax_pred)
+                    softmax_pred = join(output_folder, fname + ".npy")
+
+                results.append(export_pool.starmap_async(save_segmentation_nifti_from_softmax,
+                                                         ((softmax_pred, join(output_folder, fname + ".nii.gz"),
+                                                           properties, interpolation_order, self.regions_class_order,
+                                                           None, None,
+                                                           softmax_fname, None, force_separate_z,
+                                                           interpolation_order_z),
+                                                          )
+                                                         )
+                               )
+
+            pred_gt_tuples.append([join(output_folder, fname + ".nii.gz"),
+                                   join(self.gt_niftis_folder, fname + ".nii.gz")])
+
+        _ = [i.get() for i in results]
+        self.print_to_log_file("finished prediction")
+
+        # evaluate raw predictions
+        self.print_to_log_file("evaluation of raw predictions")
+        task = self.dataset_directory.split("/")[-1]
+        job_name = self.experiment_name
+        mean_dice_score = aggregate_scores(pred_gt_tuples, labels=list(range(self.target_num_classes)),
+                             json_output_file=join(output_folder, "summary.json"),
+                             json_name=job_name + " val tiled %s" % (str(use_sliding_window)),
+                             json_author="Fabian",
+                             json_task=task, num_threads=default_num_threads)
+        self.print_to_log_file(f"mean dice score is: {mean_dice_score}")
+
+        if run_postprocessing_on_folds:
+            # in the old nnunet we would stop here. Now we add a postprocessing. This postprocessing can remove everything
+            # except the largest connected component for each class. To see if this improves results, we do this for all
+            # classes and then rerun the evaluation. Those classes for which this resulted in an improved dice score will
+            # have this applied during inference as well
+            self.print_to_log_file("determining postprocessing")
+            determine_postprocessing(self.output_folder, self.gt_niftis_folder, validation_folder_name,
+                                     final_subf_name=validation_folder_name + "_postprocessed", debug=debug)
+            # after this the final predictions for the vlaidation set can be found in validation_folder_name_base + "_postprocessed"
+            # They are always in that folder, even if no postprocessing as applied!
+
+        # detemining postprocesing on a per-fold basis may be OK for this fold but what if another fold finds another
+        # postprocesing to be better? In this case we need to consolidate. At the time the consolidation is going to be
+        # done we won't know what self.gt_niftis_folder was, so now we copy all the niftis into a separate folder to
+        # be used later
+        gt_nifti_folder = join(self.output_folder_base, "gt_niftis")
+        maybe_mkdir_p(gt_nifti_folder)
+        for f in subfiles(self.gt_niftis_folder, suffix=".nii.gz"):
+            success = False
+            attempts = 0
+            e = None
+            while not success and attempts < 10:
+                try:
+                    shutil.copy(f, gt_nifti_folder)
+                    success = True
+                except OSError as e:
+                    attempts += 1
+                    sleep(1)
+            if not success:
+                print("Could not copy gt nifti file %s into folder %s" % (f, gt_nifti_folder))
+                if e is not None:
+                    raise e
+
+        self.network.train(current_mode)
+
+        return mean_dice_score
+
+    def predict_preprocessed_data_return_seg_and_softmax(self, data: np.ndarray, do_mirroring: bool = True,
+                                                         mirror_axes: Tuple[int] = None,
+                                                         use_sliding_window: bool = True, step_size: float = 0.5,
+                                                         use_gaussian: bool = True, pad_border_mode: str = 'constant',
+                                                         pad_kwargs: dict = None, all_in_gpu: bool = False,
+                                                         verbose: bool = True, mixed_precision=True) -> Tuple[
+        np.ndarray, np.ndarray]:
+        if pad_border_mode == 'constant' and pad_kwargs is None:
+            pad_kwargs = {'constant_values': 0}
+
+        if do_mirroring and mirror_axes is None:
+            mirror_axes = self.data_aug_params['mirror_axes']
+
+        if do_mirroring:
+            assert self.data_aug_params["do_mirror"], "Cannot do mirroring as test time augmentation when training " \
+                                                      "was done without mirroring"
+
+        net = get_network(self.network).backbone
+        assert isinstance(net, SegmentationNetwork)
+        ret = net.predict_3D(data, do_mirroring=do_mirroring, mirror_axes=mirror_axes,
+                             use_sliding_window=use_sliding_window, step_size=step_size,
+                             patch_size=self.patch_size, regions_class_order=self.regions_class_order,
+                             use_gaussian=use_gaussian, pad_border_mode=pad_border_mode,
+                             pad_kwargs=pad_kwargs, all_in_gpu=all_in_gpu, verbose=verbose,
+                             mixed_precision=mixed_precision)
+        return ret
+
+    def metric_to_wandb(self):
+        import wandb
+        if dist.is_initialized() and dist.get_rank() > 0:
+            return
+        wandb.log(
+            {k: v[-1] for k, v in self.all_tr_losses.items()}, 
+            step=self.epoch
+        )
+        wandb.log({'val/target_loss': self.all_val_losses[-1]}, step=self.epoch)
+        wandb.log({'val/target_dice': self.all_val_eval_metrics[-1]}, step=self.epoch)
+        
+    def on_epoch_end(self):
+        continue_training = super().on_epoch_end()
+        self.metric_to_wandb()
+        return continue_training
+        
\ No newline at end of file
diff -urN nnUNet/nnunet/utilities/distributed.py nnUNetNew/nnunet/utilities/distributed.py
--- nnUNet/nnunet/utilities/distributed.py	2023-02-17 12:15:29.560698699 +0000
+++ nnUNetNew/nnunet/utilities/distributed.py	2023-02-17 12:13:25.652936963 +0000
@@ -17,6 +17,8 @@
 from torch import distributed
 from torch import autograd
 from torch.nn.parallel import DistributedDataParallel as DDP
+import torch.distributed as dist
+import os
 
 
 def print_if_rank0(*args):
@@ -24,6 +26,42 @@
         print(*args)
 
 
+def setup_dist(backend):
+    if 'MASTER_ADDR' not in os.environ:
+        return 
+    
+    print(f"my master = {os.environ.get('MASTER_ADDR', 'localhost')}  my port = {os.environ.get('MASTER_PORT', '23700')}")
+
+    mpi_world_size = int(os.environ.get('PMI_SIZE', -1))
+    mpi_rank = int(os.environ.get('PMI_RANK', -1))
+    if mpi_world_size > 0:
+        os.environ['RANK'] = str(mpi_rank)
+        os.environ['WORLD_SIZE'] = str(mpi_world_size)
+    else:
+        # set the default rank and world size to 0 and 1
+        os.environ['RANK'] = str(os.environ.get('RANK', 0))
+        os.environ['WORLD_SIZE'] = str(os.environ.get('WORLD_SIZE', 1))
+    
+    print(f"my rank = {os.environ['RANK']}  my size = {os.environ['WORLD_SIZE']}")
+    
+    dist.init_process_group(backend=backend)
+
+
+def cleanup_dist():
+    if 'MASTER_ADDR' not in os.environ:
+        return 
+    
+    dist.destroy_process_group()
+
+
+def get_network(model):
+    if isinstance(model, DDP):
+        net = model.module
+    else:
+        net = model
+    return net
+
+
 class awesome_allgather_function(autograd.Function):
     @staticmethod
     def forward(ctx, input):
diff -urN nnUNet/nnunet/utilities/log.py nnUNetNew/nnunet/utilities/log.py
--- nnUNet/nnunet/utilities/log.py	1970-01-01 00:00:00.000000000 +0000
+++ nnUNetNew/nnunet/utilities/log.py	2023-02-17 12:13:25.652936963 +0000
@@ -0,0 +1,27 @@
+#    Copyright 2022, Intel Corporation.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
+from time import time
+from datetime import datetime
+
+
+def print_to_console(*args, add_timestamp=True):
+    
+    timestamp = time()
+    dt_object = datetime.fromtimestamp(timestamp)
+
+    if add_timestamp:
+        args = ("%s:" % dt_object, *args)
+
+    print(*args)
diff -urN nnUNet/nnunet/utilities/tensor_utilities.py nnUNetNew/nnunet/utilities/tensor_utilities.py
--- nnUNet/nnunet/utilities/tensor_utilities.py	2023-02-17 12:15:29.560698699 +0000
+++ nnUNetNew/nnunet/utilities/tensor_utilities.py	2023-02-17 12:13:25.653936963 +0000
@@ -52,3 +52,12 @@
     return x[tuple(indices)]
 
 
+def get_prefixed_named_param(model, prefix):
+    for name, param in model.named_parameters():
+        if name.startswith(prefix):
+            yield param
+
+def get_unprefixed_named_param(model, prefix):
+    for name, param in model.named_parameters():
+        if not name.startswith(prefix):
+            yield param
diff -urN nnUNet/readme.md nnUNetNew/readme.md
--- nnUNet/readme.md	2023-02-17 12:15:29.542698700 +0000
+++ nnUNetNew/readme.md	1970-01-01 00:00:00.000000000 +0000
@@ -1,487 +0,0 @@
-**[2020_10_21] Update:** We now have documentation for [common questions](documentation/common_questions.md) and
-[common issues](documentation/common_problems_and_solutions.md). We now also provide [reference epoch times for
-several datasets and tips on how to identify bottlenecks](documentation/expected_epoch_times.md).
-
-Please read these documents before opening a new issue!
-
-
-# nnU-Net
-
-In 3D biomedical image segmentation, dataset properties like imaging modality, image sizes, voxel spacings, class
-ratios etc vary drastically.
-For example, images in the [Liver and Liver Tumor Segmentation Challenge dataset](https://competitions.codalab.org/competitions/17094)
-are computed tomography (CT) scans, about 512x512x512 voxels large, have isotropic voxel spacings and their
-intensity values are quantitative (Hounsfield Units).
-The [Automated Cardiac Diagnosis Challenge dataset](https://acdc.creatis.insa-lyon.fr/) on the other hand shows cardiac
-structures in cine MRI with a typical image shape of 10x320x320 voxels, highly anisotropic voxel spacings and
-qualitative intensity values. In addition, the ACDC dataset suffers from slice misalignments and a heterogeneity of
-out-of-plane spacings which can cause severe interpolation artifacts if not handled properly.
-
-In current research practice, segmentation pipelines are designed manually and with one specific dataset in mind.
-Hereby, many pipeline settings depend directly or indirectly on the properties of the dataset
-and display a complex co-dependence: image size, for example, affects the patch size, which in
-turn affects the required receptive field of the network, a factor that itself influences several other
-hyperparameters in the pipeline. As a result, pipelines that were developed on one (type of) dataset are inherently
-incomaptible with other datasets in the domain.
-
-**nnU-Net is the first segmentation method that is designed to deal with the dataset diversity found in the domain. It
-condenses and automates the keys decisions for designing a successful segmentation pipeline for any given dataset.**
-
-nnU-Net makes the following contributions to the field:
-
-1. **Standardized baseline:** nnU-Net is the first standardized deep learning benchmark in biomedical segmentation.
-   Without manual effort, researchers can compare their algorithms against nnU-Net on an arbitrary number of datasets
-   to provide meaningful evidence for proposed improvements.
-2. **Out-of-the-box segmentation method:** nnU-Net is the first plug-and-play tool for state-of-the-art biomedical
-   segmentation. Inexperienced users can use nnU-Net out of the box for their custom 3D segmentation problem without
-   need for manual intervention.
-3. **Framework:** nnU-Net is a framework for fast and effective development of segmentation methods. Due to its modular
-   structure, new architectures and methods can easily be integrated into nnU-Net. Researchers can then benefit from its
-   generic nature to roll out and evaluate their modifications on an arbitrary number of datasets in a
-   standardized environment.
-
-For more information about nnU-Net, please read the following paper:
-
-
-    Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2020). nnU-Net: a self-configuring method 
-    for deep learning-based biomedical image segmentation. Nature Methods, 1-9.
-
-Please also cite this paper if you are using nnU-Net for your research!
-
-
-# Table of Contents
-- [nnU-Net](#nnu-net)
-- [Table of Contents](#table-of-contents)
-- [Installation](#installation)
-- [Usage](#usage)
-  - [How to run nnU-Net on a new dataset](#how-to-run-nnu-net-on-a-new-dataset)
-    - [Dataset conversion](#dataset-conversion)
-    - [Experiment planning and preprocessing](#experiment-planning-and-preprocessing)
-    - [Model training](#model-training)
-      - [2D U-Net](#2d-u-net)
-      - [3D full resolution U-Net](#3d-full-resolution-u-net)
-      - [3D U-Net cascade](#3d-u-net-cascade)
-        - [3D low resolution U-Net](#3d-low-resolution-u-net)
-        - [3D full resolution U-Net](#3d-full-resolution-u-net-1)
-      - [Multi GPU training](#multi-gpu-training)
-    - [Identifying the best U-Net configuration](#identifying-the-best-u-net-configuration)
-    - [Run inference](#run-inference)
-  - [How to run inference with pretrained models](#how-to-run-inference-with-pretrained-models)
-  - [Examples](#examples)
-- [Extending or Changing nnU-Net](#extending-or-changing-nnu-net)
-- [Information on run time and potential performance bottlenecks.](#information-on-run-time-and-potential-performance-bottlenecks)
-- [Common questions and issues](#common-questions-and-issues)
-- [Useful Resources](#useful-resources)
-- [Acknowledgements](#acknowledgements)
-
-
-# Installation
-nnU-Net has been tested on Linux (Ubuntu 16, 18 and 20; centOS, RHEL). We do not provide support for other operating
-systems.
-
-nnU-Net requires a GPU! For inference, the GPU should have 4 GB of VRAM. For training nnU-Net models the GPU should have at
-least 10 GB (popular non-datacenter options are the RTX 2080ti, RTX 3080 or RTX 3090). 
-
-For training, we recommend a strong CPU to go along with the GPU. At least 6 CPU cores (12 threads) are recommended. CPU
-requirements are mostly related to data augmentation and scale with the number of input channels. They are thus higher
-for datasets like BraTS which use 4 image modalities and lower for datasets like LiTS which only uses CT images.
-
-We very strongly recommend you install nnU-Net in a virtual environment.
-[Here is a quick how-to for Ubuntu.](https://linoxide.com/linux-how-to/setup-python-virtual-environment-ubuntu/)
-If you choose to compile pytorch from source, you will need to use conda instead of pip. In that case, please set the
-environment variable OMP_NUM_THREADS=1 (preferably in your bashrc using `export OMP_NUM_THREADS=1`). This is important!
-
-Python 2 is deprecated and not supported. Please make sure you are using Python 3.
-
-1) Install [PyTorch](https://pytorch.org/get-started/locally/) as described on their website (conda/pip). Please 
-install the latest version and (IMPORTANT!) choose 
-the highest CUDA version compatible with your drivers for maximum performance. 
-**DO NOT JUST `PIP INSTALL NNUNET` WITHOUT PROPERLY INSTALLING PYTORCH FIRST**
-2) Verify that a recent version of pytorch was installed by running
-    ```bash
-    python -c 'import torch;print(torch.backends.cudnn.version())'
-    python -c 'import torch;print(torch.__version__)'   
-    ```
-   This should print `8200` and `1.11.0+cu113` (Apr 1st 2022)
-3) Install nnU-Net depending on your use case:
-    1) For use as **standardized baseline**, **out-of-the-box segmentation algorithm** or for running **inference with pretrained models**:
-
-       ```pip install nnunet```
-
-    2) For use as integrative **framework** (this will create a copy of the nnU-Net code on your computer so that you can modify it as needed):
-          ```bash
-          git clone https://github.com/MIC-DKFZ/nnUNet.git
-          cd nnUNet
-          pip install -e .
-          ```
-4) nnU-Net needs to know where you intend to save raw data, preprocessed data and trained models. For this you need to
-   set a few of environment variables. Please follow the instructions [here](documentation/setting_up_paths.md).
-5) (OPTIONAL) Install [hiddenlayer](https://github.com/waleedka/hiddenlayer). hiddenlayer enables nnU-net to generate
-   plots of the network topologies it generates (see [Model training](#model-training)). To install hiddenlayer,
-   run the following commands:
-    ```bash
-    pip install --upgrade git+https://github.com/FabianIsensee/hiddenlayer.git@more_plotted_details#egg=hiddenlayer
-    ```
-
-Installing nnU-Net will add several new commands to your terminal. These commands are used to run the entire nnU-Net
-pipeline. You can execute them from any location on your system. All nnU-Net commands have the prefix `nnUNet_` for
-easy identification.
-
-Note that these commands simply execute python scripts. If you installed nnU-Net in a virtual environment, this
-environment must be activated when executing the commands.
-
-All nnU-Net commands have a `-h` option which gives information on how to use them.
-
-A typical installation of nnU-Net can be completed in less than 5 minutes. If pytorch needs to be compiled from source
-(which is what we currently recommend when using Turing GPUs), this can extend to more than an hour.
-
-# Usage
-To familiarize yourself with nnU-Net we recommend you have a look at the [Examples](#Examples) before you start with
-your own dataset.
-
-## How to run nnU-Net on a new dataset
-Given some dataset, nnU-Net fully automatically configures an entire segmentation pipeline that matches its properties.
-nnU-Net covers the entire pipeline, from preprocessing to model configuration, model training, postprocessing
-all the way to ensembling. After running nnU-Net, the trained model(s) can be applied to the test cases for inference.
-
-### Dataset conversion
-nnU-Net expects datasets in a structured format. This format closely (but not entirely) follows the data structure of
-the [Medical Segmentation Decthlon](http://medicaldecathlon.com/). Please read
-[this](documentation/dataset_conversion.md) for information on how to convert datasets to be compatible with nnU-Net.
-
-### Experiment planning and preprocessing
-As a first step, nnU-Net extracts a dataset fingerprint (a set of dataset-specific properties such as
-image sizes, voxel spacings, intensity information etc). This information is used to create three U-Net configurations:
-a 2D U-Net, a 3D U-Net that operated on full resolution images as well as a 3D U-Net cascade where the first U-Net
-creates a coarse segmentation map in downsampled images which is then refined by the second U-Net.
-
-Provided that the requested raw dataset is located in the correct folder (`nnUNet_raw_data_base/nnUNet_raw_data/TaskXXX_MYTASK`,
-also see [here](documentation/dataset_conversion.md)), you can run this step with the following command:
-
-```bash
-nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity
-```
-
-`XXX` is the integer identifier associated with your Task name `TaskXXX_MYTASK`. You can pass several task IDs at once.
-
-Running `nnUNet_plan_and_preprocess` will populate your folder with preprocessed data. You will find the output in
-nnUNet_preprocessed/TaskXXX_MYTASK. `nnUNet_plan_and_preprocess` creates subfolders with preprocessed data for the 2D
-U-Net as well as all applicable 3D U-Nets. It will also create 'plans' files (with the ending.pkl) for the 2D and
-3D configurations. These files contain the generated segmentation pipeline configuration and will be read by the
-nnUNetTrainer (see below). Note that the preprocessed data folder only contains the training cases.
-The test images are not preprocessed (they are not looked at at all!). Their preprocessing happens on the fly during
-inference.
-
-`--verify_dataset_integrity` should be run at least for the first time the command is run on a given dataset. This will execute some
-checks on the dataset to ensure that it is compatible with nnU-Net. If this check has passed once, it can be
-omitted in future runs. If you adhere to the dataset conversion guide (see above) then this should pass without issues :-)
-
-Note that `nnUNet_plan_and_preprocess` accepts several additional input arguments. Running `-h` will list all of them
-along with a description. If you run out of RAM during preprocessing, you may want to adapt the number of processes
-used with the `-tl` and `-tf` options.
-
-After `nnUNet_plan_and_preprocess` is completed, the U-Net configurations have been created and a preprocessed copy
-of the data will be located at nnUNet_preprocessed/TaskXXX_MYTASK.
-
-Extraction of the dataset fingerprint can take from a couple of seconds to several minutes depending on the properties
-of the segmentation task. Pipeline configuration given the extracted finger print is nearly instantaneous (couple
-of seconds). Preprocessing depends on image size and how powerful the CPU is. It can take between seconds and several
-tens of minutes.
-
-### Model training
-nnU-Net trains all U-Net configurations in a 5-fold cross-validation. This enables nnU-Net to determine the
-postprocessing and ensembling (see next step) on the training dataset. Per default, all U-Net configurations need to
-be run on a given dataset. There are, however situations in which only some configurations (and maybe even without
-running the cross-validation) are desired. See [FAQ](documentation/common_questions.md) for more information.
-
-Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the U-Net
-cascade is omitted because the patch size of the full resolution U-Net already covers a large part of the input images.
-
-Training models is done with the `nnUNet_train` command. The general structure of the command is:
-```bash
-nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD  --npz (additional options)
-```
-
-CONFIGURATION is a string that identifies the requested U-Net configuration. TRAINER_CLASS_NAME is the name of the
-model trainer. If you implement custom trainers (nnU-Net as a framework) you can specify your custom trainer here.
-TASK_NAME_OR_ID specifies what dataset should be trained on and FOLD specifies which fold of the 5-fold-cross-validaton
-is trained.
-
-nnU-Net stores a checkpoint every 50 epochs. If you need to continue a previous training, just add a `-c` to the
-training command.
-
-IMPORTANT: `--npz` makes the models save the softmax outputs during the final validation. It should only be used for trainings
-where you plan to run `nnUNet_find_best_configuration` afterwards
-(this is nnU-Nets automated selection of the best performing (ensemble of) configuration(s), see below). If you are developing new
-trainer classes you may not need the softmax predictions and should therefore omit the `--npz` flag. Exported softmax
-predictions are very large and therefore can take up a lot of disk space.
-If you ran initially without the `--npz` flag but now require the softmax predictions, simply run
-```bash
-nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD -val --npz
-```
-to generate them. This will only rerun the validation, not the training.
-
-See `nnUNet_train -h` for additional options.
-
-#### 2D U-Net
-For FOLD in [0, 1, 2, 3, 4], run:
-```bash
-nnUNet_train 2d nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz
-```
-
-#### 3D full resolution U-Net
-For FOLD in [0, 1, 2, 3, 4], run:
-```bash
-nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz
-```
-
-#### 3D U-Net cascade
-##### 3D low resolution U-Net
-For FOLD in [0, 1, 2, 3, 4], run:
-```bash
-nnUNet_train 3d_lowres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz
-```
-
-##### 3D full resolution U-Net
-For FOLD in [0, 1, 2, 3, 4], run:
-```bash
-nnUNet_train 3d_cascade_fullres nnUNetTrainerV2CascadeFullRes TaskXXX_MYTASK FOLD --npz
-```
-
-Note that the 3D full resolution U-Net of the cascade requires the five folds of the low resolution U-Net to be
-completed beforehand!
-
-The trained models will be written to the RESULTS_FOLDER/nnUNet folder. Each training obtains an automatically generated
-output folder name:
-
-nnUNet_preprocessed/CONFIGURATION/TaskXXX_MYTASKNAME/TRAINER_CLASS_NAME__PLANS_FILE_NAME/FOLD
-
-For Task002_Heart (from the MSD), for example, this looks like this:
-
-    RESULTS_FOLDER/nnUNet/
-    ├── 2d
-    │   └── Task02_Heart
-    │       └── nnUNetTrainerV2__nnUNetPlansv2.1
-    │           ├── fold_0
-    │           ├── fold_1
-    │           ├── fold_2
-    │           ├── fold_3
-    │           └── fold_4
-    ├── 3d_cascade_fullres
-    ├── 3d_fullres
-    │   └── Task02_Heart
-    │       └── nnUNetTrainerV2__nnUNetPlansv2.1
-    │           ├── fold_0
-    │           │   ├── debug.json
-    │           │   ├── model_best.model
-    │           │   ├── model_best.model.pkl
-    │           │   ├── model_final_checkpoint.model
-    │           │   ├── model_final_checkpoint.model.pkl
-    │           │   ├── network_architecture.pdf
-    │           │   ├── progress.png
-    │           │   └── validation_raw
-    │           │       ├── la_007.nii.gz
-    │           │       ├── la_007.pkl
-    │           │       ├── la_016.nii.gz
-    │           │       ├── la_016.pkl
-    │           │       ├── la_021.nii.gz
-    │           │       ├── la_021.pkl
-    │           │       ├── la_024.nii.gz
-    │           │       ├── la_024.pkl
-    │           │       ├── summary.json
-    │           │       └── validation_args.json
-    │           ├── fold_1
-    │           ├── fold_2
-    │           ├── fold_3
-    │           └── fold_4
-    └── 3d_lowres
-
-
-Note that 3d_lowres and 3d_cascade_fullres are not populated because this dataset did not trigger the cascade. In each
-model training output folder (each of the fold_x folder, 10 in total here), the following files will be created (only
-shown for one folder above for brevity):
-- debug.json: Contains a summary of blueprint and inferred parameters used for training this model. Not easy to read,
-  but very useful for debugging ;-)
-- model_best.model / model_best.model.pkl: checkpoint files of the best model identified during training. Not used right now.
-- model_final_checkpoint.model / model_final_checkpoint.model.pkl: checkpoint files of the final model (after training
-  has ended). This is what is used for both validation and inference.
-- network_architecture.pdf (only if hiddenlayer is installed!): a pdf document with a figure of the network architecture in it.
-- progress.png: A plot of the training (blue) and validation (red) loss during training. Also shows an approximation of
-  the evlauation metric (green). This approximation is the average Dice score of the foreground classes. It should,
-  however, only to be taken with a grain of salt because it is computed on randomly drawn patches from the validation
-  data at the end of each epoch, and the aggregation of TP, FP and FN for the Dice computation treats the patches as if
-  they all originate from the same volume ('global Dice'; we do not compute a Dice for each validation case and then
-  average over all cases but pretend that there is only one validation case from which we sample patches). The reason for
-  this is that the 'global Dice' is easy to compute during training and is still quite useful to evaluate whether a model
-  is training at all or not. A proper validation is run at the end of the training.
-- validation_raw: in this folder are the predicted validation cases after the training has finished. The summary.json
-  contains the validation metrics (a mean over all cases is provided at the end of the file).
-
-During training it is often useful to watch the progress. We therefore recommend that you have a look at the generated
-progress.png when running the first training. It will be updated after each epoch.
-
-Training times largely depend on the GPU. The smallest GPU we recommend for training is the Nvidia RTX 2080ti. With
-this GPU (and pytorch compiled with cuDNN 8.0.2), all network trainings take less than 2 days.
-
-#### Multi GPU training
-
-**Multi GPU training is experimental and NOT RECOMMENDED!**
-
-nnU-Net supports two different multi-GPU implementation: DataParallel (DP) and Distributed Data Parallel (DDP)
-(but currently only on one host!). DDP is faster than DP and should be preferred if possible. However, if you did not
-install nnunet as a framework (meaning you used the `pip install nnunet` variant), DDP is not available. It requires a
-different way of calling the correct python script (see below) which we cannot support from our terminal commands.
-
-Distributed training currently only works for the basic trainers (2D, 3D full resolution and 3D low resolution) and not
-for the second, high resolution U-Net of the cascade. The reason for this is that distributed training requires some
-changes to the network and loss function, requiring a new nnUNet trainer class. This is, as of now, simply not
-implemented for the cascade, but may be added in the future.
-
-To run distributed training (DP), use the following command:
-
-```bash
-CUDA_VISIBLE_DEVICES=0,1,2... nnUNet_train_DP CONFIGURATION nnUNetTrainerV2_DP TASK_NAME_OR_ID FOLD -gpus GPUS --dbs
-```
-
-Note that nnUNetTrainerV2 was replaced with nnUNetTrainerV2_DP. Just like before, CONFIGURATION can be 2d, 3d_lowres or
-3d_fullres. TASK_NAME_OR_ID refers to the task you would like to train and FOLD is the fold of the cross-validation.
-GPUS (integer value) specifies the number of GPUs you wish to train on. To specify which GPUs you want to use, please make use of the
-CUDA_VISIBLE_DEVICES envorinment variable to specify the GPU ids (specify as many as you configure with -gpus GPUS).
---dbs, if set, will distribute the batch size across GPUs. So if nnUNet configures a batch size of 2 and you run on 2 GPUs
-, each GPU will run with a batch size of 1. If you omit --dbs, each GPU will run with the full batch size (2 for each GPU
-in this example for a total of batch size 4).
-
-To run the DDP training you must have nnU-Net installed as a framework. Your current working directory must be the
-nnunet folder (the one that has the dataset_conversion, evaluation, experiment_planning, ... subfolders!). You can then run
-the DDP training with the following command:
-
-```bash
-CUDA_VISIBLE_DEVICES=0,1,2... python -m torch.distributed.launch --master_port=XXXX --nproc_per_node=Y run/run_training_DDP.py CONFIGURATION nnUNetTrainerV2_DDP TASK_NAME_OR_ID FOLD --dbs
-```
-
-XXXX must be an open port for process-process communication (something like 4321 will do on most systems). Y is the
-number of GPUs you wish to use. Remember that we do not (yet) support distributed training across compute nodes. This
-all happens on the same system. Again, you can use CUDA_VISIBLE_DEVICES=0,1,2 to control what GPUs are used.
-If you run more than one DDP training on the same system (say you have 4 GPUs and you run two training with 2 GPUs each)
-you need to specify a different --master_port for each training!
-
-*IMPORTANT!*
-Multi-GPU training results in models that cannot be used for inference easily (as said above, all of this is experimental ;-) ).
-After finishing the training of all folds, run `nnUNet_change_trainer_class` on the folder where the trained model is
-(see `nnUNet_change_trainer_class -h` for instructions). After that you can run inference.
-
-### Identifying the best U-Net configuration
-Once all models are trained, use the following
-command to automatically determine what U-Net configuration(s) to use for test set prediction:
-
-```bash
-nnUNet_find_best_configuration -m 2d 3d_fullres 3d_lowres 3d_cascade_fullres -t XXX
-```
-
-(all 5 folds need to be completed for all specified configurations!)
-
-On datasets for which the cascade was not configured, use `-m 2d 3d_fullres` instead. If you wish to only explore some
-subset of the configurations, you can specify that with the `-m` command. Additional options are available (use `-h` for help).
-
-### Run inference
-Remember that the data located in the input folder must adhere to the format specified
-[here](documentation/data_format_inference.md).
-
-`nnUNet_find_best_configuration` will print a string to the terminal with the inference commands you need to use.
-The easiest way to run inference is to simply use these commands.
-
-If you wish to manually specify the configuration(s) used for inference, use the following commands:
-
-For each of the desired configurations, run:
-```
-nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION --save_npz
-```
-
-Only specify `--save_npz` if you intend to use ensembling. `--save_npz` will make the command save the softmax
-probabilities alongside of the predicted segmentation masks requiring a lot of disk space.
-
-Please select a separate `OUTPUT_FOLDER` for each configuration!
-
-If you wish to run ensembling, you can ensemble the predictions from several configurations with the following command:
-```bash
-nnUNet_ensemble -f FOLDER1 FOLDER2 ... -o OUTPUT_FOLDER -pp POSTPROCESSING_FILE
-```
-
-You can specify an arbitrary number of folders, but remember that each folder needs to contain npz files that were
-generated by `nnUNet_predict`. For ensembling you can also specify a file that tells the command how to postprocess.
-These files are created when running `nnUNet_find_best_configuration` and are located in the respective trained model
-directory (RESULTS_FOLDER/nnUNet/CONFIGURATION/TaskXXX_MYTASK/TRAINER_CLASS_NAME__PLANS_FILE_IDENTIFIER/postprocessing.json or
-RESULTS_FOLDER/nnUNet/ensembles/TaskXXX_MYTASK/ensemble_X__Y__Z--X__Y__Z/postprocessing.json). You can also choose to
-not provide a file (simply omit -pp) and nnU-Net will not run postprocessing.
-
-Note that per default, inference will be done with all available folds. We very strongly recommend you use all 5 folds.
-Thus, all 5 folds must have been trained prior to running inference. The list of available folds nnU-Net found will be
-printed at the start of the inference.
-
-## How to run inference with pretrained models
-
-Trained models for all challenges we participated in are publicly available. They can be downloaded and installed
-directly with nnU-Net. Note that downloading a pretrained model will overwrite other models that were trained with
-exactly the same configuration (2d, 3d_fullres, ...), trainer (nnUNetTrainerV2) and plans.
-
-To obtain a list of available models, as well as a short description, run
-
-```bash
-nnUNet_print_available_pretrained_models
-```
-
-You can then download models by specifying their task name. For the Liver and Liver Tumor Segmentation Challenge,
-for example, this would be:
-
-```bash
-nnUNet_download_pretrained_model Task029_LiTS
-```
-After downloading is complete, you can use this model to run [inference](#run-inference). Keep in mind that each of
-these models has specific data requirements (Task029_LiTS runs on abdominal CT scans, others require several image
-modalities as input in a specific order).
-
-When using the pretrained models you must adhere to the license of the dataset they are trained on! If you run
-`nnUNet_download_pretrained_model` you will find a link where you can find the license for each dataset.
-
-## Examples
-
-To get you started we compiled two simple to follow examples:
-- run a training with the 3d full resolution U-Net on the Hippocampus dataset. See [here](documentation/training_example_Hippocampus.md).
-- run inference with nnU-Net's pretrained models on the Prostate dataset. See [here](documentation/inference_example_Prostate.md).
-
-Usability not good enough? Let us know!
-
-# Extending or Changing nnU-Net
-Please refer to [this](documentation/extending_nnunet.md) guide.
-
-# Information on run time and potential performance bottlenecks.
-
-We have compiled a list of expected epoch times on standardized datasets across many different GPUs. You can use them
-to verify that your system is performing as expected. There are also tips on how to identify bottlenecks and what
-to do about them.
-
-Click [here](documentation/expected_epoch_times.md).
-
-# Common questions and issues
-
-We have collected solutions to common [questions](documentation/common_questions.md) and
-[problems](documentation/common_problems_and_solutions.md). Please consult these documents before you open a new issue.
-
-# Useful Resources
-
-* The [nnU-Net Workshop](https://github.com/IML-DKFZ/nnunet-workshop) is a step-by-step introduction to nnU-Net and visualizing
-results using MITK. Regarding nnU-Net, it includes training and inference examples and an example to train on a new dataset.
-The workshop itself is a jupyter notebook, which can be executed in GoogleColab.
-
-* This RSNA 2021 Deep Learning Lab [notebook](https://github.com/RSNA/AI-Deep-Learning-Lab-2021/blob/main/sessions/tcia-idc/RSNA_2021_IDC_and_TCIA.ipynb) demonstrates how nnU-Net can be used to analyze public DICOM datasets available in US National Cancer Institute [Imaging Data Commons (IDC)](https://imaging.datacommons.cancer.gov). This notebook demonstrates how datasets suitable for the analysis with nnU-Net can be identified within IDC, how they can be preprocessed from the DICOM format to be usable with nnU-Net, and how the results of the analysis can be visualized in the notebook without having to download anything. NCI Imaging Data Commons is a cloud-based repository of publicly available cancer imaging data co-located with the analysis and exploration tools and resources. IDC is a node within the broader NCI [Cancer Research Data Commons (CRDC)](https://datacommons.cancer.gov/) infrastructure that provides secure access to a large, comprehensive, and expanding collection of cancer research data.
-
-* A [Google Colab notebook](documentation/celltrackingchallenge/MIC-DKFZ.ipynb) example has been added to the repository allowing to train and apply a model to some of the 
-[cell tracking challenge](http://celltrackingchallenge.net/) datasets. 
-You will need to download the data and some extra folders in your Google Drive and connect to it from the notebook 
-for the process to work.
-
-# Acknowledgements
-
-<img src="HI_Logo.png" width="512px" />
-
-nnU-Net is developed and maintained by the Applied Computer Vision Lab (ACVL) of [Helmholtz Imaging](http://helmholtz-imaging.de).
diff -urN nnUNet/setup.cfg nnUNetNew/setup.cfg
--- nnUNet/setup.cfg	2023-02-17 12:15:29.542698700 +0000
+++ nnUNetNew/setup.cfg	1970-01-01 00:00:00.000000000 +0000
@@ -1,2 +0,0 @@
-[metadata]
-description-file = readme.md
\ No newline at end of file
diff -urN nnUNet/setup.py nnUNetNew/setup.py
--- nnUNet/setup.py	2023-02-17 12:15:29.542698700 +0000
+++ nnUNetNew/setup.py	2023-02-17 12:13:25.653936963 +0000
@@ -1,3 +1,18 @@
+#    Copyright 2022, Intel Corporation.
+#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+
 from setuptools import setup, find_namespace_packages
 
 setup(name='nnunet',
@@ -9,27 +24,13 @@
       author_email='f.isensee@dkfz-heidelberg.de',
       license='Apache License Version 2.0, January 2004',
       install_requires=[
-            "torch>1.10.0",
-            "tqdm",
-            "dicom2nifti",
-            "scikit-image>=0.14",
-            "medpy",
-            "scipy",
-            "batchgenerators>=0.23",
-            "numpy",
-            "sklearn",
-            "SimpleITK",
-            "pandas",
-            "requests",
-            "nibabel", 
-            "tifffile", 
-            "matplotlib",
       ],
       entry_points={
           'console_scripts': [
               'nnUNet_convert_decathlon_task = nnunet.experiment_planning.nnUNet_convert_decathlon_task:main',
               'nnUNet_plan_and_preprocess = nnunet.experiment_planning.nnUNet_plan_and_preprocess:main',
               'nnUNet_train = nnunet.run.run_training:main',
+              'nnUNet_train_da = nnunet.run.run_training_da:main',
               'nnUNet_train_DP = nnunet.run.run_training_DP:main',
               'nnUNet_train_DDP = nnunet.run.run_training_DDP:main',
               'nnUNet_predict = nnunet.inference.predict_simple:main',
