diff --git a/modelzoo/rnnt/pytorch/Dockerfile-oneapi b/modelzoo/rnnt/pytorch/Dockerfile-oneapi
new file mode 100755
index 0000000..35ed095
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/Dockerfile-oneapi
@@ -0,0 +1,43 @@
+# Copyright (c) 2019-2020, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+FROM xuechendi/oneapi-aikit:hydro.ai
+
+ENV PATH /opt/intel/oneapi/intelpython/python3.7/condabin:$PATH
+RUN /bin/bash -c "source ~/.bashrc && conda init bash && conda activate pytorch"
+# RUN /bin/bash -c "conda activate pytorch"
+# ENV PATH /opt/intel/oneapi/intelpython/python3.7/condabin:$PATH
+# RUN conda init bash && \
+#     conda activate pytorch
+
+RUN apt install -y sox
+
+COPY requirements.txt .
+RUN pip install --no-cache --disable-pip-version-check -U -r requirements.txt
+
+RUN git clone https://github.com/HawkAaron/warp-transducer && \
+    cd warp-transducer && \
+    mkdir build && \
+    cd build && \
+    cmake .. && \
+    make && \
+    cd ../pytorch_binding && \
+    python setup.py install && \
+    cd ../../..
+
+WORKDIR /workspace/rnnt
+
+RUN pip install torch_optimizer==0.1.0
+
+RUN pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110
diff --git a/modelzoo/rnnt/pytorch/LICENSE b/modelzoo/rnnt/pytorch/LICENSE
deleted file mode 100644
index 3e04bb8..0000000
--- a/modelzoo/rnnt/pytorch/LICENSE
+++ /dev/null
@@ -1,204 +0,0 @@
-   Except where otherwise noted, the following license applies to all files in this repo. 
-        
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright 2019-2021 NVIDIA Corporation
-   Copyright 2019 Myrtle Software Limited, www.myrtle.ai
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/modelzoo/rnnt/pytorch/NOTICE b/modelzoo/rnnt/pytorch/NOTICE
deleted file mode 100644
index 9d3b1ff..0000000
--- a/modelzoo/rnnt/pytorch/NOTICE
+++ /dev/null
@@ -1,5 +0,0 @@
-RNN-T in PyTorch
-
-This repository includes source code (in "rnnt/") from:
-* https://github.com/keithito/tacotron and https://github.com/ryanleary/patter licensed under MIT license.
-
diff --git a/modelzoo/rnnt/pytorch/README.md b/modelzoo/rnnt/pytorch/README.md
index a01a940..4f2eedc 100644
--- a/modelzoo/rnnt/pytorch/README.md
+++ b/modelzoo/rnnt/pytorch/README.md
@@ -1,4 +1,4 @@
-# 1. Problem 
+# 1. Problem
 Speech recognition accepts raw audio samples and produces a corresponding text transcription.
 
 ## Requirements
@@ -208,7 +208,7 @@ collection of solutions from several works. It is based on the following article
 * Park 2019 - adaptive spec augment, internal dataset: https://arxiv.org/abs/1912.05533
 * Guo 2020 - RNN-T trained with vanilla LSTM, internal dataset: https://arxiv.org/abs/2007.13802
 
-### List of layers 
+### List of layers
 Model structure is described in the following picture:
 ![model layers structure](./rnnt_layers.svg "RNN-T model structure")
 
diff --git a/modelzoo/rnnt/pytorch/bind_launch.py b/modelzoo/rnnt/pytorch/bind_launch.py
deleted file mode 100644
index 4d79826..0000000
--- a/modelzoo/rnnt/pytorch/bind_launch.py
+++ /dev/null
@@ -1,121 +0,0 @@
-import sys
-import subprocess
-import os
-import socket
-from argparse import ArgumentParser, REMAINDER
-
-import torch
-
-
-def parse_args():
-    """
-    Helper function parsing the command line options
-    @retval ArgumentParser
-    """
-    parser = ArgumentParser(description="PyTorch distributed training launch "
-                                        "helper utilty that will spawn up "
-                                        "multiple distributed processes")
-
-    # Optional arguments for the launch helper
-    parser.add_argument("--nnodes", type=int, default=1,
-                        help="The number of nodes to use for distributed "
-                             "training")
-    parser.add_argument("--node_rank", type=int, default=0,
-                        help="The rank of the node for multi-node distributed "
-                             "training")
-    parser.add_argument("--nproc_per_node", type=int, default=1,
-                        help="The number of processes to launch on each node, "
-                             "for GPU training, this is recommended to be set "
-                             "to the number of GPUs in your system so that "
-                             "each process can be bound to a single GPU.")
-    parser.add_argument("--master_addr", default="127.0.0.1", type=str,
-                        help="Master node (rank 0)'s address, should be either "
-                             "the IP address or the hostname of node 0, for "
-                             "single node multi-proc training, the "
-                             "--master_addr can simply be 127.0.0.1")
-    parser.add_argument("--master_port", default=29500, type=int,
-                        help="Master node (rank 0)'s free port that needs to "
-                             "be used for communciation during distributed "
-                             "training")
-    parser.add_argument('--no_hyperthreads', action='store_true',
-                        help='Flag to disable binding to hyperthreads')
-    parser.add_argument('--no_membind', action='store_true',
-                        help='Flag to disable memory binding')
-
-    # non-optional arguments for binding
-    parser.add_argument("--nsockets_per_node", type=int, required=True,
-                        help="Number of CPU sockets on a node")
-    parser.add_argument("--ncores_per_socket", type=int, required=True,
-                        help="Number of CPU cores per socket")
-
-    # positional
-    parser.add_argument("training_script", type=str,
-                        help="The full path to the single GPU training "
-                             "program/script to be launched in parallel, "
-                             "followed by all the arguments for the "
-                             "training script")
-
-    # rest from the training program
-    parser.add_argument('training_script_args', nargs=REMAINDER)
-    return parser.parse_args()
-
-def main():
-    args = parse_args()
-
-    # variables for numactrl binding
-    NSOCKETS = args.nsockets_per_node
-    NGPUS_PER_SOCKET = args.nproc_per_node // args.nsockets_per_node
-    NCORES_PER_GPU = args.ncores_per_socket // NGPUS_PER_SOCKET
-
-    # world size in terms of number of processes
-    dist_world_size = args.nproc_per_node * args.nnodes
-
-    # set PyTorch distributed related environmental variables
-    current_env = os.environ.copy()
-    current_env["MASTER_ADDR"] = args.master_addr
-    current_env["MASTER_PORT"] = str(args.master_port)
-    current_env["WORLD_SIZE"] = str(dist_world_size)
-
-    processes = []
-
-    for local_rank in range(0, args.nproc_per_node):
-        # each process's rank
-        dist_rank = args.nproc_per_node * args.node_rank + local_rank
-        current_env["RANK"] = str(dist_rank)
-
-        # form numactrl binding command
-        cpu_ranges = [local_rank * NCORES_PER_GPU,
-                     (local_rank + 1) * NCORES_PER_GPU - 1,
-                     local_rank * NCORES_PER_GPU + (NCORES_PER_GPU * NGPUS_PER_SOCKET * NSOCKETS),
-                     (local_rank + 1) * NCORES_PER_GPU + (NCORES_PER_GPU * NGPUS_PER_SOCKET * NSOCKETS) - 1]
-
-        numactlargs = []
-        if args.no_hyperthreads:
-            numactlargs += [ "--physcpubind={}-{}".format(*cpu_ranges[0:2]) ]
-        else:
-            numactlargs += [ "--physcpubind={}-{},{}-{}".format(*cpu_ranges) ]
-
-        if not args.no_membind:
-            memnode = local_rank // NGPUS_PER_SOCKET
-            numactlargs += [ "--membind={}".format(memnode) ]
-
-        # spawn the processes
-        cmd = [ "/usr/bin/numactl" ] \
-            + numactlargs \
-            + [ sys.executable,
-                "-u",
-                args.training_script,
-                "--local_rank={}".format(local_rank)
-              ] \
-            + args.training_script_args
-
-        process = subprocess.Popen(cmd, env=current_env)
-        processes.append(process)
-
-    for process in processes:
-        process.wait()
-
-
-if __name__ == "__main__":
-    main()
-
diff --git a/modelzoo/rnnt/pytorch/common/audio.py b/modelzoo/rnnt/pytorch/common/audio.py
index d515832..23789ff 100644
--- a/modelzoo/rnnt/pytorch/common/audio.py
+++ b/modelzoo/rnnt/pytorch/common/audio.py
@@ -26,8 +26,8 @@ def audio_from_file(file_path, offset=0, duration=0, trim=False, target_sr=16000
     audio = AudioSegment(file_path, target_sr=target_sr, int_values=False,
                          offset=offset, duration=duration, trim=trim)
 
-    samples = torch.tensor(audio.samples, dtype=torch.float).cuda()
-    num_samples = torch.tensor(samples.shape[0]).int().cuda()
+    samples = torch.tensor(audio.samples, dtype=torch.float)
+    num_samples = torch.tensor(samples.shape[0]).int()
     return (samples.unsqueeze(0), num_samples.unsqueeze(0))
 
 
diff --git a/modelzoo/rnnt/pytorch/common/data/dali/data_loader.py b/modelzoo/rnnt/pytorch/common/data/dali/data_loader.py
index a5ac4b8..61f571b 100644
--- a/modelzoo/rnnt/pytorch/common/data/dali/data_loader.py
+++ b/modelzoo/rnnt/pytorch/common/data/dali/data_loader.py
@@ -42,7 +42,7 @@ class DaliDataLoader:
                  tokenizer, batch_size: int, sampler, pipeline_type: str, seed, grad_accumulation_steps: int = 1,
                  num_threads=multiprocessing.cpu_count(),
                  tokenized_transcript=False,
-                 device_type: str = "gpu", synthetic_seq_len=None, 
+                 device_type: str = "gpu", synthetic_seq_len=None,
                  in_mem_file_list=True, enable_prefetch=False, preproc=None, min_seq_split_len=-1,
                  pre_sort=False, jit_tensor_formation=False, dont_use_mmap=False):
 
diff --git a/modelzoo/rnnt/pytorch/common/data/dali/iterator.py b/modelzoo/rnnt/pytorch/common/data/dali/iterator.py
index a0c4d98..08c4001 100644
--- a/modelzoo/rnnt/pytorch/common/data/dali/iterator.py
+++ b/modelzoo/rnnt/pytorch/common/data/dali/iterator.py
@@ -43,7 +43,7 @@ class DaliRnntIterator(object):
     Use DataLoader instead.
     """
 
-    def __init__(   self, dali_pipelines, transcripts, tokenizer, batch_size, shard_size, pipeline_type, 
+    def __init__(   self, dali_pipelines, transcripts, tokenizer, batch_size, shard_size, pipeline_type,
                     normalize_transcripts=False, synthetic_text_seq_len=None, enable_prefetch=False,
                     tokenized_transcript=False,
                     preproc=None, min_seq_split_len=-1, jit_tensor_formation=False):
@@ -70,11 +70,10 @@ class DaliRnntIterator(object):
         self.tokenize(transcripts)
         self.synthetic_text_seq_len = synthetic_text_seq_len
         self.enable_prefetch = enable_prefetch
-        self.prefetch_stream = torch.cuda.Stream()
         self.preproc = preproc
         self.pipeline_type = pipeline_type
         self.min_seq_split_len = min_seq_split_len
-        self.pivot_len_cpu = torch.tensor(0, dtype=torch.int, device='cpu').pin_memory()      
+        self.pivot_len_cpu = torch.tensor(0, dtype=torch.int, device='cpu')
 
 
     def tokenize(self, transcripts):
@@ -108,7 +107,7 @@ class DaliRnntIterator(object):
         transcripts = [torch.tensor(self.tr[i]) for i in ids] if self.jit_tensor_formation else self.tr[ids]
         transcripts = torch.nn.utils.rnn.pad_sequence(transcripts, batch_first=True)
 
-        return transcripts.cuda(), self.t_sizes[ids].cuda()
+        return transcripts, self.t_sizes[ids]
 
     def fetch_next(self):
         data = self.dali_it.__next__()
@@ -156,9 +155,6 @@ class DaliRnntIterator(object):
 
     def __next__(self):
         if self.enable_prefetch:
-            torch.cuda.current_stream().wait_stream(self.prefetch_stream)
-            # make sure all async copies are committed
-            self.prefetch_stream.synchronize()
             if self.prefetched_data is None:
                 raise StopIteration
             else:
@@ -168,23 +164,22 @@ class DaliRnntIterator(object):
 
                 if self.pipeline_type == "train" and self.min_seq_split_len > 0:
                     # seq split path
-                    audio, audio_shape, transcripts, transcripts_lengths = self.prefetched_data 
+                    audio, audio_shape, transcripts, transcripts_lengths = self.prefetched_data
                     second_segment_len = audio.size(0) - self.pivot_len_cpu
                     if second_segment_len >= self.min_seq_split_len:
                         list_audio = [audio[:self.pivot_len_cpu], audio[self.pivot_len_cpu:, :self.split_batch_size]]
                         return list_audio, audio_shape, transcripts, transcripts_lengths
-                
+
                 # normal path
                 return self.prefetched_data
         else:
             return self.fetch_next()
 
     def prefetch(self):
-        with torch.cuda.stream(self.prefetch_stream):
-            try:
-                self.prefetched_data = self.fetch_next()
-            except StopIteration:
-                self.prefetched_data = None
+        try:
+            self.prefetched_data = self.fetch_next()
+        except StopIteration:
+            self.prefetched_data = None
 
     def __iter__(self):
         return self
diff --git a/modelzoo/rnnt/pytorch/common/data/dali/pipeline.py b/modelzoo/rnnt/pytorch/common/data/dali/pipeline.py
index 8a160cd..bf113f7 100644
--- a/modelzoo/rnnt/pytorch/common/data/dali/pipeline.py
+++ b/modelzoo/rnnt/pytorch/common/data/dali/pipeline.py
@@ -114,33 +114,33 @@ class DaliPipeline(nvidia.dali.pipeline.Pipeline):
         if synthetic_seq_len is None:
             if in_mem_file_list:
                 self.read = ops.readers.File( name="Reader", dont_use_mmap=dont_use_mmap,
-                                            pad_last_batch=(pipeline_type == 'val'), 
-                                            device="cpu", file_root=file_root, files=sampler.files, 
+                                            pad_last_batch=(pipeline_type == 'val'),
+                                            device="cpu", file_root=file_root, files=sampler.files,
                                             labels=sampler.labels, shard_id=shard_id,
-                                            num_shards=n_shards, shuffle_after_epoch=shuffle) 
-            else:   
+                                            num_shards=n_shards, shuffle_after_epoch=shuffle)
+            else:
                 self.read = ops.readers.File( name="Reader", dont_use_mmap=dont_use_mmap,
-                                            pad_last_batch=(pipeline_type == 'val'), 
-                                            device="cpu", file_root=file_root, 
-                                            file_list=sampler.get_file_list_path(), 
+                                            pad_last_batch=(pipeline_type == 'val'),
+                                            device="cpu", file_root=file_root,
+                                            file_list=sampler.get_file_list_path(),
                                             shard_id=shard_id,
                                             num_shards=n_shards, shuffle_after_epoch=shuffle)
         else:
             # sampler.get_file_list_path()    # still want to model this cost
-            self.read = ops.readers.File(name="Reader", dont_use_mmap=dont_use_mmap, pad_last_batch=(pipeline_type == 'val'), device="cpu", file_root=file_root, file_list="/workspace/rnnt/rnnt_dali.file_list.synth", shard_id=shard_id,
+            self.read = ops.readers.File(name="Reader", dont_use_mmap=dont_use_mmap, pad_last_batch=(pipeline_type == 'val'), device="cpu", file_root=file_root, file_list="./rnnt/rnnt_dali.file_list.synth", shard_id=shard_id,
                                        num_shards=n_shards, shuffle_after_epoch=shuffle)
 
         # TODO change ExternalSource to Uniform for new DALI release
         if resample_range is not None and not (resample_range[0] == 1 and resample_range[1] == 1):
             if pre_sort:
-                self.speed_perturbation_coeffs = ops.ExternalSource(source=pci)    
+                self.speed_perturbation_coeffs = ops.ExternalSource(source=pci)
             else:
                 self.speed_perturbation_coeffs = ops.random.Uniform(device="cpu", range=resample_range)
         else:
             self.speed_perturbation_coeffs = None
 
 
-        
+
 
 
         self.decode = ops.decoders.Audio(device="cpu", sample_rate=self.sample_rate if resample_range is None else None,
@@ -276,5 +276,5 @@ class DaliPipeline(nvidia.dali.pipeline.Pipeline):
         audio = self.pad(audio)
 
         # When modifying DALI pipeline returns, make sure you update `output_map` in DALIGenericIterator invocation
-        return audio.gpu(), label, audio_len.gpu()
+        return audio, label, audio_len
 
diff --git a/modelzoo/rnnt/pytorch/common/data/dali/sampler.py b/modelzoo/rnnt/pytorch/common/data/dali/sampler.py
index b3c4b5f..cbbb9f9 100644
--- a/modelzoo/rnnt/pytorch/common/data/dali/sampler.py
+++ b/modelzoo/rnnt/pytorch/common/data/dali/sampler.py
@@ -117,7 +117,6 @@ class SimpleSampler:
 
     def make_files(self, output_files):
         self.files, self.labels = self.process_output_files(output_files)
-
     def sample(self, file_names, in_mem_file_list, tokenized_transcript):
         output_files, self.transcripts = {}, {}
         max_duration = self.config_data['max_duration']
@@ -142,7 +141,6 @@ class SimpleSampler:
         else:
             self.make_file_list(output_files, file_names)
 
-
 class BucketingSampler(SimpleSampler):
     def __init__(self, config_data, num_buckets, batch_size, num_workers, num_epochs, seed, dist_sampler, pre_sort):
         super(BucketingSampler, self).__init__(config_data, dist_sampler)
@@ -220,7 +218,7 @@ class VectorizedBucketingSampler(SimpleSampler):
         lengths = [output_files[name]['duration'] for name in names]
         labels = np.array([output_files[name]['label'] for name in names])
 
-        dur = torch.tensor(lengths, device='cuda')
+        dur = torch.tensor(lengths, device='cpu')
         len_ids = dur.argsort()
         buckets = len_ids.tensor_split(self.num_buckets)
         padded_buckets = torch.nn.utils.rnn.pad_sequence(buckets, padding_value=-1, batch_first=True)
@@ -229,10 +227,10 @@ class VectorizedBucketingSampler(SimpleSampler):
             torch.random.manual_seed(self.seed)
             self.seed += 1
 
-            buckets_shuffler = torch.rand(self.num_epochs, *padded_buckets.shape, device='cuda')
+            buckets_shuffler = torch.rand(self.num_epochs, *padded_buckets.shape, device='cpu')
             shuffle_columnvise = buckets_shuffler.argsort(dim=2)
             epochs, num_buckets, samples = shuffle_columnvise.shape
-            shift = torch.arange(0, samples*num_buckets, samples, device='cuda').view(1, -1, 1)
+            shift = torch.arange(0, samples*num_buckets, samples, device='cpu').view(1, -1, 1)
             shuffle_globalvise = shuffle_columnvise + shift
 
             shuffled_buckets = padded_buckets.take(shuffle_globalvise)
@@ -242,8 +240,8 @@ class VectorizedBucketingSampler(SimpleSampler):
             epochs, samples = unpadded.shape
 
             to_drop = samples - (samples // gbs * gbs)
-            mask = torch.ones_like(unpadded, dtype=bool, device='cuda')
-            removed_samples = torch.rand(unpadded.shape, device='cuda').argsort(dim=1)[:, :to_drop]
+            mask = torch.ones_like(unpadded, dtype=bool, device='cpu')
+            removed_samples = torch.rand(unpadded.shape, device='cpu').argsort(dim=1)[:, :to_drop]
             epoch_idx = torch.arange(self.num_epochs).view(-1, 1).expand(self.num_epochs, to_drop)
             mask[epoch_idx.flatten(), removed_samples.flatten()] = False
 
@@ -251,16 +249,16 @@ class VectorizedBucketingSampler(SimpleSampler):
             _, num_iterations, _ = batch_aligned.shape
 
             epochs, num_batches, bs = batch_aligned.view(self.num_epochs, -1, gbs).shape
-            new_order = torch.rand(epochs, num_batches, device='cuda')
+            new_order = torch.rand(epochs, num_batches, device='cpu')
             nwo = new_order.argsort(dim=1).view(-1, num_batches, 1) * bs \
-                + torch.arange(0, bs, 1, device='cuda').view(1,1,-1) \
-                + torch.arange(0, epochs*num_batches*bs, num_batches*bs,device='cuda').view(-1, 1, 1)
+                + torch.arange(0, bs, 1, device='cpu').view(1,1,-1) \
+                + torch.arange(0, epochs*num_batches*bs, num_batches*bs,device='cpu').view(-1, 1, 1)
 
             out = batch_aligned.take(nwo)
             if self.pre_sort:
                 # At this point, the mini-batch has been formed. Now we can arrange work to each GPU
                 pert_range = self.config_data['speed_perturbation']['max_rate'] - self.config_data['speed_perturbation']['min_rate']
-                self.pert_coeff = torch.rand(out.size(0), out.size(1), out.size(2), device="cuda") * pert_range + self.config_data['speed_perturbation']['min_rate']
+                self.pert_coeff = torch.rand(out.size(0), out.size(1), out.size(2), device="cpu") * pert_range + self.config_data['speed_perturbation']['min_rate']
                 dur_after_pert = dur[out] * self.pert_coeff
                 idx_asc = dur_after_pert.argsort(dim=-1)
                 idx_des = torch.flip(idx_asc, dims=[-1])
@@ -272,21 +270,21 @@ class VectorizedBucketingSampler(SimpleSampler):
                 out = torch.gather(out, 2, idx_mix)
                 self.pert_coeff = torch.gather(self.pert_coeff, 2, idx_mix)
 
-                # to test, try 
+                # to test, try
                 # dur[out] * self.pert_coeff
-        
+
 
         if self.dist_sampler:
             out = out.view(epochs, -1, self.num_workers, self.batch_size).moveaxis(2, 0)
             out = out[self.rank]
             if self.pre_sort:
                 self.pert_coeff = self.pert_coeff.view(epochs, -1, self.num_workers, self.batch_size).moveaxis(2, 0)
-                self.pert_coeff = self.pert_coeff[self.rank].cpu() 
+                self.pert_coeff = self.pert_coeff[self.rank].cpu()
 
 
         self.dataset_size = num_iterations * self.batch_size
         out = out.cpu()
-        
+
         return np.array(names) [out.flatten()].tolist(), \
                np.array(labels)[out.flatten()].tolist()
 
diff --git a/modelzoo/rnnt/pytorch/common/data/dataset.py b/modelzoo/rnnt/pytorch/common/data/dataset.py
index f450c10..d65e78d 100644
--- a/modelzoo/rnnt/pytorch/common/data/dataset.py
+++ b/modelzoo/rnnt/pytorch/common/data/dataset.py
@@ -229,5 +229,5 @@ def get_data_loader(dataset, batch_size, world_size, rank, shuffle=True,
         dataset=dataset,
         collate_fn=collate_fn,
         num_workers=num_workers,
-        pin_memory=True
+        pin_memory=False
     )
diff --git a/modelzoo/rnnt/pytorch/common/data/features.py b/modelzoo/rnnt/pytorch/common/data/features.py
index 106d9f1..9fd6a5c 100644
--- a/modelzoo/rnnt/pytorch/common/data/features.py
+++ b/modelzoo/rnnt/pytorch/common/data/features.py
@@ -19,7 +19,6 @@ import librosa
 import torch
 import torch.nn as nn
 
-from apex import amp
 
 
 class BaseFeatures(nn.Module):
@@ -34,11 +33,7 @@ class BaseFeatures(nn.Module):
 
     def __call__(self, x):
         audio, audio_lens = x
-        if self.optim_level == 1:
-            with amp.disable_casts():
-                return self.calculate_features(audio, audio_lens)
-        else:
-            return self.calculate_features(audio, audio_lens)
+        return self.calculate_features(audio, audio_lens)
 
 
 class SpecAugment(BaseFeatures):
@@ -123,16 +118,16 @@ class VectorizedSpecAugment(SpecAugment):
 
         b, h, w = x.shape
 
-        time_shape   = torch.randint(self.min_time, int(round(w * self.max_time)) + 1, [b, self.time_masks, 1], device='cuda')
-        time_anchors = (torch.rand([b, self.time_masks, 1], device='cuda') * (w - time_shape)).round().int()
-        time_idx     = torch.linspace(0, w-1, w, dtype=int, device='cuda')
+        time_shape   = torch.randint(self.min_time, int(round(w * self.max_time)) + 1, [b, self.time_masks, 1], device='cpu')
+        time_anchors = (torch.rand([b, self.time_masks, 1], device='cpu') * (w - time_shape)).round().int()
+        time_idx     = torch.linspace(0, w-1, w, dtype=int, device='cpu')
         time_mask   = (
             (time_idx >= time_anchors) * (time_idx <= time_anchors + time_shape)
         ).any(dim=1)
 
-        freq_shape   = torch.randint(self.min_freq, self.max_freq + 1, [b, self.freq_masks, 1], device='cuda')
-        freq_anchors = (torch.rand([b, self.freq_masks, 1], device='cuda') * (h - freq_shape)).round().int()
-        freq_idx     = torch.linspace(0, h-1, h, dtype=int, device='cuda')
+        freq_shape   = torch.randint(self.min_freq, self.max_freq + 1, [b, self.freq_masks, 1], device='cpu')
+        freq_anchors = (torch.rand([b, self.freq_masks, 1], device='cpu') * (h - freq_shape)).round().int()
+        freq_idx     = torch.linspace(0, h-1, h, dtype=int, device='cpu')
         freq_mask   = (
             (freq_idx >= freq_anchors) * (freq_idx <= freq_anchors + freq_shape)
         ).any(dim=1)
diff --git a/modelzoo/rnnt/pytorch/common/data/text.py b/modelzoo/rnnt/pytorch/common/data/text.py
index 36b78fe..2c0011a 100644
--- a/modelzoo/rnnt/pytorch/common/data/text.py
+++ b/modelzoo/rnnt/pytorch/common/data/text.py
@@ -14,7 +14,6 @@
 
 import sentencepiece as spm
 
-
 class Tokenizer:
     def __init__(self, labels, sentpiece_model=None):
         """Converts transcript to a sequence of tokens.
diff --git a/modelzoo/rnnt/pytorch/common/helpers.py b/modelzoo/rnnt/pytorch/common/helpers.py
index 1c72f72..802d551 100644
--- a/modelzoo/rnnt/pytorch/common/helpers.py
+++ b/modelzoo/rnnt/pytorch/common/helpers.py
@@ -17,8 +17,6 @@ import os
 import re
 from collections import OrderedDict
 
-from apex import amp
-
 import torch
 import torch.distributed as dist
 
@@ -109,14 +107,14 @@ def process_evaluation_epoch(aggregates):
     if multi_gpu:
         if eloss is not None:
             eloss /= dist.get_world_size()
-            eloss_tensor = torch.tensor(eloss).cuda()
+            eloss_tensor = torch.tensor(eloss)
             dist.all_reduce(eloss_tensor)
             eloss = eloss_tensor.item()
 
-        scores_tensor = torch.tensor(scores).cuda()
+        scores_tensor = torch.tensor(scores)
         dist.all_reduce(scores_tensor)
         scores = scores_tensor.item()
-        num_words_tensor = torch.tensor(num_words).cuda()
+        num_words_tensor = torch.tensor(num_words)
         dist.all_reduce(num_words_tensor)
         num_words = num_words_tensor.item()
         wer = scores * 1.0 / num_words
@@ -176,7 +174,6 @@ class Checkpointer(object):
             'state_dict': unwrap_ddp(model).state_dict(),
             'ema_state_dict': unwrap_ddp(ema_model).state_dict() if ema_model is not None else None,
             'optimizer': optimizer.state_dict(),
-            'amp': amp.state_dict() if self.use_amp else None,
         }
 
         if is_best:
@@ -235,8 +232,6 @@ class Checkpointer(object):
 
         optimizer.load_state_dict(checkpoint['optimizer'])
 
-        if self.use_amp:
-            amp.load_state_dict(checkpoint['amp'])
 
         meta['start_epoch'] = checkpoint.get('epoch')
         meta['best_wer'] = checkpoint.get('best_wer', meta['best_wer'])
@@ -250,7 +245,7 @@ class Preproc():
         self.window_stride = cfg["input_val"]["filterbank_features"]["window_stride"]
         self.frame_subsampling =  cfg["input_val"]["frame_splicing"]["frame_subsampling"]
         self.batch_split_factor = batch_split_factor
-        self.list_packed_batch_cpu = [torch.tensor(0, dtype=torch.int64, device='cpu').pin_memory() 
+        self.list_packed_batch_cpu = [torch.tensor(0, dtype=torch.int64, device='cpu')
                                         for i in range(self.batch_split_factor)]
 
     def preproc_func(self, audio, audio_shape):
@@ -264,8 +259,8 @@ class Preproc():
         self.meta_data = []
         B_split = transcripts.size(0) // self.batch_split_factor
         for i in range(self.batch_split_factor):
-            self.meta_data.append(self.get_packing_meta_data(   max_f_len, 
-                                                                audio_shape_[i*B_split:(i+1)*B_split], 
+            self.meta_data.append(self.get_packing_meta_data(   max_f_len,
+                                                                audio_shape_[i*B_split:(i+1)*B_split],
                                                                 transcripts_lengths[i*B_split:(i+1)*B_split],
                                                                 async_cp,
                                                                 i))
diff --git a/modelzoo/rnnt/pytorch/common/optimizers.py b/modelzoo/rnnt/pytorch/common/optimizers.py
index 3d313a0..e448c3e 100644
--- a/modelzoo/rnnt/pytorch/common/optimizers.py
+++ b/modelzoo/rnnt/pytorch/common/optimizers.py
@@ -43,14 +43,14 @@ def lr_policy(step, epoch, initial_lr, optimizer, steps_per_epoch, warmup_epochs
         # we only have 1 param group now
         optimizer.param_groups[0]['lr'] = max(a * initial_lr, min_lr)
     else:
-        optimizer._lr = max(a * initial_lr, min_lr * torch.ones_like(initial_lr))
+        optimizer._lr = max(a * initial_lr, min_lr)
 
 
 class AdamW(Optimizer):
     """Implements AdamW algorithm.
-  
+
     It has been proposed in `Adam: A Method for Stochastic Optimization`_.
-  
+
     Arguments:
         params (iterable): iterable of parameters to optimize or dicts defining
             parameter groups
@@ -62,13 +62,13 @@ class AdamW(Optimizer):
         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
         amsgrad (boolean, optional): whether to use the AMSGrad variant of this
             algorithm from the paper `On the Convergence of Adam and Beyond`_
-  
+
         Adam: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
         On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
     """
-  
+
     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                   weight_decay=0, amsgrad=False):
         if not 0.0 <= lr:
@@ -82,15 +82,15 @@ class AdamW(Optimizer):
         defaults = dict(lr=lr, betas=betas, eps=eps,
                         weight_decay=weight_decay, amsgrad=amsgrad)
         super(AdamW, self).__init__(params, defaults)
-  
+
     def __setstate__(self, state):
         super(AdamW, self).__setstate__(state)
         for group in self.param_groups:
             group.setdefault('amsgrad', False)
-  
+
     def step(self, closure=None):
         """Performs a single optimization step.
-  
+
         Arguments:
             closure (callable, optional): A closure that reevaluates the model
                 and returns the loss.
@@ -98,7 +98,7 @@ class AdamW(Optimizer):
         loss = None
         if closure is not None:
             loss = closure()
-  
+
         for group in self.param_groups:
             for p in group['params']:
                 if p.grad is None:
@@ -107,9 +107,9 @@ class AdamW(Optimizer):
                 if grad.is_sparse:
                     raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
                 amsgrad = group['amsgrad']
-  
+
                 state = self.state[p]
-  
+
                 # State initialization
                 if len(state) == 0:
                     state['step'] = 0
@@ -120,12 +120,12 @@ class AdamW(Optimizer):
                     if amsgrad:
                         # Maintains max of all exp. moving avg. of sq. grad. values
                         state['max_exp_avg_sq'] = torch.zeros_like(p.data)
-  
+
                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                 if amsgrad:
                     max_exp_avg_sq = state['max_exp_avg_sq']
                 beta1, beta2 = group['betas']
-  
+
                 state['step'] += 1
                 # Decay the first and second moment running average coefficient
                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
@@ -137,15 +137,15 @@ class AdamW(Optimizer):
                     denom = max_exp_avg_sq.sqrt().add_(group['eps'])
                 else:
                     denom = exp_avg_sq.sqrt().add_(group['eps'])
-  
+
                 bias_correction1 = 1 - beta1 ** state['step']
                 bias_correction2 = 1 - beta2 ** state['step']
                 step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1
                 p.data.add_(torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom), alpha=-step_size)
-  
+
         return loss
 
-  
+
 class Novograd(Optimizer):
     """
     Implements Novograd algorithm.
@@ -250,5 +250,5 @@ class Novograd(Optimizer):
                 exp_avg.mul_(beta1).add_(grad)
 
                 p.data.add_(exp_avg, alpha=-group['lr'])
-        
+
         return loss
diff --git a/modelzoo/rnnt/pytorch/common/rnn.py b/modelzoo/rnnt/pytorch/common/rnn.py
index 393a276..a6a9f55 100644
--- a/modelzoo/rnnt/pytorch/common/rnn.py
+++ b/modelzoo/rnnt/pytorch/common/rnn.py
@@ -19,10 +19,9 @@ import torch
 from torch.nn import Parameter
 from mlperf import logging
 
-
 def rnn(input_size, hidden_size, num_layers,
         forget_gate_bias=1.0, dropout=0.0,
-        decoupled=False, **kwargs):
+        decoupled=False, rnn_type='lstm', **kwargs):
 
     kwargs = dict(
         input_size=input_size,
@@ -36,7 +35,10 @@ def rnn(input_size, hidden_size, num_layers,
     if decoupled:
         return DecoupledLSTM(**kwargs)
     else:
-        return LSTM(**kwargs)
+        if rnn_type == 'lstm':
+            return LSTM(**kwargs)
+        else:
+            return GRU(**kwargs)
 
 
 class LSTM(torch.nn.Module):
@@ -65,6 +67,7 @@ class LSTM(torch.nn.Module):
             num_layers=num_layers,
             dropout=dropout,
         )
+        self.layer_norm = torch.nn.LayerNorm(hidden_size)
 
         self.dropout = torch.nn.Dropout(dropout) if dropout else None
 
@@ -102,10 +105,77 @@ class LSTM(torch.nn.Module):
             if self.dropout:
                 y0 = self.dropout(y0)
                 y1 = self.dropout(y1)
+            y0 = self.layer_norm(y0)
+            y1 = self.layer_norm(y1)
             # h will not be used in training any way. Return None.
             # We guarantee this path will only be taken by training
             return [y0, y1], None
 
+class GRU(torch.nn.Module):
+
+    def __init__(self, input_size, hidden_size, num_layers, dropout,
+                 forget_gate_bias, weights_init_scale=1.0,
+                 hidden_hidden_bias_scale=0.0, **kwargs):
+        """Returns an GRU with bias init to `forget_gate_bias`.
+
+        Args:
+            input_size: See `torch.nn.GRU`.
+            hidden_size: See `torch.nn.GRU`.
+            num_layers: See `torch.nn.GRU`.
+            dropout: See `torch.nn.GRU`.
+            forget_gate_bias: For each layer and each direction, the total value of
+                to initialise the forget gate bias to.
+
+        Returns:
+            A `torch.nn.GRU`.
+        """
+        super(GRU, self).__init__()
+
+        self.gru = torch.nn.GRU(
+            input_size=input_size,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            dropout=dropout,
+        )
+
+        self.dropout = torch.nn.Dropout(dropout) if dropout else None
+
+        if forget_gate_bias is not None:
+            for name, v in self.gru.named_parameters():
+                if "bias_ih" in name:
+                    bias = getattr(self.gru, name)
+                    bias.data[hidden_size:2*hidden_size].fill_(forget_gate_bias)
+                if "bias_hh" in name:
+                    bias = getattr(self.gru, name)
+                    bias.data[hidden_size:2*hidden_size] *= float(hidden_hidden_bias_scale)
+
+        for name, v in self.named_parameters():
+            if 'weight' in name or 'bias' in name:
+                v.data *= float(weights_init_scale)
+        tensor_name = kwargs['tensor_name']
+        logging.log_event(logging.constants.WEIGHTS_INITIALIZATION,
+                          metadata=dict(tensor=tensor_name))
+
+
+    def forward(self, x, h=None):
+        if type(x) is not list:
+            x, h = self.gru(x, h)
+            if self.dropout:
+                x = self.dropout(x)
+            return x, h
+        else:
+            # seq splitting path
+            if len(x) != 2:
+                raise NotImplementedError("Only number of seq segments equal to 2 is supported")
+            y0, h0 = self.gru(x[0], h)
+            hid0 = h0[:, :x[1].size(1)].contiguous()
+            y1, h1 = self.gru(x[1], hid0)
+            if self.dropout:
+                y0 = self.dropout(y0)
+                y1 = self.dropout(y1)
+            # h will not be used in training any way. Return None.
+            # We guarantee this path will only be taken by training
+            return [y0, y1], None
 
 class DecoupledBase(torch.nn.Module):
     """Base class for decoupled RNNs.
diff --git a/modelzoo/rnnt/pytorch/common/tb_dllogger.py b/modelzoo/rnnt/pytorch/common/tb_dllogger.py
index ad6e3a6..9b83066 100644
--- a/modelzoo/rnnt/pytorch/common/tb_dllogger.py
+++ b/modelzoo/rnnt/pytorch/common/tb_dllogger.py
@@ -107,7 +107,7 @@ def stdout_metric_format(metric, metadata, value):
 
 
 def init_log(args):
-    
+
     enabled = not dist.is_initialized() or dist.get_rank() == 0
     if enabled:
         fpath = args.log_file or os.path.join(args.output_dir, 'nvlog.json')
diff --git a/modelzoo/rnnt/pytorch/common/text/cleaners.py b/modelzoo/rnnt/pytorch/common/text/cleaners.py
index 08ef5d8..9b514ea 100644
--- a/modelzoo/rnnt/pytorch/common/text/cleaners.py
+++ b/modelzoo/rnnt/pytorch/common/text/cleaners.py
@@ -13,7 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-""" from https://github.com/keithito/tacotron 
+""" from https://github.com/keithito/tacotron
 Modified to add puncturation removal
 """
 
diff --git a/modelzoo/rnnt/pytorch/common/text/numbers.py b/modelzoo/rnnt/pytorch/common/text/numbers.py
index 46ce110..1b55c3a 100644
--- a/modelzoo/rnnt/pytorch/common/text/numbers.py
+++ b/modelzoo/rnnt/pytorch/common/text/numbers.py
@@ -12,7 +12,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-""" from https://github.com/keithito/tacotron 
+""" from https://github.com/keithito/tacotron
 Modifed to add support for time and slight tweaks to _expand_number
 """
 
diff --git a/modelzoo/rnnt/pytorch/config.sh b/modelzoo/rnnt/pytorch/config.sh
new file mode 100644
index 0000000..7e9a5f2
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/config.sh
@@ -0,0 +1,36 @@
+## Run specific params
+export DATADIR="/raid/datasets/rnnt/"
+export METADATA_DIR="/lustre/fsw/mlperf-ci/tokenized/"
+export SENTENCEPIECES_DIR="/lustre/fsw/mlperf-ci/sentpiece"
+export BATCHSIZE=32
+export EVAL_BATCHSIZE=338
+export GRAD_ACCUMULATION_STEPS=1
+export WALLTIME=04:00:00
+export MAX_SYMBOL=300
+export DATA_CPU_THREADS=4
+
+#source $(dirname ${BASH_SOURCE[0]})/hyperparameters_2048.sh
+
+## Opt flag
+export FUSE_RELU_DROPOUT=true
+export MULTI_TENSOR_EMA=true
+# export BATCH_EVAL_MODE=cg_unroll_pipeline
+export APEX_LOSS=fp16
+export APEX_JOINT=pack
+
+export AMP_LVL=1
+export BUFFER_PREALLOC=true
+export VECTORIZED_SA=true
+export EMA_UPDATE_TYPE=fp16
+export DIST_LAMB=false
+export MULTILAYER_LSTM=true
+export ENABLE_PREFETCH=true
+export BATCH_SPLIT_FACTOR=1
+export TOKENIZED_TRANSCRIPT=true
+export VECTORIZED_SAMPLER=true
+export DIST_SAMPLER=true
+export MIN_SEQ_SPLIT_LEN=20
+export APEX_MLP=true
+export PRE_SORT_FOR_SEQ_SPLIT=true
+export LOG_FREQUENCY=1
+export JIT_TENSOR_FORMATION=true
diff --git a/modelzoo/rnnt/pytorch/configs/baseline_v3-1023sp.yaml b/modelzoo/rnnt/pytorch/configs/baseline_v3-1023sp.yaml
index 83393d1..2948575 100644
--- a/modelzoo/rnnt/pytorch/configs/baseline_v3-1023sp.yaml
+++ b/modelzoo/rnnt/pytorch/configs/baseline_v3-1023sp.yaml
@@ -17,7 +17,7 @@
 #
 
 tokenizer:
-    sentpiece_model: /sentencepieces/librispeech1023.model
+    sentpiece_model: /home/vmagent/app/dataset/LibriSpeech/sentencepieces/librispeech1023.model
     labels: [" ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
              "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", "'"]
 
@@ -67,23 +67,23 @@ input_train:
     min_time: 0
     max_time: 0.03
 
-rnnt:
-  in_feats: 256  # n_filt x frame_stacking (+ padding to 32)
-  # init: xavier_uniform
-  # activation: relu
+# rnnt:
+#   in_feats: 256  # n_filt x frame_stacking (+ padding to 32)
+#   # init: xavier_uniform
+#   # activation: relu
 
-  enc_n_hid: 1024
-  enc_pre_rnn_layers: 2
-  enc_post_rnn_layers: 3
-  enc_stack_time_factor: 2
-  enc_dropout: 0.1
+#   enc_n_hid: 1024
+#   enc_pre_rnn_layers: 2
+#   enc_post_rnn_layers: 3
+#   enc_stack_time_factor: 2
+#   enc_dropout: 0.1
 
-  pred_n_hid: 512
-  pred_rnn_layers: 2
-  pred_dropout: 0.3
+#   pred_n_hid: 512
+#   pred_rnn_layers: 2
+#   pred_dropout: 0.3
 
-  joint_n_hid: 512
-  joint_dropout: 0.3
+#   joint_n_hid: 512
+#   joint_dropout: 0.3
 
-  forget_gate_bias: 1.0
-  decoupled_rnns: true
+#   forget_gate_bias: 1.0
+#   decoupled_rnns: true
diff --git a/modelzoo/rnnt/pytorch/decoder.py b/modelzoo/rnnt/pytorch/decoder.py
new file mode 100644
index 0000000..ad4de18
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/decoder.py
@@ -0,0 +1,165 @@
+# Copyright (c) 2019, Myrtle Software Limited. All rights reserved.
+# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#           http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+import numpy as np
+
+import torch.nn.functional as F
+
+class TransducerDecoder:
+    """Decoder base class.
+
+    Args:
+        alphabet: An Alphabet object.
+        blank_symbol: The symbol in `alphabet` to use as the blank during CTC
+            decoding.
+        model: Model to use for prediction.
+    """
+
+    def __init__(self, blank_index, model):
+        self._model = model
+        self._SOS = -1   # start of sequence
+        self._blank_id = blank_index
+
+    def _pred_step(self, label, hidden, device):
+        if label == self._SOS:
+            return self._model.predict(None, hidden, add_sos=False)
+        if label > self._blank_id:
+            label -= 1
+        label = label_collate([[label]]).to(device)
+        return self._model.predict(label, hidden, add_sos=False)
+
+    def _joint_step(self, enc, pred, log_normalize=False):
+        logits = self._model.joint(enc, pred)[:, 0, 0, :]
+        if not log_normalize:
+            return logits
+
+        probs = F.log_softmax(logits, dim=len(logits.shape) - 1)
+
+        return probs
+
+    def _get_last_symb(self, labels):
+        return self._SOS if labels == [] else labels[-1]
+
+
+class RNNTGreedyDecoder(TransducerDecoder):
+    """A greedy transducer decoder.
+
+    Args:
+        blank_symbol: See `Decoder`.
+        model: Model to use for prediction.
+        max_symbols_per_step: The maximum number of symbols that can be added
+            to a sequence in a single time step; if set to None then there is
+            no limit.
+        cutoff_prob: Skip to next step in search if current highest character
+            probability is less than this.
+    """
+    def __init__(self, blank_index, model, max_symbols_per_step=30):
+        super().__init__(blank_index, model)
+        assert max_symbols_per_step is None or max_symbols_per_step > 0
+        self.max_symbols = max_symbols_per_step
+
+    def decode(self, x, out_lens):
+        """Returns a list of sentences given an input batch.
+
+        Args:
+            x: A tensor of size (batch, channels, features, seq_len)
+                TODO was (seq_len, batch, in_features).
+            out_lens: list of int representing the length of each sequence
+                output sequence.
+
+        Returns:
+            list containing batch number of sentences (strings).
+        """
+        with torch.no_grad():
+            # Apply optional preprocessing
+
+            logits, out_lens = self._model.encode((x, out_lens))
+
+            output = []
+            for batch_idx in range(logits.size(0)):
+                inseq = logits[batch_idx, :, :].unsqueeze(1)
+                logitlen = out_lens[batch_idx]
+                sentence = self._greedy_decode(inseq, logitlen)
+                output.append(sentence)
+
+        return output
+
+    def _greedy_decode(self, x, out_len):
+        training_state = self._model.training
+        self._model.eval()
+
+        device = x.device
+
+        hidden = None
+        label = []
+        for time_idx in range(out_len):
+            f = x[time_idx, :, :].unsqueeze(0)
+
+            not_blank = True
+            symbols_added = 0
+
+            while not_blank and (
+                    self.max_symbols is None or
+                    symbols_added < self.max_symbols):
+                g, hidden_prime = self._pred_step(
+                    self._get_last_symb(label),
+                    hidden,
+                    device
+                )
+                logp = self._joint_step(f, g, log_normalize=False)[0, :]
+
+                # get index k, of max prob
+                v, k = logp.max(0)
+                k = k.item()
+
+                if k == self._blank_id:
+                    not_blank = False
+                else:
+                    label.append(k)
+                    hidden = hidden_prime
+                symbols_added += 1
+
+        self._model.train(training_state)
+        return label
+
+def label_collate(labels):
+    """Collates the label inputs for the rnn-t prediction network.
+
+    If `labels` is already in torch.Tensor form this is a no-op.
+
+    Args:
+        labels: A torch.Tensor List of label indexes or a torch.Tensor.
+
+    Returns:
+        A padded torch.Tensor of shape (batch, max_seq_len).
+    """
+
+    if isinstance(labels, torch.Tensor):
+        return labels.type(torch.int64)
+    if not isinstance(labels, (list, tuple)):
+        raise ValueError(
+            f"`labels` should be a list or tensor not {type(labels)}"
+        )
+
+    batch_size = len(labels)
+    max_len = max(len(l) for l in labels)
+
+    cat_labels = np.full((batch_size, max_len), fill_value=0.0, dtype=np.int32)
+    for e, l in enumerate(labels):
+        cat_labels[e, :len(l)] = l
+    labels = torch.LongTensor(cat_labels)
+
+    return labels
\ No newline at end of file
diff --git a/modelzoo/rnnt/pytorch/dgxa100_nic_affinity.xml b/modelzoo/rnnt/pytorch/dgxa100_nic_affinity.xml
deleted file mode 100755
index 4260c59..0000000
--- a/modelzoo/rnnt/pytorch/dgxa100_nic_affinity.xml
+++ /dev/null
@@ -1,50 +0,0 @@
-<system version="1">
-  <cpu numaid="3" affinity="00000000,00000000,ffff0000,00000000,00000000,00000000,ffff0000,00000000" arch="x86_64" vendor="AuthenticAMD" familyid="143" modelid="49">
-    <pci busid="0000:01:00.0" class="0x060400" link_speed="16 GT/s" link_width="16">
-      <pci busid="0000:03:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:07:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:12:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-      <pci busid="0000:0a:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:0f:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:0c:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-    </pci>
-  </cpu>
-  <cpu numaid="1" affinity="00000000,00000000,00000000,ffff0000,00000000,00000000,00000000,ffff0000" arch="x86_64" vendor="AuthenticAMD" familyid="143" modelid="49">
-    <pci busid="0000:41:00.0" class="0x060400" link_speed="16 GT/s" link_width="16">
-      <pci busid="0000:43:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:47:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:54:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-      <pci busid="0000:49:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:4e:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:4b:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-    </pci>
-  </cpu>
-  <cpu numaid="7" affinity="ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000,00000000" arch="x86_64" vendor="AuthenticAMD" familyid="143" modelid="49">
-    <pci busid="0000:81:00.0" class="0x060400" link_speed="16 GT/s" link_width="16">
-      <pci busid="0000:83:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:87:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:94:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-      <pci busid="0000:8b:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:90:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:8d:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-    </pci>
-  </cpu>
-  <cpu numaid="5" affinity="00000000,ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000" arch="x86_64" vendor="AuthenticAMD" familyid="143" modelid="49">
-    <pci busid="0000:b1:00.0" class="0x060400" link_speed="16 GT/s" link_width="16">
-      <pci busid="0000:b3:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:b7:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:cc:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-      <pci busid="0000:b8:00.0" class="0x060400" link_speed="16 GT/s" link_width="16" subsystem_device="0x0000">
-        <pci busid="0000:bd:00.0" class="0x030200" link_speed="16 GT/s" link_width="16"/>
-        <pci busid="0000:ba:00.0" class="0x020700" link_speed="16 GT/s" link_width="16"/>
-      </pci>
-    </pci>
-  </cpu>
-</system>
diff --git a/modelzoo/rnnt/pytorch/distributed.py b/modelzoo/rnnt/pytorch/distributed.py
new file mode 100644
index 0000000..e2f7644
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/distributed.py
@@ -0,0 +1,49 @@
+import os
+import builtins
+import numpy as np
+import torch
+from torch.autograd import Function
+from torch.nn.parallel import DistributedDataParallel as DDP
+import torch.distributed as dist
+try:
+    import torch_ccl
+except ImportError as e:
+    print(e)
+    torch_ccl = False
+
+my_rank = -1
+my_size = -1
+my_local_rank = -1
+my_local_size = -1
+
+def env2int(env_list, default = -1):
+    for e in env_list:
+        val = int(os.environ.get(e, -1))
+        if val >= 0: return val
+    return default
+
+
+def init_distributed(rank = -1, size = -1, backend='gloo'):
+    global my_rank
+    global my_size
+    global my_local_rank
+    global my_local_size
+    #guess Rank and size
+    if rank == -1:
+        rank = env2int(['PMI_RANK', 'OMPI_COMM_WORLD_RANK', 'MV2_COMM_WORLD_RANK', 'RANK'], 0)
+    if size == -1:
+        size = env2int(['PMI_SIZE', 'OMPI_COMM_WORLD_SIZE', 'MV2_COMM_WORLD_SIZE', 'WORLD_SIZE'], 1)
+    if not os.environ.get('RANK', None) and rank != -1: os.environ['RANK'] = str(rank)
+    if not os.environ.get('WORLD_SIZE', None) and size != -1: os.environ['WORLD_SIZE'] = str(size)
+    if not os.environ.get('MASTER_PORT', None): os.environ['MASTER_PORT'] = '29500'
+    if not os.environ.get('MASTER_ADDR', None): os.environ['MASTER_ADDR'] = '127.0.0.1'
+    if size > 1:
+        print(F"world_size:{size},rank:{rank}")
+        dist.init_process_group(backend, rank=rank, world_size=size)
+        my_rank = dist.get_rank()
+        my_size = dist.get_world_size()
+        my_local_rank = env2int(['MPI_LOCALRANKID', 'OMPI_COMM_WORLD_LOCAL_RANK', 'MV2_COMM_WORLD_LOCAL_RANK'], 0)
+        my_local_size = env2int(['MPI_LOCALNRANKS', 'OMPI_COMM_WORLD_LOCAL_SIZE', 'MV2_COMM_WORLD_LOCAL_SIZE'], 1)
+        if backend == 'ccl':
+            print(f"Using CCL_ATL_TRANSPORT={os.environ.get('CCL_ATL_TRANSPORT', '(default)')}")
+            print(f"Using CCL_ATL_SHM={os.environ.get('CCL_ATL_SHM', '(default)')}")
diff --git a/modelzoo/rnnt/pytorch/hyperparameters_2048.sh b/modelzoo/rnnt/pytorch/hyperparameters_2048.sh
deleted file mode 100644
index 9bec453..0000000
--- a/modelzoo/rnnt/pytorch/hyperparameters_2048.sh
+++ /dev/null
@@ -1,6 +0,0 @@
-export EPOCH=80
-export LR=0.007
-export EMA=0.995
-export HOLD_EPOCHS=33
-export LR_DECAY_POWER=0.939
-
diff --git a/modelzoo/rnnt/pytorch/hyperparameters_3072.sh b/modelzoo/rnnt/pytorch/hyperparameters_3072.sh
deleted file mode 100644
index 195fa3f..0000000
--- a/modelzoo/rnnt/pytorch/hyperparameters_3072.sh
+++ /dev/null
@@ -1,10 +0,0 @@
-export EPOCH=120
-export LR=0.009
-export EMA=0.985
-export WARMUP=6
-export HOLD_EPOCHS=40
-export LR_DECAY_POWER=0.940
-export WEIGHTS_INIT_SCALE=0.35
-export BETA1=0.9
-export BETA2=0.995
-
diff --git a/modelzoo/rnnt/pytorch/inference.py b/modelzoo/rnnt/pytorch/inference.py
new file mode 100644
index 0000000..171fd64
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/inference.py
@@ -0,0 +1,513 @@
+# Copyright (c) 2019-2021, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#           http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import argparse
+import copy
+import os
+import random
+import time
+
+import torch
+import multiprocessing
+import numpy as np
+# import torch.distributed as dist
+# from torch.cuda.amp import GradScaler
+import math
+import intel_extension_for_pytorch as ipex
+from torch.nn.parallel import DistributedDataParallel as DDP
+import distributed as dist
+
+from common import helpers
+from common.data.dali import sampler as dali_sampler
+from common.data.dali.data_loader import DaliDataLoader
+from common.data.text import Tokenizer
+from common.data import features
+from common.helpers import (Checkpointer, greedy_wer, num_weights, print_once,
+                            process_evaluation_epoch, Preproc)
+from common.optimizers import lr_policy
+from common.tb_dllogger import flush_log, init_log, log
+from rnnt import config
+from rnnt.decoder import RNNTGreedyDecoder
+from rnnt.loss import RNNTLoss
+# from rnnt.loss import apexTransducerLoss
+from rnnt.model import RNNT
+from rnnt.rnnt_graph import RNNTGraph
+
+from mlperf import logging
+
+
+# TODO Eval batch size
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='RNN-T Training Reference')
+
+    training = parser.add_argument_group('training setup')
+    training.add_argument('--epochs', default=100, type=int,
+                          help='Number of epochs for the entire training')
+    training.add_argument("--warmup_epochs", default=6, type=int,
+                          help='Initial epochs of increasing learning rate')
+    training.add_argument("--hold_epochs", default=40, type=int,
+                          help='Constant max learning rate epochs after warmup')
+    training.add_argument('--epochs_this_job', default=0, type=int,
+                          help=('Run for a number of epochs with no effect on the lr schedule.'
+                                'Useful for re-starting the training.'))
+    training.add_argument('--cudnn_benchmark', action='store_true', default=True,
+                          help='Enable cudnn benchmark')
+    training.add_argument('--amp_level', default=1, type=int, choices=[0, 1, 2, 3],
+                          help='APEX AMP optimization level')
+    training.add_argument('--seed', default=None, type=int, help='Random seed')
+    training.add_argument('--local_rank', default=os.getenv('LOCAL_RANK', 0), type=int,
+                          help='GPU id used for distributed training')
+    training.add_argument('--target', default=0.058, type=float, help='Target WER accuracy')
+    training.add_argument('--apex_transducer_loss', default=None, type=str, choices=['fp16', 'fp32'],
+                            help='what precision of apex transducer_loss to use')
+    training.add_argument('--fuse_relu_dropout', action='store_true',
+                            help='Fuse ReLU and dropout in the joint net')
+    training.add_argument('--weights_init_scale', default=0.5, type=float, help='If set, overwrites value in config.')
+    training.add_argument('--hidden_hidden_bias_scale', type=float, help='If set, overwrites value in config.')
+    training.add_argument('--batch_eval_mode', default=None, type=str, choices=['no_cg', 'cg', 'cg_unroll_pipeline'],
+                    help='do evaluation in batch')
+    training.add_argument('--cg_unroll_factor', default=4, type=int, help='Unrolling factor for batch eval mode cg_unroll_pipeline')
+    training.add_argument('--apex_transducer_joint', default=None, type=str, choices=['pack', 'not_pack'],
+                            help='whether or not to pack the sequence with apex transducer_joint')
+    training.add_argument('--buffer_pre_alloc', action='store_true',
+                            help='Pre-allocate buffer in PyTorch')
+    training.add_argument('--multilayer_lstm', action='store_true',
+                            help='Use multilayer LSTMs instead of splitting them into multiple single-layer ones')
+    training.add_argument('--batch_split_factor', default=1, type=int, help='Split batches feed into the joint net')
+    training.add_argument('--apex_mlp', action='store_true',
+                            help='Use apex MLP')
+
+    training.add_argument("--num_cg", default=0, type=int,
+                          help='number of graphs needed for training')
+    training.add_argument('--min_seq_split_len', default=-1, type=int, help='Split sequences in a mini-batch to improve performance')
+    training.add_argument('--pre_sort_for_seq_split', action='store_true',
+                            help='Presort samples in a mini-batch so that seq split is more effective')
+    training.add_argument('--dist', action='store_true', default=False, help='Enable distributed training')
+    training.add_argument('--dist_backend', type=str, default='gloo', help='Distributed training backend')
+    training.add_argument('--use_ipex', action='store_true', default=False, help='Enable IPEX backend')
+
+
+    optim = parser.add_argument_group('optimization setup')
+    optim.add_argument('--batch_size', default=128, type=int,
+                       help='Effective batch size per GPU (might require grad accumulation')
+    optim.add_argument('--val_batch_size', default=2, type=int,
+                       help='Evalution time batch size')
+    optim.add_argument('--lr', default=4e-3, type=float,
+                       help='Peak learning rate')
+    optim.add_argument("--min_lr", default=1e-5, type=float,
+                       help='minimum learning rate')
+    optim.add_argument("--lr_exp_gamma", default=0.935, type=float,
+                       help='gamma factor for exponential lr scheduler')
+    optim.add_argument('--weight_decay', default=1e-3, type=float,
+                       help='Weight decay for the optimizer')
+    optim.add_argument('--grad_accumulation_steps', default=8, type=int,
+                       help='Number of accumulation steps')
+    optim.add_argument('--clip_norm', default=1, type=float,
+                       help='If provided, gradients will be clipped above this norm')
+    optim.add_argument('--beta1', default=0.9, type=float, help='Beta 1 for optimizer')
+    optim.add_argument('--beta2', default=0.999, type=float, help='Beta 2 for optimizer')
+    optim.add_argument('--ema', type=float, default=0.999,
+                       help='Discount factor for exp averaging of model weights')
+    optim.add_argument('--multi_tensor_ema', action='store_true',
+                            help='Use multi_tensor_apply for EMA')
+    optim.add_argument('--dist_lamb', action='store_true',
+                            help='Use distributed LAMB')
+    optim.add_argument('--ema_update_type', default='fp32', type=str, choices=['fp16', 'fp32'],
+                            help='is ema applied on the fp32 master weight or fp16 weight')
+    optim.add_argument('--dwu_group_size', default=8, type=int,
+                       help='Group size for distributed optimizer. Will be ignored if non-distributed optimizer is used')
+
+
+
+    io = parser.add_argument_group('feature and checkpointing setup')
+    io.add_argument('--dali_device', type=str, choices=['cpu', 'gpu'],
+                    default='cpu', help='Use DALI pipeline for fast data processing')
+    io.add_argument('--resume', action='store_true',
+                    help='Try to resume from last saved checkpoint.')
+    io.add_argument('--ckpt', default=None, type=str,
+                    help='Path to a checkpoint for resuming training')
+    io.add_argument('--save_at_the_end', action='store_true',
+                    help='Saves model checkpoint at the end of training')
+    io.add_argument('--save_frequency', default=None, type=int,
+                    help='Checkpoint saving frequency in epochs')
+    io.add_argument('--keep_milestones', default=[], type=int, nargs='+',
+                    help='Milestone checkpoints to keep from removing')
+    io.add_argument('--save_best_from', default=200, type=int,
+                    help='Epoch on which to begin tracking best checkpoint (dev WER)')
+    io.add_argument('--val_frequency', default=1, type=int,
+                    help='Number of epochs between evaluations on dev set')
+    io.add_argument('--log_frequency', default=25, type=int,
+                    help='Number of steps between printing training stats')
+    io.add_argument('--prediction_frequency', default=None, type=int,
+                    help='Number of steps between printing sample decodings')
+    io.add_argument('--model_config', default='configs/baseline_v3-1023sp.yaml',
+                    type=str, required=True,
+                    help='Path of the model configuration file')
+    io.add_argument('--num_buckets', type=int, default=6,
+                    help='If provided, samples will be grouped by audio duration, '
+                         'to this number of backets, for each bucket, '
+                         'random samples are batched, and finally '
+                         'all batches are randomly shuffled')
+    io.add_argument('--vectorized_sampler', action='store_true',
+                    help='Use optimized bucketing sampler implementation')
+    io.add_argument('--dist_sampler', action='store_true',
+                    help='Each rank owns an unique copy of file list')
+    io.add_argument('--train_manifests', type=str, required=True, nargs='+',
+                    help='Paths of the training dataset manifest file')
+    io.add_argument('--val_manifests', type=str, required=True, nargs='+',
+                    help='Paths of the evaluation datasets manifest files')
+    io.add_argument('--max_duration', type=float,
+                    help='Discard samples longer than max_duration')
+    io.add_argument('--max_txt_len', type=int, default=125,
+                    help='The longest text length in the sample')
+    io.add_argument('--max_eval_sample_duration', type=float, default=32.7,
+                    help='The max duration of samples in the eval set')
+    io.add_argument('--train_dataset_dir', required=True, type=str,
+                    help='Root dir of train dataset')
+    io.add_argument('--valid_dataset_dir', required=True, type=str,
+                    help='Root dir of valid dataset')
+    io.add_argument('--output_dir', type=str, required=True,
+                    help='Directory for logs and checkpoints')
+    io.add_argument('--log_file', type=str, default=None,
+                    help='Path to save the training logfile.')
+    io.add_argument('--max_symbol_per_sample', type=int, default=None,
+                    help='maximum number of symbols per sample can have during eval')
+    io.add_argument('--data_cpu_threads', type=int, default=multiprocessing.cpu_count(),
+            help='Number of CPU threads used for data loading and preprocessing.')
+    io.add_argument('--synthetic_audio_seq_len', type=int, default=None,
+                    help='length for synthetic audio sequence.')
+    io.add_argument('--synthetic_text_seq_len', type=int, default=None,
+                    help='length for synthetic text sequence.')
+    io.add_argument('--enable_seq_len_stats', action='store_true',
+                            help='Store and output seq len stats')
+    io.add_argument('--vectorized_sa', action='store_true',
+                    help='Vectorized implementation of SpecAugment')
+    io.add_argument('--in_mem_file_list', action='store_true',
+                    help='prepare file list in memory instead of on the disk')
+    io.add_argument('--enable_prefetch', action='store_true',
+                    help='prefetch and preprocess input data for next iteration')
+    io.add_argument('--tokenized_transcript', action='store_true',
+                    help='loads transcript in tokenized form')
+    io.add_argument('--jit_tensor_formation', action='store_true',
+                    help='just-in-time tensor formation. Form the input txt tensor on the fly.')
+    io.add_argument('--dali_dont_use_mmap', action='store_true',
+                    help='Disable mmap for DALI')
+    io.add_argument('--training_time_threshold', type=int, default=None,
+                    help='Max training time before stopping training')
+
+    io.add_argument('--enc_n_hid', type=int, default=1024)
+    io.add_argument('--enc_pre_rnn_layers', type=int, default=2)
+    io.add_argument('--enc_stack_time_factor', type=int, default=2)
+    io.add_argument('--enc_post_rnn_layers', type=int, default=3)
+    io.add_argument('--enc_dropout', type=float, default=0.1)
+    io.add_argument('--pred_n_hid', type=int, default=512)
+    io.add_argument('--pred_rnn_layers', type=int, default=2)
+    io.add_argument('--pred_dropout', type=float, default=0.3)
+    io.add_argument('--joint_n_hid', type=int, default=512)
+    io.add_argument('--joint_dropout', type=float, default=0.3)
+    return parser.parse_args()
+
+
+@torch.no_grad()
+def apply_ema(model, ema_model, decay):
+    if not decay:
+        return
+
+    sd = getattr(model, 'module', model).state_dict()
+    for k, v in ema_model.state_dict().items():
+        v.copy_(decay * v + (1 - decay) * sd[k])
+
+
+@torch.no_grad()
+def evaluate(epoch, step, val_loader, val_feat_proc, detokenize,
+             ema_model, loss_fn, greedy_decoder, amp_level):
+    logging.log_start(logging.constants.EVAL_START, metadata=dict(epoch_num=epoch))
+    start_time = time.time()
+    agg = {'preds': [], 'txts': [], 'idx': []}
+    greedy_decoder.update_ema_model_eval(ema_model)
+
+    for i, batch in enumerate(val_loader):
+        print(f'{val_loader.pipeline_type} evaluation: {i:>10}/{len(val_loader):<10}', end='\r')
+        audio, audio_lens, txt, txt_lens = batch
+        feats, feat_lens = val_feat_proc([audio, audio_lens])
+        pred = greedy_decoder.decode(feats, feat_lens)
+        agg['preds'] += helpers.gather_predictions([pred], detokenize)
+        agg['txts'] += helpers.gather_transcripts([txt.cpu()], [txt_lens.cpu()], detokenize)
+
+    wer, loss = process_evaluation_epoch(agg)
+
+    logging.log_event(logging.constants.EVAL_ACCURACY, value=wer, metadata=dict(epoch_num=epoch))
+    logging.log_end(logging.constants.EVAL_STOP, metadata=dict(epoch_num=epoch))
+
+    log((epoch,), step, 'dev_ema', {'wer': 100.0 * wer,
+                                 'took': time.time() - start_time})
+    return wer
+
+def apply_model_config(args, cfg):
+    cfg['rnnt'] = {}
+    cfg['rnnt']['in_feats'] = 256
+    cfg['rnnt']['enc_n_hid'] = args.enc_n_hid
+    cfg['rnnt']['enc_pre_rnn_layers'] = args.enc_pre_rnn_layers
+    cfg['rnnt']['enc_post_rnn_layers'] = args.enc_post_rnn_layers
+    cfg['rnnt']['enc_stack_time_factor'] = args.enc_stack_time_factor
+    cfg['rnnt']['enc_dropout'] = args.enc_dropout
+    cfg['rnnt']['pred_n_hid'] = args.pred_n_hid
+    cfg['rnnt']['pred_rnn_layers'] = args.pred_rnn_layers
+    cfg['rnnt']['pred_dropout'] = args.pred_dropout
+    cfg['rnnt']['joint_n_hid'] = args.joint_n_hid
+    cfg['rnnt']['joint_dropout'] = args.joint_dropout
+    cfg['rnnt']['forget_gate_bias'] = 1.0
+    cfg['rnnt']['decoupled_rnns'] = True
+
+def main():
+    args = parse_args()
+    logging.configure_logger(args.output_dir, 'RNNT')
+    logging.log_start(logging.constants.INIT_START)
+
+    # set up distributed training
+    if args.dist or (int(os.environ.get('WORLD_SIZE', 1)) > 1):
+        dist.init_distributed(backend=args.dist_backend)
+        world_size = dist.my_size
+    else:
+        world_size = 1
+
+    if args.seed is not None:
+        logging.log_event(logging.constants.SEED, value=args.seed)
+        torch.manual_seed(args.seed + args.local_rank)
+        np.random.seed(args.seed + args.local_rank)
+        random.seed(args.seed + args.local_rank)
+        # np_rng is used for buckets generation, and needs the same seed on every worker
+        sampler_seed = args.seed
+        if world_size > 1:
+            dali_seed = args.seed + dist.my_rank
+        else:
+            dali_seed = args.seed
+
+    init_log(args)
+    cfg = config.load(args.model_config)
+    apply_model_config(args, cfg)
+    config.apply_duration_flags(cfg, args.max_duration)
+
+    assert args.grad_accumulation_steps >= 1
+    assert args.batch_size % args.grad_accumulation_steps == 0, f'{args.batch_size} % {args.grad_accumulation_steps} != 0'
+    logging.log_event(logging.constants.GRADIENT_ACCUMULATION_STEPS, value=args.grad_accumulation_steps)
+    batch_size = args.batch_size // args.grad_accumulation_steps
+
+    # set up the model
+    tokenizer_kw = config.tokenizer(cfg)
+    tokenizer = Tokenizer(**tokenizer_kw)
+
+    rnnt_config = config.rnnt(cfg)
+    logging.log_event(logging.constants.MODEL_WEIGHTS_INITIALIZATION_SCALE, value=args.weights_init_scale)
+    if args.fuse_relu_dropout:
+        rnnt_config["fuse_relu_dropout"] = True
+    if args.weights_init_scale is not None:
+        rnnt_config['weights_init_scale'] = args.weights_init_scale
+    if args.hidden_hidden_bias_scale is not None:
+        rnnt_config['hidden_hidden_bias_scale'] = args.hidden_hidden_bias_scale
+    if args.multilayer_lstm:
+        rnnt_config["decoupled_rnns"] = False
+    if args.use_ipex:
+        rnnt_config['use_ipex'] = True
+    model = RNNT(n_classes=tokenizer.num_labels + 1, **rnnt_config)
+    blank_idx = tokenizer.num_labels
+    loss_fn = RNNTLoss(blank_idx=blank_idx)
+
+    # set up evaluation
+    logging.log_event(logging.constants.EVAL_MAX_PREDICTION_SYMBOLS, value=args.max_symbol_per_sample)
+    greedy_decoder = RNNTGreedyDecoder( blank_idx=blank_idx,
+                                        batch_eval_mode=args.batch_eval_mode,
+                                        cg_unroll_factor = args.cg_unroll_factor,
+                                        rnnt_config=rnnt_config,
+                                        max_symbol_per_sample=args.max_symbol_per_sample,
+                                        amp_level=args.amp_level)
+
+    print_once(f'Model size: {num_weights(model) / 10**6:.1f}M params\n')
+
+    if args.ema > 0:
+        ema_model = copy.deepcopy(model)
+    else:
+        ema_model = None
+    logging.log_event(logging.constants.MODEL_EVAL_EMA_FACTOR, value=args.ema)
+
+    # set up optimization
+    opt_eps=1e-9
+    kw = {'params': model.parameters(), 'lr': args.lr,
+          'weight_decay': args.weight_decay}
+
+    optimizer = ipex.optim._lamb.Lamb(betas=(args.beta1, args.beta2), eps=opt_eps, **kw)
+
+    # data parallel
+    if world_size > 1:
+        model = DDP(model)
+    print_once(model)
+
+    print_once('Setting up datasets...')
+    (
+        train_dataset_kw,
+        train_features_kw,
+        train_splicing_kw,
+        train_padalign_kw,
+        train_specaugm_kw,
+    ) = config.input(cfg, 'train')
+    (
+        val_dataset_kw,
+        val_features_kw,
+        val_splicing_kw,
+        val_padalign_kw,
+        val_specaugm_kw,
+    ) = config.input(cfg, 'val')
+
+    class PermuteAudio(torch.nn.Module):
+        def forward(self, x):
+            return (x[0].permute(2, 0, 1).contiguous(), *x[1:])
+
+    if not train_specaugm_kw:
+        train_specaugm = torch.nn.Identity()
+    elif not args.vectorized_sa:
+        train_specaugm = features.SpecAugment(optim_level=args.amp_level, **train_specaugm_kw)
+    else:
+        train_specaugm = features.VectorizedSpecAugment(optim_level=args.amp_level, **train_specaugm_kw)
+    train_augmentations = torch.nn.Sequential(
+        train_specaugm,
+        features.FrameSplicing(optim_level=args.amp_level, **train_splicing_kw),
+        features.FillPadding(optim_level=args.amp_level, ),
+        features.PadAlign(optim_level=args.amp_level, **train_padalign_kw),
+        PermuteAudio(),
+    )
+
+    if not val_specaugm_kw:
+        val_specaugm = torch.nn.Identity()
+    elif not args.vectorized_sa:
+        val_specaugm = features.SpecAugment(optim_level=args.amp_level, **val_specaugm_kw)
+    else:
+        val_specaugm = features.VectorizedSpecAugment(optim_level=args.amp_level, **val_specaugm_kw)
+    val_augmentations = torch.nn.Sequential(
+        val_specaugm,
+        features.FrameSplicing(optim_level=args.amp_level, **val_splicing_kw),
+        features.FillPadding(optim_level=args.amp_level, ),
+        features.PadAlign(optim_level=args.amp_level, **val_padalign_kw),
+        PermuteAudio(),
+    )
+
+    train_feat_proc = train_augmentations
+    val_feat_proc   = val_augmentations
+
+    train_preproc = Preproc(train_feat_proc, args.dist_lamb, args.apex_transducer_joint, args.batch_split_factor, cfg)
+
+
+    # graphing
+    if args.num_cg > 0:
+        if not args.dist_lamb:
+            raise NotImplementedError("Currently CUDA graph training only works with dist LAMB")
+        if args.batch_split_factor != 1:
+            raise NotImplementedError("Currently CUDA graph training does not work with batch split")
+
+        max_seq_len = math.ceil(train_preproc.audio_duration_to_seq_len(
+                                                    cfg['input_train']['audio_dataset']['max_duration'],
+                                                    after_subsampling=True,
+                                                    after_stack_time=False
+                                                    )
+                        * cfg["input_train"]["audio_dataset"]["speed_perturbation"]["max_rate"])
+
+        print_once(f'Graph with max_seq_len of %d' % max_seq_len)
+        rnnt_graph = RNNTGraph(model, rnnt_config, batch_size, max_seq_len, args.max_txt_len, args.num_cg)
+        rnnt_graph.capture_graph()
+    else:
+        rnnt_graph = None
+
+    # capture CG for eval
+    if type(args.batch_eval_mode) == str and args.batch_eval_mode.startswith("cg"):
+        max_seq_len = train_preproc.audio_duration_to_seq_len(  args.max_eval_sample_duration,
+                                                                after_subsampling=True,
+                                                                after_stack_time=True)
+        dict_meta_data = {"batch": args.val_batch_size, "max_feat_len": max_seq_len}
+        greedy_decoder.capture_cg_for_eval(ema_model, dict_meta_data)
+
+
+    logging.log_end(logging.constants.INIT_STOP)
+    if world_size > 1:
+        torch.distributed.barrier()
+    logging.log_start(logging.constants.RUN_START)
+    if world_size > 1:
+        torch.distributed.barrier()
+
+    if args.pre_sort_for_seq_split and not args.vectorized_sampler:
+        raise NotImplementedError("Pre sort only works with vectorized sampler for now")
+    logging.log_event(logging.constants.DATA_TRAIN_NUM_BUCKETS, value=args.num_buckets)
+
+    sampler = dali_sampler.SimpleSampler(val_dataset_kw)
+
+    sampler.sample(   file_names=args.val_manifests,
+                            in_mem_file_list=args.in_mem_file_list,
+                            tokenized_transcript=args.tokenized_transcript)
+
+
+    # Setup DALI pipeline
+    if args.synthetic_audio_seq_len is None and args.synthetic_text_seq_len is None:
+        synthetic_seq_len = None
+    elif args.synthetic_audio_seq_len is not None and args.synthetic_text_seq_len is not None:
+        synthetic_seq_len = [args.synthetic_audio_seq_len, args.synthetic_text_seq_len]
+    else:
+        raise Exception("synthetic seq length for both text and audio need to be specified")
+
+    val_loader = DaliDataLoader(gpu_id=None,
+                                dataset_path=args.valid_dataset_dir,
+                                config_data=val_dataset_kw,
+                                config_features=val_features_kw,
+                                json_names=args.val_manifests,
+                                batch_size=args.val_batch_size,
+                                sampler=sampler,
+                                pipeline_type="val",
+                                device_type=args.dali_device,
+                                tokenizer=tokenizer,
+                                num_threads=args.data_cpu_threads,
+                                seed=dali_seed,
+                                tokenized_transcript=args.tokenized_transcript,
+                                in_mem_file_list=args.in_mem_file_list,
+                                jit_tensor_formation=args.jit_tensor_formation,
+                                dont_use_mmap=args.dali_dont_use_mmap)
+
+
+    logging.log_event(logging.constants.EVAL_SAMPLES, value=val_loader.dataset_size)
+
+    # load checkpoint
+    meta = {'best_wer': 10**6, 'start_epoch': 0}
+    checkpointer = Checkpointer(args.output_dir, 'RNN-T',
+                                args.keep_milestones, use_amp=True)
+    # if args.resume:
+    #     args.ckpt = checkpointer.last_checkpoint() or args.ckpt
+
+    if args.ckpt is not None:
+        checkpointer.load(args.ckpt, model, ema_model, optimizer, meta)
+
+    epoch = 1
+    step = 1
+
+    # eval loop
+    model.eval()
+
+    start_time = time.time()
+    wer = evaluate(epoch, step, val_loader, val_feat_proc, tokenizer.detokenize,
+            ema_model, loss_fn, greedy_decoder, args.amp_level)
+    eval_time = time.time() - start_time
+    print_once(f'evaluate throughput: {len(val_loader) * world_size * args.val_batch_size / eval_time}')
+    print_once(f'evaluate wer: {wer}')
+
+    flush_log()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/modelzoo/rnnt/pytorch/launch.py b/modelzoo/rnnt/pytorch/launch.py
new file mode 100644
index 0000000..67b6001
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/launch.py
@@ -0,0 +1,730 @@
+from __future__ import absolute_import, division, print_function, unicode_literals
+import sys
+import platform
+import subprocess
+import os
+from os.path import expanduser
+import re
+import glob
+import numpy as np
+from argparse import ArgumentParser, REMAINDER
+from argparse import RawTextHelpFormatter
+import logging
+import psutil
+from datetime import datetime
+
+format_str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+logging.basicConfig(level=logging.INFO, format=format_str)
+logger = logging.getLogger(__name__)
+
+r"""
+This is a script for launching PyTorch training and inference on Intel Xeon CPU with optimal configurations.
+Now, single instance inference/training, multi-instance inference/training and distributed training
+with oneCCL backend is enabled.
+
+To get the peak performance on Intel Xeon CPU, the script optimizes the configuration of thread and memory
+management. For thread management, the script configures thread affinity and the preload of Intel OMP library.
+For memory management, it configures NUMA binding and preload optimized memory allocation library (e.g. tcmalloc, jemalloc).
+
+**How to use this module:**
+
+*** Single instance inference/training ***
+
+1. Run single-instance inference or training on a single node with all CPU sockets.
+
+::
+
+   >>> python -m intel_extension_for_pytorch.launch --throughput_mode script.py args
+
+2. Run single-instance inference or training on a single CPU socket.
+
+::
+
+   >>> python -m intel_extension_for_pytorch.launch --socket_id 1 script.py args
+
+*** Multi-instance inference ***
+
+1. Multi-instance
+   By default, one instance per socket. if you want to set the instance numbers and core per instance,
+   --ninstances and  --ncore_per_instance should be set.
+
+
+   >>> python -m intel_extension_for_pytorch.launch -- python_script args
+
+   eg: on CLX8280 with 14 instance, 4 cores per instance
+::
+
+   >>> python -m intel_extension_for_pytorch.launch  --ninstances 14 --ncore_per_instance 4 python_script args
+
+
+*** Distributed Training ***
+
+spawns up multiple distributed training processes on each of the training nodes. For intel_extension_for_pytorch, oneCCL
+is used as the communication backend and MPI used to launch multi-proc. To get the better
+performance, you should specify the different cores for oneCCL communication and computation
+process seperately. This tool can automatically set these ENVs(such as I_MPI_PIN_DOMIN) and launch
+multi-proc for you.
+
+The utility can be used for single-node distributed training, in which one or
+more processes per node will be spawned.  It can also be used in
+multi-node distributed training, by spawning up multiple processes on each node
+for well-improved multi-node distributed training performance as well.
+
+
+1. Single-Node multi-process distributed training
+
+::
+
+    >>> python  -m intel_extension_for_pytorch.launch --distributed  python_script  --arg1 --arg2 --arg3 and all other
+                arguments of your training script
+
+2. Multi-Node multi-process distributed training: (e.g. two nodes)
+
+
+rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*
+
+::
+
+    >>> python -m intel_extension_for_pytorch.launch --distributed --nproc_per_node=xxx
+               --nnodes=2 --hostfile hostfile python_sript --arg1 --arg2 --arg3
+               and all other arguments of your training script)
+
+
+3. To look up what optional arguments this module offers:
+
+::
+
+    >>> python -m intel_extension_for_pytorch.launch --help
+
+*** Memory allocator  ***
+
+"--enable_tcmalloc" and "--enable_jemalloc" can be used to enable different memory allcator.
+
+"""
+
+class CPUinfo():
+    '''
+    Get CPU inforamation, such as cores list and NUMA information.
+    '''
+    def __init__(self):
+
+        self.cpuinfo = []
+        if platform.system() == "Windows":
+            raise RuntimeError("Windows platform is not supported!!!")
+        elif platform.system() == "Linux":
+            args = ["lscpu", "--parse=CPU,Core,Socket,Node"]
+            lscpu_info = subprocess.check_output(args, universal_newlines=True).split("\n")
+
+            # Get information about  cpu, core, socket and node
+            for line in lscpu_info:
+                pattern = r"^([\d]+,[\d]+,[\d]+,[\d]?)"
+                regex_out = re.search(pattern, line)
+                if regex_out:
+                    self.cpuinfo.append(regex_out.group(1).strip().split(","))
+            self.get_socket_info()
+
+    def get_socket_info(self):
+
+        self.socket_physical_cores = []  # socket_id is index
+        self.socket_logical_cores = []   # socket_id is index
+        self.physical_core_node_map = {}  # phyical core to numa node id
+        self.logical_core_node_map = {}   # logical core to numa node id
+        self.sockets = int(max([line[2] for line in self.cpuinfo])) + 1
+        for socket_id in range(self.sockets):
+            cur_socket_physical_core = []
+            cur_socket_logical_core = []
+            for line in self.cpuinfo:
+                if socket_id == int(line[2]):
+                    node_id = line[3] if line[3] != '' else '0'
+                    if int(line[1]) not in cur_socket_physical_core:
+                        cur_socket_physical_core.append(int(line[1]))
+                        self.physical_core_node_map[int(line[1])] = int(node_id)
+                    cur_socket_logical_core.append(int(line[0]))
+                    self.logical_core_node_map[int(line[0])] = int(node_id)
+            self.socket_physical_cores.append(cur_socket_physical_core)
+            self.socket_logical_cores.append(cur_socket_logical_core)
+
+    def socket_nums(self):
+        return self.sockets
+
+    def physical_core_nums(self):
+        return len(self.socket_physical_cores) * len(self.socket_physical_cores[0])
+
+    def logical_core_nums(self):
+        return len(self.socket_logical_cores) * len(self.socket_logical_cores[0])
+
+    def get_socket_physical_cores(self, socket_id):
+        if socket_id < 0 or socket_id > self.sockets - 1:
+            logger.error("Invalid socket id")
+        return self.socket_physical_cores[socket_id]
+
+    def get_socket_logical_cores(self, socket_id):
+        if socket_id < 0 or socket_id > self.sockets - 1:
+            logger.error("Invalid socket id")
+        return self.socket_logical_cores[socket_id]
+
+    def get_all_physical_cores(self):
+        return np.array(self.socket_physical_cores).flatten().tolist()
+
+    def get_all_logical_cores(self):
+        return np.array(self.socket_logical_cores).flatten().tolist()
+
+    def numa_aware_check(self, core_list):
+        '''
+        Check whether all cores in core_list are in the same NUMA node. cross NUMA will reduce perforamnce.
+        We strongly advice to not use cores in different node.
+        '''
+        cores_numa_map = self.logical_core_node_map
+        if len(core_list) <= 1:
+            return True
+        numa_id = cores_numa_map[core_list[0]]
+        for core in core_list:
+            if numa_id != cores_numa_map[core]:
+                logger.warning("Numa Aware: cores:{} in different NUMA node".format(str(core_list)))
+                return False
+        return True
+
+class Launcher():
+    r"""
+     Base class for launcher
+    """
+    def __init__(self):
+        self.cpuinfo = CPUinfo()
+
+    def launch(self, args):
+        pass
+
+    def add_lib_preload(self, lib_type=None):
+        '''
+        Enale TCMalloc/JeMalloc/intel OpenMP
+        '''
+        library_paths = []
+        if "CONDA_PREFIX" in os.environ:
+            library_paths.append(os.environ["CONDA_PREFIX"] + "/lib/")
+        if "VIRTUAL_ENV" in os.environ:
+            library_paths.append(os.environ["VIRTUAL_ENV"] + "/lib/")
+
+        library_paths += ["{}/.local/lib/".format(expanduser("~")), "/usr/local/lib/",
+                          "/usr/local/lib64/", "/usr/lib/", "/usr/lib64/"]
+        lib_find = False
+        for lib_path in library_paths:
+            library_file = lib_path + "lib" + lib_type + ".so"
+            matches = glob.glob(library_file)
+            if len(matches) > 0:
+                if "LD_PRELOAD" in os.environ:
+                    os.environ["LD_PRELOAD"] = matches[0] + ":" + os.environ["LD_PRELOAD"]
+                else:
+                    os.environ["LD_PRELOAD"] = matches[0]
+                lib_find = True
+                break
+        return lib_find
+
+
+    def set_memory_allocator(self, enable_tcmalloc=True, enable_jemalloc=False, use_default_allocator=False):
+        '''
+        Enable TCMalloc/JeMalloc with LD_PRELOAD and set configuration for JeMalloc.
+        By default, PTMalloc will be used for PyTorch, but TCMalloc and JeMalloc can get better
+        memory resue and reduce page fault to improve performance.
+        '''
+        if enable_tcmalloc and enable_jemalloc:
+            logger.error("Unable to enable TCMalloc and JEMalloc at the same time")
+            exit(-1)
+
+        if enable_tcmalloc:
+            find_tc = self.add_lib_preload(lib_type="tcmalloc")
+            if not find_tc:
+                logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
+                               " or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+                               "{}/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+                               "you can use 'conda install -c conda-forge gperftools' to install tcmalloc"
+                               .format("TCmalloc", "tcmalloc", expanduser("~")))
+            else:
+                logger.info("Use TCMalloc memory allocator")
+
+        elif enable_jemalloc:
+            find_je = self.add_lib_preload(lib_type="jemalloc")
+            if not find_je:
+                logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
+                               " or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+                               "{}/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+                               "you can use 'conda install -c conda-forge jemalloc' to install jemalloc"
+                               .format("JeMalloc", "jemalloc", expanduser("~")))
+            else:
+                logger.info("Use JeMallocl memory allocator")
+                self.set_env('MALLOC_CONF', "oversize_threshold:1,background_thread:true,metadata_thp:auto")
+
+        elif use_default_allocator:
+            pass
+
+        else:
+            find_tc = self.add_lib_preload(lib_type="tcmalloc")
+            if find_tc:
+                logger.info("Use TCMalloc memory allocator")
+                return
+            find_je = self.add_lib_preload(lib_type="jemalloc")
+            if find_je:
+                logger.info("Use JeMallocl memory allocator")
+                return
+            logger.warning("Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
+                           " or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+                           "{}/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance"
+                           .format(expanduser("~")))
+
+    def logger_env(self, env_name=""):
+        if env_name in os.environ:
+            logger.info("{}={}".format(env_name, os.environ[env_name]))
+
+    def set_env(self, env_name, env_value=None):
+        if not env_value:
+            logger.warning("{} is None".format(env_name))
+        if env_name not in os.environ:
+            os.environ[env_name] = env_value
+        elif os.environ[env_name] != env_value:
+            logger.warning("{} in environment variable is {} while the value you set is {}".format(env_name, os.environ[env_name], env_value))
+        self.logger_env(env_name)
+
+    # set_kmp_affinity is used to control whether to set KMP_AFFINITY or not. In scenario that use all cores on all sockets, including logical cores, setting KMP_AFFINITY disables logical cores. In this case, KMP_AFFINITY should not be set.
+    def set_multi_thread_and_allocator(self, ncore_per_instance, disable_iomp=False, set_kmp_affinity=True, enable_tcmalloc=True, enable_jemalloc=False, use_default_allocator=False):
+        '''
+        Set multi-thread configuration and enable Intel openMP and TCMalloc/JeMalloc.
+        By default, GNU openMP and PTMalloc are used in PyTorch. but Intel openMP and TCMalloc/JeMalloc are better alternatives
+        to get performance benifit.
+        '''
+        self.set_memory_allocator(enable_tcmalloc, enable_jemalloc, use_default_allocator)
+        self.set_env("OMP_NUM_THREADS", str(ncore_per_instance))
+        if not disable_iomp:
+            find_iomp = self.add_lib_preload(lib_type="iomp5")
+            if not find_iomp:
+                logger.warning("Unable to find the {} library file lib{}.so in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
+                               " or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
+                               "{}/.local/lib/ so the LD_PRELOAD environment variable will not be set."
+                               "you can use 'conda install intel-openm' to install intel openMP"
+                               .format("iomp", "iomp5", expanduser("~")))
+            else:
+                logger.info("Using Intel OpenMP")
+                if set_kmp_affinity:
+                    self.set_env("KMP_AFFINITY", "granularity=fine,compact,1,0")
+                self.set_env("KMP_BLOCKTIME", "1")
+        self.logger_env("LD_PRELOAD")
+
+class MultiInstanceLauncher(Launcher):
+    r"""
+     Launcher for single instance and multi-instance
+     """
+    def launch(self, args):
+        processes = []
+        cores = []
+        set_kmp_affinity = True
+        if args.core_list:  # user specify what cores will be used by params
+            cores = args.core_list.strip().split(",")
+            if args.ncore_per_instance == -1:
+                logger.error("please specify the '--ncore_per_instance' if you have pass the --core_list params")
+                exit(-1)
+            elif args.ninstances > 1 and args.ncore_per_instance * args.ninstances < len(cores):
+                logger.warning("only first {} cores will be used, but you specify {} cores in core_list".format(args.ncore_per_instance * args.ninstances, len(cores)))
+            else:
+                args.ninstances = len(cores) // args.ncore_per_instance
+        else:
+            if args.use_logical_core:
+                if args.socket_id != -1:
+                    cores = self.cpuinfo.get_socket_logical_cores(args.socket_id)
+                else:
+                    cores = self.cpuinfo.get_all_logical_cores()
+                    # When using all cores on all sockets, including logical cores, setting KMP_AFFINITY disables logical cores. Thus, KMP_AFFINITY should not be set.
+                    set_kmp_affinity = False
+            else:
+                if args.socket_id != -1:
+                    cores = self.cpuinfo.get_socket_physical_cores(args.socket_id)
+                else:
+                    cores = self.cpuinfo.get_all_physical_cores()
+            if not args.multi_instance and args.ninstances == -1 and args.ncore_per_instance == -1:
+                args.ninstances = 1
+                args.ncore_per_instance = len(cores)
+            elif args.multi_instance and args.ninstances == -1 and args.ncore_per_instance == -1:
+                args.throughput_mode = True
+            elif args.ncore_per_instance == -1 and args.ninstances != -1:
+                args.ncore_per_instance = len(cores) // args.ninstances
+            elif args.ncore_per_instance != -1 and args.ninstances == -1:
+                args.ninstances = len(cores) // args.ncore_per_instance
+            else:
+                if args.ninstances * args.ncore_per_instance > len(cores):
+                    logger.error("Please make sure ninstances * ncore_per_instance <= total_cores")
+                    exit(-1)
+            if args.latency_mode:
+                print('--latency_mode is exclusive to --ninstances, --ncore_per_instance, --socket_id and --use_logical_core. They won\'t take effect even they are set explicitly.')
+                args.ncore_per_instance = 4
+                cores = self.cpuinfo.get_all_physical_cores()
+                args.ninstances = len(cores) // args.ncore_per_instance
+
+            if args.throughput_mode:
+                print('--throughput_mode is exclusive to --ninstances, --ncore_per_instance, --socket_id and --use_logical_core. They won\'t take effect even they are set explicitly.')
+                args.ninstances = self.cpuinfo.socket_nums()
+                cores = self.cpuinfo.get_all_physical_cores()
+                args.ncore_per_instance = len(cores) // args.ninstances
+
+        self.set_multi_thread_and_allocator(args.ncore_per_instance,
+                                            args.disable_iomp,
+                                            set_kmp_affinity,
+                                            args.enable_tcmalloc,
+                                            args.enable_jemalloc,
+                                            args.use_default_allocator)
+        os.environ["LAUNCH_CMD"] = "#"
+        for i in range(args.ninstances):
+            cmd = []
+            cur_process_cores = ""
+            if not args.disable_numactl:
+                cmd = ["numactl"]
+                core_list = cores[i * args.ncore_per_instance:(i + 1) * args.ncore_per_instance]
+                core_list = sorted(core_list)
+                same_numa = self.cpuinfo.numa_aware_check(core_list)
+                core_ranges = []
+                for core in core_list:
+                    if len(core_ranges) == 0:
+                        range_elem = {'start': core, 'end': core}
+                        core_ranges.append(range_elem)
+                    else:
+                        if core - core_ranges[-1]['end'] == 1:
+                            core_ranges[-1]['end'] = core
+                        else:
+                            range_elem = {'start': core, 'end': core}
+                            core_ranges.append(range_elem)
+                for r in core_ranges:
+                    cur_process_cores = cur_process_cores + "{}-{},".format(r['start'], r['end'])
+                cur_process_cores = cur_process_cores[:-1]
+                numa_params = "-C {} ".format(cur_process_cores)
+                if same_numa:
+                    numa_params += "-m {}".format(self.cpuinfo.logical_core_node_map[core_list[0]])
+                cmd.extend(numa_params.split())
+            with_python = not args.no_python
+            if with_python:
+                cmd.append(sys.executable)
+                cmd.append("-u")
+            if args.module:
+                cmd.append("-m")
+            cmd.append(args.program)
+            log_name = args.log_file_prefix + "_instance_{}_cores_".format(i) + cur_process_cores.replace(',', '_') + ".log"
+            log_name = os.path.join(args.log_path, log_name)
+            cmd.extend(args.program_args)
+            os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
+            cmd_s = " ".join(cmd)
+            if args.log_path:
+                cmd_s = "{} 2>&1 | tee {}".format(cmd_s, log_name)
+            logger.info(cmd_s)
+            process = subprocess.Popen(cmd_s, env=os.environ, shell=True)
+            processes.append(process)
+        os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
+        for process in processes:
+            process.wait()
+            if process.returncode != 0:
+                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd_s)
+
+class DistributedTrainingLauncher(Launcher):
+    r"""
+     Launcher for distributed traning with MPI launcher
+     """
+    def get_mpi_pin_domain(self, nproc_per_node, ccl_worker_count, total_cores):
+        '''
+        I_MPI_PIN_DOMAIN specify the cores used for every MPI process.
+        The first ccl_worker_count cores of every rank for ccl communication
+        and the other cores will be used to do computation.
+        For example: on CascadeLake 8280 CPU, 2 ranks on one node. ccl_worker_count=4
+        CCL_WORKER_COUNT=4
+        CCL_WORKER_AFFINITY="0,1,2,3,28,29,30,31"
+        I_MPI_PIN_DOMAIN=[0xffffff0,0xffffff0000000]
+        '''
+        ppn = nproc_per_node
+        cores_per_rank = total_cores // ppn
+        pin_domain = "["
+        for proc in range(ppn):
+            domain_binary = 0
+            begin = proc * cores_per_rank + ccl_worker_count
+            end = proc * cores_per_rank + cores_per_rank - 1
+            for i in range(begin, end + 1):
+                domain_binary |= (1 << i)
+            pin_domain += hex(domain_binary) + ","
+        pin_domain += "]"
+        return pin_domain
+
+    def get_ccl_worker_affinity(self, nproc_per_node, ccl_worker_count, total_cores):
+        '''
+        Computation and communication use different cores when using oneCCL
+        backend for distributed training. we use first ccl_worker_count cores of
+        every rank for ccl communication
+        '''
+        ppn = nproc_per_node
+        cores_per_rank = total_cores // ppn
+        affinity = ''
+        for proc in range(ppn):
+            for ccl_worker in range(ccl_worker_count):
+                affinity += str(proc * cores_per_rank + ccl_worker) + ","
+        affinity = affinity[:-1]
+        return affinity
+
+    def launch(self, args):
+        '''
+        Set ENVs and launch MPI process for distributed training.
+        '''
+        if args.nnodes > 1 and not os.path.exists(args.hostfile):
+            raise ValueError("hostfile is necessary when you use multi-node distributed training,"
+                             "Please create hostfile which include the ip list you used for distributed running")
+        elif args.nnodes > 1:
+            ipv4_addr_pattern = r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
+            ip_list = []
+            with open(args.hostfile) as f:
+                for line in f:
+                    line = line.strip().strip("\n")
+                    # is_valid = re.match(ipv4_addr_pattern, line)
+                    # if not is_valid:
+                    #     logger.error("{} is not valid IPV4 address".format(line))
+                    #     exit(-1)
+                    # else:
+                    #     ip_list.append(line)
+                    ip_list.append(line)
+            if len(ip_list) < args.nnodes:
+                logger.error("The number of IP {} should greater than nnodes parameters {}".format(len(ip_list), args.nnodes))
+                exit(-1)
+            master_check = False
+            dic = psutil.net_if_addrs()
+            for adapter in dic:
+                snicList = dic[adapter]
+                for snic in snicList:
+                    if snic.address == ip_list[0]:
+                        master_check = True
+            if not master_check:
+                logger.error("MASTER_ADDR is incorrect. Please make sure the first line {} in your hostfile is ip address of the current node".format(ip_list[0]))
+                exit(-1)
+
+            logger.info("Begin to validate the ip connect")
+            args.master_addr = ip_list[0]
+            for ip in ip_list[1:]:
+                completed_process = subprocess.run("ssh -o PasswordAuthentication=no {} ':'".format(ip), shell=True)
+                if completed_process.returncode != 0:
+                    logger.error("Passwordless SSH login to {} failed, please make sure you have setup SSH public key right")
+                    exit(-1)
+                else:
+                    logger.info("connection from master node {} to slave node {} is OK".format(args.master_addr, ip))
+
+        total_cores_per_node = self.cpuinfo.physical_core_nums()
+        if args.use_logical_core:
+            total_cores_per_node = self.cpuinfo.logical_core_nums()
+
+        # set distributed related environmental variables
+        self.set_env("MASTER_ADDR", args.master_addr)
+        self.set_env("MASTER_PORT", str(args.master_port))
+        mpi_pin_domain = self.get_mpi_pin_domain(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node)
+        if args.set_affinity:
+            self.set_env("I_MPI_PIN_DOMAIN", mpi_pin_domain)
+
+        ppn = args.nproc_per_node
+        cores_per_rank = total_cores_per_node // ppn
+
+        opm_num_threads = cores_per_rank - args.ccl_worker_count
+        self.set_multi_thread_and_allocator(opm_num_threads,
+                                            args.disable_iomp,
+                                            args.set_affinity,
+                                            args.enable_tcmalloc,
+                                            args.enable_jemalloc,
+                                            args.use_default_allocator)
+
+        self.set_env("CCL_WORKER_COUNT", str(args.ccl_worker_count))
+        ccl_affinity = self.get_ccl_worker_affinity(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node)
+        if args.set_affinity:
+            self.set_env("CCL_WORKER_AFFINITY", ccl_affinity)
+
+        os.environ["LAUNCH_CMD"] = "#"
+        cmd = ['mpiexec.hydra']
+        mpi_config = "-l -np {} -ppn {} -genv OMP_NUM_THREADS={} ".format(args.nnodes * args.nproc_per_node, args.nproc_per_node, opm_num_threads)
+        mpi_config += args.more_mpi_params
+        if args.nnodes > 1:
+            mpi_config += " -hostfile {}".format(args.hostfile)
+        cmd.extend(mpi_config.split())
+        with_python = not args.no_python
+        if with_python:
+            cmd.append(sys.executable)
+            cmd.append("-u")
+        if args.module:
+            cmd.append("-m")
+        cmd.append(args.program)
+        cmd.extend(args.program_args)
+        logger.info(cmd)
+        process = subprocess.Popen(cmd, env=os.environ)
+        process.wait()
+        os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
+        os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
+
+def add_distributed_training_params(parser):
+
+    cpuinfo = CPUinfo()
+    socket_nums = cpuinfo.socket_nums()
+
+    group = parser.add_argument_group("Distributed Training Parameters With oneCCL backend")
+    group.add_argument("--nnodes", metavar='\b', type=int, default=1,
+                       help="The number of nodes to use for distributed "
+                       "training")
+    group.add_argument("--nproc_per_node", metavar='\b', type=int, default=socket_nums,
+                       help="The number of processes to launch on each node")
+    # ccl control
+    group.add_argument("--ccl_worker_count", metavar='\b', default=4, type=int,
+                       help="Core numbers per rank used for ccl communication")
+    # mpi control
+    group.add_argument("--master_addr", metavar='\b', default="127.0.0.1", type=str,
+                       help="Master node (rank 0)'s address, should be either "
+                            "the IP address or the hostname of node 0, for "
+                            "single node multi-proc training, the "
+                            "--master_addr can simply be 127.0.0.1")
+    group.add_argument("--master_port", metavar='\b', default=29500, type=int,
+                       help="Master node (rank 0)'s free port that needs to "
+                            "be used for communication during distributed "
+                            "training")
+    group.add_argument("--hostfile", metavar='\b', default="hostfile", type=str,
+                       help="Hostfile is necessary for multi-node multi-proc "
+                            "training. hostfile includes the node address list "
+                            "node address which should be either the IP address"
+                            "or the hostname.")
+    group.add_argument("--more_mpi_params", metavar='\b', default="", type=str,
+                       help="User can pass more parameters for mpiexec.hydra "
+                            "except for -np -ppn -hostfile and -genv I_MPI_PIN_DOMAIN")
+
+def add_memory_allocator_params(parser):
+
+    group = parser.add_argument_group("Memory Allocator Parameters")
+    # allocator control
+    group.add_argument("--enable_tcmalloc", action='store_true', default=False,
+                       help="Enable tcmalloc allocator")
+    group.add_argument("--enable_jemalloc", action='store_true', default=False,
+                       help="Enable jemalloc allocator")
+    group.add_argument("--use_default_allocator", action='store_true', default=False,
+                       help="Use default memory allocator")
+
+def add_multi_instance_params(parser):
+
+    group = parser.add_argument_group("Multi-instance Parameters")
+    # multi-instance control
+    group.add_argument("--ncore_per_instance", metavar='\b', default=-1, type=int,
+                       help="Cores per instance")
+    group.add_argument("--ninstances", metavar='\b', default=-1, type=int,
+                       help="For multi-instance, you should give the cores number you used for per instance.")
+    group.add_argument("--latency_mode", action='store_true', default=False,
+                       help="By detault 4 core per instance and use all physical cores")
+    group.add_argument("--throughput_mode", action='store_true', default=False,
+                       help="By default one instance per socket and use all physical cores")
+    group.add_argument("--socket_id", metavar='\b', default=-1, type=int,
+                       help="Socket id for multi-instance, by default all sockets will be used")
+    group.add_argument("--use_logical_core", action='store_true', default=False,
+                       help="Whether only use physical cores")
+    group.add_argument("--disable_numactl", action='store_true', default=False,
+                       help="Disable numactl")
+    group.add_argument("--core_list", metavar='\b', default=None, type=str,
+                       help="Specify the core list as 'core_id, core_id, ....', otherwise, all the cores will be used.")
+    group.add_argument("--log_path", metavar='\b', default="", type=str,
+                       help="The log file directory. Default path is '', which means disable logging to files.")
+    group.add_argument("--log_file_prefix", metavar='\b', default="run", type=str,
+                       help="log file prefix")
+    group.add_argument("--set_affinity", action='store_true', default=True,
+                       help="Set thread affinity for MPI and CCL")
+
+def add_kmp_iomp_params(parser):
+
+    group = parser.add_argument_group("IOMP Parameters")
+    group.add_argument("--disable_iomp", action='store_true', default=False,
+                       help="By default, we use Intel OpenMP and libiomp5.so will be add to LD_PRELOAD")
+
+
+def parse_args():
+    """
+    Helper function parsing the command line options
+    @retval ArgumentParser
+    """
+    parser = ArgumentParser(description="This is a script for launching PyTorch training and inference on Intel Xeon CPU "
+                                        "with optimal configurations. Now, single instance inference/training, multi-instance "
+                                        "inference/training and distributed training with oneCCL backend is enabled. "
+                                        "To get the peak performance on Intel Xeon CPU, the script optimizes the configuration "
+                                        "of thread and memory management. For thread management, the script configures thread "
+                                        "affinity and the preload of Intel OMP library. For memory management, it configures "
+                                        "NUMA binding and preload optimized memory allocation library (e.g. tcmalloc, jemalloc) "
+                                        "\n################################# Basic usage ############################# \n"
+                                        "\n 1. single instance\n"
+                                        "\n   >>> python -m intel_extension_for_pytorch.launch python_script args \n"
+                                        "\n2. multi-instance \n"
+                                        "\n    >>> python -m intel_extension_for_pytorch.launch --ninstances xxx --ncore_per_instance xx python_script args\n"
+                                        "\n3. Single-Node multi-process distributed training\n"
+                                        "\n    >>> python  -m intel_extension_for_pytorch.launch --distributed  python_script args\n"
+                                        "\n4. Multi-Node multi-process distributed training: (e.g. two nodes)\n"
+                                        "\n   rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*\n"
+                                        "\n   >>> python -m intel_extension_for_pytorch.launch --distributed --nproc_per_node=2\n"
+                                        "\n       --nnodes=2 --hostfile hostfile python_script args\n"
+                                        "\n############################################################################# \n",
+                                        formatter_class=RawTextHelpFormatter)
+
+    parser.add_argument("--multi_instance", action='store_true', default=False,
+                        help="Enable multi-instance, by default one instance per socket")
+
+    parser.add_argument('--distributed', action='store_true', default=False,
+                        help='Enable distributed training.')
+    parser.add_argument("-m", "--module", default=False, action="store_true",
+                        help="Changes each process to interpret the launch script "
+                             "as a python module, executing with the same behavior as"
+                             "'python -m'.")
+
+    parser.add_argument("--no_python", default=False, action="store_true",
+                        help="Do not prepend the --program script with \"python\" - just exec "
+                             "it directly. Useful when the script is not a Python script.")
+
+    add_memory_allocator_params(parser)
+    add_kmp_iomp_params(parser)
+
+    add_distributed_training_params(parser)
+    add_multi_instance_params(parser)
+    # positional
+    parser.add_argument("program", type=str,
+                        help="The full path to the proram/script to be launched. "
+                             "followed by all the arguments for the script")
+
+    # rest from the training program
+    parser.add_argument('program_args', nargs=REMAINDER)
+    return parser.parse_args()
+
+def main():
+
+    env_before = set(os.environ.keys())
+    if platform.system() == "Windows":
+        raise RuntimeError("Windows platform is not supported!!!")
+
+    args = parse_args()
+    if args.log_path:
+        path = os.path.dirname(args.log_path if args.log_path.endswith('/') else args.log_path + '/')
+        if not os.path.exists(path):
+            os.makedirs(path)
+        args.log_path = path
+
+        args.log_file_prefix = '{}_{}'.format(args.log_file_prefix, datetime.now().strftime("%Y%m%d%H%M%S"))
+        fileHandler = logging.FileHandler("{0}/{1}_instances.log".format(args.log_path, args.log_file_prefix))
+        logFormatter = logging.Formatter(format_str)
+        fileHandler.setFormatter(logFormatter)
+        logger.addHandler(fileHandler)
+
+    if args.distributed and args.multi_instance:
+        raise RuntimeError("Either args.distributed or args.multi_instance should be set")
+
+    if args.latency_mode and args.throughput_mode:
+        raise RuntimeError("Either args.latency_mode or args.throughput_mode should be set")
+
+    if args.nnodes > 1:
+        args.distributed = True
+
+    if not args.no_python and not args.program.endswith(".py"):
+        logger.error("For non Python script, you should use '--no_python' parameter.")
+        exit()
+
+    launcher = None
+    if args.distributed:
+        launcher = DistributedTrainingLauncher()
+    else:
+        launcher = MultiInstanceLauncher()
+
+    launcher.launch(args)
+    for x in sorted(set(os.environ.keys()) - env_before):
+        logger.debug('{0}={1}'.format(x, os.environ[x]))
+
+if __name__ == "__main__":
+    main()
diff --git a/modelzoo/rnnt/pytorch/meta.yaml b/modelzoo/rnnt/pytorch/meta.yaml
new file mode 100644
index 0000000..9c89fdd
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/meta.yaml
@@ -0,0 +1,4 @@
+train_manifests:
+  - /home/vmagent/app/dataset/LibriSpeech/metadata/librispeech-train-clean-100-wav-tokenized.pkl
+val_manifests:
+  - /home/vmagent/app/dataset/LibriSpeech/metadata/librispeech-dev-clean-wav-tokenized.pkl
diff --git a/modelzoo/rnnt/pytorch/mlperf/logging.py b/modelzoo/rnnt/pytorch/mlperf/logging.py
index 55fc7f5..ab68a33 100644
--- a/modelzoo/rnnt/pytorch/mlperf/logging.py
+++ b/modelzoo/rnnt/pytorch/mlperf/logging.py
@@ -23,8 +23,10 @@ from mlperf_logging.mllog import constants
 mllogger = mllog.get_mllogger()
 
 
-def configure_logger(output_dir, benchmark):
-    mllog.config(filename=os.path.join(output_dir, f'{benchmark}.log'))
+def configure_logger(output_dir, benchmark, local_rank):
+    if local_rank == 0 and not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+        mllog.config(filename=os.path.join(output_dir, f'{benchmark}.log'))
     mllogger = mllog.get_mllogger()
     mllogger.logger.propagate = False
 
diff --git a/modelzoo/rnnt/pytorch/rnnt/config.py b/modelzoo/rnnt/pytorch/rnnt/config.py
index d23b7ee..741be58 100644
--- a/modelzoo/rnnt/pytorch/rnnt/config.py
+++ b/modelzoo/rnnt/pytorch/rnnt/config.py
@@ -46,10 +46,8 @@ def load(fpath, max_duration=None):
 
     return cfg
 
-
 def validate_and_fill(klass, user_conf, ignore=[], optional=[]):
     conf = default_args(klass)
-
     for k,v in user_conf.items():
         assert k in conf or k in ignore, f'Unknown parameter {k} for {klass}'
         conf[k] = v
diff --git a/modelzoo/rnnt/pytorch/rnnt/decoder.py b/modelzoo/rnnt/pytorch/rnnt/decoder.py
index bf1887f..a0607d8 100644
--- a/modelzoo/rnnt/pytorch/rnnt/decoder.py
+++ b/modelzoo/rnnt/pytorch/rnnt/decoder.py
@@ -20,38 +20,32 @@ import torch.nn.functional as F
 from .model import label_collate
 import math
 import copy
-import amp_C
 
 def graph_simple(func_or_module,
                  sample_args,
-                 graph_stream=None,
                  warmup_iters=2,
                  warmup_only=False):
     assert isinstance(sample_args, tuple)
-    stream = torch.cuda.Stream() if graph_stream is None else graph_stream
-    ambient_stream = torch.cuda.current_stream()
-    stream.wait_stream(ambient_stream)
-    with torch.cuda.stream(stream):
-        # warmup iters before capture
-        for _ in range(warmup_iters):
-            outputs  = func_or_module(*sample_args)
-
-        if warmup_iters > 0:
-            del outputs
-        # print("Graphing\n", flush=True)
-        # Capture forward pass
-        fwd_graph = torch.cuda._Graph()
-        fwd_graph.capture_begin()
+    # warmup iters before capture
+    for _ in range(warmup_iters):
         outputs  = func_or_module(*sample_args)
-        fwd_graph.capture_end()
 
-    ambient_stream.wait_stream(stream)
+    if warmup_iters > 0:
+        del outputs
+    # print("Graphing\n", flush=True)
+    # Capture forward pass
+    # fwd_graph = torch.cuda.CUDAGraph()
+    # fwd_graph.capture_begin()
+    # outputs  = func_or_module(*sample_args)
+    # fwd_graph.capture_end()
+
     def functionalized(*inputs):
         with torch.no_grad():
             for i, arg in zip(sample_args, inputs):
                 if i.data_ptr() != arg.data_ptr():
                     i.copy_(arg)
-        fwd_graph.replay()      
+        # fwd_graph.replay()
+        outputs  = func_or_module(*sample_args)
         return outputs
     return functionalized
 
@@ -130,21 +124,21 @@ class RNNTGreedyDecoder:
         with torch.no_grad():
             ema_model_eval = self._handle_ema_model(ema_model)
             self.model = ema_model_eval
-            feats = torch.ones(dict_meta_data["batch"], dict_meta_data["max_feat_len"], self.rnnt_config["joint_n_hid"], dtype=torch.float16, device='cuda')
-            feat_lens = torch.ones(dict_meta_data["batch"], dtype=torch.int32, device='cuda') * dict_meta_data["max_feat_len"]
+            feats = torch.ones(dict_meta_data["batch"], dict_meta_data["max_feat_len"], self.rnnt_config["joint_n_hid"], dtype=torch.float, device='cpu')
+            feat_lens = torch.ones(dict_meta_data["batch"], dtype=torch.int32, device='cpu') * dict_meta_data["max_feat_len"]
             self._capture_cg(feats, feat_lens)
 
     def update_ema_model_eval(self, ema_model):
         ema_model_eval = self._handle_ema_model(ema_model)
-        if type(self.batch_eval_mode) == str and self.batch_eval_mode.startswith("cg"): 
+        if type(self.batch_eval_mode) == str and self.batch_eval_mode.startswith("cg"):
             if self.cg_captured == False:
                 raise Exception("CUDA graph for eval should be captured first before updating")
             else:
-                overflow_buf = torch.cuda.IntTensor([0])
-                amp_C.multi_tensor_scale(65536, overflow_buf, [list(ema_model_eval.parameters()), list(self.model.parameters())], 1.0)
+                # overflow_buf = torch.cuda.IntTensor([0])
+                # amp_C.multi_tensor_scale(65536, overflow_buf, [list(ema_model_eval.parameters()), list(self.model.parameters())], 1.0)
 
-                # for p, p_eval in zip(self.model.parameters(), ema_model_eval.parameters()):
-                #     p.copy_(p_eval)
+                for p, p_eval in zip(self.model.parameters(), ema_model_eval.parameters()):
+                    p.copy_(p_eval)
         else:
             self.model = ema_model_eval
 
@@ -238,7 +232,7 @@ class RNNTGreedyDecoder:
         # advance time_idx as needed
         num_symbol_added += non_blank_mask
         num_total_symbol += non_blank_mask
-        
+
         time_out_mask = num_total_symbol >= self.max_symbol_per_sample
 
         exceed_mask = num_symbol_added >= self.max_symbols
@@ -246,7 +240,7 @@ class RNNTGreedyDecoder:
         time_idx += advance_mask
         label_idx += non_blank_mask & ~time_out_mask
         num_symbol_added.masked_fill_(advance_mask, 0)
-        
+
         complete_mask = (time_idx >= out_len) | time_out_mask
         batch_complete = complete_mask.all()
 
@@ -269,33 +263,23 @@ class RNNTGreedyDecoder:
         non_blank_mask = (k != self.blank_idx)
 
         # update current label according to non_blank_mask
-        self.label_upd_stream.wait_stream(torch.cuda.current_stream())
-        with torch.cuda.stream(self.label_upd_stream):
-            current_label = current_label * ~non_blank_mask + k * non_blank_mask
-            label_tensor[arange_tensor, label_idx] = label_tensor[arange_tensor, label_idx] * complete_mask + current_label * ~complete_mask
+        current_label = current_label * ~non_blank_mask + k * non_blank_mask
+        label_tensor[arange_tensor, label_idx] = label_tensor[arange_tensor, label_idx] * complete_mask + current_label * ~complete_mask
+
+        for i in range(2):
+            expand_mask = non_blank_mask.unsqueeze(0).unsqueeze(2).expand(hidden[0].size())
+            hidden[i] = hidden[i] * ~expand_mask + hidden_prime[i] * expand_mask
 
-        self.hidden_upd_stream.wait_stream(torch.cuda.current_stream())
-        with torch.cuda.stream(self.hidden_upd_stream):
-            for i in range(2):
-                expand_mask = non_blank_mask.unsqueeze(0).unsqueeze(2).expand(hidden[0].size())
-                hidden[i] = hidden[i] * ~expand_mask + hidden_prime[i] * expand_mask
-        
         # advance time_idx as needed
-        self.time_idx_upd_stream.wait_stream(torch.cuda.current_stream())
-        with torch.cuda.stream(self.time_idx_upd_stream):
-            num_symbol_added += non_blank_mask
-            exceed_mask = num_symbol_added >= self.max_symbols
-            advance_mask = (~non_blank_mask | exceed_mask)  & ~complete_mask
-            time_idx += advance_mask
-            num_symbol_added.masked_fill_(advance_mask, 0)
+        num_symbol_added += non_blank_mask
+        exceed_mask = num_symbol_added >= self.max_symbols
+        advance_mask = (~non_blank_mask | exceed_mask)  & ~complete_mask
+        time_idx += advance_mask
+        num_symbol_added.masked_fill_(advance_mask, 0)
 
         # handle time out
         num_total_symbol += non_blank_mask
         time_out_mask = num_total_symbol >= self.max_symbol_per_sample
-        
-        torch.cuda.current_stream().wait_stream(self.label_upd_stream)
-        torch.cuda.current_stream().wait_stream(self.hidden_upd_stream)
-        torch.cuda.current_stream().wait_stream(self.time_idx_upd_stream)
 
         label_idx += non_blank_mask & ~time_out_mask
         complete_mask = (time_idx >= out_len) | time_out_mask
@@ -316,13 +300,9 @@ class RNNTGreedyDecoder:
             func_to_be_captured = self._eval_main_loop_unroll
         else:
             func_to_be_captured = self._eval_main_loop_stream
-        self.label_upd_stream = torch.cuda.Stream()
-        self.hidden_upd_stream = torch.cuda.Stream()
-        self.time_idx_upd_stream = torch.cuda.Stream()
 
         cg = graph_simple(  func_to_be_captured,
                             tuple(t.clone() for t in list_input_tensor),
-                            torch.cuda.Stream(),
                             warmup_iters=2)
         return cg
 
@@ -376,7 +356,7 @@ class RNNTGreedyDecoder:
         assert self.max_symbol_per_sample is not None, "max_symbol_per_sample needs to be specified in order to use batch_eval"
         label_tensor = torch.zeros(B, self.max_symbol_per_sample, dtype=torch.int, device=device)
         current_label = torch.ones(B, dtype=torch.int, device=device) * -1
-        
+
         time_idx = torch.zeros(B, dtype=torch.int64, device=device)
         label_idx = torch.zeros(B, dtype=torch.int64, device=device)
         complete_mask = time_idx >= out_len
@@ -419,10 +399,10 @@ class RNNTGreedyDecoder:
                 hidden = [None, None]
                 hidden[0] = torch.zeros_like(hidden_prime[0])
                 hidden[1] = torch.zeros_like(hidden_prime[1])
-            
+
             ''' We might need to do the following dynamic resizing '''
             # if (symbol_i >= label_tensor.size(1)):
-            #     # resize label tensor 
+            #     # resize label tensor
             #     label_tensor = torch.cat((label_tensor, torch.zeros_like(label_tensor)), dim=1)
 
             label_tensor[arange_tensor, label_idx] = label_tensor[arange_tensor, label_idx] * complete_mask + current_label * ~complete_mask
@@ -431,7 +411,7 @@ class RNNTGreedyDecoder:
             # for i in range(B):
             #     if non_blank_mask[i] and time_idx[i] < out_len[i]:
             #         # pdb.set_trace()
-            #         label_ref[i].append(current_label[i].item()) 
+            #         label_ref[i].append(current_label[i].item())
 
             # update hidden if the inference result is non-blank
             for i in range(2):
@@ -445,18 +425,18 @@ class RNNTGreedyDecoder:
             else:
                 time_out_mask = num_total_symbol >= self.max_symbol_per_sample
 
-            
+
             exceed_mask = num_symbol_added >= self.max_symbols
             advance_mask = (~non_blank_mask | exceed_mask)  & ~complete_mask
             time_idx += advance_mask
             label_idx += non_blank_mask & ~time_out_mask
             num_symbol_added.masked_fill_(advance_mask, 0)
-            
+
             complete_mask = (time_idx >= out_len) | time_out_mask
-            
+
 
         label = []
-        
+
         for i in range(B):
             label.append(label_tensor[i, :label_idx[i]].tolist())
         return label
@@ -467,12 +447,12 @@ class RNNTGreedyDecoder:
         B = x.size()[0]
         self.cg_batch_size = B
 
-        hidden = [torch.zeros((self.rnnt_config["pred_rnn_layers"], B, self.rnnt_config["pred_n_hid"]), 
+        hidden = [torch.zeros((self.rnnt_config["pred_rnn_layers"], B, self.rnnt_config["pred_n_hid"]),
                                 dtype=x.dtype, device=device)]*2
         assert self.max_symbol_per_sample is not None, "max_symbol_per_sample needs to be specified in order to use batch_eval"
         label_tensor = torch.zeros(B, self.max_symbol_per_sample, dtype=torch.int, device=device)
         current_label = torch.ones(B, dtype=torch.int, device=device) * -1
-        
+
         time_idx = torch.zeros(B, dtype=torch.int64, device=device)
         label_idx = torch.zeros(B, dtype=torch.int64, device=device)
         complete_mask = time_idx >= out_len
@@ -480,11 +460,11 @@ class RNNTGreedyDecoder:
         num_total_symbol = torch.zeros(B, dtype=torch.int, device=device)
         arange_tensor = torch.arange(B, device=device)
 
-        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, 
+        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask,
                                 num_symbol_added, num_total_symbol, current_label]
 
         self.stashed_tensor = x, out_len, arange_tensor
-        
+
         self.main_loop_cg = self._capture_cg_for_main_loop(list_input_tensor)
         self.cg_captured = True
 
@@ -500,7 +480,7 @@ class RNNTGreedyDecoder:
         hidden = [torch.zeros((2, self.cg_batch_size, self.rnnt_config["pred_n_hid"]), dtype=x.dtype, device=device)]*2
         label_tensor = torch.zeros(self.cg_batch_size, self.max_symbol_per_sample, dtype=torch.int, device=device)
         current_label = torch.ones(self.cg_batch_size, dtype=torch.int, device=device) * -1
-        
+
         time_idx = torch.zeros(self.cg_batch_size, dtype=torch.int64, device=device)
         label_idx = torch.zeros(self.cg_batch_size, dtype=torch.int64, device=device)
         complete_mask = time_idx >= self.stashed_tensor[1] # i.e. padded out_len
@@ -509,18 +489,18 @@ class RNNTGreedyDecoder:
         num_total_symbol = torch.zeros(self.cg_batch_size, dtype=torch.int, device=device)
         arange_tensor = torch.arange(self.cg_batch_size, device=device)
 
-        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol, 
+        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol,
             current_label]
 
-        
+
         while batch_complete == False:
-            list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol, 
+            list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol,
             current_label]
             label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, batch_complete, num_symbol_added, num_total_symbol, \
             current_label = self.main_loop_cg(*list_input_tensor)
 
         label = []
-        
+
         for i in range(B):
             label.append(label_tensor[i, :label_idx[i]].tolist())
         return label
@@ -537,7 +517,7 @@ class RNNTGreedyDecoder:
         hidden = [torch.zeros((2, self.cg_batch_size, self.rnnt_config["pred_n_hid"]), dtype=x.dtype, device=device)]*2
         label_tensor = torch.zeros(self.cg_batch_size, self.max_symbol_per_sample, dtype=torch.int, device=device)
         current_label = torch.ones(self.cg_batch_size, dtype=torch.int, device=device) * -1
-        
+
         time_idx = torch.zeros(self.cg_batch_size, dtype=torch.int64, device=device)
         label_idx = torch.zeros(self.cg_batch_size, dtype=torch.int64, device=device)
         complete_mask = time_idx >= self.stashed_tensor[1] # i.e. padded out_len
@@ -546,30 +526,25 @@ class RNNTGreedyDecoder:
         num_total_symbol = torch.zeros(self.cg_batch_size, dtype=torch.int, device=device)
         arange_tensor = torch.arange(self.cg_batch_size, device=device)
 
-        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol, 
+        list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol,
             current_label]
 
-        
-        batch_complete_cpu = torch.tensor(False, dtype=torch.bool, device='cpu').pin_memory()
-        copy_stream = torch.cuda.Stream()
+
+        batch_complete_cpu = torch.tensor(False, dtype=torch.bool, device='cpu')
         while True:
-            list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol, 
+            list_input_tensor = [label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, num_symbol_added, num_total_symbol,
             current_label]
             label_tensor, hidden[0], hidden[1], time_idx, label_idx, complete_mask, batch_complete, num_symbol_added, num_total_symbol, \
             current_label = self.main_loop_cg(*list_input_tensor)
 
-            copy_stream.synchronize()
             # print(batch_complete_cpu)
             if torch.any(batch_complete_cpu):
                 break
 
-            copy_stream.wait_stream(torch.cuda.current_stream())
-
-            with torch.cuda.stream(copy_stream):
-                batch_complete_cpu.copy_(batch_complete, non_blocking=True)
+            batch_complete_cpu.copy_(batch_complete, non_blocking=True)
 
         label = []
-        
+
         for i in range(B):
             label.append(label_tensor[i, :label_idx[i]].tolist())
         return label
diff --git a/modelzoo/rnnt/pytorch/rnnt/loss.py b/modelzoo/rnnt/pytorch/rnnt/loss.py
index b833b61..a045ef2 100644
--- a/modelzoo/rnnt/pytorch/rnnt/loss.py
+++ b/modelzoo/rnnt/pytorch/rnnt/loss.py
@@ -17,7 +17,6 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from warprnnt_pytorch import RNNTLoss as WarpRNNTLoss
-from apex.contrib.transducer import TransducerLoss
 
 
 class RNNTLoss(torch.nn.Module):
@@ -87,27 +86,27 @@ class RNNTLoss(torch.nn.Module):
 
         return loss
 
-class apexTransducerLoss(torch.nn.Module):
-    def __init__(self, blank_idx, precision, packed_input):
-        super().__init__()
-        self.t_loss = TransducerLoss(packed_input=packed_input)
-        self.blank_idx = blank_idx
-        self.precision = precision
+# class apexTransducerLoss(torch.nn.Module):
+#     def __init__(self, blank_idx, precision, packed_input):
+#         super().__init__()
+#         self.t_loss = TransducerLoss(packed_input=packed_input)
+#         self.blank_idx = blank_idx
+#         self.precision = precision
 
-    def forward(self, logits, logit_lens, y, y_lens, dict_meta_data=None):
-        if self.precision == "fp32" and logits.dtype != torch.float32:
-            logits = logits.float()
+#     def forward(self, logits, logit_lens, y, y_lens, dict_meta_data=None):
+#         if self.precision == "fp32" and logits.dtype != torch.float32:
+#             logits = logits.float()
 
-        if y.dtype != torch.int32:
-            y = y.int()
+#         if y.dtype != torch.int32:
+#             y = y.int()
 
-        if logit_lens.dtype != torch.int32:
-            logit_lens = logit_lens.int()
+#         if logit_lens.dtype != torch.int32:
+#             logit_lens = logit_lens.int()
 
-        if y_lens.dtype != torch.int32:
-            y_lens = y_lens.int()
+#         if y_lens.dtype != torch.int32:
+#             y_lens = y_lens.int()
 
-        loss = self.t_loss(logits, y, logit_lens, y_lens, self.blank_idx, 
-                            batch_offset=dict_meta_data["batch_offset"], 
-                            max_f_len=dict_meta_data["max_f_len"]).mean()
-        return loss
+#         loss = self.t_loss(logits, y, logit_lens, y_lens, self.blank_idx,
+#                             batch_offset=dict_meta_data["batch_offset"],
+#                             max_f_len=dict_meta_data["max_f_len"]).mean()
+#         return loss
diff --git a/modelzoo/rnnt/pytorch/rnnt/model.py b/modelzoo/rnnt/pytorch/rnnt/model.py
index f052a53..4315e6a 100644
--- a/modelzoo/rnnt/pytorch/rnnt/model.py
+++ b/modelzoo/rnnt/pytorch/rnnt/model.py
@@ -20,16 +20,15 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mlperf import logging
-from apex.mlp import MLP
+import intel_extension_for_pytorch as ipex
 
 from common.rnn import rnn
-from apex.contrib.transducer import TransducerJoint
 
-torch._C._jit_set_nvfuser_enabled(True)
-torch._C._jit_set_texpr_fuser_enabled(False)
-torch._C._jit_override_can_fuse_on_cpu(False)
-torch._C._jit_override_can_fuse_on_gpu(False)
-torch._C._jit_set_bailout_depth(20)
+# torch._C._jit_set_nvfuser_enabled(True)
+# torch._C._jit_set_texpr_fuser_enabled(False)
+# torch._C._jit_override_can_fuse_on_cpu(False)
+# torch._C._jit_override_can_fuse_on_gpu(False)
+# torch._C._jit_set_bailout_depth(20)
 
 class StackTime(nn.Module):
     def __init__(self, factor):
@@ -46,7 +45,7 @@ class StackTime(nn.Module):
         x = x.transpose(0, 1)
         return x
 
-        
+
     def forward(self, x, x_lens):
         # T, B, U
         if type(x) is not list:
@@ -57,7 +56,7 @@ class StackTime(nn.Module):
             # seq splitting path
             if len(x) != 2:
                 raise NotImplementedError("Only number of seq segments equal to 2 is supported")
-            # For simplicty, we assume except or the last segment, all seq segments should be 
+            # For simplicty, we assume except or the last segment, all seq segments should be
             # multiple of self.factor. Therefore, we can ensure each segment can be done independently.
             assert x[0].size(1) % self.factor == 0, "The length of the 1st seq segment should be multiple of stack factor"
             y0 = self.stack(x[0])
@@ -109,7 +108,7 @@ class RNNT(nn.Module):
                  hidden_hidden_bias_scale=0.0, weights_init_scale=1.0,
                  enc_lr_factor=1.0, pred_lr_factor=1.0, joint_lr_factor=1.0,
                  fuse_relu_dropout=False, apex_transducer_joint=None,
-                 min_lstm_bs=8, apex_mlp=False):
+                 min_lstm_bs=8, apex_mlp=False, rnn_type='lstm'):
         super(RNNT, self).__init__()
 
         self.enc_lr_factor = enc_lr_factor
@@ -123,6 +122,7 @@ class RNNT(nn.Module):
         post_rnn_input_size = enc_stack_time_factor * enc_n_hid
 
         enc_mod = {}
+        enc_mod["batch_norm"] = nn.BatchNorm1d(pre_rnn_input_size)
         enc_mod["pre_rnn"] = rnn(input_size=pre_rnn_input_size,
                                  hidden_size=enc_n_hid,
                                  num_layers=enc_pre_rnn_layers,
@@ -131,6 +131,7 @@ class RNNT(nn.Module):
                                  weights_init_scale=weights_init_scale,
                                  dropout=enc_dropout,
                                  decoupled=decoupled_rnns,
+                                 rnn_type=rnn_type,
                                  tensor_name='pre_rnn',
                                 )
 
@@ -144,6 +145,7 @@ class RNNT(nn.Module):
                                   weights_init_scale=weights_init_scale,
                                   dropout=enc_dropout,
                                   decoupled=decoupled_rnns,
+                                  rnn_type=rnn_type,
                                   tensor_name='post_rnn',
                                 )
 
@@ -163,6 +165,7 @@ class RNNT(nn.Module):
                 weights_init_scale=weights_init_scale,
                 dropout=pred_dropout,
                 decoupled=decoupled_rnns,
+                rnn_type=rnn_type,
                 tensor_name='dec_rnn',
             ),
         })
@@ -178,16 +181,7 @@ class RNNT(nn.Module):
         logging.log_event(logging.constants.WEIGHTS_INITIALIZATION,
                           metadata=dict(tensor='joint_enc'))
 
-        if apex_mlp:
-            # make sure we use the same weight initialization 
-            linear_dummy = torch.nn.Linear(joint_n_hid, n_classes)
-            fc = MLP([joint_n_hid, n_classes], activation='none')
-            with torch.no_grad():
-                fc.weights[0].copy_(linear_dummy.weight)
-                fc.biases[0].copy_(linear_dummy.bias)
-            del linear_dummy
-        else:
-            fc = torch.nn.Linear(joint_n_hid, n_classes)  
+        fc = torch.nn.Linear(joint_n_hid, n_classes)
 
         if fuse_relu_dropout:
             self.joint_net = nn.Sequential(
@@ -204,9 +198,6 @@ class RNNT(nn.Module):
                               metadata=dict(tensor='joint_net'))
         self.apex_transducer_joint = apex_transducer_joint
 
-        if self.apex_transducer_joint is not None:
-            self.my_transducer_joint= TransducerJoint(
-                                        pack_output=(self.apex_transducer_joint=='pack'))
         self.min_lstm_bs = min_lstm_bs
 
     def forward(self, x, x_lens, y, y_lens, dict_meta_data=None, state=None):
@@ -221,15 +212,12 @@ class RNNT(nn.Module):
 
         return out, x_lens
 
-    def enc_pred(self, x, x_lens, y, y_lens, pred_stream, state=None):
-        pred_stream.wait_stream(torch.cuda.current_stream())
+    def enc_pred(self, x, x_lens, y, y_lens, state=None):
         f, x_lens = self.encode(x, x_lens)
 
-        with torch.cuda.stream(pred_stream):
-            y = label_collate(y)
-            g, _ = self.predict(y, state)
+        y = label_collate(y)
+        g, _ = self.predict(y, state)
 
-        torch.cuda.current_stream().wait_stream(pred_stream)
         return f, g, x_lens
 
     def _seq_merge(self, x):
@@ -253,6 +241,16 @@ class RNNT(nn.Module):
         if require_padding:
             bs = x.size(1)
             x = torch.nn.functional.pad(x, (0, 0, 0, self.min_lstm_bs - bs))
+        if type(x) is not list:
+            x = x.transpose(1, 2)
+            x = self.encoder["batch_norm"](x)
+            x = x.transpose(1, 2)
+        else:
+            x[0] = x[0].transpose(1, 2)
+            x[1] = x[1].transpose(1, 2)
+            x = [self.encoder["batch_norm"](x[0]), self.encoder["batch_norm"](x[1])]
+            x[0] = x[0].transpose(1, 2)
+            x[1] = x[1].transpose(1, 2)
         x, _ = self.encoder["pre_rnn"](x, None)
         x, x_lens = self.encoder["stack_time"](x, x_lens)
         x, _ = self.encoder["post_rnn"](x, None)
@@ -354,19 +352,12 @@ class RNNT(nn.Module):
         """
         # Combine the input states and the output states
 
-        if apex_transducer_joint is None:
-            f = f.unsqueeze(dim=2)   # (B, T, 1, H)
-            g = g.unsqueeze(dim=1)   # (B, 1, U + 1, H)
-            h = f + g
-
-            B, T, U, H = h.size()
-            res = self.joint_net(h.view(-1, H))
-            res = res.view(B, T, U, -1)
-        else:
-            h = self.my_transducer_joint(f, g, f_len, dict_meta_data["g_len"], 
-                                            dict_meta_data["batch_offset"], 
-                                            dict_meta_data["packed_batch"])  
-            res = self.joint_net(h)
+        f = f.unsqueeze(dim=2)   # (B, T, 1, H)
+        g = g.unsqueeze(dim=1)   # (B, 1, U + 1, H)
+        h = f + g
+        B, T, U, H = h.size()
+        res = self.joint_net(h.view(-1, H))
+        res = res.view(B, T, U, -1)
 
         del f, g
         return res
@@ -429,7 +420,7 @@ class RNNTPredict(nn.Module):
 
         if require_padding:
             g = g[:bs]
-        
+
         return g
 
 def label_collate(labels):
diff --git a/modelzoo/rnnt/pytorch/rnnt/rnnt_graph.py b/modelzoo/rnnt/pytorch/rnnt/rnnt_graph.py
index 535ecbe..2798cba 100755
--- a/modelzoo/rnnt/pytorch/rnnt/rnnt_graph.py
+++ b/modelzoo/rnnt/pytorch/rnnt/rnnt_graph.py
@@ -73,7 +73,7 @@ class RNNTGraph():
     def _model_segment(self, encode_block, predict_block, x, x_lens, y, y_lens, dict_meta_data=None):
         f, x_lens = encode_block(x, x_lens)
         # g, _ = self.model.predict(y)
-        g = predict_block(y) 
+        g = predict_block(y)
         out = self.model.joint(f, g, self.model.apex_transducer_joint, x_lens, dict_meta_data)
         return out, x_lens
 
diff --git a/modelzoo/rnnt/pytorch/run_and_time.sh b/modelzoo/rnnt/pytorch/run_and_time.sh
index 5069d36..eacde82 100755
--- a/modelzoo/rnnt/pytorch/run_and_time.sh
+++ b/modelzoo/rnnt/pytorch/run_and_time.sh
@@ -20,7 +20,7 @@
 
 set -e
 
-# Only rank print 
+# Only rank print
 [ "${SLURM_LOCALID-}" -ne 0 ] && set +x
 
 
diff --git a/modelzoo/rnnt/pytorch/scripts/create_sentencepieces.sh b/modelzoo/rnnt/pytorch/scripts/create_sentencepieces.sh
index e8d5192..760f161 100644
--- a/modelzoo/rnnt/pytorch/scripts/create_sentencepieces.sh
+++ b/modelzoo/rnnt/pytorch/scripts/create_sentencepieces.sh
@@ -12,9 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+DATA_DIR="/home/vmagent/app/dataset/LibriSpeech"
 OUTPUT_DIR=${1:-/datasets/sentencepieces}
 
 mkdir -p $OUTPUT_DIR
-jq -r '.[]["transcript"]' /datasets/LibriSpeech/librispeech-train-*-wav.json > /tmp/txt.txt
+jq -r '.[]["transcript"]' ${DATA_DIR}/librispeech-train-*-wav.json > /tmp/txt.txt
 python -c "import sentencepiece as spm; spm.SentencePieceTrainer.train(input='/tmp/txt.txt', model_prefix='librispeech1023', vocab_size=1023, character_coverage=1.0, bos_id=-1, eos_id=-1, model_type='unigram')"
 cp librispeech1023.* $OUTPUT_DIR
diff --git a/modelzoo/rnnt/pytorch/scripts/download_librispeech.sh b/modelzoo/rnnt/pytorch/scripts/download_librispeech.sh
index 76a57e3..263d4ee 100755
--- a/modelzoo/rnnt/pytorch/scripts/download_librispeech.sh
+++ b/modelzoo/rnnt/pytorch/scripts/download_librispeech.sh
@@ -16,13 +16,13 @@
 #!/usr/bin/env bash
 
 DATA_SET="LibriSpeech"
-DATA_ROOT_DIR="/datasets"
+DATA_ROOT_DIR="/home/vmagent/app/dataset"
 DATA_DIR="${DATA_ROOT_DIR}/${DATA_SET}"
 if [ ! -d "$DATA_DIR" ]
 then
     mkdir $DATA_DIR
     chmod go+rx $DATA_DIR
-    python utils/download_librispeech.py utils/librispeech.csv $DATA_DIR -e ${DATA_ROOT_DIR}/
+    python utils/download_librispeech.py utils/librispeech.csv $DATA_DIR -e ${DATA_ROOT_DIR}/ --skip_download
 else
     echo "Directory $DATA_DIR already exists."
 fi
diff --git a/modelzoo/rnnt/pytorch/scripts/inference.sh b/modelzoo/rnnt/pytorch/scripts/inference.sh
new file mode 100644
index 0000000..49afc08
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/scripts/inference.sh
@@ -0,0 +1,185 @@
+#!/bin/bash
+
+# Copyright (c) 2018-2021, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# runs benchmark and reports time to convergence
+# to use the script:
+#   run_and_time.sh
+
+set -e
+source config.sh
+# Only rank print
+[ "${SLURM_LOCALID-}" -ne 0 ] && set +x
+
+
+: "${DALIDEVICE:=cpu}"
+: "${MODELCONFIG:=configs/baseline_v3-1023sp.yaml}"
+: "${BATCHSIZE:=32}"
+: "${EVAL_BATCHSIZE:=338}"
+: "${EPOCH:=1}"
+: "${SEED:=2021}"
+: "${LR:=0.004}"
+: "${WARMUP:=4}"
+: "${GRAD_ACCUMULATION_STEPS:=1}"
+: "${VAL_FREQUENCY:=1}"
+: "${HOLD_EPOCHS:=40}"
+: "${EMA:=0.999}"
+: "${LR_DECAY_POWER:=0.935}"
+: "${WEIGHTS_INIT_SCALE:=0.5}"
+: "${TRAIN_DATASET_DIR:="/home/vmagent/app/dataset/LibriSpeech/train"}"
+: "${VALID_DATASET_DIR:="/home/vmagent/app/dataset/LibriSpeech/valid"}"
+: "${DALI_ONLY:=false}"
+: "${VECTORIZED_SA:=false}"
+: "${VECTORIZED_SAMPLER=false}"
+: "${LOG_FREQUENCY=1}"
+: "${BETA1:=0.9}"
+: "${BETA2:=0.999}"
+: "${MAX_TRAIN_DURATION:=16.7}"
+
+: ${META_DIR:="/home/vmagent/app/dataset/LibriSpeech/metadata"}
+: ${TRAIN_MANIFESTS:="$META_DIR/librispeech-train-clean-100-wav-tokenized.pkl"}
+: ${VAL_MANIFESTS:="$META_DIR/librispeech-dev-clean-wav-tokenized.pkl"}
+: ${OUTPUT_DIR:="/results"}
+: ${TARGET:=0.058}
+
+: "${DIST:=true}"
+: "${DIST_BACKEND:=ccl}"
+
+: "${CKPT:=/results/RNN-T_epoch22_checkpoint.pt}"
+
+# start timing
+start=$(date +%s)
+start_fmt=$(date +%Y-%m-%d\ %r)
+echo "STARTING TIMING RUN AT $start_fmt"
+
+export DATASET_DIR
+export TORCH_HOME="$(pwd)/torch-model-cache"
+
+mkdir -p /results
+
+MODEL_ARGS="--enc_n_hid 256 \
+  --enc_pre_rnn_layers 1 \
+  --enc_stack_time_factor 8 \
+  --enc_post_rnn_layers 1 \
+  --enc_dropout 0.1 \
+  --pred_n_hid 128 \
+  --pred_rnn_layers 2 \
+  --pred_dropout 0.3 \
+  --joint_n_hid 128 \
+  --joint_dropout 0.3"
+
+# run training
+ARGS="inference.py \
+  --batch_size=$BATCHSIZE \
+  --beta1=${BETA1} \
+  --beta2=${BETA2} \
+  --max_duration=${MAX_TRAIN_DURATION} \
+  --val_batch_size=$EVAL_BATCHSIZE \
+  --target=${TARGET} \
+  --lr=${LR} \
+  --min_lr=1e-5 \
+  --lr_exp_gamma=${LR_DECAY_POWER} \
+  --epochs=$EPOCH \
+  --warmup_epochs=$WARMUP \
+  --hold_epochs=$HOLD_EPOCHS \
+  --epochs_this_job=0 \
+  --ema=$EMA \
+  --output_dir ${OUTPUT_DIR} \
+  --model_config=$MODELCONFIG \
+  --seed $SEED \
+  --train_dataset_dir=${TRAIN_DATASET_DIR} \
+  --valid_dataset_dir=${VALID_DATASET_DIR} \
+  --cudnn_benchmark \
+  --dali_device $DALIDEVICE \
+  --weight_decay=1e-3 \
+  --log_frequency=${LOG_FREQUENCY} \
+  --val_frequency=$VAL_FREQUENCY \
+  --grad_accumulation_steps=$GRAD_ACCUMULATION_STEPS \
+  --prediction_frequency=1000000 \
+  --weights_init_scale=${WEIGHTS_INIT_SCALE} \
+  --val_manifests=${VAL_MANIFESTS} \
+  --train_manifests ${TRAIN_MANIFESTS} \
+  --ckpt ${CKPT}"
+
+if [ $BUCKET -ne 0 ]; then
+  ARGS="${ARGS} --num_buckets=${BUCKET}"
+fi
+if [ $MAX_SYMBOL -gt 0 ]; then
+  ARGS="${ARGS} --max_symbol_per_sample=${MAX_SYMBOL}"
+fi
+if [ "$APEX_LOSS" = "fp16" ] || [ "$APEX_LOSS" = "fp32" ]; then
+  ARGS="${ARGS} --apex_transducer_loss=${APEX_LOSS}"
+fi
+if [ "$FUSE_RELU_DROPOUT" = true ]; then
+  ARGS="${ARGS} --fuse_relu_dropout"
+fi
+if [ "$MULTI_TENSOR_EMA" = true ]; then
+  ARGS="${ARGS} --multi_tensor_ema"
+fi
+if [ "$BATCH_EVAL_MODE" = "no_cg" ] || [ "$BATCH_EVAL_MODE" = "cg" ] || [ "$BATCH_EVAL_MODE" = "cg_unroll_pipeline" ]; then
+  ARGS="${ARGS} --batch_eval_mode ${BATCH_EVAL_MODE}"
+fi
+if [ "$DIST_LAMB" = true ]; then
+  ARGS="${ARGS} --dist_lamb"
+fi
+if [ "$APEX_JOINT" = "pack" ] || [ "$APEX_JOINT" = "not_pack" ]; then
+  ARGS="${ARGS} --apex_transducer_joint=${APEX_JOINT}"
+fi
+if [ "$BUFFER_PREALLOC" = true ]; then
+  ARGS="${ARGS} --buffer_pre_alloc"
+fi
+if [ "$EMA_UPDATE_TYPE" = "fp16" ] || [ "$EMA_UPDATE_TYPE" = "fp32" ]; then
+  ARGS="${ARGS} --ema_update_type=${EMA_UPDATE_TYPE}"
+fi
+if [ "$DIST" = true ]; then
+  ARGS="${ARGS} --dist --dist_backend=${DIST_BACKEND}"
+fi
+ARGS="${ARGS} ${MODEL_ARGS}"
+
+[ ! -z "${AMP_LVL}" ] && ARGS+=" --amp_level ${AMP_LVL}"
+[ ! -z "${DATA_CPU_THREADS}" ] && ARGS+=" --data_cpu_threads ${DATA_CPU_THREADS}"
+[ ! -z "${BATCH_SPLIT_FACTOR}" ] && ARGS+=" --batch_split_factor ${BATCH_SPLIT_FACTOR}"
+[ ! -z "${NUM_CG}" ] && ARGS+=" --num_cg ${NUM_CG}"
+[ ! -z "${MIN_SEQ_SPLIT_LEN}" ] && ARGS+=" --min_seq_split_len ${MIN_SEQ_SPLIT_LEN}"
+[ ! -z "${DWU_GROUP_SIZE}" ] && ARGS+=" --dwu_group_size ${DWU_GROUP_SIZE}"
+[ "${VECTORIZED_SA}" = true ] && ARGS+=" --vectorized_sa"
+[ "${MULTILAYER_LSTM}" = true ] && ARGS+=" --multilayer_lstm"
+[ "${IN_MEM_FILE_LIST}" = true ] && ARGS+=" --in_mem_file_list"
+[ "${ENABLE_PREFETCH}" = true ] && ARGS+=" --enable_prefetch"
+[ "${TOKENIZED_TRANSCRIPT}" = true ] && ARGS+=" --tokenized_transcript"
+[ "${VECTORIZED_SAMPLER}" = true ] && ARGS+=" --vectorized_sampler"
+[ "${SEQ_LEN_STATS}" = true ] && ARGS+=" --enable_seq_len_stats"
+[ "${DIST_SAMPLER}" = true ] && ARGS+=" --dist_sampler"
+[ "${APEX_MLP}" = true ] && ARGS+=" --apex_mlp"
+[ "${PRE_SORT_FOR_SEQ_SPLIT}" = true ] && ARGS+=" --pre_sort_for_seq_split"
+[ "${JIT_TENSOR_FORMATION}" = true ] && ARGS+=" --jit_tensor_formation"
+[ "${DALI_DONT_USE_MMAP}" = true ] && ARGS+=" --dali_dont_use_mmap"
+
+CONDA_PREFIX=/opt/intel/oneapi/intelpython/latest/envs/pytorch_1.10
+# if [ "$DIST" = true ]; then
+#   echo "Distributed inference"
+#   ${CONDA_PREFIX}/bin/python -m intel_extension_for_pytorch.cpu.launch --distributed --nproc_per_node=2 --nnodes=1 --hostfile hosts \
+#     ${ARGS} 2>&1 | tee dist_train.log
+#   ret_code=$?
+
+# else
+#   echo "Inference"
+#   ${CONDA_PREFIX}/bin/python -m intel_extension_for_pytorch.cpu.launch ${ARGS} 2>&1 | tee train.log
+#   ret_code=$?
+# fi
+
+${CONDA_PREFIX}/bin/python -m intel_extension_for_pytorch.cpu.launch --distributed --nproc_per_node=2 --nnodes=1 --hostfile hosts ${ARGS} 2>&1 | tee inference.log
+
+set +x
diff --git a/modelzoo/rnnt/pytorch/scripts/preprocess_librispeech.sh b/modelzoo/rnnt/pytorch/scripts/preprocess_librispeech.sh
index e9ac375..dbdb707 100755
--- a/modelzoo/rnnt/pytorch/scripts/preprocess_librispeech.sh
+++ b/modelzoo/rnnt/pytorch/scripts/preprocess_librispeech.sh
@@ -14,39 +14,49 @@
 
 #!/usr/bin/env bash
 
+DATA_DIR="/home/vmagent/app/dataset/LibriSpeech"
+META_DIR="/home/vmagent/app/dataset/LibriSpeech/metadata"
+SENTENCE_DIR="/home/vmagent/app/dataset/LibriSpeech/sentencepieces"
+
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/train-clean-100 \
-    --dest_dir /datasets/LibriSpeech/train-clean-100-wav \
-    --output_json /datasets/LibriSpeech/librispeech-train-clean-100-wav.json
+    --input_dir ${DATA_DIR}/train-clean-100 \
+    --dest_dir ${DATA_DIR}/train-clean-100-wav \
+    --output_json ${DATA_DIR}/librispeech-train-clean-100-wav.json
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/train-clean-360 \
-    --dest_dir /datasets/LibriSpeech/train-clean-360-wav \
-    --output_json /datasets/LibriSpeech/librispeech-train-clean-360-wav.json
+    --input_dir ${DATA_DIR}/train-clean-360 \
+    --dest_dir ${DATA_DIR}/train-clean-360-wav \
+    --output_json ${DATA_DIR}/librispeech-train-clean-360-wav.json
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/train-other-500 \
-    --dest_dir /datasets/LibriSpeech/train-other-500-wav \
-    --output_json /datasets/LibriSpeech/librispeech-train-other-500-wav.json
+    --input_dir ${DATA_DIR}/train-other-500 \
+    --dest_dir ${DATA_DIR}/train-other-500-wav \
+    --output_json ${DATA_DIR}/librispeech-train-other-500-wav.json
 
 
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/dev-clean \
-    --dest_dir /datasets/LibriSpeech/dev-clean-wav \
-    --output_json /datasets/LibriSpeech/librispeech-dev-clean-wav.json
+    --input_dir ${DATA_DIR}/dev-clean \
+    --dest_dir ${DATA_DIR}/dev-clean-wav \
+    --output_json ${DATA_DIR}/librispeech-dev-clean-wav.json
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/dev-other \
-    --dest_dir /datasets/LibriSpeech/dev-other-wav \
-    --output_json /datasets/LibriSpeech/librispeech-dev-other-wav.json
+    --input_dir ${DATA_DIR}/dev-other \
+    --dest_dir ${DATA_DIR}/dev-other-wav \
+    --output_json ${DATA_DIR}/librispeech-dev-other-wav.json
 
 
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/test-clean \
-    --dest_dir /datasets/LibriSpeech/test-clean-wav \
-    --output_json /datasets/LibriSpeech/librispeech-test-clean-wav.json
+    --input_dir ${DATA_DIR}/test-clean \
+    --dest_dir ${DATA_DIR}/test-clean-wav \
+    --output_json ${DATA_DIR}/librispeech-test-clean-wav.json
 python ./utils/convert_librispeech.py \
-    --input_dir /datasets/LibriSpeech/test-other \
-    --dest_dir /datasets/LibriSpeech/test-other-wav \
-    --output_json /datasets/LibriSpeech/librispeech-test-other-wav.json
+    --input_dir ${DATA_DIR}/test-other \
+    --dest_dir ${DATA_DIR}/test-other-wav \
+    --output_json ${DATA_DIR}/librispeech-test-other-wav.json
+
+bash scripts/create_sentencepieces.sh $SENTENCE_DIR
 
-bash scripts/create_sentencepieces.sh /sentencepieces
+mkdir $META_DIR
+python scripts/tokenize_transcripts.py --output_dir $META_DIR --model $SENTENCE_DIR/librispeech1023.model ${DATA_DIR}/librispeech-*.json
 
-python scripts/tokenize_transcripts.py --output_dir /metadata/ --model /sentencepieces/librispeech1023.model /datasets/LibriSpeech/librispeech-*.json
+mkdir $DATA_DIR/train
+mkdir $DATA_DIR/valid
+mv $DATA_DIR/train-*-wav $DATA_DIR/train
+mv $DATA_DIR/dev-*-wav $DATA_DIR/valid
\ No newline at end of file
diff --git a/modelzoo/rnnt/pytorch/scripts/train.sh b/modelzoo/rnnt/pytorch/scripts/train.sh
new file mode 100644
index 0000000..758849c
--- /dev/null
+++ b/modelzoo/rnnt/pytorch/scripts/train.sh
@@ -0,0 +1,211 @@
+#!/bin/bash
+
+# Copyright (c) 2018-2021, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# runs benchmark and reports time to convergence
+# to use the script:
+#   run_and_time.sh
+
+set -e
+
+source config.sh
+# Only rank print
+[ "${SLURM_LOCALID-}" -ne 0 ] && set +x
+
+
+: "${AMP_LVL:=1}"
+: "${DALIDEVICE:=cpu}"
+: "${MODELCONFIG:=configs/baseline_v3-1023sp.yaml}"
+: "${BATCHSIZE:=${BATCHSIZE}}"
+: "${EVAL_BATCHSIZE:=${BATCHSIZE}}"
+: "${EPOCH:=80}"
+: "${SEED:=2021}"
+: "${LR:=0.007}"
+: "${WARMUP:=10}"
+: "${GRAD_ACCUMULATION_STEPS:=1}"
+: "${VAL_FREQUENCY:=1}"
+: "${HOLD_EPOCHS:=33}"
+: "${EMA:=0.999}"
+: "${LR_DECAY_POWER:=0.939}"
+: "${WEIGHTS_INIT_SCALE:=0.5}"
+: "${TRAIN_DATASET_DIR:="/home/vmagent/app/dataset/LibriSpeech/train"}"
+: "${VALID_DATASET_DIR:="/home/vmagent/app/dataset/LibriSpeech/valid"}"
+: "${DALI_ONLY:=false}"
+: "${VECTORIZED_SA:=false}"
+: "${VECTORIZED_SAMPLER=false}"
+: "${LOG_FREQUENCY=1}"
+: "${BETA1:=0.9}"
+: "${BETA2:=0.999}"
+: "${MAX_TRAIN_DURATION:=16.7}"
+
+: ${META_DIR:="/home/vmagent/app/dataset/LibriSpeech/metadata"}
+# : ${TRAIN_MANIFESTS:="$META_DIR/librispeech-train-clean-100-wav-tokenized.pkl \
+#                       $META_DIR/librispeech-train-clean-360-wav-tokenized.pkl \
+#                       $META_DIR/librispeech-train-other-500-wav-tokenized.pkl"}
+: ${TRAIN_MANIFESTS:="$META_DIR/librispeech-train-clean-100-wav-tokenized.pkl"}
+: ${VAL_MANIFESTS:="$META_DIR/librispeech-dev-clean-wav-tokenized.pkl"}
+: ${OUTPUT_DIR:="/results"}
+: ${TARGET:=0.25}
+
+: "${DIST:=true}"
+: "${DIST_BACKEND:=ccl}"
+# variable to control wether enable thread affinity in docker
+: "${SET_AFFINITY:=true}"
+
+# start timing
+start=$(date +%s)
+start_fmt=$(date +%Y-%m-%d\ %r)
+echo "STARTING TIMING RUN AT $start_fmt"
+
+# run benchmark
+echo "running benchmark"
+
+export DATASET_DIR
+export TORCH_HOME="$(pwd)/torch-model-cache"
+
+mkdir -p /results
+
+MODEL_ARGS="--enc_n_hid 1024 \
+  --enc_pre_rnn_layers 2 \
+  --enc_stack_time_factor 8 \
+  --enc_post_rnn_layers 3 \
+  --enc_dropout 0.1 \
+  --pred_n_hid 512 \
+  --pred_rnn_layers 2 \
+  --pred_dropout 0.3 \
+  --joint_n_hid 512 \
+  --joint_dropout 0.3 \
+  --rnn_type lstm"
+# run training
+ARGS="train.py \
+  --batch_size=$BATCHSIZE \
+  --beta1=${BETA1} \
+  --beta2=${BETA2} \
+  --max_duration=${MAX_TRAIN_DURATION} \
+  --val_batch_size=$EVAL_BATCHSIZE \
+  --target=${TARGET} \
+  --lr=${LR} \
+  --min_lr=1e-5 \
+  --lr_exp_gamma=${LR_DECAY_POWER} \
+  --epochs=$EPOCH \
+  --warmup_epochs=$WARMUP \
+  --hold_epochs=$HOLD_EPOCHS \
+  --epochs_this_job=0 \
+  --ema=$EMA \
+  --output_dir ${OUTPUT_DIR} \
+  --model_config=$MODELCONFIG \
+  --seed $SEED \
+  --train_dataset_dir=${TRAIN_DATASET_DIR} \
+  --valid_dataset_dir=${VALID_DATASET_DIR} \
+  --cudnn_benchmark \
+  --dali_device $DALIDEVICE \
+  --weight_decay=1e-3 \
+  --log_frequency=${LOG_FREQUENCY} \
+  --val_frequency=$VAL_FREQUENCY \
+  --grad_accumulation_steps=$GRAD_ACCUMULATION_STEPS \
+  --prediction_frequency=1000000 \
+  --weights_init_scale=${WEIGHTS_INIT_SCALE} \
+  --val_manifests=${VAL_MANIFESTS} \
+  --train_manifests ${TRAIN_MANIFESTS} \
+  --save_at_the_end" # \
+  # --use_ipex"
+
+if [ $BUCKET -ne 0 ]; then
+  ARGS="${ARGS} --num_buckets=${BUCKET}"
+fi
+if [ $MAX_SYMBOL -gt 0 ]; then
+  ARGS="${ARGS} --max_symbol_per_sample=${MAX_SYMBOL}"
+fi
+if [ "$APEX_LOSS" = "fp16" ] || [ "$APEX_LOSS" = "fp32" ]; then
+  ARGS="${ARGS} --apex_transducer_loss=${APEX_LOSS}"
+fi
+if [ "$FUSE_RELU_DROPOUT" = true ]; then
+  ARGS="${ARGS} --fuse_relu_dropout"
+fi
+if [ "$MULTI_TENSOR_EMA" = true ]; then
+  ARGS="${ARGS} --multi_tensor_ema"
+fi
+if [ "$BATCH_EVAL_MODE" = "no_cg" ] || [ "$BATCH_EVAL_MODE" = "cg" ] || [ "$BATCH_EVAL_MODE" = "cg_unroll_pipeline" ]; then
+  ARGS="${ARGS} --batch_eval_mode ${BATCH_EVAL_MODE}"
+fi
+if [ "$DIST_LAMB" = true ]; then
+  ARGS="${ARGS} --dist_lamb"
+fi
+if [ "$APEX_JOINT" = "pack" ] || [ "$APEX_JOINT" = "not_pack" ]; then
+  ARGS="${ARGS} --apex_transducer_joint=${APEX_JOINT}"
+fi
+if [ "$BUFFER_PREALLOC" = true ]; then
+  ARGS="${ARGS} --buffer_pre_alloc"
+fi
+if [ "$EMA_UPDATE_TYPE" = "fp16" ] || [ "$EMA_UPDATE_TYPE" = "fp32" ]; then
+  ARGS="${ARGS} --ema_update_type=${EMA_UPDATE_TYPE}"
+fi
+if [ "$DIST" = true ]; then
+  ARGS="${ARGS} --dist --dist_backend=${DIST_BACKEND}"
+fi
+ARGS="${ARGS} ${MODEL_ARGS}"
+
+[ ! -z "${AMP_LVL}" ] && ARGS+=" --amp_level ${AMP_LVL}"
+[ ! -z "${DATA_CPU_THREADS}" ] && ARGS+=" --data_cpu_threads ${DATA_CPU_THREADS}"
+[ ! -z "${BATCH_SPLIT_FACTOR}" ] && ARGS+=" --batch_split_factor ${BATCH_SPLIT_FACTOR}"
+[ ! -z "${NUM_CG}" ] && ARGS+=" --num_cg ${NUM_CG}"
+[ ! -z "${MIN_SEQ_SPLIT_LEN}" ] && ARGS+=" --min_seq_split_len ${MIN_SEQ_SPLIT_LEN}"
+[ ! -z "${DWU_GROUP_SIZE}" ] && ARGS+=" --dwu_group_size ${DWU_GROUP_SIZE}"
+[ "${VECTORIZED_SA}" = true ] && ARGS+=" --vectorized_sa"
+[ "${MULTILAYER_LSTM}" = true ] && ARGS+=" --multilayer_lstm"
+[ "${IN_MEM_FILE_LIST}" = true ] && ARGS+=" --in_mem_file_list"
+[ "${ENABLE_PREFETCH}" = true ] && ARGS+=" --enable_prefetch"
+[ "${TOKENIZED_TRANSCRIPT}" = true ] && ARGS+=" --tokenized_transcript"
+[ "${VECTORIZED_SAMPLER}" = true ] && ARGS+=" --vectorized_sampler"
+[ "${SEQ_LEN_STATS}" = true ] && ARGS+=" --enable_seq_len_stats"
+[ "${DIST_SAMPLER}" = true ] && ARGS+=" --dist_sampler"
+[ "${APEX_MLP}" = true ] && ARGS+=" --apex_mlp"
+[ "${PRE_SORT_FOR_SEQ_SPLIT}" = true ] && ARGS+=" --pre_sort_for_seq_split"
+[ "${JIT_TENSOR_FORMATION}" = true ] && ARGS+=" --jit_tensor_formation"
+[ "${DALI_DONT_USE_MMAP}" = true ] && ARGS+=" --dali_dont_use_mmap"
+
+CONDA_PREFIX=/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0
+if [ "$DIST" = true ]; then
+  echo "Distributed training"
+  if [ "$SET_AFFINITY" = true ]; then
+    ${CONDA_PREFIX}/bin/python -m intel_extension_for_pytorch.cpu.launch --distributed --nproc_per_node=2 --nnodes=1 --hostfile hosts \
+      ${ARGS} 2>&1 | tee dist_train.log
+  else
+    echo "disable thread affinity"
+    ${CONDA_PREFIX}/bin/python -m launch --distributed --nproc_per_node=2 --nnodes=1 --hostfile hosts ${ARGS} 2>&1 | tee dist_train.log
+  fi
+  ret_code=$?
+
+else
+  echo "Training"
+  ${CONDA_PREFIX}/bin/python -m intel_extension_for_pytorch.cpu.launch ${ARGS} 2>&1 | tee train.log
+  ret_code=$?
+fi
+
+set +x
+
+sleep 3
+if [[ $ret_code != 0 ]]; then exit $ret_code; fi
+
+# end timing
+end=$(date +%s)
+end_fmt=$(date +%Y-%m-%d\ %r)
+echo "ENDING TIMING RUN AT $end_fmt"
+
+# report result
+result=$(( $end - $start ))
+result_name="RNN_SPEECH_RECOGNITION"
+
+echo "RESULT,$result_name,,$result,nvidia,$start_fmt"
diff --git a/modelzoo/rnnt/pytorch/train.py b/modelzoo/rnnt/pytorch/train.py
index de45eb0..8bb5542 100644
--- a/modelzoo/rnnt/pytorch/train.py
+++ b/modelzoo/rnnt/pytorch/train.py
@@ -21,14 +21,12 @@ import time
 import torch
 import multiprocessing
 import numpy as np
-import torch.distributed as dist
-from apex import amp
-from torch.cuda.amp import GradScaler
-from apex.optimizers import FusedLAMB
-from apex.parallel import DistributedDataParallel
-from apex.contrib.optimizers.distributed_fused_lamb import DistributedFusedLAMB
-import amp_C
+# import torch.distributed as dist
+# from torch.cuda.amp import GradScaler
 import math
+import intel_extension_for_pytorch as ipex
+from torch.nn.parallel import DistributedDataParallel as DDP
+import distributed as dist
 
 from common import helpers
 from common.data.dali import sampler as dali_sampler
@@ -42,7 +40,7 @@ from common.tb_dllogger import flush_log, init_log, log
 from rnnt import config
 from rnnt.decoder import RNNTGreedyDecoder
 from rnnt.loss import RNNTLoss
-from rnnt.loss import apexTransducerLoss
+# from rnnt.loss import apexTransducerLoss
 from rnnt.model import RNNT
 from rnnt.rnnt_graph import RNNTGraph
 
@@ -72,30 +70,33 @@ def parse_args():
     training.add_argument('--local_rank', default=os.getenv('LOCAL_RANK', 0), type=int,
                           help='GPU id used for distributed training')
     training.add_argument('--target', default=0.058, type=float, help='Target WER accuracy')
-    training.add_argument('--apex_transducer_loss', default=None, type=str, choices=['fp16', 'fp32'], 
+    training.add_argument('--apex_transducer_loss', default=None, type=str, choices=['fp16', 'fp32'],
                             help='what precision of apex transducer_loss to use')
-    training.add_argument('--fuse_relu_dropout', action='store_true', 
+    training.add_argument('--fuse_relu_dropout', action='store_true',
                             help='Fuse ReLU and dropout in the joint net')
     training.add_argument('--weights_init_scale', default=0.5, type=float, help='If set, overwrites value in config.')
     training.add_argument('--hidden_hidden_bias_scale', type=float, help='If set, overwrites value in config.')
     training.add_argument('--batch_eval_mode', default=None, type=str, choices=['no_cg', 'cg', 'cg_unroll_pipeline'],
                     help='do evaluation in batch')
     training.add_argument('--cg_unroll_factor', default=4, type=int, help='Unrolling factor for batch eval mode cg_unroll_pipeline')
-    training.add_argument('--apex_transducer_joint', default=None, type=str, choices=['pack', 'not_pack'], 
+    training.add_argument('--apex_transducer_joint', default=None, type=str, choices=['pack', 'not_pack'],
                             help='whether or not to pack the sequence with apex transducer_joint')
-    training.add_argument('--buffer_pre_alloc', action='store_true', 
+    training.add_argument('--buffer_pre_alloc', action='store_true',
                             help='Pre-allocate buffer in PyTorch')
-    training.add_argument('--multilayer_lstm', action='store_true', 
+    training.add_argument('--multilayer_lstm', action='store_true',
                             help='Use multilayer LSTMs instead of splitting them into multiple single-layer ones')
     training.add_argument('--batch_split_factor', default=1, type=int, help='Split batches feed into the joint net')
-    training.add_argument('--apex_mlp', action='store_true', 
+    training.add_argument('--apex_mlp', action='store_true',
                             help='Use apex MLP')
 
     training.add_argument("--num_cg", default=0, type=int,
                           help='number of graphs needed for training')
     training.add_argument('--min_seq_split_len', default=-1, type=int, help='Split sequences in a mini-batch to improve performance')
-    training.add_argument('--pre_sort_for_seq_split', action='store_true', 
+    training.add_argument('--pre_sort_for_seq_split', action='store_true',
                             help='Presort samples in a mini-batch so that seq split is more effective')
+    training.add_argument('--dist', action='store_true', default=False, help='Enable distributed training')
+    training.add_argument('--dist_backend', type=str, default='gloo', help='Distributed training backend')
+    training.add_argument('--use_ipex', action='store_true', default=False, help='Enable IPEX backend')
 
 
     optim = parser.add_argument_group('optimization setup')
@@ -119,11 +120,11 @@ def parse_args():
     optim.add_argument('--beta2', default=0.999, type=float, help='Beta 2 for optimizer')
     optim.add_argument('--ema', type=float, default=0.999,
                        help='Discount factor for exp averaging of model weights')
-    optim.add_argument('--multi_tensor_ema', action='store_true', 
+    optim.add_argument('--multi_tensor_ema', action='store_true',
                             help='Use multi_tensor_apply for EMA')
-    optim.add_argument('--dist_lamb', action='store_true', 
+    optim.add_argument('--dist_lamb', action='store_true',
                             help='Use distributed LAMB')
-    optim.add_argument('--ema_update_type', default='fp32', type=str, choices=['fp16', 'fp32'], 
+    optim.add_argument('--ema_update_type', default='fp32', type=str, choices=['fp16', 'fp32'],
                             help='is ema applied on the fp32 master weight or fp16 weight')
     optim.add_argument('--dwu_group_size', default=8, type=int,
                        help='Group size for distributed optimizer. Will be ignored if non-distributed optimizer is used')
@@ -173,8 +174,10 @@ def parse_args():
                     help='The longest text length in the sample')
     io.add_argument('--max_eval_sample_duration', type=float, default=32.7,
                     help='The max duration of samples in the eval set')
-    io.add_argument('--dataset_dir', required=True, type=str,
-                    help='Root dir of dataset')
+    io.add_argument('--train_dataset_dir', required=True, type=str,
+                    help='Root dir of train dataset')
+    io.add_argument('--valid_dataset_dir', required=True, type=str,
+                    help='Root dir of valid dataset')
     io.add_argument('--output_dir', type=str, required=True,
                     help='Directory for logs and checkpoints')
     io.add_argument('--log_file', type=str, default=None,
@@ -187,7 +190,7 @@ def parse_args():
                     help='length for synthetic audio sequence.')
     io.add_argument('--synthetic_text_seq_len', type=int, default=None,
                     help='length for synthetic text sequence.')
-    io.add_argument('--enable_seq_len_stats', action='store_true', 
+    io.add_argument('--enable_seq_len_stats', action='store_true',
                             help='Store and output seq len stats')
     io.add_argument('--vectorized_sa', action='store_true',
                     help='Vectorized implementation of SpecAugment')
@@ -201,40 +204,32 @@ def parse_args():
                     help='just-in-time tensor formation. Form the input txt tensor on the fly.')
     io.add_argument('--dali_dont_use_mmap', action='store_true',
                     help='Disable mmap for DALI')
+    io.add_argument('--training_time_threshold', type=int, default=None,
+                    help='Max training time before stopping training')
+
+    io.add_argument('--enc_n_hid', type=int, default=1024)
+    io.add_argument('--enc_pre_rnn_layers', type=int, default=2)
+    io.add_argument('--enc_stack_time_factor', type=int, default=2)
+    io.add_argument('--enc_post_rnn_layers', type=int, default=3)
+    io.add_argument('--enc_dropout', type=float, default=0.1)
+    io.add_argument('--pred_n_hid', type=int, default=512)
+    io.add_argument('--pred_rnn_layers', type=int, default=2)
+    io.add_argument('--pred_dropout', type=float, default=0.3)
+    io.add_argument('--joint_n_hid', type=int, default=512)
+    io.add_argument('--joint_dropout', type=float, default=0.3)
+    io.add_argument('--rnn_type', type=str, default='lstm')
+
     return parser.parse_args()
 
 
 @torch.no_grad()
-def apply_ema(optimizer, model, ema_model, decay, ema_update_type):
-    if not decay:
-        return
-
-    ema_model_weight_list = list(ema_model.parameters())
-    if ema_update_type == "fp32":
-        model_weight_list = list(amp.master_params(optimizer))
-    elif ema_update_type == "fp16":
-        model_weight_list = list(model.parameters())
-    for ema, source in zip(ema_model_weight_list, model_weight_list):
-        ema.copy_(decay * ema + (1 - decay) * source)
-
-def init_multi_tensor_ema(optimizer, model, ema_model, ema_update_type):
-    # Regardless of the ema_update_type, ema_accumulation_type is FP32, which is
-    # ensured by FP32 ema_model
-    ema_model_weight_list = list(ema_model.parameters())
-    if ema_update_type == "fp32":
-        model_weight_list = list(amp.master_params(optimizer))
-    elif ema_update_type == "fp16":
-        model_weight_list = list(model.parameters())
-    overflow_buf_for_ema = torch.cuda.IntTensor([0])
-    return ema_model_weight_list, model_weight_list, overflow_buf_for_ema
-
-def apply_multi_tensor_ema(model_weight_list, ema_model_weight_list, decay, overflow_buf):
+def apply_ema(model, ema_model, decay):
     if not decay:
         return
 
-    amp_C.multi_tensor_axpby(65536, overflow_buf, [ema_model_weight_list, model_weight_list, ema_model_weight_list], decay, 1-decay, -1)
-
-
+    sd = getattr(model, 'module', model).state_dict()
+    for k, v in ema_model.state_dict().items():
+        v.copy_(decay * v + (1 - decay) * sd[k])
 
 
 @torch.no_grad()
@@ -246,14 +241,15 @@ def evaluate(epoch, step, val_loader, val_feat_proc, detokenize,
     greedy_decoder.update_ema_model_eval(ema_model)
 
     for i, batch in enumerate(val_loader):
-        # print(f'{val_loader.pipeline_type} evaluation: {i:>10}/{len(val_loader):<10}', end='\r')
+        print(f'{val_loader.pipeline_type} evaluation: {i:>10}/{len(val_loader):<10}', end='\r')
 
         audio, audio_lens, txt, txt_lens = batch
 
         feats, feat_lens = val_feat_proc([audio, audio_lens])
-        if amp_level == 2:
-            feats = feats.half()
-        
+
+        # if amp_level == 2:
+        #     feats = feats.half()
+
         pred = greedy_decoder.decode(feats, feat_lens)
         agg['preds'] += helpers.gather_predictions([pred], detokenize)
         agg['txts'] += helpers.gather_transcripts([txt.cpu()], [txt_lens.cpu()], detokenize)
@@ -268,15 +264,12 @@ def evaluate(epoch, step, val_loader, val_feat_proc, detokenize,
     return wer
 
 
-def train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_lens, optimizer, grad_scaler, 
-                meta_data, train_loader, rnnt_graph, copy_stream, pred_stream):
-    # sync free loss
-    lr_cpu = torch.tensor(0, dtype=torch.float, device='cpu').pin_memory()
-    loss_cpu = torch.tensor(0, dtype=torch.float16, device='cpu').pin_memory()
+def train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_lens,
+                meta_data, train_loader, rnnt_graph):
     if args.batch_split_factor == 1:
         if rnnt_graph is not None:
             log_probs, log_prob_lens = rnnt_graph.step(feats, feat_lens, txt, txt_lens, meta_data[0])
-        else:    
+        else:
             log_probs, log_prob_lens = model(feats, feat_lens, txt, txt_lens, meta_data[0])
 
         loss = loss_fn(log_probs, log_prob_lens, txt, txt_lens, meta_data[0])
@@ -286,96 +279,83 @@ def train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_len
             train_loader.data_iterator().prefetch()
         loss /= args.grad_accumulation_steps
 
-        # copy loss to cpu asynchronously
-        copy_stream.wait_stream(torch.cuda.current_stream())
-        with torch.cuda.stream(copy_stream):
-            loss_cpu.copy_(loss.detach(), non_blocking=True)
-            if args.dist_lamb:
-                lr_cpu.copy_(optimizer._lr, non_blocking=True)
-
         del log_probs, log_prob_lens
 
-        if args.dist_lamb:
-            grad_scaler.scale(loss).backward()
-        else:
-            with amp.scale_loss(loss, optimizer) as scaled_loss:
-                scaled_loss.backward()
-
-        # sync before return         
-        copy_stream.synchronize()
-        if torch.isnan(loss_cpu).any():
+        loss.backward()
+        if torch.isnan(loss).any():
             raise Exception("Loss is NaN")
 
-        return loss_cpu.item(), lr_cpu.item()
+        return loss.item()
 
     else:
-        f, g, log_prob_lens = model.enc_pred(feats, feat_lens, txt, txt_lens, pred_stream)
+        f, g, log_prob_lens = model.enc_pred(feats, feat_lens, txt, txt_lens)
         f_2, g_2 = f.detach(), g.detach()
         f_2.requires_grad = True
         g_2.requires_grad = True
         B_split = batch_size // args.batch_split_factor
         loss_item = 0
         for i in range(args.batch_split_factor):
-            
-            log_probs = model.joint(f_2[i*B_split:(i+1)*B_split], g_2[i*B_split:(i+1)*B_split], args.apex_transducer_joint, 
+
+            log_probs = model.joint(f_2[i*B_split:(i+1)*B_split], g_2[i*B_split:(i+1)*B_split], args.apex_transducer_joint,
                                     log_prob_lens[i*B_split:(i+1)*B_split], meta_data[i])
-            loss = loss_fn( log_probs, log_prob_lens[i*B_split:(i+1)*B_split], txt[i*B_split:(i+1)*B_split], 
+            loss = loss_fn( log_probs, log_prob_lens[i*B_split:(i+1)*B_split], txt[i*B_split:(i+1)*B_split],
                             txt_lens[i*B_split:(i+1)*B_split], meta_data[i])
 
             if args.enable_prefetch and train_loader is not None and i == 0:
                 # if train_loader is None, that means we are doing dummy runs,
                 # so we don't need to prefetch
                 train_loader.data_iterator().prefetch()
-            
-            loss /= (args.grad_accumulation_steps*args.batch_split_factor)
-
-            # copy loss to cpu asynchronously
-            copy_stream.wait_stream(torch.cuda.current_stream())
-            with torch.cuda.stream(copy_stream):
-                loss_cpu.copy_(loss.detach(), non_blocking=True)
-                if args.dist_lamb and i == 0:
-                    # only need to copy lr for once
-                    lr_cpu.copy_(optimizer._lr, non_blocking=True)
 
+            loss /= (args.grad_accumulation_steps*args.batch_split_factor)
             del log_probs
+            loss.backward()
 
-            if args.dist_lamb:
-                grad_scaler.scale(loss).backward()
-            else:
-                with amp.scale_loss(loss, optimizer) as scaled_loss:
-                    scaled_loss.backward()
-
-            copy_stream.synchronize()
-            if torch.isnan(loss_cpu).any():
+            if torch.isnan(loss).any():
                 raise Exception("Loss is NaN")
 
-            loss_item += loss_cpu.item()
-        
+            loss_item += loss.item()
+
         f.backward(f_2.grad)
         g.backward(g_2.grad)
 
-        return loss_item, lr_cpu.item()
+        return loss_item,
+
+def apply_model_config(args, cfg):
+    cfg['rnnt'] = {}
+    cfg['rnnt']['in_feats'] = 256
+    cfg['rnnt']['enc_n_hid'] = args.enc_n_hid
+    cfg['rnnt']['enc_pre_rnn_layers'] = args.enc_pre_rnn_layers
+    cfg['rnnt']['enc_post_rnn_layers'] = args.enc_post_rnn_layers
+    cfg['rnnt']['enc_stack_time_factor'] = args.enc_stack_time_factor
+    cfg['rnnt']['enc_dropout'] = args.enc_dropout
+    cfg['rnnt']['pred_n_hid'] = args.pred_n_hid
+    cfg['rnnt']['pred_rnn_layers'] = args.pred_rnn_layers
+    cfg['rnnt']['pred_dropout'] = args.pred_dropout
+    cfg['rnnt']['joint_n_hid'] = args.joint_n_hid
+    cfg['rnnt']['joint_dropout'] = args.joint_dropout
+    cfg['rnnt']['forget_gate_bias'] = 1.0
+    cfg['rnnt']['decoupled_rnns'] = True
 
 def main():
     args = parse_args()
-    logging.configure_logger(args.output_dir, 'RNNT')
-    logging.log_start(logging.constants.INIT_START)
-
-    assert(torch.cuda.is_available())
-    assert args.prediction_frequency is None or args.prediction_frequency % args.log_frequency == 0
-
-    torch.backends.cudnn.benchmark = args.cudnn_benchmark
 
     # set up distributed training
-    multi_gpu = args.dist_lamb or (int(os.environ.get('WORLD_SIZE', 1)) > 1)
-    if multi_gpu:
-        torch.cuda.set_device(args.local_rank)
-        dist.init_process_group(backend='nccl', init_method='env://')
-        world_size = dist.get_world_size()
-        print_once(f'Distributed training with {world_size} GPUs\n')
+    if args.dist or (int(os.environ.get('WORLD_SIZE', 1)) > 1):
+        dist.init_distributed(backend=args.dist_backend)
+        world_size = dist.my_size
     else:
         world_size = 1
 
+    if world_size > 1:
+        torch.distributed.barrier()
+    logging.configure_logger(args.output_dir, 'RNNT', dist.my_local_rank)
+    if world_size > 1:
+        torch.distributed.barrier()
+
+    logging.log_start(logging.constants.INIT_START)
+
+    assert args.prediction_frequency is None or args.prediction_frequency % args.log_frequency == 0
+
     if args.seed is not None:
         logging.log_event(logging.constants.SEED, value=args.seed)
         torch.manual_seed(args.seed + args.local_rank)
@@ -383,13 +363,14 @@ def main():
         random.seed(args.seed + args.local_rank)
         # np_rng is used for buckets generation, and needs the same seed on every worker
         sampler_seed = args.seed
-        if multi_gpu:
-            dali_seed = args.seed + dist.get_rank()
+        if world_size > 1:
+            dali_seed = args.seed + dist.my_rank
         else:
             dali_seed = args.seed
 
     init_log(args)
     cfg = config.load(args.model_config)
+    apply_model_config(args, cfg)
     config.apply_duration_flags(cfg, args.max_duration)
 
     assert args.grad_accumulation_steps >= 1
@@ -400,12 +381,11 @@ def main():
         assert batch_size % args.batch_split_factor == 0, f'{batch_size} % {args.batch_split_factor} != 0'
         assert args.dist_lamb, "dist LAMB must be used when batch split is enabled"
 
-    num_nodes = os.environ.get('SLURM_JOB_NUM_NODES', 1)
     logging.log_event(logging.constants.SUBMISSION_BENCHMARK, value=logging.constants.RNNT)
-    logging.log_event(logging.constants.SUBMISSION_ORG, value='NVIDIA')
+    logging.log_event(logging.constants.SUBMISSION_ORG, value='Intel')
     logging.log_event(logging.constants.SUBMISSION_DIVISION, value=logging.constants.CLOSED) # closed or open
     logging.log_event(logging.constants.SUBMISSION_STATUS, value=logging.constants.ONPREM) # on-prem/cloud/research
-    logging.log_event(logging.constants.SUBMISSION_PLATFORM, value=f'{num_nodes}xSUBMISSION_PLATFORM_PLACEHOLDER')
+    logging.log_event(logging.constants.SUBMISSION_PLATFORM, value='CPU')
 
     # set up the model
     tokenizer_kw = config.tokenizer(cfg)
@@ -415,26 +395,22 @@ def main():
     logging.log_event(logging.constants.MODEL_WEIGHTS_INITIALIZATION_SCALE, value=args.weights_init_scale)
     if args.fuse_relu_dropout:
         rnnt_config["fuse_relu_dropout"] = True
-    if args.apex_transducer_joint is not None:
-        rnnt_config["apex_transducer_joint"] = args.apex_transducer_joint
+    # if args.apex_transducer_joint is not None:
+    #     rnnt_config["apex_transducer_joint"] = args.apex_transducer_joint
     if args.weights_init_scale is not None:
         rnnt_config['weights_init_scale'] = args.weights_init_scale
     if args.hidden_hidden_bias_scale is not None:
         rnnt_config['hidden_hidden_bias_scale'] = args.hidden_hidden_bias_scale
     if args.multilayer_lstm:
         rnnt_config["decoupled_rnns"] = False
-    if args.apex_mlp:
-        rnnt_config["apex_mlp"] = True
-    enc_stack_time_factor = rnnt_config["enc_stack_time_factor"]
+    if args.use_ipex:
+        rnnt_config['use_ipex'] = True
+    rnnt_config['rnn_type'] = args.rnn_type
+    # if args.apex_mlp:
+    #     rnnt_config["apex_mlp"] = True
     model = RNNT(n_classes=tokenizer.num_labels + 1, **rnnt_config)
-    model.cuda()
     blank_idx = tokenizer.num_labels
-    if args.apex_transducer_loss == None:
-        loss_fn = RNNTLoss(blank_idx=blank_idx)
-    else:
-        if args.apex_transducer_loss == "fp16":
-            assert args.amp_level in [1, 2], "model in FP32 and loss in FP16 is not a valid use case"
-        loss_fn = apexTransducerLoss(blank_idx, args.apex_transducer_loss, packed_input=args.apex_transducer_joint=="pack")
+    loss_fn = RNNTLoss(blank_idx=blank_idx)
 
     # set up evaluation
     logging.log_event(logging.constants.EVAL_MAX_PREDICTION_SYMBOLS, value=args.max_symbol_per_sample)
@@ -448,63 +424,33 @@ def main():
     print_once(f'Model size: {num_weights(model) / 10**6:.1f}M params\n')
 
     if args.ema > 0:
-        ema_model = copy.deepcopy(model).cuda()
+        ema_model = copy.deepcopy(model)
     else:
         ema_model = None
     logging.log_event(logging.constants.MODEL_EVAL_EMA_FACTOR, value=args.ema)
-    
+
     # set up optimization
     opt_eps=1e-9
-    if args.dist_lamb:
-        model.half()
-        initial_lrs = args.lr * torch.tensor(1, device='cuda', dtype=torch.float)
-        kw = {  'params': model.parameters(), 
-                'lr': initial_lrs,
-                'weight_decay': args.weight_decay,
-                'eps': opt_eps,
-                'betas': (args.beta1, args.beta2),
-                'max_grad_norm': args.clip_norm,
-                'overlap_reductions': True,
-                'dwu_group_size': args.dwu_group_size,
-                'use_nvlamb': False,
-                "dwu_num_blocks": 2,
-                "dwu_num_chunks": 2,
-                "dwu_num_rs_pg": 1,
-                "dwu_num_ar_pg": 1,
-                "dwu_num_ag_pg": 2,
-                "bias_correction": True
-                }
-        optimizer = DistributedFusedLAMB(**kw)
-        # Prevent following communicators to lock the tree
-        os.environ["NCCL_SHARP_DISABLE"] = "1"
-        grad_scaler = GradScaler(init_scale=512)
-        print_once(f'Starting with LRs: {initial_lrs}')
-    else:
-        grad_scaler = None
-        kw = {'params': model.parameters(), 'lr': args.lr,
-              'max_grad_norm': args.clip_norm,
-              'weight_decay': args.weight_decay}
+    kw = {'params': model.parameters(), 'lr': args.lr,
+          'weight_decay': args.weight_decay}
 
-        initial_lrs = args.lr
+    initial_lrs = args.lr
 
-        print_once(f'Starting with LRs: {initial_lrs}')
-        optimizer = FusedLAMB(betas=(args.beta1, args.beta2), eps=opt_eps, **kw)
+    print_once(f'Starting with LRs: {initial_lrs}')
+    optimizer = ipex.optim._lamb.Lamb(betas=(args.beta1, args.beta2), eps=opt_eps, **kw)
 
-        model, optimizer = amp.initialize(
-            models=model,
-            optimizers=optimizer,
-            opt_level=f'O{args.amp_level}',
-            max_loss_scale=512.0,
-            cast_model_outputs=torch.float16 if args.amp_level == 2 else None)
+    # optimize with ipex
+    if args.use_ipex:
+        model, optimizer = ipex.optimize(model, optimizer=optimizer)
 
     adjust_lr = lambda step, epoch: lr_policy(
             step, epoch, initial_lrs, optimizer, steps_per_epoch=steps_per_epoch,
             warmup_epochs=args.warmup_epochs, hold_epochs=args.hold_epochs,
             min_lr=args.min_lr, exp_gamma=args.lr_exp_gamma, dist_lamb=args.dist_lamb)
-
-    if not args.dist_lamb and multi_gpu:
-        model = DistributedDataParallel(model)
-
+    # data parallel
+    if world_size > 1:
+        model = DDP(model, find_unused_parameters=True)
+    print_once(model)
     print_once('Setting up datasets...')
     (
         train_dataset_kw,
@@ -544,7 +490,7 @@ def main():
 
     class PermuteAudio(torch.nn.Module):
         def forward(self, x):
-            return (x[0].permute(2, 0, 1), *x[1:])
+            return (x[0].permute(2, 0, 1).contiguous(), *x[1:])
 
     if not train_specaugm_kw:
         train_specaugm = torch.nn.Identity()
@@ -577,9 +523,6 @@ def main():
     train_feat_proc = train_augmentations
     val_feat_proc   = val_augmentations
 
-    train_feat_proc.cuda()
-    val_feat_proc.cuda()
-
     train_preproc = Preproc(train_feat_proc, args.dist_lamb, args.apex_transducer_joint, args.batch_split_factor, cfg)
 
 
@@ -591,10 +534,10 @@ def main():
             raise NotImplementedError("Currently CUDA graph training does not work with batch split")
 
         max_seq_len = math.ceil(train_preproc.audio_duration_to_seq_len(
-                                                    cfg['input_train']['audio_dataset']['max_duration'], 
+                                                    cfg['input_train']['audio_dataset']['max_duration'],
                                                     after_subsampling=True,
                                                     after_stack_time=False
-                                                    ) 
+                                                    )
                         * cfg["input_train"]["audio_dataset"]["speed_perturbation"]["max_rate"])
 
         print_once(f'Graph with max_seq_len of %d' % max_seq_len)
@@ -605,50 +548,18 @@ def main():
 
     # capture CG for eval
     if type(args.batch_eval_mode) == str and args.batch_eval_mode.startswith("cg"):
-        max_seq_len = train_preproc.audio_duration_to_seq_len(  args.max_eval_sample_duration, 
-                                                                after_subsampling=True, 
-                                                                after_stack_time=True) 
+        max_seq_len = train_preproc.audio_duration_to_seq_len(  args.max_eval_sample_duration,
+                                                                after_subsampling=True,
+                                                                after_stack_time=True)
         dict_meta_data = {"batch": args.val_batch_size, "max_feat_len": max_seq_len}
         greedy_decoder.capture_cg_for_eval(ema_model, dict_meta_data)
 
-    # warm up optimizer
-    def optimizer_warm_up():
-        WARMUP_LEN = 8
-        feats = torch.ones(WARMUP_LEN , batch_size, rnnt_config["in_feats"], dtype=torch.float16, device='cuda')
-        feat_lens = torch.ones(batch_size, dtype=torch.int32, device='cuda') * WARMUP_LEN 
-        txt = torch.ones(batch_size, WARMUP_LEN , dtype=torch.int64, device='cuda')
-        txt_lens = torch.ones(batch_size, dtype=torch.int32, device='cuda') * WARMUP_LEN 
-        dict_meta_data = train_preproc.get_packing_meta_data(feats.size(0), feat_lens, txt_lens)
-        log_probs, log_prob_lens = model(feats, feat_lens, txt, txt_lens, dict_meta_data)
-        loss = loss_fn(log_probs, log_prob_lens, txt, txt_lens, dict_meta_data)
-        loss /= args.grad_accumulation_steps
-        del log_probs, log_prob_lens
-
-        assert not torch.isnan(loss).any(), "should not have happened"
-        if args.dist_lamb:
-            optimizer._lazy_init_stage1()
-            grad_scaler.scale(loss).backward()
-            optimizer._lazy_init_stage2()
-        else:
-            with amp.scale_loss(loss, optimizer) as scaled_loss:
-                scaled_loss.backward()
-        optimizer.zero_grad()   # Don't really want to do the update
-        if args.dist_lamb:
-            optimizer.complete_reductions()
-            optimizer.set_global_scale(grad_scaler._get_scale_async())
-            grad_scaler.step(optimizer)
-            grad_scaler.update()
-        else:
-            optimizer.step()
-
-    if args.dist_lamb:
-        optimizer_warm_up()
 
     logging.log_end(logging.constants.INIT_STOP)
-    if multi_gpu:
+    if world_size > 1:
         torch.distributed.barrier()
     logging.log_start(logging.constants.RUN_START)
-    if multi_gpu:
+    if world_size > 1:
         torch.distributed.barrier()
 
     if args.pre_sort_for_seq_split and not args.vectorized_sampler:
@@ -676,10 +587,10 @@ def main():
 
     eval_sampler = dali_sampler.SimpleSampler(val_dataset_kw)
 
-    train_sampler.sample(   file_names=args.train_manifests, 
+    train_sampler.sample(   file_names=args.train_manifests,
                             in_mem_file_list=args.in_mem_file_list,
                             tokenized_transcript=args.tokenized_transcript)
-    eval_sampler.sample(file_names=args.val_manifests, 
+    eval_sampler.sample(file_names=args.val_manifests,
                         in_mem_file_list=args.in_mem_file_list,
                         tokenized_transcript=args.tokenized_transcript)
 
@@ -691,8 +602,8 @@ def main():
         synthetic_seq_len = [args.synthetic_audio_seq_len, args.synthetic_text_seq_len]
     else:
         raise Exception("synthetic seq length for both text and audio need to be specified")
-    train_loader = DaliDataLoader(gpu_id=args.local_rank,
-                                  dataset_path=args.dataset_dir,
+    train_loader = DaliDataLoader(gpu_id=None,
+                                  dataset_path=args.train_dataset_dir,
                                   config_data=train_dataset_kw,
                                   config_features=train_features_kw,
                                   json_names=args.train_manifests,
@@ -715,8 +626,8 @@ def main():
                                   dont_use_mmap=args.dali_dont_use_mmap)
 
 
-    val_loader = DaliDataLoader(gpu_id=args.local_rank,
-                                dataset_path=args.dataset_dir,
+    val_loader = DaliDataLoader(gpu_id=None,
+                                dataset_path=args.valid_dataset_dir,
                                 config_data=val_dataset_kw,
                                 config_features=val_features_kw,
                                 json_names=args.val_manifests,
@@ -729,6 +640,7 @@ def main():
                                 seed=dali_seed,
                                 tokenized_transcript=args.tokenized_transcript,
                                 in_mem_file_list=args.in_mem_file_list,
+                                jit_tensor_formation=args.jit_tensor_formation,
                                 dont_use_mmap=args.dali_dont_use_mmap)
 
 
@@ -771,189 +683,145 @@ def main():
 
     # training loop
     model.train()
-    if args.multi_tensor_ema == True:
-        ema_model_weight_list, model_weight_list, overflow_buf_for_ema = init_multi_tensor_ema(optimizer, model, ema_model, args.ema_update_type)
-
-    copy_stream = torch.cuda.Stream()
-    pred_stream = torch.cuda.Stream()
-    def buffer_pre_allocation():
-        max_seq_len = math.ceil(train_preproc.audio_duration_to_seq_len(
-                                                    cfg['input_train']['audio_dataset']['max_duration'], 
-                                                    after_subsampling=False,
-                                                    after_stack_time=False
-                                                    ) 
-                        * cfg["input_train"]["audio_dataset"]["speed_perturbation"]["max_rate"])
-        max_txt_len = train_loader.data_iterator().max_txt_len
-        print_once(f'Pre-allocate buffer with max_seq_len of %d and max_txt_len of %d' % (max_seq_len, max_txt_len))
-        audio = torch.ones(batch_size, cfg["input_val"]["filterbank_features"]["n_filt"], max_seq_len, dtype=torch.float32, device='cuda')
-        audio_lens = torch.ones(batch_size, dtype=torch.int32, device='cuda') * max_seq_len
-        txt = torch.ones(batch_size, max_txt_len, dtype=torch.int64, device='cuda')
-        txt_lens = torch.ones(batch_size, dtype=torch.int32, device='cuda') * max_txt_len
-        feats, feat_lens = train_feat_proc([audio, audio_lens])
-        if args.dist_lamb:
-            feats = feats.half()
-
-        meta_data = []
-        B_split = batch_size // args.batch_split_factor
-        for i in range(args.batch_split_factor):
-            meta_data.append(train_preproc.get_packing_meta_data(   feats.size(0), 
-                                                                    feat_lens[i*B_split:(i+1)*B_split], 
-                                                                    txt_lens[i*B_split:(i+1)*B_split]))
-
-        train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_lens, optimizer, 
-                    grad_scaler, meta_data, None, rnnt_graph, copy_stream, pred_stream)
-
-
-    if args.buffer_pre_alloc:
-        buffer_pre_allocation()
 
     training_start_time = time.time()
     training_utts = 0
-    for epoch in range(start_epoch + 1, args.epochs + 1):
+    with torch.profiler.profile(
+        schedule=torch.profiler.schedule(wait=0, warmup=0, active=2, repeat=1, skip_first=5),
+        on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(args.output_dir, 'trace')),
+        with_modules=True) as prof:
+        for epoch in range(start_epoch + 1, args.epochs + 1):
 
-        logging.log_start(logging.constants.BLOCK_START,
-                          metadata=dict(first_epoch_num=epoch,
-                                        epoch_count=1))
-        logging.log_start(logging.constants.EPOCH_START,
-                          metadata=dict(epoch_num=epoch))
+            logging.log_start(logging.constants.BLOCK_START,
+                              metadata=dict(first_epoch_num=epoch,
+                                            epoch_count=1))
+            logging.log_start(logging.constants.EPOCH_START,
+                              metadata=dict(epoch_num=epoch))
 
-        epoch_utts = 0
-        accumulated_batches = 0
-        epoch_start_time = time.time()
-        
+            epoch_utts = 0
+            accumulated_batches = 0
+            epoch_start_time = time.time()
 
-        if args.enable_prefetch:
-            train_loader.data_iterator().prefetch()
 
-        step_start_time = time.time()
+            if args.enable_prefetch:
+                train_loader.data_iterator().prefetch()
 
-        for batch in train_loader:
-            if accumulated_batches == 0:
-                if not args.dist_lamb:
+            step_start_time = time.time()
+
+            for batch in train_loader:
+                if accumulated_batches == 0:
                     optimizer.zero_grad()
+                    adjust_lr(step, epoch)
+                    step_utts = 0
+                    all_feat_lens = []
+                # feature proc
+                if args.enable_prefetch:
+                    # when prefetch is enabled, train_feat_proc at prefetch time
+                    feats, feat_lens, txt, txt_lens = batch
+                    meta_data = train_preproc.meta_data
                 else:
-                    optimizer.zero_grad(set_to_none=True)
-                adjust_lr(step, epoch)
-                step_utts = 0
-                all_feat_lens = []
+                    audio, audio_lens, txt, txt_lens = batch
+                    feats, feat_lens = train_feat_proc([audio, audio_lens])
+                    meta_data = []
+                    B_split = batch_size // args.batch_split_factor
+                    for i in range(args.batch_split_factor):
+                        meta_data.append(train_preproc.get_packing_meta_data(   feats.size(0),
+                                                                                feat_lens[i*B_split:(i+1)*B_split],
+                                                                                txt_lens[i*B_split:(i+1)*B_split]))
 
-            if args.enable_prefetch:
-                # when prefetch is enabled, train_feat_proc at prefetch time
-                feats, feat_lens, txt, txt_lens = batch
-                meta_data = train_preproc.meta_data
-            else:
-                audio, audio_lens, txt, txt_lens = batch
-                feats, feat_lens = train_feat_proc([audio, audio_lens])
-                if args.dist_lamb:
-                    feats = feats.half()
-                meta_data = []
-                B_split = batch_size // args.batch_split_factor
-                for i in range(args.batch_split_factor):
-                    meta_data.append(train_preproc.get_packing_meta_data(   feats.size(0), 
-                                                                            feat_lens[i*B_split:(i+1)*B_split], 
-                                                                            txt_lens[i*B_split:(i+1)*B_split]))
-                        
-            if args.enable_seq_len_stats:
-                all_feat_lens += feat_lens
-
-
-
-            loss_item, lr_item = train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_lens, optimizer, 
-                                    grad_scaler, meta_data, train_loader, rnnt_graph, copy_stream, pred_stream)
-
-
-            step_utts += txt_lens.size(0) * world_size
-            epoch_utts += txt_lens.size(0) * world_size
-            accumulated_batches += 1
-            if accumulated_batches % args.grad_accumulation_steps == 0:
-
-                if args.dist_lamb:
-                    optimizer.complete_reductions()
-                    optimizer.set_global_scale(grad_scaler._get_scale_async())
-                    grad_scaler.step(optimizer)
-                    grad_scaler.update()
+                if args.enable_seq_len_stats:
+                    all_feat_lens += feat_lens
 
-                else:
-                    optimizer.step()
 
-                if args.multi_tensor_ema == True:
-                    apply_multi_tensor_ema(model_weight_list, ema_model_weight_list, args.ema, overflow_buf_for_ema)
-                else:
-                    apply_ema(optimizer, ema_model, args.ema)
-
-                if step % args.log_frequency == 0:
-
-                    if args.prediction_frequency is None or step % args.prediction_frequency == 0:
-                        preds = greedy_decoder.decode(feats.half(), feat_lens)
-                        wer, pred_utt, ref = greedy_wer(
-                                preds,
-                                txt,
-                                txt_lens,
-                                tokenizer.detokenize)
-                        print_once(f'  Decoded:   {pred_utt[:90]}')
-                        print_once(f'  Reference: {ref[:90]}')
-                        wer = {'wer': 100 * wer}
-                    else:
-                        wer = {}
-
-                    step_time = time.time() - step_start_time
-                    step_start_time = time.time()
-                    dict_log = {'loss': loss_item,
-                                 **wer,  # optional entry
-                                'throughput': step_utts / step_time,
-                                'took': step_time,
-                                'lrate': optimizer._lr.item() if args.dist_lamb else optimizer.param_groups[0]['lr']} # TODO: eliminate sync
-
-                    if args.enable_seq_len_stats:
-                        dict_log["seq-len-min"] = min(all_feat_lens).item()
-                        dict_log["seq-len-max"] = max(all_feat_lens).item()
-
-                    log((epoch, step % steps_per_epoch or steps_per_epoch, steps_per_epoch),
-                        step, 'train', dict_log)
-
-
-
-                step += 1
-                accumulated_batches = 0
-                # end of step
-
-        logging.log_end(logging.constants.EPOCH_STOP,
-                        metadata=dict(epoch_num=epoch))
-
-        epoch_time = time.time() - epoch_start_time
-        log((epoch,), None, 'train_avg', {'throughput': epoch_utts / epoch_time,
-                                          'took': epoch_time})
-        # logging throughput for dashboard
-        logging.log_event(key='throughput', value= epoch_utts / epoch_time)
-
-        if epoch % args.val_frequency == 0:
-            wer = evaluate(epoch, step, val_loader, val_feat_proc,
-                           tokenizer.detokenize, ema_model, loss_fn,
-                           greedy_decoder, args.amp_level)
-
-            last_wer = wer
-            if wer < best_wer and epoch >= args.save_best_from:
-                checkpointer.save(model, ema_model, optimizer, epoch,
-                                  step, best_wer, is_best=True)
-                best_wer = wer
-
-        save_this_epoch = (args.save_frequency is not None and epoch % args.save_frequency == 0) \
-                       or (epoch in args.keep_milestones)
-        if save_this_epoch:
-            checkpointer.save(model, ema_model, optimizer, epoch, step, best_wer)
-
-        training_utts += epoch_utts
-        logging.log_end(logging.constants.BLOCK_STOP, metadata=dict(first_epoch_num=epoch))
-
-        if last_wer <= args.target:
-            logging.log_end(logging.constants.RUN_STOP, metadata={'status': 'success'})
-            print_once(f'Finished after {args.epochs_this_job} epochs.')
-            break
-        if 0 < args.epochs_this_job <= epoch - start_epoch:
-            print_once(f'Finished after {args.epochs_this_job} epochs.')
-            break
-        # end of epoch
 
+                loss_item= train_step( model, loss_fn, args, batch_size, feats, feat_lens, txt, txt_lens,
+                                        meta_data, train_loader, rnnt_graph)
+                step_utts += txt_lens.size(0) * world_size
+                epoch_utts += txt_lens.size(0) * world_size
+                accumulated_batches += 1
+                if accumulated_batches % args.grad_accumulation_steps == 0:
+
+                    optimizer.step()
+                    apply_ema(model, ema_model, args.ema)
+
+                    if step % args.log_frequency == 0:
+
+                        if args.prediction_frequency is None or step % args.prediction_frequency == 0:
+                            preds = greedy_decoder.decode(feats, feat_lens)
+                            wer, pred_utt, ref = greedy_wer(
+                                    preds,
+                                    txt,
+                                    txt_lens,
+                                    tokenizer.detokenize)
+                            print_once(f'  Decoded:   {pred_utt[:90]}')
+                            print_once(f'  Reference: {ref[:90]}')
+                            wer = {'wer': 100 * wer}
+                        else:
+                            wer = {}
+
+                        step_time = time.time() - step_start_time
+                        step_start_time = time.time()
+                        dict_log = {'loss': loss_item,
+                                     **wer,  # optional entry
+                                    'throughput': step_utts / step_time,
+                                    'took': step_time,
+                                    'lrate': optimizer._lr.item() if args.dist_lamb else optimizer.param_groups[0]['lr']} # TODO: eliminate sync
+
+                        if args.enable_seq_len_stats:
+                            dict_log["seq-len-min"] = min(all_feat_lens).item()
+                            dict_log["seq-len-max"] = max(all_feat_lens).item()
+
+                        log((epoch, step % steps_per_epoch or steps_per_epoch, steps_per_epoch),
+                            step, 'train', dict_log)
+
+
+
+                    step += 1
+                    accumulated_batches = 0
+                    # end of step
+                prof.step()
+
+            logging.log_end(logging.constants.EPOCH_STOP,
+                            metadata=dict(epoch_num=epoch))
+
+            epoch_time = time.time() - epoch_start_time
+            log((epoch,), None, 'train_avg', {'throughput': epoch_utts / epoch_time,
+                                              'took': epoch_time})
+            # logging throughput for dashboard
+            logging.log_event(key='throughput', value= epoch_utts / epoch_time)
+
+            if epoch % args.val_frequency == 0:
+                wer = evaluate(epoch, step, val_loader, val_feat_proc,
+                               tokenizer.detokenize, ema_model, loss_fn,
+                               greedy_decoder, args.amp_level)
+                last_wer = wer
+                if wer < best_wer and epoch >= args.save_best_from:
+                    checkpointer.save(model, ema_model, optimizer, epoch,
+                                      step, best_wer, is_best=True)
+                    best_wer = wer
+
+            save_this_epoch = (args.save_frequency is not None and epoch % args.save_frequency == 0) \
+                           or (epoch in args.keep_milestones)
+            if save_this_epoch:
+                checkpointer.save(model, ema_model, optimizer, epoch, step, best_wer)
+
+            training_utts += epoch_utts
+            logging.log_end(logging.constants.BLOCK_STOP, metadata=dict(first_epoch_num=epoch))
+
+            if last_wer <= args.target:
+                logging.log_end(logging.constants.RUN_STOP, metadata={'status': 'success'})
+                print_once(f'Finished after {args.epochs_this_job} epochs.')
+                break
+            if 0 < args.epochs_this_job <= epoch - start_epoch:
+                print_once(f'Finished after {args.epochs_this_job} epochs.')
+                break
+            # end of epoch
+            training_time = time.time() - training_start_time
+            if args.training_time_threshold is not None and training_time > args.training_time_threshold:
+                print_once('training time exceed threshold, stop training')
+                break
+
+    print_once(prof.key_averages().table(sort_by='self_cpu_time_total'))
 
     training_time = time.time() - training_start_time
     log((), None, 'train_avg', {'throughput': training_utts / training_time})
@@ -968,6 +836,10 @@ def main():
     flush_log()
     if args.save_at_the_end:
         checkpointer.save(model, ema_model, optimizer, epoch, step, best_wer)
+    if dist.my_rank == 0:
+            file = os.path.join(args.output_dir, 'metric.txt')
+            with open(file, 'w') as f:
+                f.writelines(str(last_wer))
 
 
 if __name__ == "__main__":
diff --git a/modelzoo/rnnt/pytorch/utils/download_librispeech.py b/modelzoo/rnnt/pytorch/utils/download_librispeech.py
index ad36ad4..d2a1545 100644
--- a/modelzoo/rnnt/pytorch/utils/download_librispeech.py
+++ b/modelzoo/rnnt/pytorch/utils/download_librispeech.py
@@ -43,12 +43,11 @@ df = pd.read_csv(args.csv, delimiter=',')
 if not args.skip_download:
     for url in df.url:
         fname = url.split('/')[-1]
-        print("Downloading %s:" % fname)
+        print(f"Downloading {fname}")
         download_file(url=url, dest_folder=args.dest, fname=fname)
 else:
     print("Skipping file download")
 
-
 if not args.skip_checksum:
     for index, row in df.iterrows():
         url = row['url']
@@ -61,12 +60,11 @@ if not args.skip_checksum:
 else:
     print("Skipping checksum")
 
-
 if not args.skip_extract:
     for url in df.url:
         fname = url.split('/')[-1]
         fpath = os.path.join(args.dest, fname)
-        print("Decompressing %s:" % fpath)
+        print(f"Decompressing {fpath}")
         extract(fpath=fpath, dest_folder=args.e)
 else:
     print("Skipping file extraction")
diff --git a/modelzoo/rnnt/pytorch/utils/download_utils.py b/modelzoo/rnnt/pytorch/utils/download_utils.py
index e881388..2765ca5 100644
--- a/modelzoo/rnnt/pytorch/utils/download_utils.py
+++ b/modelzoo/rnnt/pytorch/utils/download_utils.py
@@ -42,7 +42,6 @@ def download_file(url, dest_folder, fname, overwrite=False):
                            unit='MB', desc=fpath, leave=True)
         for chunk in chunks:
             fp.write(chunk)
-
     os.rename(tmp_fpath, fpath)
 
 
diff --git a/modelzoo/rnnt/pytorch/utils/preprocessing_utils.py b/modelzoo/rnnt/pytorch/utils/preprocessing_utils.py
index 15605ce..ef006da 100644
--- a/modelzoo/rnnt/pytorch/utils/preprocessing_utils.py
+++ b/modelzoo/rnnt/pytorch/utils/preprocessing_utils.py
@@ -15,7 +15,7 @@
 #!/usr/bin/env python
 import os
 import multiprocessing
-import librosa
+# import librosa
 import functools
 
 import sox
@@ -28,7 +28,6 @@ def preprocess(data, input_dir, dest_dir, target_sr=None, speed=None,
     speed = speed or []
     speed.append(1)
     speed = list(set(speed))  # Make uniqe
-
     input_fname = os.path.join(input_dir,
                                data['input_relpath'],
                                data['input_fname'])
@@ -40,7 +39,6 @@ def preprocess(data, input_dir, dest_dir, target_sr=None, speed=None,
     output_dict = {}
     output_dict['transcript'] = data['transcript'].lower().strip()
     output_dict['files'] = []
-
     fname = os.path.splitext(data['input_fname'])[0]
     for s in speed:
         output_fname = fname + '{}.wav'.format('' if s==1 else '-{}'.format(s))
