{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RecDP LLM - fuzzy deduplication\n",
        "\n",
        "Near Dedup is to Detect duplicated documents and output as a duplicates list.\n",
        "\n",
        "* Step 1. We use DataSketch minHash as the base algorithm to calculate (hash, band_id) pair for each documents.\n",
        "\n",
        "* Step 2. We use Spark and SlimPajama connected component detect documents who sharing the same (hash, band_id) pair.\n",
        "\n",
        "* Step 3(Optional). We apply the duplication list to original file to elimate duplicated documents.\n",
        "\n",
        "\n",
        "### We support two types of input and output:\n",
        "\n",
        "use case 1:\n",
        "* Expect Input format: a folder of *.jsonl.\n",
        "* Expect Output format: a folder of *.jsonl after reduction.\n",
        "\n",
        "use case 2:\n",
        "* Expect Input format: spark dataframe.\n",
        "* Expect Output format: spark dataframe."
      ],
      "metadata": {
        "id": "NUdnJWOJLGvF"
      },
      "id": "NUdnJWOJLGvF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get started"
      ],
      "metadata": {
        "id": "C1FFwhr3MwA1"
      },
      "id": "C1FFwhr3MwA1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install pyrecdp and dependencies"
      ],
      "metadata": {
        "id": "tMaSNIKIMz3T"
      },
      "id": "tMaSNIKIMz3T"
    },
    {
      "cell_type": "code",
      "source": [
        "! DEBIAN_FRONTEND=noninteractive apt-get install -y openjdk-8-jre\n",
        "! pip install pyrecdp --pre\n",
        "# ! pip install 'git+https://github.com/intel/e2eAIOK.git#egg=pyrecdp&subdirectory=RecDP'"
      ],
      "metadata": {
        "id": "ro4yOk5IMvRY"
      },
      "id": "ro4yOk5IMvRY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. prepare your own data"
      ],
      "metadata": {
        "id": "pnHK2VTvM5rg"
      },
      "id": "pnHK2VTvM5rg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736fb211-dbe6-4ca9-a1b1-db2cff2d287a",
      "metadata": {
        "id": "736fb211-dbe6-4ca9-a1b1-db2cff2d287a"
      },
      "outputs": [],
      "source": [
        "%mkdir -p /content/test_data\n",
        "%cd /content/test_data\n",
        "file_names = ['NIH_sample.jsonl']\n",
        "file_list = [f\"https://raw.githubusercontent.com/intel/e2eAIOK/main/RecDP/tests/data/llm_data/PILE/{i}\" for i in file_names]\n",
        "!wget -P /content/test_data {\" \".join(file_list)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. fuzzy deduplicate (seperate detection and reduction)"
      ],
      "metadata": {
        "id": "J-125gFtNyOg"
      },
      "id": "J-125gFtNyOg"
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /content/test_data"
      ],
      "metadata": {
        "id": "vkGRn9uJN4B0",
        "outputId": "8aa9d021-99f5-4de8-932e-2fccb933f70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vkGRn9uJN4B0",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIH_sample.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = [\"/content/test_data/NIH_sample.jsonl\"]\n",
        "dup_dir = \"/content/fuzzy_dedup\"\n",
        "\n",
        "ngram_size = 13 # num_words to do compare\n",
        "num_perm = 256 # num_permutation to hold this whole document.\n",
        "# ranges and bands will impact the probabilities of false positive and false negative.\n",
        "ranges = 13\n",
        "bands = 9\n",
        "\n",
        "from pyrecdp.primitives.llmutils import near_dedup\n",
        "import pandas as pd\n",
        "\n",
        "near_dedup(data_files, dup_dir, ngram_size, num_perm, bands, ranges)\n",
        "\n",
        "## Validate codes\n",
        "import pickle\n",
        "print(\"Detected duplications are:\")\n",
        "connects, num_pair, index_list = pickle.load(open(f\"{dup_dir}/connected_components.pickle\", 'rb'))\n",
        "connected_component_reverse = [[index_list[j] for j in i] for i in connects]\n",
        "connected_component_reverse"
      ],
      "metadata": {
        "id": "SMK9KcV4N7al",
        "outputId": "7e93835c-a7d0-4612-e9aa-7c6f452d5bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SMK9KcV4N7al",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will assign 1 cores and 10386 M memory for spark\n",
            "per core memory size is 10.143 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
            "Load data with RowID started ...\n",
            "Load data with RowID took 2.592839109000124 sec\n",
            "num_bands is 9, ranges is 13\n",
            "generate minHashLsh started ...\n",
            "generate minHashLsh took 63.16276241699961 sec\n",
            "generate_connected_components all started ...\n",
            "Started graph building\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loop on file: 100%|██████████| 2/2 [00:00<00:00, 5312.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the set of duplicates: 13 0.01398324966430664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 71275.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of connected components: 7 0.02257537841796875\n",
            "Graph generated duplicates list!!! 0.022832632064819336\n",
            "generate_connected_components all took 0.02385874599985982 sec\n",
            "generate_duplicates_dict all started ...\n",
            "Processing duplicates!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 46163.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of duplicate documents that will be removed: 12\n",
            "generate_duplicates_dict all took 0.009320153000317077 sec\n",
            "Completed!!\n",
            "    total processed 10000 documents\n",
            "    total detected 12 duplicated documents\n",
            "    duplicate ratio is 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected duplications are:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['NIH_sample.jsonl@1769', 'NIH_sample.jsonl@1764', 'NIH_sample.jsonl@1765'],\n",
              " ['NIH_sample.jsonl@245',\n",
              "  'NIH_sample.jsonl@243',\n",
              "  'NIH_sample.jsonl@246',\n",
              "  'NIH_sample.jsonl@248',\n",
              "  'NIH_sample.jsonl@244',\n",
              "  'NIH_sample.jsonl@247'],\n",
              " ['NIH_sample.jsonl@1191', 'NIH_sample.jsonl@1190'],\n",
              " ['NIH_sample.jsonl@7746', 'NIH_sample.jsonl@7745'],\n",
              " ['NIH_sample.jsonl@9026', 'NIH_sample.jsonl@8561'],\n",
              " ['NIH_sample.jsonl@8200', 'NIH_sample.jsonl@7354'],\n",
              " ['NIH_sample.jsonl@3037', 'NIH_sample.jsonl@3024']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply duplication list to original data to remove duplication\n",
        "\n",
        "from pyrecdp.primitives.llmutils import shrink_document_MP\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/test_data/\"\n",
        "dup_dir = \"/content/fuzzy_dedup\"\n",
        "dup_dict = os.path.join(dup_dir, \"duplicates.pickle\")\n",
        "out_dir = os.path.join(dup_dir, \"output\")\n",
        "\n",
        "shrink_document_MP(data_dir, dup_dict, out_dir)\n",
        "\n",
        "# validate\n",
        "print(\"\\nReduction is completed, checkout the new jsonl filesize\")\n",
        "! ls \"/content/fuzzy_dedup/output\"\n",
        "! cat /content/fuzzy_dedup/output/* | wc -l"
      ],
      "metadata": {
        "id": "81NiajWFTy2n",
        "outputId": "d8efdf8d-6561-411a-b190-acf8d289b331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "81NiajWFTy2n",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting to 1 for number of processes\n",
            "parallelize with 1 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reduction is completed, checkout the new jsonl filesize\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIH_sample.jsonl\n",
            "9988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual compare based on detection\n",
        "print(f\"First duplication is {connected_component_reverse[0]}\")\n",
        "print(\"You'll see the similar content in above documents\")\n",
        "\n",
        "for f_id in connected_component_reverse[0]:\n",
        "  print(f_id)\n",
        "  f_name, rid = f_id.split(\"@\")\n",
        "  ! sed -n {rid}p {f_name}\n"
      ],
      "metadata": {
        "id": "jFOflHdsQ2XI",
        "outputId": "88c1b3aa-8999-4563-cf4a-a65f5efbf0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jFOflHdsQ2XI",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First duplication is ['NIH_sample.jsonl@1769', 'NIH_sample.jsonl@1764', 'NIH_sample.jsonl@1765']\n",
            "You'll see the similar content in above documents\n",
            "NIH_sample.jsonl@1769\n",
            "{\"meta\": {\"APPLICATION_ID\": 2044519}, \"text\": \"The overall aim of study is to test the \\\"matching hypothesis\\\" that alcohol treatment effectiveness can be increased by assigning clients with certain characteristics to particular treatments. The present application proposes to continue work initiated and conducted over the past five years. The specific aims of study are: to test primary and secondary a priori matching hypotheses over the course of 15 months of follow-up; to conduct psychometric and other analyses of patient, treatment process, and outcome variables to test these matching hypotheses; to examine alternative analytic strategies and variables for testing matching; and to determine the extent to which matching effects persist over a three year period following treatment completion. Data sets collected from the Project MATCH outpatient (N=954) and aftercare (N=774) randomized clinical trial studies will be analyzed to achieve these aims. Clients randomly assigned to one of three treatments, Twelve Step, Cognitive Behavioral, or Motivational Enhancement, have been assessed prior to treatment on baseline and matching variables and followed, at 3 month intervals, for 15 months from treatment assignment to measure their alcohol consumption and other dimensions of outcome. To test longer term matching effects, clients in the outpatient arm of the study will be contacted and assessed at 39 months after treatment initiation. In year 08 these data will be analyzed to address this question. It is anticipated that study results will impact on alcohol treatment research and delivery for the next decade. The Project MATCH Steering Committee, following discussion with NIAAA Staff, has chosen to submit a generic proposal across each of the CRUs, reflecting the common protocol and cooperative process that has guided the project across its first five years of operation. However, each CRU has provided site-specific information in the budget, budget justifications, listing of key personnel, consultant/consortium agreements, and human subjects sections of their respective proposals.\"}\n",
            "NIH_sample.jsonl@1764\n",
            "{\"meta\": {\"APPLICATION_ID\": 2044494}, \"text\": \"The overall aim of this study is to test the \\\"matching hypothesis\\\" that alcohol treatment effectiveness can be increased by assigning clients with certain characteristics to particular treatments. The present application proposes to continue work initiated and conducted over the past five years. The specific aims of study are: to test primary and secondary a priori matching hypotheses over the course of 15 months of follow-up; to conduct psychometric and other analyses of patient, treatment process, and outcome variables to test these matching hypotheses; to examine alternative analytic strategies and variables for testing matching- and to determine the extent to which matching effects persist over a three year period following treatment completion. Data sets collected from the Project MATCH outpatient (N=954) and aftercare (N=774) randomized clinical trial studies will be analyzed to achieve these aims. Clients randomly assigned to one of three treatments (Twelve Step, Cognitive Behavioral, or Motivational Enhancement) have been assessed prior to treatment on baseline and matching variables and followed, at 3 month intervals, for 15 months from treatment assignment to measure their alcohol consumption and other dimensions of outcome. To test longer term matching effects clients in the outpatient arm of the study will be contacted and assessed at 39 months after treatment initiation. In year 08 these data will be analyzed to address this question. It is anticipated that study results will impact on alcohol treatment research and delivery for the next decade. Finally, it should be noted that the Project MATCH Steering Committee, following discussion with NIAAA Staff, has chosen to submit a generic proposal across each of the CRUs reflecting the common protocol and cooperative process that has guided the project across its first five years of operation. However, each CRU has provided site-specific information in the budget, budget justifications, listing of key personnel, consultant/consortium agreements, and human subjects sections of their respective proposals.\"}\n",
            "NIH_sample.jsonl@1765\n",
            "{\"meta\": {\"APPLICATION_ID\": 2044499}, \"text\": \"The overall aim of study is to test the \\\"matching hypothesis\\\" that alcohol treatment effectiveness can be increased by assigning clients with certain characteristics to particular treatments. The present application proposes to continue work initiated and conducted over the past five years. The specific aims of study are: to test primary and secondary a priori matching hypotheses over the course of 15 months of follow-up; to conduct psychometric and other analyses of patient, treatment process, and outcome variables to test these matching hypotheses; to examine alternative analytic strategies and variables for testing matching; and to determine the extent to which matching effects persist over a three year period following treatment completion. Data sets collected from the Project MATCH outpatient (N=954) and aftercare (N=774) randomized clinical trial studies will be analyzed to achieve these aims. Clients randomly assigned to one of three treatments, Twelve Step, Cognitive Behavioral, or Motivational Enhancement, have been assessed prior to treatment on baseline and matching variables and followed, at 3 month intervals, for 15 months from treatment assignment to measure their alcohol consumption and other dimensions of outcome. To test longer term matching effects clients in the outpatient arm of the study will be contacted and assessed at 39 months after treatment initiation. In year 08 these data will be analyzed to address this question. It is anticipated that study results will impact on alcohol treatment research and delivery for the next decade. The Project MATCH Steering Committee, following discussion with NIAAA Staff, has chosen to submit a generic proposal across each of the CRUs, reflecting the common protocol and cooperative process that has guided the project across its first five years of operation. However, each CRU has provided site-specific information in the budget, budget justifications, listing of key personnel, consultant/consortium agreements, and human subjects sections of their respective proposals.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. fuzzy deduplicate (unified detection and reduction)"
      ],
      "metadata": {
        "id": "okSSgtKDTnuE"
      },
      "id": "okSSgtKDTnuE"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyrecdp.core import SparkDataProcessor\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "import pyspark.sql.functions as F\n",
        "from pyrecdp.primitives.llmutils import near_dedup_spk\n",
        "\n",
        "data_files = [\"/content/test_data/NIH_sample.jsonl\"]\n",
        "dup_dir = \"/content/fuzzy_dedup_spark\"\n",
        "\n",
        "ngram_size = 13\n",
        "num_perm = 256\n",
        "bands = 9\n",
        "ranges = 13\n",
        "rdp = SparkDataProcessor()\n",
        "spark = rdp.spark\n",
        "schema = StructType([\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"meta\", StringType(), True)\n",
        "])\n",
        "spark_df = spark.read.text(data_files)\n",
        "spark_df = spark_df.withColumn('jsonData', F.from_json(F.col('value'), schema)).select(\"jsonData.*\")\n",
        "print(\"input is \")\n",
        "spark_df.show()\n",
        "print(f\"Total num_rows of input is {spark_df.count()}\")\n",
        "\n",
        "ret_df = near_dedup_spk(spark_df, ngram_size, num_perm, bands, ranges)\n",
        "\n",
        "print(\"output is\")\n",
        "ret_df.show()\n",
        "print(f\"Total num_rows of output is {ret_df.count()}\")\n",
        "del rdp"
      ],
      "metadata": {
        "id": "HQ_Ax_wxTnC9",
        "outputId": "98031a36-2956-434d-a576-b86bd782ea5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HQ_Ax_wxTnC9",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will assign 1 cores and 10386 M memory for spark\n",
            "per core memory size is 10.143 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
            "input is \n",
            "+--------------------+--------------------+\n",
            "|                text|                meta|\n",
            "+--------------------+--------------------+\n",
            "|The National Dome...|{\"APPLICATION_ID\"...|\n",
            "|The Office of Pla...|{\"APPLICATION_ID\"...|\n",
            "|Improving outcome...|{\"APPLICATION_ID\"...|\n",
            "|This project is i...|{\"APPLICATION_ID\"...|\n",
            "|The CCDF Policies...|{\"APPLICATION_ID\"...|\n",
            "|The overall purpo...|{\"APPLICATION_ID\"...|\n",
            "|This contract wil...|{\"APPLICATION_ID\"...|\n",
            "|The purpose of th...|{\"APPLICATION_ID\"...|\n",
            "|The purpose of th...|{\"APPLICATION_ID\"...|\n",
            "|Intimate partner ...|{\"APPLICATION_ID\"...|\n",
            "|ACF's Office of R...|{\"APPLICATION_ID\"...|\n",
            "|The Temporary Ass...|{\"APPLICATION_ID\"...|\n",
            "|Investing in Qual...|{\"APPLICATION_ID\"...|\n",
            "|Current developme...|{\"APPLICATION_ID\"...|\n",
            "|The proposed diss...|{\"APPLICATION_ID\"...|\n",
            "|As the US populat...|{\"APPLICATION_ID\"...|\n",
            "|Through employing...|{\"APPLICATION_ID\"...|\n",
            "|The proposed mixe...|{\"APPLICATION_ID\"...|\n",
            "|To better serve D...|{\"APPLICATION_ID\"...|\n",
            "|A long standing r...|{\"APPLICATION_ID\"...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Total num_rows of input is 10000\n",
            "num_bands is 9, ranges is 13\n",
            "generate minHashLsh started ...\n",
            "generate minHashLsh took 64.54865999799995 sec\n",
            "generate_connected_components => duplicates started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 69283.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate_connected_components => duplicates took 0.8337461669998447 sec\n",
            "deduplicate input data started ...\n",
            "deduplicate input data took 0.9185702700001457 sec\n",
            "Completed!!\n",
            "    total processed 10000 documents\n",
            "    total detected 12 duplicated documents, exact deduplicated counts is 12\n",
            "    duplicate ratio is 0.0012\n",
            "output is\n",
            "+--------------+--------------------+--------------------+\n",
            "|filename_docid|                text|                meta|\n",
            "+--------------+--------------------+--------------------+\n",
            "|   global_id@0|The National Dome...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@1|The Office of Pla...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@2|Improving outcome...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@3|This project is i...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@4|The CCDF Policies...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@5|The overall purpo...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@6|This contract wil...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@7|The purpose of th...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@8|The purpose of th...|{\"APPLICATION_ID\"...|\n",
            "|   global_id@9|Intimate partner ...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@10|ACF's Office of R...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@11|The Temporary Ass...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@12|Investing in Qual...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@13|Current developme...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@14|The proposed diss...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@15|As the US populat...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@16|Through employing...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@17|The proposed mixe...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@18|To better serve D...|{\"APPLICATION_ID\"...|\n",
            "|  global_id@19|A long standing r...|{\"APPLICATION_ID\"...|\n",
            "+--------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Total num_rows of output is 9988\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}