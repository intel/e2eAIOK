{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb0f0d6",
   "metadata": {},
   "source": [
    "# RecDP LLM - Downsize public dataset\n",
    "\n",
    "This notebook shows how to use RecDP tools and pipeline to downsize a public fine tuning dataset and calculate kinds of evaluation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba4a07",
   "metadata": {},
   "source": [
    "# Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a56be1",
   "metadata": {},
   "source": [
    "## 1. Install pyrecdp and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! DEBIAN_FRONTEND=noninteractive apt-get install -q -y openjdk-8-jre\n",
    "# ! pip install recdp\n",
    "! pip install 'git+https://github.com/intel/e2eAIOK.git#egg=pyrecdp&subdirectory=RecDP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de2779",
   "metadata": {},
   "source": [
    "## 2. Prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22acbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p content/test_data\n",
    "!cp ../../../tests/data/alpaca/alpaca_data_50.jsonl /content/test_data\n",
    "!cp ../../../tests/data/dolly/dolly_sample_50.parquet /content/test_data\n",
    "!cp ../../../tests/data/openorca/openorca_sample_50.parquet /content/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3886440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:09:07.594\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m219\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "[DatasetReader, PerfileSourcedJsonlReader, TextPrompt, PerfileParquetWriter]\n",
      "init ray with total mem of 162212234035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:11:13,934\tWARNING services.py:1889 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:11:54,463\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-12-04 03:11:57,227\tINFO read_api.py:406 -- To satisfy the requested parallelism of 144, each read task output is split into 144 smaller blocks.\n",
      "2023-12-04 03:11:57,254\tINFO dataset.py:2380 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-12-04 03:11:57,258\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON->SplitBlocks(144)] -> LimitOperator[limit=20]\n",
      "2023-12-04 03:11:57,260\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-04 03:11:57,262\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.'}\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n",
      "{'instruction': 'Identify the odd one out.', 'input': 'Twitter, Instagram, Telegram', 'output': 'Telegram'}\n",
      "{'instruction': 'Explain why the following fraction is equivalent to 1/4', 'input': '4/16', 'output': 'The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.'}\n",
      "{'instruction': 'Write a short story in third person narration about a protagonist who has to make an important career decision.', 'input': '', 'output': 'John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.'}\n",
      "{'instruction': 'Render a 3D model of a house', 'input': '', 'output': '<nooutput> This type of instruction cannot be fulfilled by a GPT model.'}\n",
      "{'instruction': 'Evaluate this sentence for spelling and grammar mistakes', 'input': 'He finnished his meal and left the resturant', 'output': 'He finished his meal and left the restaurant.'}\n",
      "{'instruction': 'How did Julius Caesar die?', 'input': '', 'output': 'Julius Caesar was assassinated by a group of up to 60 conspirators, led by Gaius Cassius Longinus and Marcus Junius Brutus, in the Senate House on the Ides of March (15 March) of 44 BC.'}\n",
      "{'instruction': 'What is the capital of France?', 'input': '', 'output': 'The capital of France is Paris.'}\n",
      "{'instruction': 'Generate a list of ten items a person might need for a camping trip', 'input': '', 'output': '1. Tent\\n2. Sleeping bags\\n3. Flashlight\\n4. Matches/lighter\\n5. Insect repellent\\n6. Extra clothes\\n7. First aid kit\\n8. Camp stove\\n9. Cooking utensils\\n10. Biodegradable soaps/shampoos'}\n",
      "{'instruction': 'Discuss the causes of the Great Depression', 'input': '', 'output': 'The Great Depression was a severe global economic downturn that lasted from 1929 to 1939. It was caused by several factors, including the stock market crash of 1929, structural weaknesses in the economy, low consumer spending, high unemployment, and a lack of confidence in the banking system. Other contributing factors included overproduction of commodities, reliance on borrowing to finance investments, agricultural overproduction, and a decrease in international trade.'}\n",
      "{'instruction': 'Classify the following into animals, plants, and minerals', 'input': 'Oak tree, copper ore, elephant', 'output': 'Oak tree: Plant\\n Copper ore: Mineral\\n Elephant: Animal'}\n",
      "{'instruction': 'Explain the use of word embeddings in Natural Language Processing', 'input': '', 'output': 'Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured. Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies. They allow for words to be processed as numerical values, giving machines an easier way to perform NLP tasks.'}\n",
      "{'instruction': 'Describe the function of a computer motherboard', 'input': '', 'output': 'A computer motherboard is the main circuit board in a computer. It is the backbone of the system, connecting all other components such as the processor, RAM, storage, graphics card and peripheral ports. It provides communication pathways between all of these components, allowing them to interact and work together. The motherboard is responsible for regulating power, and also houses slots for expansion cards, as well as ports for USB, audio, Ethernet, and other connection types.'}\n",
      "{'instruction': 'Reverse engineer this code to create a new version', 'input': 'def factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial', 'output': 'def factorialize(num):\\n  factorial = 1\\n  for i in range(num, 0, -1):\\n    factorial *= i\\n  \\n  return factorial'}\n",
      "{'instruction': 'Propose an ethical solution to the problem of data privacy', 'input': '', 'output': 'One ethical solution to the problem of data privacy is to implement an opt-in data collection policy. This means that companies cannot collect and use personal data without the explicit permission of the data owners. This approach gives users the power to determine which data companies can use and for what purpose. Companies should also be transparent about how they use the collected data, so that users can make informed decisions and have control over their own data.'}\n",
      "{'instruction': 'Generate three verbs that mean the same as \"to apologize\"', 'input': '', 'output': 'Acknowledge, express regret, make amends.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 03:11:58,479\tINFO read_api.py:406 -- To satisfy the requested parallelism of 144, each read task output is split into 144 smaller blocks.\n",
      "2023-12-04 03:11:58,495\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON->SplitBlocks(144)] -> TaskPoolMapOperator[Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)]\n",
      "2023-12-04 03:11:58,499\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-04 03:11:58,500\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error while sending or receiving.\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
       "    self.socket.sendall(command.encode(\"utf-8\"))\n",
       "ConnectionResetError: [Errno 104] Connection reset by peer\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error while sending or receiving.\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
       "    self.socket.sendall(command.encode(\"utf-8\"))\n",
       "ConnectionResetError: [Errno 104] Connection reset by peer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Closing down clientserver connection\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Closing down clientserver connection\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Exception while sending command.\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
       "    self.socket.sendall(command.encode(\"utf-8\"))\n",
       "ConnectionResetError: [Errno 104] Connection reset by peer\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
       "    response = connection.send_command(command)\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
       "    raise Py4JNetworkError(\n",
       "py4j.protocol.Py4JNetworkError: Error while sending\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Exception while sending command.\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
       "    self.socket.sendall(command.encode(\"utf-8\"))\n",
       "ConnectionResetError: [Errno 104] Connection reset by peer\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
       "    response = connection.send_command(command)\n",
       "  File \"/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
       "    raise Py4JNetworkError(\n",
       "py4j.protocol.Py4JNetworkError: Error while sending\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Closing down clientserver connection\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Closing down clientserver connection\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on alpaca_data_50.jsonl:   0%|          | 0/1 [00:00<?, ?it/s]2023-12-04 03:12:00,635\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON->SplitBlocks(144)] -> TaskPoolMapOperator[Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write]\n",
      "2023-12-04 03:12:00,636\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-04 03:12:00,637\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:alpaca, prompt_name:causal_llm_1, subset_name:None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54471)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54471)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54483)\u001b[0m Skipping writing empty dataset with UUID 9cc563c56c61472dadfed22e817182ea at ResumableTextPipeline_output-alpaca/alpaca_data_50.jsonl\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54461)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\u001b[32m [repeated 71x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54461)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\u001b[32m [repeated 71x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:12:18.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-04 03:12:18.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on alpaca_data_50.jsonl: 100%|██████████| 1/1 [00:17<00:00, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:12:18.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m425\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output-alpaca for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=54507)\u001b[0m Skipping writing empty dataset with UUID 9cc563c56c61472dadfed22e817182ea at ResumableTextPipeline_output-alpaca/alpaca_data_50.jsonl\u001b[32m [repeated 93x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:12:20.645\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m219\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "[DatasetReader, PerfileSourcedParquetReader, TextPrompt, PerfileParquetWriter]\n",
      "init ray with total mem of 162212234035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:14:26,261\tWARNING services.py:1889 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:15:06,725\tINFO worker.py:1642 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=59535) Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:259: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m   pq_ds.pieces, **prefetch_remote_args\n",
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:269: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m   self._pq_pieces = [_SerializedPiece(p) for p in pq_ds.pieces]\n",
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:270: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=59535)\u001b[0m   self._pq_paths = [p.path for p in pq_ds.pieces]\n",
      "2023-12-04 03:15:10,367\tINFO read_api.py:406 -- To satisfy the requested parallelism of 144, each read task output is split into 144 smaller blocks.\n",
      "ResumableTextPipeline, current on openorca_sample_50.parquet:   0%|          | 0/1 [00:00<?, ?it/s]2023-12-04 03:15:10,425\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet->SplitBlocks(144)] -> TaskPoolMapOperator[Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write]\n",
      "2023-12-04 03:15:10,426\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-04 03:15:10,428\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:openorca, prompt_name:causal_llm_1, subset_name:None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59532)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59532)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59532)\u001b[0m Skipping writing empty dataset with UUID 26cfaa1c1eac4a198b0dab09036e0abc at ResumableTextPipeline_output-openorca/openorca_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59470)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59470)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59532)\u001b[0m Skipping writing empty dataset with UUID 26cfaa1c1eac4a198b0dab09036e0abc at ResumableTextPipeline_output-openorca/openorca_sample_50.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:15:29.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-04 03:15:29.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on openorca_sample_50.parquet: 100%|██████████| 1/1 [00:18<00:00, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:15:29.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m425\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output-openorca for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=59510)\u001b[0m Skipping writing empty dataset with UUID 26cfaa1c1eac4a198b0dab09036e0abc at ResumableTextPipeline_output-openorca/openorca_sample_50.parquet\u001b[32m [repeated 92x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:15:32.103\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m219\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "recdp_promptsource: /host/mnt/DP_disk1/code/recllm/e2eAIOK/RecDP/pyrecdp/promptsource\n",
      "promptsource_templates_path: /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/promptsource/templates\n",
      "[DatasetReader, PerfileSourcedParquetReader, TextPrompt, PerfileParquetWriter]\n",
      "init ray with total mem of 162212234035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:17:37,761\tWARNING services.py:1889 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "2023-12-04 03:18:18,229\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:259: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m   pq_ds.pieces, **prefetch_remote_args\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:269: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m   self._pq_pieces = [_SerializedPiece(p) for p in pq_ds.pieces]\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/ray/data/datasource/parquet_datasource.py:270: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "\u001b[2m\u001b[36m(_get_reader pid=64482)\u001b[0m   self._pq_paths = [p.path for p in pq_ds.pieces]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=64482) Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 03:18:21,765\tINFO read_api.py:406 -- To satisfy the requested parallelism of 144, each read task output is split into 144 smaller blocks.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "ResumableTextPipeline, current on dolly_sample_50.parquet:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A2023-12-04 03:18:21,841\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet->SplitBlocks(144)] -> TaskPoolMapOperator[Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write]\n",
      "2023-12-04 03:18:21,842\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-04 03:18:21,844\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:dolly, prompt_name:causal_llm_1, subset_name:None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64424)\u001b[0m /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64424)\u001b[0m   warnings.warn(\"Setuptools is replacing distutils.\")\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64485)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:18:40.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-04 03:18:40.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ResumableTextPipeline, current on dolly_sample_50.parquet: 100%|██████████| 1/1 [00:18<00:00, 18.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 03:18:40.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m425\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output-dolly for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(Map(<lambda>)->Map(<lambda>)->MapBatches(<lambda>)->Map(<lambda>)->Write pid=64449)\u001b[0m Skipping writing empty dataset with UUID ed229cdea4a4496da3e49f5a6c3d44e3 at ResumableTextPipeline_output-dolly/dolly_sample_50.parquet\u001b[32m [repeated 86x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pyrecdp.LLM import TextPipeline, ResumableTextPipeline\n",
    "from pyrecdp.primitives.operations import *\n",
    "import os\n",
    "\n",
    "alpaca_prompt = ResumableTextPipeline()\n",
    "alpaca_prompt.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output-alpaca\"\n",
    "ops = [\n",
    "    JsonlReader(\"/content/test_data/alpaca_data_50.jsonl\"),\n",
    "    TextPrompt(dataset_name=\"alpaca\", prompt_name=\"causal_llm_1\"),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "alpaca_prompt.add_operations(ops)\n",
    "ret = alpaca_prompt.execute()\n",
    "del alpaca_prompt\n",
    "\n",
    "openorca_prompt = ResumableTextPipeline()\n",
    "openorca_prompt.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output-openorca\"\n",
    "ops = [\n",
    "    ParquetReader(\"/content/test_data/openorca_sample_50.parquet\"),\n",
    "    TextPrompt(dataset_name=\"openorca\", prompt_name=\"causal_llm_1\"),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "openorca_prompt.add_operations(ops)\n",
    "ret = openorca_prompt.execute()\n",
    "del openorca_prompt\n",
    "\n",
    "dolly_prompt = ResumableTextPipeline()\n",
    "dolly_prompt.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output-dolly\"\n",
    "ops = [\n",
    "    ParquetReader(\"/content/test_data/dolly_sample_50.parquet\"),\n",
    "    TextPrompt(dataset_name=\"dolly\", prompt_name=\"causal_llm_1\"),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "dolly_prompt.add_operations(ops)\n",
    "ret = dolly_prompt.execute()\n",
    "del dolly_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5affc7",
   "metadata": {},
   "source": [
    "## 3. Downsize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27020a1c",
   "metadata": {},
   "source": [
    "### 3.1 Pipeline on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee3486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set, use default value of /usr/lib/jvm/java-8-openjdk-amd64/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['psutil', 'tqdm', 'pyyaml', 'pandas', 'pyarrow', 'transformers', 'graphviz', 'requests', 'distro', 'pyspark==3.4.0', 'matplotlib', 'datasketch==1.5.9', 'ftfy==6.1.1', 'jsonlines==3.1.0', 'networkit==10.1', 'nltk==3.8.1', 'regex==2023.6.3', 'scipy==1.10.1', 'typer>=0.6.1', 'phonenumbers', 'fasttext==0.9.2', 'wget==3.2', 'alt-profanity-check==1.3.0', 'huggingface-hub', 'loguru==0.7.2', 'tabulate==0.9.0', 'sentencepiece', 'selectolax', 'spacy', 'torch', 'Faker', 'ray==2.7.1', 'loguru', 'detoxify', 'emoji==2.2.0', 'kenlm', 'rouge-score']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: alt-profanity-check==1.3.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (1.3.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: alt-profanity-check==1.3.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (1.3.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scikit-learn==1.3.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from alt-profanity-check==1.3.0) (1.3.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: scikit-learn==1.3.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from alt-profanity-check==1.3.0) (1.3.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: joblib&gt;=1.3.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from alt-profanity-check==1.3.0) (1.3.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: joblib>=1.3.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from alt-profanity-check==1.3.0) (1.3.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy&gt;=1.17.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0-&gt;alt-profanity-check==1.3.0) (1.24.4)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0->alt-profanity-check==1.3.0) (1.24.4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scipy&gt;=1.5.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0-&gt;alt-profanity-check==1.3.0) (1.10.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0->alt-profanity-check==1.3.0) (1.10.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0-&gt;alt-profanity-check==1.3.0) (3.2.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from scikit-learn==1.3.0->alt-profanity-check==1.3.0) (3.2.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: huggingface-hub in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (0.19.4)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: huggingface-hub in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (0.19.4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: filelock in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (3.13.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: filelock in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (3.13.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: fsspec&gt;=2023.5.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (2023.10.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (2023.10.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: requests in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (2.31.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: requests in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (2.31.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm&gt;=4.42.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (4.66.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (4.66.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pyyaml&gt;=5.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (6.0.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (6.0.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (4.8.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (4.8.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: packaging&gt;=20.9 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (23.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from huggingface-hub) (23.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests-&gt;huggingface-hub) (3.3.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests->huggingface-hub) (3.3.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests-&gt;huggingface-hub) (3.4)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests->huggingface-hub) (3.4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests-&gt;huggingface-hub) (2.1.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests->huggingface-hub) (2.1.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests-&gt;huggingface-hub) (2023.11.17)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from requests->huggingface-hub) (2023.11.17)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: Faker in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (20.0.3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: Faker in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (20.0.3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: python-dateutil&gt;=2.4 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from Faker) (2.8.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: python-dateutil>=2.4 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from Faker) (2.8.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: six&gt;=1.5 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from python-dateutil&gt;=2.4-&gt;Faker) (1.16.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: six>=1.5 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: rouge-score in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (0.1.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: rouge-score in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (0.1.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: absl-py in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (2.0.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: absl-py in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (2.0.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: nltk in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (3.8.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: nltk in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (3.8.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (1.24.4)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: numpy in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (1.24.4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: six&gt;=1.14.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: click in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (7.1.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: click in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk->rouge-score) (7.1.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: joblib in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (1.3.2)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: joblib in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk->rouge-score) (1.3.2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: regex&gt;=2021.8.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (2023.6.3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk->rouge-score) (2023.6.3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (4.66.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: tqdm in /opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages (from nltk->rouge-score) (4.66.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:45:44.850\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m222\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "[DatasetReader, PerfileSourcedJsonlReader, TextPrompt, RandomSelect, TextToxicity, TextDiversityIndicate, TextQualityScorer, RougeScoreDedup, PerfileParquetWriter]\n",
      "Will assign 36 cores and 206263 M memory for spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 23:45:52 WARN Utils: Your hostname, sr533 resolves to a loopback address: 127.0.1.1; using 10.0.2.133 instead (on interface eno0)\n",
      "23/12/04 23:45:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/04 23:45:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per core memory size is 5.595 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
      "execute with spark for global tasks started ...\n",
      "DatasetReader\n",
      "\u001b[32m2023-12-04 23:45:57.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mDatasetReader: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "execute with spark for global tasks took 0.0023543089628219604 sec\n",
      "PerfileSourcedJsonlReader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on alpaca_data_50.jsonl:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpaca_data_50.jsonl\n",
      "TextPrompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSelect\n",
      "TextToxicity\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDiversityIndicate\n",
      "statistics_decorator spark\n",
      "\u001b[32m2023-12-04 23:47:19.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.core.model_utils\u001b[0m:\u001b[36mprepare_diversity_model\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mLoading spacy model [en_core_web_md-3.5.0]...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextQualityScorer\n",
      "statistics_decorator spark\n",
      "model_name is gpt3\n",
      "\u001b[32m2023-12-04 23:47:35.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mprepare_model\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mPreparing scorer model in [/root/.cache/recdp/models/gpt3_quality_model]...\u001b[0m\n",
      "real_model_path is /root/.cache/recdp/models/gpt3_quality_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:47:38.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mStart scoring dataset...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RougeScoreDedup\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using default tokenizer.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using default tokenizer.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=================================>                   (126 + 36) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:47:49.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 0: total processing num_samples is 55, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      " 50%|█████     | 1/2 [00:09<00:09,  9.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 took 9.555126171559095 sec\n",
      "Round 1 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:================================================>    (184 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:47:52.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 1: total processing num_samples is 0, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 took 2.8002537600696087 sec\n",
      "generate_connected_components => duplicates started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_connected_components => duplicates took 0.03802505135536194 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:47:53.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mRandomSelect: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextToxicity: A total of 11 rows of data were processed, using 69.80172777175903 seconds, Get max toxicity 0.0010136101627722383, Get min toxicity 0.0001716656406642869, Get average toxicity 0.00036636507800060576,Get the std of toxicity 0.0002271174606949808\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextDiversityIndicate: A total of 11 rows of data were processed, using 15.65165090560913 seconds, Get max diversity types 11, Get average diversity types 1.625,Get the std of diversity types 2.5\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextQualityScorer: A total of 11 rows of data were processed, using 5.69097900390625 seconds, Get average quality score 0.9509534303108631\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mRougeScoreDedup: A total of 11 rows of data were processed, using 12.74691891670227 seconds, A duplication list containing 0 found, around 0.0% of total data, Sampled, duplication preview: Empty DataFrame\n",
      "Columns: [id_1, id_2, id_pair, similarity_left, similarity_right, score]\n",
      "Index: []\u001b[0m\n",
      "\u001b[32m2023-12-04 23:47:53.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on alpaca_data_50.jsonl: 100%|██████████| 1/1 [01:47<00:00, 107.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-04 23:47:53.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m429\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output_alpaca_result for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyrecdp.LLM import TextPipeline, ResumableTextPipeline\n",
    "from pyrecdp.primitives.operations import *\n",
    "import os\n",
    "alpaca_pipeline = ResumableTextPipeline()\n",
    "alpaca_pipeline.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output_alpaca_result\"\n",
    "ops = [\n",
    "    JsonlReader(\"/content/test_data/alpaca_data_50.jsonl\"),\n",
    "    TextPrompt(dataset_name=\"alpaca\", prompt_name=\"causal_llm_1\"),\n",
    "    RandomSelect(fraction=0.3),\n",
    "    TextToxicity(huggingface_config_path=\"/root/.cache/huggingface/hub/models--xlm-roberta-base\"),\n",
    "    TextDiversityIndicate(out_dir=out_dir, language=\"en\", first_sent=False),\n",
    "    TextQualityScorer(model=\"gpt3\"),\n",
    "    RougeScoreDedup(max_ratio=0.7, batch_size=10,score_store_path=os.path.join(out_dir,'RougeScorefiltered.parquet')),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "alpaca_pipeline.add_operations(ops)\n",
    "ret = alpaca_pipeline.execute()\n",
    "del alpaca_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4070937-9873-4575-bf34-fcdcfc8993ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:12:03.708\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m222\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "[DatasetReader, PerfileSourcedParquetReader, TextPrompt, RandomSelect, TextDiversityIndicate, TextQualityScorer, RougeScoreDedup, PerfileParquetWriter]\n",
      "Will assign 36 cores and 206263 M memory for spark\n",
      "per core memory size is 5.595 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
      "execute with spark for global tasks started ...\n",
      "DatasetReader\n",
      "\u001b[32m2023-12-05 00:12:04.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mDatasetReader: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "execute with spark for global tasks took 0.0032089054584503174 sec\n",
      "PerfileSourcedParquetReader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on openorca_sample_50.parquet:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openorca_sample_50.parquet\n",
      "TextPrompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSelect\n",
      "TextDiversityIndicate\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextQualityScorer\n",
      "statistics_decorator spark\n",
      "model_name is gpt3\n",
      "\u001b[32m2023-12-05 00:12:20.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mprepare_model\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mPreparing scorer model in [/root/.cache/recdp/models/gpt3_quality_model]...\u001b[0m\n",
      "real_model_path is /root/.cache/recdp/models/gpt3_quality_model\n",
      "\u001b[32m2023-12-05 00:12:21.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mStart scoring dataset...\u001b[0m\n",
      "RougeScoreDedup\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using default tokenizer.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using default tokenizer.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:====================================>                (139 + 36) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:12:28.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 0: total processing num_samples is 55, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      " 50%|█████     | 1/2 [00:06<00:06,  6.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 took 6.891998417675495 sec\n",
      "Round 1 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=========================================>           (158 + 36) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:12:30.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 1: total processing num_samples is 0, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "100%|██████████| 2/2 [00:09<00:00,  4.62s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 took 2.336260261014104 sec\n",
      "generate_connected_components => duplicates started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_connected_components => duplicates took 0.015356998890638351 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:12:31.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-05 00:12:31.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mRandomSelect: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-05 00:12:31.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextDiversityIndicate: A total of 11 rows of data were processed, using 14.471717834472656 seconds, Get max diversity types 11, Get average diversity types 1.826086956521739,Get the std of diversity types 2.328717436956612\u001b[0m\n",
      "\u001b[32m2023-12-05 00:12:31.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextQualityScorer: A total of 11 rows of data were processed, using 1.4916315078735352 seconds, Get average quality score 0.9467587080940774\u001b[0m\n",
      "\u001b[32m2023-12-05 00:12:31.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mRougeScoreDedup: A total of 11 rows of data were processed, using 9.504194974899292 seconds, A duplication list containing 0 found, around 0.0% of total data, Sampled, duplication preview: Empty DataFrame\n",
      "Columns: [id_1, id_2, id_pair, similarity_left, similarity_right, score]\n",
      "Index: []\u001b[0m\n",
      "\u001b[32m2023-12-05 00:12:31.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on openorca_sample_50.parquet: 100%|██████████| 1/1 [00:27<00:00, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:12:31.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m429\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output_openorca_result for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyrecdp.LLM import TextPipeline, ResumableTextPipeline\n",
    "from pyrecdp.primitives.operations import *\n",
    "import os\n",
    "openorca_pipeline = ResumableTextPipeline()\n",
    "openorca_pipeline.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output_openorca_result\"\n",
    "ops = [\n",
    "    ParquetReader(\"/content/test_data/openorca_sample_50.parquet\"),\n",
    "    TextPrompt(dataset_name=\"openorca\", prompt_name=\"causal_llm_1\"),\n",
    "    RandomSelect(fraction=0.3),\n",
    "    # TextToxicity(huggingface_config_path=\"/root/.cache/huggingface/hub/models--xlm-roberta-base\"),\n",
    "    TextDiversityIndicate(out_dir=out_dir, language=\"en\", first_sent=False),\n",
    "    TextQualityScorer(model=\"gpt3\"),\n",
    "    RougeScoreDedup(max_ratio=0.7, batch_size=10,score_store_path=os.path.join(out_dir,'RougeScorefiltered.parquet')),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "openorca_pipeline.add_operations(ops)\n",
    "ret = openorca_pipeline.execute()\n",
    "del openorca_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7b3104-87c3-4a8b-8d4b-41d579ab0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:13:50.408\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36menable_statistics\u001b[0m:\u001b[36m222\u001b[0m - \u001b[33m\u001b[1mEnabling this option will result in a decrease in execution speed\u001b[0m\n",
      "[DatasetReader, PerfileSourcedParquetReader, TextPrompt, RandomSelect, TextDiversityIndicate, TextQualityScorer, RougeScoreDedup, PerfileParquetWriter]\n",
      "Will assign 36 cores and 206263 M memory for spark\n",
      "per core memory size is 5.595 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
      "execute with spark for global tasks started ...\n",
      "DatasetReader\n",
      "\u001b[32m2023-12-05 00:13:50.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mDatasetReader: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "execute with spark for global tasks took 0.00310460664331913 sec\n",
      "PerfileSourcedParquetReader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on dolly_sample_50.parquet:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolly_sample_50.parquet\n",
      "TextPrompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSelect\n",
      "TextDiversityIndicate\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatbot-finetuning/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextQualityScorer\n",
      "statistics_decorator spark\n",
      "model_name is gpt3\n",
      "\u001b[32m2023-12-05 00:14:08.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mprepare_model\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mPreparing scorer model in [/root/.cache/recdp/models/gpt3_quality_model]...\u001b[0m\n",
      "real_model_path is /root/.cache/recdp/models/gpt3_quality_model\n",
      "\u001b[32m2023-12-05 00:14:09.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mStart scoring dataset...\u001b[0m\n",
      "RougeScoreDedup\n",
      "statistics_decorator spark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using default tokenizer.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using default tokenizer.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=================================================>   (187 + 13) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:14:15.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 0: total processing num_samples is 55, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      " 50%|█████     | 1/2 [00:06<00:06,  6.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 took 6.102168790996075 sec\n",
      "Round 1 started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:===========================================>         (164 + 36) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:14:17.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_compare_dedup\u001b[0m:\u001b[36mprocess_spark\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mRound 1: total processing num_samples is 0, detected high score num_samples is 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "100%|██████████| 2/2 [00:08<00:00,  4.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 took 2.2937835920602083 sec\n",
      "generate_connected_components => duplicates started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_connected_components => duplicates took 0.014340104535222054 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:14:18.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mTextPrompt: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-05 00:14:18.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mRandomSelect: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n",
      "\u001b[32m2023-12-05 00:14:18.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextDiversityIndicate: A total of 11 rows of data were processed, using 14.846248388290405 seconds, Get max diversity types 11, Get average diversity types 1.7692307692307692,Get the std of diversity types 2.7735009811261455\u001b[0m\n",
      "\u001b[32m2023-12-05 00:14:18.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mTextQualityScorer: A total of 11 rows of data were processed, using 1.127403974533081 seconds, Get average quality score 0.9174850873367049\u001b[0m\n",
      "\u001b[32m2023-12-05 00:14:18.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mRougeScoreDedup: A total of 11 rows of data were processed, using 8.664161920547485 seconds, A duplication list containing 0 found, around 0.0% of total data, Sampled, duplication preview: Empty DataFrame\n",
      "Columns: [id_1, id_2, id_pair, similarity_left, similarity_right, score]\n",
      "Index: []\u001b[0m\n",
      "\u001b[32m2023-12-05 00:14:18.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mop_summary\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mPerfileParquetWriter: A total of 0 rows of data were processed, using 0 seconds, with 0 rows modified or removed, 0 rows of data remaining.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResumableTextPipeline, current on dolly_sample_50.parquet: 100%|██████████| 1/1 [00:27<00:00, 27.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-05 00:14:18.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.LLM.TextPipeline\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m429\u001b[0m - \u001b[1mCompleted! ResumableTextPipeline will not return dataset, please check ResumableTextPipeline_output_dolly_result for verification.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyrecdp.LLM import TextPipeline, ResumableTextPipeline\n",
    "from pyrecdp.primitives.operations import *\n",
    "import os\n",
    "\n",
    "dolly_pipeline = ResumableTextPipeline()\n",
    "dolly_pipeline.enable_statistics()\n",
    "out_dir = \"ResumableTextPipeline_output_dolly_result\"\n",
    "ops = [\n",
    "    ParquetReader(\"/content/test_data/dolly_sample_50.parquet\"),\n",
    "    TextPrompt(dataset_name=\"dolly\", prompt_name=\"causal_llm_1\"),\n",
    "    RandomSelect(fraction=0.3),\n",
    "    # TextToxicity(huggingface_config_path=\"/root/.cache/huggingface/hub/models--xlm-roberta-base\"),\n",
    "    TextDiversityIndicate(out_dir=out_dir, language=\"en\", first_sent=False),\n",
    "    TextQualityScorer(model=\"gpt3\"),\n",
    "    RougeScoreDedup(max_ratio=0.7, batch_size=10,score_store_path=os.path.join(out_dir,'RougeScorefiltered.parquet')),\n",
    "    ParquetWriter(out_dir)\n",
    "]\n",
    "dolly_pipeline.add_operations(ops)\n",
    "ret = dolly_pipeline.execute()\n",
    "del dolly_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7743121",
   "metadata": {},
   "source": [
    "### 3.2 Check the result and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68620b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Alpaca Result ============\n",
      "The Rouge score:  {'dup_num': 0, 'dup_ratio': 0.0}\n",
      "The toxicity:  {'min': 0.0001716656406642869, 'max': 0.0010136101627722383, 'mean': 0.00036636507800060576, 'std': 0.0002271174606949808}\n",
      "The diversity intricate:  {'max': 11, 'mean': 1.625, 'std': 2.5}\n",
      "The quality score : {'mean': 0.9509534303108631}\n",
      "The original dataset length: 50, the processed dataset length: 11\n",
      "============ Dolly Result ============\n",
      "The Rouge score:  {'dup_num': 0, 'dup_ratio': 0.0}\n",
      "The diversity intricate:  {'max': 11, 'mean': 1.7692307692307692, 'std': 2.7735009811261455}\n",
      "The quality score : {'mean': 0.9174850873367049}\n",
      "The original dataset length: 50, the processed dataset length: 11\n",
      "============ Openorca Result ============\n",
      "The Rouge score:  {'dup_num': 0, 'dup_ratio': 0.0}\n",
      "The diversity intricate:  {'max': 11, 'mean': 1.826086956521739, 'std': 2.328717436956612}\n",
      "The quality score : {'mean': 0.9467587080940774}\n",
      "The original dataset length: 50, the processed dataset length: 11\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "print('============ Alpaca Result ============')\n",
    "out_dir = 'ResumableTextPipeline_output_alpaca_result'\n",
    "print('The Rouge score: ', json.load(open(f'{out_dir}/RougeScoreDedup-statistics', 'r')))\n",
    "print('The toxicity: ', json.load(open(f'{out_dir}/TextToxicity-statistics', 'r')))\n",
    "print('The diversity intricate: ', json.load(open(f'{out_dir}/TextDiversityIndicate-statistics', 'r')))\n",
    "print('The quality score :', json.load(open(f'{out_dir}/TextQualityScorer-statistics', 'r')))\n",
    "\n",
    "\n",
    "origin_dataset_length = (len(pd.read_json(\"/content/test_data/alpaca_data_50.jsonl\", lines=True))) \n",
    "downsized_dataset_length = len(pd.read_parquet(glob.glob(f'{out_dir}/alpaca_data_50.jsonl/*.parquet')))\n",
    "\n",
    "print(f'The original dataset length: {origin_dataset_length}, the processed dataset length: {downsized_dataset_length}')\n",
    "\n",
    "print('============ Dolly Result ============')\n",
    "out_dir = 'ResumableTextPipeline_output_dolly_result'\n",
    "print('The Rouge score: ', json.load(open(f'{out_dir}/RougeScoreDedup-statistics', 'r')))\n",
    "# print('The toxicity: ', json.load(open(f'{out_dir}/TextToxicity-statistics', 'r')))\n",
    "print('The diversity intricate: ', json.load(open(f'{out_dir}/TextDiversityIndicate-statistics', 'r')))\n",
    "print('The quality score :', json.load(open(f'{out_dir}/TextQualityScorer-statistics', 'r')))\n",
    "\n",
    "\n",
    "origin_dataset_length = (len(pd.read_parquet(\"/content/test_data/dolly_sample_50.parquet\"))) \n",
    "downsized_dataset_length = len(pd.read_parquet(glob.glob(f'{out_dir}/dolly_sample_50.parquet/*.parquet')))\n",
    "\n",
    "print(f'The original dataset length: {origin_dataset_length}, the processed dataset length: {downsized_dataset_length}')\n",
    "\n",
    "print('============ Openorca Result ============')\n",
    "out_dir = 'ResumableTextPipeline_output_openorca_result'\n",
    "print('The Rouge score: ', json.load(open(f'{out_dir}/RougeScoreDedup-statistics', 'r')))\n",
    "# print('The toxicity: ', json.load(open(f'{out_dir}/TextToxicity-statistics', 'r')))\n",
    "print('The diversity intricate: ', json.load(open(f'{out_dir}/TextDiversityIndicate-statistics', 'r')))\n",
    "print('The quality score :', json.load(open(f'{out_dir}/TextQualityScorer-statistics', 'r')))\n",
    "\n",
    "\n",
    "origin_dataset_length = (len(pd.read_parquet(\"/content/test_data/openorca_sample_50.parquet\"))) \n",
    "downsized_dataset_length = len(pd.read_parquet(glob.glob(f'{out_dir}/openorca_sample_50.parquet/*.parquet')))\n",
    "\n",
    "print(f'The original dataset length: {origin_dataset_length}, the processed dataset length: {downsized_dataset_length}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
