experiment:
  project: "distiller"
  tag: "cifar10_kd_res50PretrainI21k_denascnn_LRdecay12"
  strategy: "OnlyDistillationStrategy"
  model_save: "/home/vmagent/app/data/model"
  loss:
    backbone: 0.1
    distiller: 0.9
dataset:
  type: "cifar10"
  num_workers: 10
  path: "/home/vmagent/app/data/dataset"
  ###For save
  # train_transform: "pretrain112"
  # test_transform: "pretrain112"
  ###For train
  train_transform: "denascnn"
model:
  type: "denas_cnn"
distiller:
  type: "kd"
  teacher: 
      type: "resnet50"
      pretrain: "/home/vmagent/app/data/model/baselines/cifar10_res50PretrainI21k/pretrain_resnet50_cifar10.pth_epoch_15"
  # save_logits: True
  use_saved_logits: True
  # check_logits: True
  logits_path: "/home/vmagent/app/data/model/distiller/cifar10_kd_res50PretrainI21k_denascnn_LRdecay10/logits"
  logits_topk: 0
solver:
  batch_size: 128
  epochs: 200
  optimizer:
    type: "SGD"
    lr: 0.1
    weight_decay: 5e-4
    momentum: 0.9
  scheduler:
    type: "ReduceLROnPlateau"
    lr_decay_rate: 0.2
    ReduceLROnPlateau:
      patience: 12
  early_stop:
    flag: True
    tolerance_epoch: 20
