experiment:
  project: "distiller"
  tag: "cifar100_kd_res50PretrainI21k_res18_LRdecay_lr008_t3"
  strategy: "OnlyDistillationStrategy"
  
output_dir: "/home/vmagent/app/data/model"
train_epochs: 200

### dataset
data_set: "cifar100"
data_path:  "/home/vmagent/app/data/dataset/cifar"
num_workers: 4

### model
model_type: "resnet18_cifar"

# loss
loss_weight:
    backbone: 0.1
    distiller: 0.9

distiller:
    type: "kd"
    teacher: 
        type: "resnet50"
        pretrain: "/home/vmagent/app/data/model/baseline/cifar100_res50PretrainI21k/cifar100_res50_pretrain_imagenet21k.pth"
    use_saved_logits: True
    logits_path: "/home/vmagent/app/data/model/distiller/cifar100_kd_res50PretrainI21k/logits"
    logits_topk: 0

## optimizer
optimizer: "SGD"
learning_rate: 0.08
weight_decay: 0.0001
momentum: 0.9

### scheduler
lr_scheduler: "ReduceLROnPlateau"
lr_scheduler_config:
    decay_rate: 0.2
    decay_patience: 3

### early stop
early_stop: "EarlyStopping"
metric_threshold: 76.71
early_stop_config:
    is_max: True
