{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE-NAS & TLK on BERT with Hugging Face Demo\n",
    "This demo mainly introduces the Hugging Face, TLK and DE-NAS joint application on the BERT, which is mainly expected to express how to leverage them together for optimizing the BERT-structure model from Hugging Face to a lighter and faster model through DE-NAS and TLK, and the optimized model can be uploaded into Hugging Face repo for broader usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "* [Background](#1)\n",
    "* [Motivation](#2)\n",
    "* [Hugging Face](#3)\n",
    "* [Experiment](#4)\n",
    "* [Summary](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"1\"></p>\n",
    "\n",
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conventional NAS is extremely computation-intensive, and poor scalability on diverse models. As shown in the figure, the CNN-based model and RNN-based model are commonly used in the CV and NLP respectively, and the conventional NAS work within these two domains are always computation-intensive.\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/Nas_problem.png\" width=\"400\"/><figure>Conventional NAS</figure>\n",
    "</center>\n",
    "\n",
    "* NAS with transfer learning (knowledge distillation) has been proved useful in almost all of the recent NLP NAS on BERT.\n",
    "\n",
    "<style>\n",
    "table {\n",
    "margin: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "| Institution | Representative Work | Hugging Face Repo |\n",
    "|:----|:----|:----|\n",
    "| Huawei | [DynaBert(NeurIPS-20)](https://arxiv.org/abs/2004.04037), [TinyBert (EMNLP-20)](https://arxiv.org/abs/1909.10351), [AutoTinyBert (ACL-21)](https://arxiv.org/abs/2107.13686), [EfficientBERT (EMNLP-21)](https://arxiv.org/abs/2109.07222), [AutoBERT-Zero (AAAI-22)](https://arxiv.org/abs/2107.07445) | Almost |\n",
    "| Alibaba | [AdaBERT (IJCAI-20)](https://arxiv.org/abs/2001.04246) | No |\n",
    "| Microsoft | [NAS-BERT (KDD-21)](https://arxiv.org/abs/2105.14444), [AutoDistil (NeurIPS-22)](https://arxiv.org/abs/2201.08539) | Others |\n",
    "\n",
    "* Noted: \n",
    "    * \"Almost\" means that almost of the above work has its hugging face repo.\n",
    "    * \"Others\" means that other models except BERT (etc., Transformer) or other light BERT (etc., by compression) have its hugging face repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"2\"></p> \n",
    "\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "The latest major innovation in the world of NLP is undoubtedly large pretrained language models. The language model benefits from a good pre-trained model and fine-tuning on the target task, which is a transfer learning process indeed.\n",
    "\n",
    "The most representive work of language model, BERT, is pretrained on large unannotated text corpora, fine-tuned on 11 NLP tasks and achieves the state-of-art results. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned.\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/BERT.png\" width=\"500\"/><figure>Overall pre-training and fine-tuning procedures for BERT</figure>\n",
    "</center>\n",
    "\n",
    "### DE-NAS with Hugging Face\n",
    "* DE-NAS is a train-free, and cross-domain (unified <u>transformer</u>) NAS.\n",
    "* Hugging Face is <u>transformer-based</u>.\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/DENAS_Huggingface.png\" width=\"500\"/><figure>DE-NAS with Hugging Face</figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE-NAS with Transfer Learning\n",
    "* DE-NAS can provide a lighter and faster model, but it can be further improved.\n",
    "* Transfer Learning can use a light model as the target model to inject knowledge from others.\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/DENAS_TLK.png\" width=\"500\"/><figure>DE-NAS with TLK</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"3\"></p> \n",
    "\n",
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face was originally a chatbot start-up service provider at New York, then open sourced a Transformers library on github, which began bigger and bigger as its development.\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/Huggingface.png\" width=\"600\"/><figure>Hugging Face</figure>\n",
    "</center>\n",
    "\n",
    "* Database: [Datasets](https://huggingface.co/datasets), [Models](https://huggingface.co/models)\n",
    "* API: [Transformer](https://huggingface.co/docs/transformers/index)...\n",
    "* Community: [Intel Page](https://huggingface.co/Intel?sort_models=downloads#models), [Forum](https://discuss.huggingface.co/), [Course](https://huggingface.co/course/chapter1/1)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"4\"></p> \n",
    "\n",
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "* Build docker image\n",
    "\n",
    "```\n",
    "cd Dockerfile-ubuntu18.04\n",
    "docker build -t aidk-pytorch110 . -f DockerfilePytorch110 --build-arg http_proxy --build-arg https_proxy\n",
    "```\n",
    "\n",
    "```\n",
    "docker run -itd --name aidk-denas-bert --privileged --network host --device=/dev/dri -v ${dataset_path}:/home/vmagent/app/dataset -v ${aidk_code_path}:/home/vmagent/app/aidk -w /home/vmagent/app/ aidk-pytorch110 /bin/bash\n",
    "```\n",
    "* Enter container with `docker exec -it aidk-denas-bert bash`\n",
    "\n",
    "* Install the jupyter and Huggingface API\n",
    "\n",
    "```\n",
    "source /opt/intel/oneapi/setvars.sh --ccl-configuration=cpu_icc --force\n",
    "conda activate pytorch-1.10.0\n",
    "pip install jupyter\n",
    "pip install transformers[torch]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch DE-NAS with TLK \n",
    "\n",
    "* Prepare dataset and pre-trained BERT from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-11 08:21:00--  https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 226.08K   266KB/s    in 0.8s    \n",
      "\n",
      "2022-10-11 08:21:02 (266 KB/s) - ‘vocab.txt’ saved [231508/231508]\n",
      "\n",
      "--2022-10-11 08:21:02--  https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/bert-base-uncased/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22&Expires=1665727214&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL2JlcnQtYmFzZS11bmNhc2VkLzA5NzQxNzM4MWQ2YzcyMzBiZDllMzU1NzQ1NmQ3MjZkZTZlODMyNDVlYzhiMjRmNTI5ZjYwMTk4YTY3YjIwM2E~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj1hdHRhY2htZW50JTNCJTIwZmlsZW5hbWUlM0QlMjJweXRvcmNoX21vZGVsLmJpbiUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY2NTcyNzIxNH19fV19&Signature=yjzYvJibuuJThNevTUNE7L3eKEQtffmO064kcv1ZRteIFXyr7PB0kADoP-4xWHbh5NR2FIwSGv6-oD64UxRvqAtEEvg0pRDIdUiqUMpMBbjuX1dLplwxzwhdHDzoQu0PgxV5yqqvr7ItIObi6ubfOHuQ6aRCHV4z752gfeFfkS3sMlDFPti-f50MpPQu0Pd5TdqEhCpwH6gpaoPuBv5NA4MpvEvDQU8Z6-rvlCeG0ksKb5gog4xj1mdJpDu0TtX1b9OQzi~fqVb8d8QumumeFS8CZbcq3RgLn~i49LA0dixlDPJQ7cyJWbnwxpWih5a1f2R-O9R1TNEmUKj11r0ZOg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2022-10-11 08:21:02--  https://cdn-lfs.huggingface.co/bert-base-uncased/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22&Expires=1665727214&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL2JlcnQtYmFzZS11bmNhc2VkLzA5NzQxNzM4MWQ2YzcyMzBiZDllMzU1NzQ1NmQ3MjZkZTZlODMyNDVlYzhiMjRmNTI5ZjYwMTk4YTY3YjIwM2E~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj1hdHRhY2htZW50JTNCJTIwZmlsZW5hbWUlM0QlMjJweXRvcmNoX21vZGVsLmJpbiUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY2NTcyNzIxNH19fV19&Signature=yjzYvJibuuJThNevTUNE7L3eKEQtffmO064kcv1ZRteIFXyr7PB0kADoP-4xWHbh5NR2FIwSGv6-oD64UxRvqAtEEvg0pRDIdUiqUMpMBbjuX1dLplwxzwhdHDzoQu0PgxV5yqqvr7ItIObi6ubfOHuQ6aRCHV4z752gfeFfkS3sMlDFPti-f50MpPQu0Pd5TdqEhCpwH6gpaoPuBv5NA4MpvEvDQU8Z6-rvlCeG0ksKb5gog4xj1mdJpDu0TtX1b9OQzi~fqVb8d8QumumeFS8CZbcq3RgLn~i49LA0dixlDPJQ7cyJWbnwxpWih5a1f2R-O9R1TNEmUKj11r0ZOg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 440473133 (420M) [application/octet-stream]\n",
      "Saving to: ‘pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>] 420.07M  10.6MB/s    in 43s     \n",
      "\n",
      "2022-10-11 08:21:46 (9.81 MB/s) - ‘pytorch_model.bin’ saved [440473133/440473133]\n",
      "\n",
      "--2022-10-11 08:21:46--  https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 570 [text/plain]\n",
      "Saving to: ‘bert_config.json’\n",
      "\n",
      "bert_config.json    100%[===================>]     570  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-11 08:21:47 (99.3 MB/s) - ‘bert_config.json’ saved [570/570]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/dataset && mkdir -p bert-base-uncased && cd bert-base-uncased && wget https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt -O vocab.txt && wget https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin -O pytorch_model.bin && wget https://huggingface.co/bert-base-uncased/resolve/main/config.json -O bert_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading the pre-trained model from Hugging Face\n",
    "\n",
    "After downloading the pre-trained model from Hugging Face, we can load it into the DE-NAS supernet and searched candidate for further optimization as followings:\n",
    "\n",
    "``` python\n",
    "# using pytorch_model.bin and bert_config.json Hugging Face to construct and initialize the DE-NAS model\n",
    "\"\"\" SuperBertForQuestionAnswering Parameters:\n",
    "    pretrained_model_name_or_path: the path that places the \"pytorch_model.bin\" and \"bert_config.json\"\n",
    "    config: the path that points to the \"bert_config.json\"\n",
    "\"\"\"\n",
    "model = SuperBertForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Launch DE-NAS search process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths: /home/vmagent/app/aidk/DeNas/asr/utils, /home/vmagent/app/aidk/DeNas/asr\n",
      "['/home/vmagent/app/aidk/DeNas', '/opt/intel/oneapi/advisor/2022.1.0/pythonapi', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python39.zip', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9/lib-dynload', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9/site-packages', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9/site-packages/warprnnt_pytorch-0.1-py3.9-linux-x86_64.egg', '', '..', '/home/vmagent/app/aidk/DeNas', '/home/vmagent/app/aidk/DeNas', '/home/vmagent/app/aidk/DeNas', '/home/vmagent/app/aidk/DeNas', '/home/vmagent/app/aidk/DeNas', '/home/vmagent/app/aidk/DeNas/asr']\n",
      "loading archive file /home/vmagent/app/dataset/bert-base-uncased\n",
      "10/10/2022 07:32:06 - INFO - nlp.supernet_bert -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Weights of SuperBertModel not initialized from pretrained model: ['bert.dense_fit.weight', 'bert.dense_fit.bias']\n",
      "Weights from pretrained model not used in SuperBertModel: ['bert.cls.predictions.bias', 'bert.cls.predictions.transform.dense.weight', 'bert.cls.predictions.transform.dense.bias', 'bert.cls.predictions.decoder.weight', 'bert.cls.seq_relationship.weight', 'bert.cls.seq_relationship.bias', 'bert.cls.predictions.transform.LayerNorm.weight', 'bert.cls.predictions.transform.LayerNorm.bias']\n",
      "10/10/2022 07:32:07 - INFO - DENAS -   epoch = 0\n",
      "10/10/2022 07:32:11 - INFO - DENAS -   random 1/50 structure (11, 10, 640, 720, 992) nas_score 218.20506286621094 params 58.934512\n",
      "10/10/2022 07:32:14 - INFO - DENAS -   random 2/50 structure (6, 8, 512, 752, 2816) nas_score 174.31529235839844 params 58.612176\n",
      "10/10/2022 07:32:18 - INFO - DENAS -   random 3/50 structure (12, 11, 704, 656, 1248) nas_score 218.89505004882812 params 62.695536\n",
      "10/10/2022 07:32:22 - INFO - DENAS -   random 4/50 structure (10, 12, 768, 640, 1376) nas_score 261.1836242675781 params 57.62336\n",
      "10/10/2022 07:32:25 - INFO - DENAS -   random 5/50 structure (8, 10, 640, 720, 2720) nas_score 143.83590698242188 params 69.01816\n",
      "10/10/2022 07:32:30 - INFO - DENAS -   random 6/50 structure (11, 11, 704, 704, 1824) nas_score 281.92169189453125 params 72.494048\n",
      "10/10/2022 07:32:35 - INFO - DENAS -   random 7/50 structure (11, 11, 704, 768, 1888) nas_score 287.23193359375 params 80.21168\n",
      "10/10/2022 07:32:39 - INFO - DENAS -   random 8/50 structure (12, 10, 640, 576, 2112) nas_score 203.54104614257812 params 65.191104\n",
      "10/10/2022 07:32:44 - INFO - DENAS -   random 9/50 structure (10, 11, 704, 656, 2144) nas_score 222.54742431640625 params 67.47608\n",
      "10/10/2022 07:32:48 - INFO - DENAS -   random 10/50 structure (9, 11, 704, 624, 2720) nas_score 184.26817321777344 params 66.200592\n",
      "10/10/2022 07:32:53 - INFO - DENAS -   random 11/50 structure (11, 10, 640, 640, 1664) nas_score 224.1397705078125 params 61.807744\n",
      "10/10/2022 07:32:57 - INFO - DENAS -   random 12/50 structure (10, 11, 704, 512, 2592) nas_score 227.76925659179688 params 57.191872\n",
      "10/10/2022 07:33:01 - INFO - DENAS -   random 13/50 structure (11, 12, 768, 768, 1504) nas_score 290.2223205566406 params 75.884192\n",
      "10/10/2022 07:33:04 - INFO - DENAS -   random 14/50 structure (8, 8, 512, 656, 2848) nas_score 116.59851837158203 params 61.498992\n",
      "10/10/2022 07:33:09 - INFO - DENAS -   random 15/50 structure (12, 12, 768, 544, 2176) nas_score 203.98484802246094 params 65.737952\n",
      "10/10/2022 07:33:12 - INFO - DENAS -   random 16/50 structure (8, 10, 640, 688, 2720) nas_score 144.92169189453125 params 65.93032\n",
      "10/10/2022 07:33:16 - INFO - DENAS -   random 17/50 structure (10, 10, 640, 768, 1568) nas_score 288.4649658203125 params 68.254016\n",
      "10/10/2022 07:33:19 - INFO - DENAS -   random 18/50 structure (8, 8, 512, 688, 2592) nas_score 148.94033813476562 params 61.699152\n",
      "10/10/2022 07:33:24 - INFO - DENAS -   random 19/50 structure (9, 11, 704, 768, 2720) nas_score 164.78244018554688 params 81.578208\n",
      "10/10/2022 07:33:29 - INFO - DENAS -   random 20/50 structure (11, 9, 576, 672, 1760) nas_score 218.46766662597656 params 64.44352\n",
      "10/10/2022 07:33:34 - INFO - DENAS -   random 21/50 structure (12, 12, 768, 528, 1920) nas_score 163.92921447753906 params 60.550512\n",
      "10/10/2022 07:33:39 - INFO - DENAS -   random 22/50 structure (11, 9, 576, 688, 2848) nas_score 260.3940124511719 params 82.46792\n",
      "10/10/2022 07:33:43 - INFO - DENAS -   random 23/50 structure (8, 10, 640, 768, 2048) nas_score 129.556640625 params 65.390848\n",
      "10/10/2022 07:33:48 - INFO - DENAS -   random 24/50 structure (10, 9, 576, 608, 2112) nas_score 195.41104125976562 params 59.006496\n",
      "10/10/2022 07:33:54 - INFO - DENAS -   random 25/50 structure (12, 10, 640, 624, 2784) nas_score 197.55532836914062 params 80.721552\n",
      "10/10/2022 07:33:58 - INFO - DENAS -   random 26/50 structure (11, 8, 512, 752, 992) nas_score 254.7490997314453 params 57.336976\n",
      "10/10/2022 07:34:01 - INFO - DENAS -   random 27/50 structure (11, 9, 576, 656, 1408) nas_score 287.49908447265625 params 57.815632\n",
      "10/10/2022 07:34:07 - INFO - DENAS -   random 28/50 structure (12, 12, 768, 560, 2496) nas_score 143.3026580810547 params 71.98344\n",
      "10/10/2022 07:34:12 - INFO - DENAS -   random 29/50 structure (11, 8, 512, 544, 2144) nas_score 220.51329040527344 params 55.17216\n",
      "10/10/2022 07:34:16 - INFO - DENAS -   random 30/50 structure (10, 9, 576, 576, 2624) nas_score 255.239990234375 params 61.78784\n",
      "10/10/2022 07:34:19 - INFO - DENAS -   random 31/50 structure (8, 11, 704, 640, 2592) nas_score 145.3800506591797 params 61.302912\n",
      "10/10/2022 07:34:24 - INFO - DENAS -   random 32/50 structure (11, 12, 768, 768, 1184) nas_score 255.58216857910156 params 70.473952\n",
      "10/10/2022 07:34:28 - INFO - DENAS -   random 33/50 structure (9, 9, 576, 672, 2816) nas_score 175.90150451660156 params 69.383904\n",
      "10/10/2022 07:34:34 - INFO - DENAS -   random 34/50 structure (12, 10, 640, 496, 2496) nas_score 120.64695739746094 params 60.679568\n",
      "10/10/2022 07:34:38 - INFO - DENAS -   random 35/50 structure (7, 11, 704, 704, 2976) nas_score 157.3531951904297 params 65.62096\n",
      "10/10/2022 07:34:43 - INFO - DENAS -   random 36/50 structure (10, 10, 640, 736, 2528) nas_score 225.96742248535156 params 79.5288\n",
      "10/10/2022 07:34:47 - INFO - DENAS -   random 37/50 structure (11, 11, 704, 672, 1408) nas_score 287.51177978515625 params 63.024608\n",
      "10/10/2022 07:34:50 - INFO - DENAS -   random 38/50 structure (12, 8, 512, 768, 736) nas_score 114.89789581298828 params 56.950656\n",
      "10/10/2022 07:34:55 - INFO - DENAS -   random 39/50 structure (9, 8, 512, 624, 2688) nas_score 145.3809051513672 params 61.522608\n",
      "10/10/2022 07:34:59 - INFO - DENAS -   random 40/50 structure (10, 10, 640, 672, 2304) nas_score 278.4458923339844 params 69.561312\n",
      "10/10/2022 07:35:05 - INFO - DENAS -   random 41/50 structure (12, 9, 576, 752, 2816) nas_score 218.4855499267578 params 95.629968\n",
      "10/10/2022 07:35:09 - INFO - DENAS -   random 42/50 structure (11, 9, 576, 752, 2592) nas_score 281.8876647949219 params 85.94472\n",
      "10/10/2022 07:35:13 - INFO - DENAS -   random 43/50 structure (12, 12, 768, 688, 1184) nas_score 137.29002380371094 params 66.832208\n",
      "10/10/2022 07:35:18 - INFO - DENAS -   random 44/50 structure (9, 9, 576, 720, 2880) nas_score 190.080322265625 params 75.201552\n",
      "10/10/2022 07:35:23 - INFO - DENAS -   random 45/50 structure (12, 12, 768, 736, 1696) nas_score 157.52377319335938 params 80.57744\n",
      "10/10/2022 07:35:29 - INFO - DENAS -   random 46/50 structure (12, 12, 768, 688, 1408) nas_score 127.19538879394531 params 70.533584\n",
      "10/10/2022 07:35:35 - INFO - DENAS -   random 47/50 structure (12, 11, 704, 768, 2624) nas_score 253.5316162109375 params 98.857728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10/2022 07:35:38 - INFO - DENAS -   random 48/50 structure (8, 10, 640, 656, 1984) nas_score 155.35948181152344 params 55.113584\n",
      "10/10/2022 07:35:41 - INFO - DENAS -   random 49/50 structure (9, 9, 576, 608, 2400) nas_score 200.03761291503906 params 58.184448\n",
      "10/10/2022 07:35:46 - INFO - DENAS -   random 50/50 structure (11, 10, 640, 512, 2208) nas_score 233.29034423828125 params 55.522144\n",
      "10/10/2022 07:35:46 - INFO - DENAS -   random_num = 50\n",
      "10/10/2022 07:35:49 - INFO - DENAS -   mutation 1/25 structure (6, 9, 576, 768, 2912) nas_score 173.75631713867188 params 61.937088\n",
      "10/10/2022 07:35:52 - INFO - DENAS -   mutation 2/25 structure (10, 10, 640, 768, 1088) nas_score 278.1615295410156 params 60.876416\n",
      "10/10/2022 07:35:57 - INFO - DENAS -   mutation 3/25 structure (9, 11, 704, 768, 1728) nas_score 170.14674377441406 params 67.855872\n",
      "10/10/2022 07:36:01 - INFO - DENAS -   mutation 4/25 structure (11, 12, 768, 640, 1632) nas_score 212.39134216308594 params 64.965536\n",
      "10/10/2022 07:36:05 - INFO - DENAS -   mutation 5/25 structure (10, 11, 704, 672, 2304) nas_score 266.0469665527344 params 71.283552\n",
      "10/10/2022 07:36:11 - INFO - DENAS -   mutation 6/25 structure (11, 8, 512, 752, 2592) nas_score 264.3286437988281 params 83.824976\n",
      "10/10/2022 07:36:15 - INFO - DENAS -   mutation 7/25 structure (8, 8, 512, 688, 2528) nas_score 126.30126953125 params 60.994128\n",
      "10/10/2022 07:36:19 - INFO - DENAS -   mutation 8/25 structure (10, 11, 704, 640, 1376) nas_score 220.7818145751953 params 55.98304\n",
      "10/10/2022 07:36:25 - INFO - DENAS -   mutation 9/25 structure (11, 11, 704, 592, 2656) nas_score 205.80685424804688 params 71.746608\n",
      "10/10/2022 07:36:28 - INFO - DENAS -   mutation 10/25 structure (8, 8, 512, 608, 2848) nas_score 126.77200317382812 params 56.972448\n",
      "10/10/2022 07:36:32 - INFO - DENAS -   mutation 11/25 structure (12, 10, 640, 544, 2176) nas_score 181.7292938232422 params 62.391008\n",
      "10/10/2022 07:36:37 - INFO - DENAS -   mutation 12/25 structure (12, 10, 640, 480, 2176) nas_score 111.8777084350586 params 55.025952\n",
      "10/10/2022 07:36:40 - INFO - DENAS -   mutation 13/25 structure (11, 12, 768, 768, 1088) nas_score 303.3016662597656 params 68.85088\n",
      "10/10/2022 07:36:45 - INFO - DENAS -   mutation 14/25 structure (10, 11, 704, 624, 2528) nas_score 252.95208740234375 params 68.962832\n",
      "10/10/2022 07:36:48 - INFO - DENAS -   mutation 15/25 structure (8, 12, 768, 688, 2592) nas_score 157.00741577148438 params 67.341392\n",
      "10/10/2022 07:36:52 - INFO - DENAS -   mutation 16/25 structure (12, 8, 512, 576, 1696) nas_score 106.09970092773438 params 55.891776\n",
      "10/10/2022 07:36:56 - INFO - DENAS -   mutation 17/25 structure (12, 10, 640, 576, 1728) nas_score 174.0391845703125 params 59.87808\n",
      "10/10/2022 07:36:58 - INFO - DENAS -   mutation 18/25 structure (8, 9, 576, 720, 1888) nas_score 160.63900756835938 params 57.950768\n",
      "10/10/2022 07:37:02 - INFO - DENAS -   mutation 19/25 structure (8, 10, 640, 768, 2336) nas_score 151.16505432128906 params 68.932096\n",
      "10/10/2022 07:37:06 - INFO - DENAS -   mutation 20/25 structure (9, 8, 512, 752, 2464) nas_score 163.00070190429688 params 71.197008\n",
      "10/10/2022 07:37:10 - INFO - DENAS -   mutation 21/25 structure (12, 12, 768, 576, 1216) nas_score 124.21073913574219 params 56.3376\n",
      "10/10/2022 07:37:15 - INFO - DENAS -   mutation 22/25 structure (12, 11, 704, 688, 2624) nas_score 244.43292236328125 params 88.510928\n",
      "10/10/2022 07:37:19 - INFO - DENAS -   mutation 23/25 structure (9, 9, 576, 688, 2496) nas_score 182.14459228515625 params 67.080176\n",
      "10/10/2022 07:37:24 - INFO - DENAS -   mutation 24/25 structure (12, 12, 768, 688, 1344) nas_score 125.21382904052734 params 69.476048\n",
      "10/10/2022 07:37:29 - INFO - DENAS -   mutation 25/25 structure (10, 10, 640, 672, 2688) nas_score 227.6558074951172 params 74.726112\n",
      "10/10/2022 07:37:29 - INFO - DENAS -   mutation_num = 25\n",
      "10/10/2022 07:37:34 - INFO - DENAS -   crossover 1/25 structure (12, 12, 768, 544, 2112) nas_score 203.22036743164062 params 64.9016\n",
      "10/10/2022 07:37:39 - INFO - DENAS -   crossover 2/25 structure (12, 11, 704, 656, 1984) nas_score 245.7374725341797 params 74.291952\n",
      "10/10/2022 07:37:43 - INFO - DENAS -   crossover 3/25 structure (12, 8, 512, 496, 2496) nas_score 84.44166564941406 params 57.627536\n",
      "10/10/2022 07:37:47 - INFO - DENAS -   crossover 4/25 structure (9, 8, 512, 688, 2592) nas_score 192.52456665039062 params 66.683024\n",
      "10/10/2022 07:37:53 - INFO - DENAS -   crossover 5/25 structure (12, 11, 704, 704, 2784) nas_score 254.67593383789062 params 93.284544\n",
      "10/10/2022 07:37:59 - INFO - DENAS -   crossover 6/25 structure (12, 10, 640, 720, 2176) nas_score 209.92872619628906 params 82.687152\n",
      "10/10/2022 07:38:03 - INFO - DENAS -   crossover 7/25 structure (12, 12, 768, 512, 1920) nas_score 135.7181396484375 params 58.708992\n",
      "10/10/2022 07:38:08 - INFO - DENAS -   crossover 8/25 structure (9, 11, 704, 624, 2688) nas_score 152.36793518066406 params 65.84088\n",
      "10/10/2022 07:38:13 - INFO - DENAS -   crossover 9/25 structure (8, 8, 512, 736, 2528) nas_score 120.19763946533203 params 65.282592\n",
      "10/10/2022 07:38:19 - INFO - DENAS -   crossover 10/25 structure (12, 12, 768, 768, 2624) nas_score 236.24420166015625 params 101.219328\n",
      "10/10/2022 07:38:25 - INFO - DENAS -   crossover 11/25 structure (12, 9, 576, 672, 2816) nas_score 223.5365753173828 params 85.408608\n",
      "10/10/2022 07:38:29 - INFO - DENAS -   crossover 12/25 structure (11, 8, 512, 752, 2816) nas_score 253.05535888671875 params 87.533296\n",
      "10/10/2022 07:38:33 - INFO - DENAS -   crossover 13/25 structure (11, 11, 704, 752, 1408) nas_score 320.4615783691406 params 70.583088\n",
      "10/10/2022 07:38:39 - INFO - DENAS -   crossover 14/25 structure (10, 8, 512, 752, 2816) nas_score 196.9762725830078 params 81.749072\n",
      "10/10/2022 07:38:42 - INFO - DENAS -   crossover 15/25 structure (12, 11, 704, 704, 736) nas_score 159.0723114013672 params 58.65696\n",
      "10/10/2022 07:38:46 - INFO - DENAS -   crossover 16/25 structure (9, 10, 640, 672, 2528) nas_score 214.7054901123047 params 67.44768\n",
      "10/10/2022 07:38:49 - INFO - DENAS -   crossover 17/25 structure (10, 11, 704, 704, 1824) nas_score 269.6324462890625 params 67.935232\n",
      "10/10/2022 07:38:55 - INFO - DENAS -   crossover 18/25 structure (12, 10, 640, 768, 2624) nas_score 257.56976318359375 params 96.496128\n",
      "10/10/2022 07:38:59 - INFO - DENAS -   crossover 19/25 structure (9, 12, 768, 672, 2816) nas_score 207.1142578125 params 74.033952\n",
      "10/10/2022 07:39:06 - INFO - DENAS -   crossover 20/25 structure (12, 9, 576, 656, 2880) nas_score 183.73788452148438 params 84.374256\n",
      "10/10/2022 07:39:10 - INFO - DENAS -   crossover 21/25 structure (10, 11, 704, 768, 2144) nas_score 271.7447814941406 params 79.075136\n",
      "10/10/2022 07:39:14 - INFO - DENAS -   crossover 22/25 structure (10, 10, 640, 656, 2304) nas_score 265.11224365234375 params 67.8956\n",
      "10/10/2022 07:39:18 - INFO - DENAS -   crossover 23/25 structure (12, 10, 640, 624, 2528) nas_score 272.0043029785156 params 76.884624\n",
      "10/10/2022 07:39:21 - INFO - DENAS -   crossover 24/25 structure (12, 9, 576, 656, 1248) nas_score 197.30198669433594 params 58.660464\n",
      "10/10/2022 07:39:27 - INFO - DENAS -   crossover 25/25 structure (12, 11, 704, 576, 2624) nas_score 165.89894104003906 params 74.046912\n",
      "10/10/2022 07:39:27 - INFO - DENAS -   crossover_num = 25\n",
      "DE-NAS search best structure took 440.6090347841382 sec\n",
      "10/10/2022 07:39:27 - INFO - DENAS -   best structure (11, 11, 704, 752, 1408) nas_score 320.4615783691406 params 70.583088\n",
      "DE-NAS completed, best structure is (11, 11, 704, 752, 1408)\n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/aidk/DeNas && python -u search.py --domain bert --conf ../conf/denas/nlp/aidk_denas_bert.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Launch DE-NAS Training with TLK\n",
    "\n",
    "1. DE-NAS model wrapped with TLK\n",
    "\n",
    "``` python\n",
    "# ---bert_trainer.py---\n",
    "\n",
    "# import tlk\n",
    "import sys\n",
    "sys.path.append(\"/home/vmagent/app/aidk/AIDK/\")\n",
    "sys.path.append(\"/home/vmagent/app/aidk/AIDK/TransferLearningKit\")\n",
    "from TransferLearningKit.src.engine_core import transferrable_model\n",
    "from TransferLearningKit.src.engine_core.distiller import kd\n",
    "...\n",
    "...\n",
    "...\n",
    "class BertTrainer(BaseTrainer):\n",
    "    def __init__(self, args):\n",
    "        ...\n",
    "        ...\n",
    "        # construct teacher model builder\n",
    "        if self.args.is_transfer_learning:\n",
    "            self.teacher_model_builder = BertModelBuilder(self.args)\n",
    "            self.teacher_model_builder.model_dir = self.args.teacher_model_dir\n",
    "        ...\n",
    "        ...\n",
    "    def fit(self):\n",
    "        ...\n",
    "        ...\n",
    "        if self.args.is_transfer_learning:\n",
    "            # construct teacher model\n",
    "            self.teacher_model = self.teacher_model_builder.init_model()\n",
    "            self.teacher_model_config = self.teacher_model_builder.decode_arch(filename = \"best_model_structure_bert.txt\")\n",
    "            self.teacher_model.module.set_sample_config(self.teacher_model_config) if hasattr(model, 'module') \\\n",
    "            else self.teacher_model.set_sample_config(self.teacher_model_config)\n",
    "            # warp DE-NAS model with knowledge distillation\n",
    "            self.teacher_distiller = kd.KD(pretrained_model=self.teacher_model, is_frozen=True, temperature=4)\n",
    "            model = transferrable_model.make_transferrable_with_knowledge_distillation(model, model.loss, self.teacher_distiller, None, \"x\", True, 0.1, 0.9)\n",
    "        ...\n",
    "        ...\n",
    "```\n",
    "2. DE-NAS with TLK training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9/runpy.py:127: RuntimeWarning: 'intel_extension_for_pytorch.cpu.launch' found in sys.modules after import of package 'intel_extension_for_pytorch.cpu', but prior to execution of 'intel_extension_for_pytorch.cpu.launch'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-10-10 13:01:37,943 - __main__ - INFO - MASTER_ADDR=127.0.0.1\n",
      "2022-10-10 13:01:37,943 - __main__ - INFO - MASTER_PORT=29500\n",
      "2022-10-10 13:01:37,943 - __main__ - INFO - I_MPI_PIN_DOMAIN=[0xfffffffffff0,]\n",
      "2022-10-10 13:01:37,944 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /root/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance\n",
      "2022-10-10 13:01:37,944 - __main__ - INFO - OMP_NUM_THREADS=44\n",
      "2022-10-10 13:01:37,944 - __main__ - INFO - Using Intel OpenMP\n",
      "2022-10-10 13:01:37,944 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "2022-10-10 13:01:37,944 - __main__ - INFO - KMP_BLOCKTIME=1\n",
      "2022-10-10 13:01:37,945 - __main__ - INFO - LD_PRELOAD=/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/libiomp5.so\n",
      "2022-10-10 13:01:37,945 - __main__ - INFO - CCL_WORKER_COUNT=4\n",
      "2022-10-10 13:01:37,945 - __main__ - INFO - CCL_WORKER_AFFINITY=0,1,2,3\n",
      "2022-10-10 13:01:37,945 - __main__ - INFO - ['mpiexec.hydra', '-l', '-np', '1', '-ppn', '1', '-genv', 'I_MPI_PIN_DOMAIN=[0xfffffffffff0,]', '-genv', 'OMP_NUM_THREADS=44', '/opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/bin/python', '-u', './trainer/train.py', '--domain', 'bert', '--conf', '/home/vmagent/app/aidk/conf/denas/nlp/aidk_denas_train_bert.conf', '--do_lower_case', '--is_transfer_learning']\n",
      "[0] 10/10/2022 13:01:39 - INFO - module.nlp.tokenization -   loading vocabulary file\n",
      "[0] 10/10/2022 13:01:39 - INFO - trainer.data.nlp_build_datasets -   load 1027 examples!\n",
      "[0] 10/10/2022 13:01:41 - INFO - module.nlp.tokenization -   loading vocabulary file\n",
      "[0] 10/10/2022 13:01:41 - INFO - trainer.data.nlp_build_datasets -   load 1680 examples!\n",
      "[0] /opt/intel/oneapi/intelpython/latest/envs/pytorch-1.10.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "[0]   warnings.warn(_create_warning_msg(\n",
      "[0] loading archive file /home/vmagent/app/dataset/bert-base-uncased/\n",
      "[0] 10/10/2022 13:01:44 - INFO - nlp.supernet_bert -   Model config {\n",
      "[0]   \"architectures\": [\n",
      "[0]     \"BertForMaskedLM\"\n",
      "[0]   ],\n",
      "[0]   \"attention_probs_dropout_prob\": 0.1,\n",
      "[0]   \"gradient_checkpointing\": false,\n",
      "[0]   \"hidden_act\": \"gelu\",\n",
      "[0]   \"hidden_dropout_prob\": 0.1,\n",
      "[0]   \"hidden_size\": 768,\n",
      "[0]   \"initializer_range\": 0.02,\n",
      "[0]   \"intermediate_size\": 3072,\n",
      "[0]   \"layer_norm_eps\": 1e-12,\n",
      "[0]   \"max_position_embeddings\": 512,\n",
      "[0]   \"model_type\": \"bert\",\n",
      "[0]   \"num_attention_heads\": 12,\n",
      "[0]   \"num_hidden_layers\": 12,\n",
      "[0]   \"pad_token_id\": 0,\n",
      "[0]   \"position_embedding_type\": \"absolute\",\n",
      "[0]   \"transformers_version\": \"4.6.0.dev0\",\n",
      "[0]   \"type_vocab_size\": 2,\n",
      "[0]   \"use_cache\": true,\n",
      "[0]   \"vocab_size\": 30522\n",
      "[0] }\n",
      "[0] \n",
      "[0] 10/10/2022 13:01:44 - INFO - nlp.supernet_bert -   Model config {\n",
      "[0]   \"architectures\": [\n",
      "[0]     \"BertForMaskedLM\"\n",
      "[0]   ],\n",
      "[0]   \"attention_probs_dropout_prob\": 0.1,\n",
      "[0]   \"gradient_checkpointing\": false,\n",
      "[0]   \"hidden_act\": \"gelu\",\n",
      "[0]   \"hidden_dropout_prob\": 0.1,\n",
      "[0]   \"hidden_size\": 768,\n",
      "[0]   \"initializer_range\": 0.02,\n",
      "[0]   \"intermediate_size\": 3072,\n",
      "[0]   \"layer_norm_eps\": 1e-12,\n",
      "[0]   \"max_position_embeddings\": 512,\n",
      "[0]   \"model_type\": \"bert\",\n",
      "[0]   \"num_attention_heads\": 12,\n",
      "[0]   \"num_hidden_layers\": 12,\n",
      "[0]   \"pad_token_id\": 0,\n",
      "[0]   \"position_embedding_type\": \"absolute\",\n",
      "[0]   \"transformers_version\": \"4.6.0.dev0\",\n",
      "[0]   \"type_vocab_size\": 2,\n",
      "[0]   \"use_cache\": true,\n",
      "[0]   \"vocab_size\": 30522\n",
      "[0] }\n",
      "[0] \n",
      "[0] Weights of SuperBertForQuestionAnswering not initialized from pretrained model: ['bert.dense_fit.weight', 'bert.dense_fit.bias', 'qa_outputs.weight', 'qa_outputs.bias']\n",
      "[0] Weights from pretrained model not used in SuperBertForQuestionAnswering: ['bert.cls.predictions.bias', 'bert.cls.predictions.transform.dense.weight', 'bert.cls.predictions.transform.dense.bias', 'bert.cls.predictions.decoder.weight', 'bert.cls.seq_relationship.weight', 'bert.cls.seq_relationship.bias', 'bert.cls.predictions.transform.LayerNorm.weight', 'bert.cls.predictions.transform.LayerNorm.bias']\n",
      "[0] 10/10/2022 13:01:45 - INFO - model.nlp.bert_model_builder -   subbert_config: {'sample_layer_num': 11, 'sample_num_attention_heads': [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], 'sample_qkv_sizes': [704, 704, 704, 704, 704, 704, 704, 704, 704, 704, 704], 'sample_hidden_size': 768, 'sample_intermediate_sizes': [1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408]}\n",
      "[0] 10/10/2022 13:01:45 - INFO - model.nlp.bert_model_builder -   Total parameters: 72096320\n",
      "[0] loading archive file /home/vmagent/app/dataset/bert-base-uncased/\n",
      "[0] 10/10/2022 13:01:45 - INFO - nlp.supernet_bert -   Model config {\n",
      "[0]   \"architectures\": [\n",
      "[0]     \"BertForMaskedLM\"\n",
      "[0]   ],\n",
      "[0]   \"attention_probs_dropout_prob\": 0.1,\n",
      "[0]   \"gradient_checkpointing\": false,\n",
      "[0]   \"hidden_act\": \"gelu\",\n",
      "[0]   \"hidden_dropout_prob\": 0.1,\n",
      "[0]   \"hidden_size\": 768,\n",
      "[0]   \"initializer_range\": 0.02,\n",
      "[0]   \"intermediate_size\": 3072,\n",
      "[0]   \"layer_norm_eps\": 1e-12,\n",
      "[0]   \"max_position_embeddings\": 512,\n",
      "[0]   \"model_type\": \"bert\",\n",
      "[0]   \"num_attention_heads\": 12,\n",
      "[0]   \"num_hidden_layers\": 12,\n",
      "[0]   \"pad_token_id\": 0,\n",
      "[0]   \"position_embedding_type\": \"absolute\",\n",
      "[0]   \"transformers_version\": \"4.6.0.dev0\",\n",
      "[0]   \"type_vocab_size\": 2,\n",
      "[0]   \"use_cache\": true,\n",
      "[0]   \"vocab_size\": 30522\n",
      "[0] }\n",
      "[0] \n",
      "[0] 10/10/2022 13:01:45 - INFO - nlp.supernet_bert -   Model config {\n",
      "[0]   \"architectures\": [\n",
      "[0]     \"BertForMaskedLM\"\n",
      "[0]   ],\n",
      "[0]   \"attention_probs_dropout_prob\": 0.1,\n",
      "[0]   \"gradient_checkpointing\": false,\n",
      "[0]   \"hidden_act\": \"gelu\",\n",
      "[0]   \"hidden_dropout_prob\": 0.1,\n",
      "[0]   \"hidden_size\": 768,\n",
      "[0]   \"initializer_range\": 0.02,\n",
      "[0]   \"intermediate_size\": 3072,\n",
      "[0]   \"layer_norm_eps\": 1e-12,\n",
      "[0]   \"max_position_embeddings\": 512,\n",
      "[0]   \"model_type\": \"bert\",\n",
      "[0]   \"num_attention_heads\": 12,\n",
      "[0]   \"num_hidden_layers\": 12,\n",
      "[0]   \"pad_token_id\": 0,\n",
      "[0]   \"position_embedding_type\": \"absolute\",\n",
      "[0]   \"transformers_version\": \"4.6.0.dev0\",\n",
      "[0]   \"type_vocab_size\": 2,\n",
      "[0]   \"use_cache\": true,\n",
      "[0]   \"vocab_size\": 30522\n",
      "[0] }\n",
      "[0] \n",
      "[0] Weights of SuperBertForQuestionAnswering not initialized from pretrained model: ['bert.dense_fit.weight', 'bert.dense_fit.bias', 'qa_outputs.weight', 'qa_outputs.bias']\n",
      "[0] Weights from pretrained model not used in SuperBertForQuestionAnswering: ['bert.cls.predictions.bias', 'bert.cls.predictions.transform.dense.weight', 'bert.cls.predictions.transform.dense.bias', 'bert.cls.predictions.decoder.weight', 'bert.cls.seq_relationship.weight', 'bert.cls.seq_relationship.bias', 'bert.cls.predictions.transform.LayerNorm.weight', 'bert.cls.predictions.transform.LayerNorm.bias']\n",
      "[0] 10/10/2022 13:01:47 - INFO - model.nlp.bert_model_builder -   subbert_config: {'sample_layer_num': 12, 'sample_num_attention_heads': [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12], 'sample_qkv_sizes': [768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768], 'sample_hidden_size': 768, 'sample_intermediate_sizes': [3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072]}\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Use model.loss\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Set distiller_feature_size for model\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Set adapter_feature_size for model\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Use new_model.loss\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Use new_model.distiller_feature_size\n",
      "[0] 10/10/2022 13:01:47 - INFO - root -   Use new_model.adapter_feature_size\n",
      "[0]   Num examples = 1027\n",
      "[0]   Batch size = 12\n",
      "[0]   Num steps = 171\n",
      "[0] 10/10/2022 13:01:47 - INFO - model.nlp.bert_trainer -   ***** Running training *****\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s][0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A[0] /home/vmagent/app/aidk/DeNas/module/nlp/optimization.py:249: UserWarning: This overload of add_ is deprecated:\n",
      "[0] \tadd_(Number alpha, Tensor other)\n",
      "[0] Consider using one of the following signatures instead:\n",
      "[0] \tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "[0]   next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "[0] \n",
      "Iteration:   1%|1         | 1/88 [00:09<13:42,  9.45s/it][0] \u001b[A[0] \n",
      "Iteration:   2%|2         | 2/88 [00:12<08:23,  5.86s/it]\u001b[A[0] \n",
      "Iteration:   3%|3         | 3/88 [00:16<07:01,  4.96s/it]\u001b[A[0] \n",
      "Iteration:   5%|4         | 4/88 [00:19<05:40,  4.05s/it]\u001b[A[0] \n",
      "Iteration:   6%|5         | 5/88 [00:21<04:47,  3.46s/it]\u001b[A[0] \n",
      "Iteration:   7%|6         | 6/88 [00:24<04:13,  3.09s/it][0] \u001b[A[0] \n",
      "Iteration:   8%|7         | 7/88 [00:26<03:52,  2.87s/it]\u001b[A[0] \n",
      "Iteration:   9%|9         | 8/88 [00:29<03:40,  2.75s/it]\u001b[A[0] \n",
      "Iteration:  10%|#         | 9/88 [00:31<03:33,  2.70s/it]\u001b[A[0] \n",
      "Iteration:  11%|#1        | 10/88 [00:34<03:25,  2.63s/it]\u001b[A[0] \n",
      "Iteration:  12%|#2        | 11/88 [00:36<03:16,  2.55s/it]\u001b[A[0] \n",
      "Iteration:  14%|#3        | 12/88 [00:38<03:09,  2.49s/it]\u001b[A[0] \n",
      "Iteration:  15%|#4        | 13/88 [00:41<03:03,  2.44s/it]\u001b[A[0] \n",
      "Iteration:  16%|#5        | 14/88 [00:43<02:59,  2.42s/it]\u001b[A[0] \n",
      "Iteration:  17%|#7        | 15/88 [00:45<02:54,  2.40s/it]\u001b[A[0] \n",
      "Iteration:  18%|#8        | 16/88 [00:48<03:03,  2.55s/it]\u001b[A[0] \n",
      "Iteration:  19%|#9        | 17/88 [00:51<02:59,  2.53s/it]\u001b[A[0] \n",
      "Iteration:  20%|##        | 18/88 [00:53<02:54,  2.50s/it]\u001b[A[0] \n",
      "Iteration:  22%|##1       | 19/88 [00:56<02:50,  2.47s/it]\u001b[A[0] \n",
      "Iteration:  23%|##2       | 20/88 [00:58<02:45,  2.44s/it]\u001b[A[0] \n",
      "Iteration:  24%|##3       | 21/88 [01:00<02:43,  2.44s/it]\u001b[A[0] \n",
      "Iteration:  25%|##5       | 22/88 [01:03<02:48,  2.56s/it]\u001b[A[0] \n",
      "Iteration:  26%|##6       | 23/88 [01:06<02:44,  2.54s/it]\u001b[A[0] \n",
      "Iteration:  27%|##7       | 24/88 [01:08<02:40,  2.50s/it]\u001b[A[0] \n",
      "Iteration:  28%|##8       | 25/88 [01:10<02:34,  2.45s/it]\u001b[A[0] \n",
      "Iteration:  30%|##9       | 26/88 [01:13<02:29,  2.41s/it]\u001b[A[0] \n",
      "Iteration:  31%|###       | 27/88 [01:15<02:26,  2.41s/it]\u001b[A[0] \n",
      "Iteration:  32%|###1      | 28/88 [01:18<02:24,  2.40s/it]\u001b[A[0] \n",
      "Iteration:  33%|###2      | 29/88 [01:20<02:27,  2.49s/it]\u001b[A[0] \n",
      "Iteration:  34%|###4      | 30/88 [01:23<02:24,  2.48s/it]\u001b[A[0] \n",
      "Iteration:  35%|###5      | 31/88 [01:25<02:18,  2.42s/it]\u001b[A[0] \n",
      "Iteration:  36%|###6      | 32/88 [01:27<02:13,  2.38s/it]\u001b[A[0] \n",
      "Iteration:  38%|###7      | 33/88 [01:30<02:09,  2.36s/it]\u001b[A[0] \n",
      "Iteration:  39%|###8      | 34/88 [01:32<02:07,  2.36s/it][0] \u001b[A[0] \n",
      "Iteration:  40%|###9      | 35/88 [01:34<02:04,  2.36s/it]\u001b[A[0] \n",
      "Iteration:  41%|####      | 36/88 [01:37<02:02,  2.35s/it]\u001b[A[0] \n",
      "Iteration:  42%|####2     | 37/88 [01:39<01:59,  2.34s/it]\u001b[A[0] \n",
      "Iteration:  43%|####3     | 38/88 [01:42<02:02,  2.46s/it]\u001b[A[0] \n",
      "Iteration:  44%|####4     | 39/88 [01:44<02:01,  2.47s/it]\u001b[A[0] \n",
      "Iteration:  45%|####5     | 40/88 [01:47<01:59,  2.49s/it][0] \u001b[A[0] \n",
      "Iteration:  47%|####6     | 41/88 [01:49<01:56,  2.47s/it][0] \u001b[A[0] \n",
      "Iteration:  48%|####7     | 42/88 [01:51<01:51,  2.43s/it][0] \u001b[A[0] \n",
      "Iteration:  49%|####8     | 43/88 [01:54<01:46,  2.37s/it][0] \u001b[A[0] \n",
      "Iteration:  50%|#####     | 44/88 [01:56<01:42,  2.34s/it][0] \u001b[A[0] \n",
      "Iteration:  51%|#####1    | 45/88 [01:58<01:39,  2.31s/it][0] \u001b[A[0] \n",
      "Iteration:  52%|#####2    | 46/88 [02:00<01:36,  2.30s/it][0] \u001b[A[0] \n",
      "Iteration:  53%|#####3    | 47/88 [02:03<01:33,  2.29s/it][0] \u001b[A[0] \n",
      "Iteration:  55%|#####4    | 48/88 [02:06<01:37,  2.44s/it][0] \u001b[A[0] 10/10/2022 13:03:55 - INFO - model.nlp.bert_trainer -   ***** Running evaluation *****\n",
      "[0] 10/10/2022 13:03:55 - INFO - model.nlp.bert_trainer -     Epoch = 0 iter 49 step\n",
      "[0] 10/10/2022 13:03:55 - INFO - model.nlp.bert_trainer -     Num examples = 1680\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -   ***** Eval results *****\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     cls_loss = 0.5925437479603047\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     em = 8.988095238095237\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     f1 = 13.238008435468874\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     global_step = 49\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     infer_cnt = 54\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     infer_time = 784.5092592592592\n",
      "[0] 10/10/2022 13:05:50 - INFO - model.nlp.utils -     loss = 0.5925437479603047\n",
      "[0] 10/10/2022 13:05:50 - INFO - root -   ** ** * Saving fine-tuned model ** ** * \n",
      "[0] \n",
      "Iteration:  56%|#####5    | 49/88 [04:04<24:15, 37.32s/it][0] \u001b[A[0] \n",
      "Iteration:  57%|#####6    | 50/88 [04:07<16:58, 26.80s/it][0] \u001b[A[0] \n",
      "Iteration:  58%|#####7    | 51/88 [04:09<11:56, 19.37s/it][0] \u001b[A[0] \n",
      "Iteration:  59%|#####9    | 52/88 [04:10<08:29, 14.14s/it][0] \u001b[A[0] \n",
      "Iteration:  60%|######    | 53/88 [04:12<06:07, 10.49s/it][0] \u001b[A[0] \n",
      "Iteration:  61%|######1   | 54/88 [04:14<04:29,  7.93s/it][0] \u001b[A[0] \n",
      "Iteration:  62%|######2   | 55/88 [04:16<03:22,  6.15s/it][0] \u001b[A[0] \n",
      "Iteration:  64%|######3   | 56/88 [04:19<02:38,  4.94s/it][0] \u001b[A[0] \n",
      "Iteration:  65%|######4   | 57/88 [04:21<02:06,  4.08s/it][0] \u001b[A[0] \n",
      "Iteration:  66%|######5   | 58/88 [04:23<01:43,  3.45s/it][0] \u001b[A[0] \n",
      "Iteration:  67%|######7   | 59/88 [04:25<01:27,  3.02s/it][0] \u001b[A[0] \n",
      "Iteration:  68%|######8   | 60/88 [04:27<01:16,  2.73s/it][0] \u001b[A[0] \n",
      "Iteration:  69%|######9   | 61/88 [04:29<01:07,  2.51s/it][0] \u001b[A[0] \n",
      "Iteration:  70%|#######   | 62/88 [04:31<01:01,  2.35s/it][0] \u001b[A[0] \n",
      "Iteration:  72%|#######1  | 63/88 [04:33<00:55,  2.23s/it][0] \u001b[A[0] \n",
      "Iteration:  73%|#######2  | 64/88 [04:35<00:51,  2.15s/it][0] \u001b[A[0] \n",
      "Iteration:  74%|#######3  | 65/88 [04:37<00:48,  2.10s/it][0] \u001b[A[0] \n",
      "Iteration:  75%|#######5  | 66/88 [04:38<00:45,  2.06s/it][0] \u001b[A[0] \n",
      "Iteration:  76%|#######6  | 67/88 [04:40<00:42,  2.03s/it][0] \u001b[A[0] \n",
      "Iteration:  77%|#######7  | 68/88 [04:42<00:40,  2.01s/it][0] \u001b[A[0] \n",
      "Iteration:  78%|#######8  | 69/88 [04:44<00:37,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  80%|#######9  | 70/88 [04:46<00:35,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  81%|########  | 71/88 [04:48<00:33,  1.97s/it][0] \u001b[A[0] \n",
      "Iteration:  82%|########1 | 72/88 [04:50<00:31,  1.97s/it][0] \u001b[A[0] \n",
      "Iteration:  83%|########2 | 73/88 [04:52<00:29,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  84%|########4 | 74/88 [04:54<00:27,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  85%|########5 | 75/88 [04:56<00:25,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  86%|########6 | 76/88 [04:58<00:23,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  88%|########7 | 77/88 [05:00<00:21,  1.97s/it][0] \u001b[A[0] \n",
      "Iteration:  89%|########8 | 78/88 [05:02<00:19,  1.97s/it][0] \u001b[A[0] \n",
      "Iteration:  90%|########9 | 79/88 [05:04<00:17,  1.96s/it][0] \u001b[A[0] \n",
      "Iteration:  91%|######### | 80/88 [05:06<00:15,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  92%|#########2| 81/88 [05:08<00:13,  1.97s/it][0] \u001b[A[0] \n",
      "Iteration:  93%|#########3| 82/88 [05:10<00:11,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  94%|#########4| 83/88 [05:12<00:09,  1.98s/it][0] \u001b[A[0] \n",
      "Iteration:  95%|#########5| 84/88 [05:14<00:08,  2.08s/it][0] \u001b[A[0] \n",
      "Iteration:  97%|#########6| 85/88 [05:16<00:06,  2.08s/it][0] \u001b[A[0] \n",
      "Iteration:  98%|#########7| 86/88 [05:18<00:04,  2.06s/it][0] \u001b[A[0] \n",
      "Iteration:  99%|#########8| 87/88 [05:20<00:02,  2.07s/it][0] \u001b[A[0] \n",
      "Iteration: 100%|##########| 88/88 [05:23<00:00,  3.67s/it][0] \u001b[A[0] \n",
      "Epoch:  50%|█████     | 1/2 [05:23<05:23, 323.15s/it][0] \n",
      "Iteration:   0%|          | 0/88 [00:00<?, ?it/s][0] \u001b[A[0] \n",
      "Iteration:   1%|1         | 1/88 [00:12<17:28, 12.05s/it][0] \u001b[A[0] \n",
      "Iteration:   2%|2         | 2/88 [00:14<09:03,  6.32s/it]\u001b[A[0] \n",
      "Iteration:   3%|3         | 3/88 [00:16<06:21,  4.49s/it][0] \u001b[A[0] \n",
      "Iteration:   5%|4         | 4/88 [00:18<05:03,  3.62s/it][0] \u001b[A[0] \n",
      "Iteration:   6%|5         | 5/88 [00:21<04:20,  3.14s/it][0] \u001b[A[0] \n",
      "Iteration:   7%|6         | 6/88 [00:23<03:58,  2.91s/it][0] \u001b[A[0] \n",
      "Iteration:   8%|7         | 7/88 [00:25<03:38,  2.70s/it][0] \u001b[A[0] \n",
      "Iteration:   9%|9         | 8/88 [00:28<03:24,  2.56s/it][0] \u001b[A[0] \n",
      "Iteration:  10%|#         | 9/88 [00:30<03:14,  2.47s/it][0] \u001b[A[0] \n",
      "Iteration:  11%|#1        | 10/88 [00:32<03:07,  2.40s/it][0] \u001b[A[0] 10/10/2022 13:07:45 - INFO - model.nlp.bert_trainer -   ***** Running evaluation *****\n",
      "[0] 10/10/2022 13:07:45 - INFO - model.nlp.bert_trainer -     Epoch = 1 iter 99 step\n",
      "[0] 10/10/2022 13:07:45 - INFO - model.nlp.bert_trainer -     Num examples = 1680\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -   ***** Eval results *****\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     cls_loss = 0.5066110789775848\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     em = 8.392857142857142\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     f1 = 12.822193941418774\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     global_step = 99\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     infer_cnt = 54\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     infer_time = 845.5888333333331\n",
      "[0] 10/10/2022 13:09:43 - INFO - model.nlp.utils -     loss = 0.5066110789775848\n",
      "[0] \n",
      "Iteration:  12%|#2        | 11/88 [02:32<49:15, 38.39s/it][0] \u001b[A[0] \n",
      "Iteration:  14%|#3        | 12/88 [02:35<34:43, 27.41s/it][0] \u001b[A[0] \n",
      "Iteration:  15%|#4        | 13/88 [02:37<24:40, 19.74s/it][0] \u001b[A[0] \n",
      "Iteration:  16%|#5        | 14/88 [02:39<17:43, 14.37s/it][0] \u001b[A[0] \n",
      "Iteration:  17%|#7        | 15/88 [02:41<12:55, 10.63s/it][0] \u001b[A[0] \n",
      "Iteration:  18%|#8        | 16/88 [02:43<09:37,  8.02s/it][0] \u001b[A[0] \n",
      "Iteration:  19%|#9        | 17/88 [02:44<07:19,  6.20s/it][0] \u001b[A[0] \n",
      "Iteration:  20%|##        | 18/88 [02:46<05:45,  4.93s/it][0] \u001b[A[0] \n",
      "Iteration:  22%|##1       | 19/88 [02:48<04:38,  4.04s/it][0] \u001b[A[0] \n",
      "Iteration:  23%|##2       | 20/88 [02:50<03:52,  3.41s/it][0] \u001b[A[0] \n",
      "Iteration:  24%|##3       | 21/88 [02:52<03:19,  2.98s/it][0] \u001b[A[0] \n",
      "Iteration:  25%|##5       | 22/88 [02:54<02:56,  2.68s/it][0] \u001b[A[0] \n",
      "Iteration:  26%|##6       | 23/88 [02:56<02:40,  2.47s/it][0] \u001b[A[0] \n",
      "Iteration:  27%|##7       | 24/88 [02:58<02:28,  2.32s/it][0] \u001b[A[0] \n",
      "Iteration:  28%|##8       | 25/88 [03:00<02:19,  2.21s/it][0] \u001b[A[0] \n",
      "Iteration:  30%|##9       | 26/88 [03:02<02:13,  2.15s/it][0] \u001b[A[0] \n",
      "Iteration:  31%|###       | 27/88 [03:04<02:08,  2.11s/it][0] \u001b[A[0] \n",
      "Iteration:  32%|###1      | 28/88 [03:06<02:03,  2.06s/it][0] \u001b[A[0] \n",
      "Iteration:  33%|###2      | 29/88 [03:08<02:00,  2.03s/it][0] \u001b[A[0] \n",
      "Iteration:  34%|###4      | 30/88 [03:10<01:56,  2.01s/it][0] \u001b[A[0] \n",
      "Iteration:  35%|###5      | 31/88 [03:12<01:59,  2.10s/it][0] \u001b[A[0] \n",
      "Iteration:  36%|###6      | 32/88 [03:15<01:56,  2.08s/it][0] \u001b[A[0] \n",
      "Iteration:  38%|###7      | 33/88 [03:16<01:53,  2.06s/it][0] \u001b[A[0] \n",
      "Iteration:  39%|###8      | 34/88 [03:19<01:51,  2.06s/it][0] \u001b[A[0] \n",
      "Iteration:  40%|###9      | 35/88 [03:21<01:48,  2.04s/it][0] \u001b[A[0] \n",
      "Iteration:  41%|####      | 36/88 [03:23<01:45,  2.03s/it][0] \u001b[A[0] \n",
      "Iteration:  42%|####2     | 37/88 [03:25<01:43,  2.03s/it][0] \u001b[A[0] \n",
      "Iteration:  43%|####3     | 38/88 [03:27<01:40,  2.01s/it][0] \u001b[A[0] \n",
      "Iteration:  44%|####4     | 39/88 [03:29<01:37,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  45%|####5     | 40/88 [03:30<01:35,  1.99s/it]\u001b[A[0] \n",
      "Iteration:  47%|####6     | 41/88 [03:32<01:33,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  48%|####7     | 42/88 [03:35<01:32,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  49%|####8     | 43/88 [03:37<01:29,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  50%|#####     | 44/88 [03:39<01:28,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  51%|#####1    | 45/88 [03:40<01:25,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  52%|#####2    | 46/88 [03:42<01:23,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  53%|#####3    | 47/88 [03:44<01:21,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  55%|#####4    | 48/88 [03:46<01:19,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  56%|#####5    | 49/88 [03:48<01:17,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  57%|#####6    | 50/88 [03:50<01:16,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  58%|#####7    | 51/88 [03:52<01:14,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  59%|#####9    | 52/88 [03:54<01:11,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  60%|######    | 53/88 [03:56<01:10,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  61%|######1   | 54/88 [03:58<01:07,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  62%|######2   | 55/88 [04:00<01:05,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  64%|######3   | 56/88 [04:02<01:03,  1.99s/it][0] \u001b[A[0] \n",
      "Iteration:  65%|######4   | 57/88 [04:04<01:01,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  66%|######5   | 58/88 [04:06<01:00,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  67%|######7   | 59/88 [04:08<00:57,  2.00s/it][0] \u001b[A[0] \n",
      "Iteration:  68%|######8   | 60/88 [04:10<00:55,  1.99s/it][0] \u001b[A[0] 10/10/2022 13:11:23 - INFO - model.nlp.bert_trainer -   ***** Running evaluation *****\n",
      "[0] 10/10/2022 13:11:23 - INFO - model.nlp.bert_trainer -     Epoch = 1 iter 149 step\n",
      "[0] 10/10/2022 13:11:23 - INFO - model.nlp.bert_trainer -     Num examples = 1680\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -   ***** Eval results *****\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     cls_loss = 0.5030578531202723\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     em = 10.178571428571429\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     f1 = 14.333607124473287\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     global_step = 149\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     infer_cnt = 54\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     infer_time = 792.659111111111\n",
      "[0] 10/10/2022 13:13:15 - INFO - model.nlp.utils -     loss = 0.5030578531202723\n",
      "[0] 10/10/2022 13:13:15 - INFO - root -   ** ** * Saving fine-tuned model ** ** * \n",
      "[0] \n",
      "Iteration:  69%|######9   | 61/88 [06:06<16:10, 35.95s/it][0] \u001b[A[0] \n",
      "Iteration:  70%|#######   | 62/88 [06:08<11:11, 25.84s/it][0] \u001b[A[0] \n",
      "Iteration:  72%|#######1  | 63/88 [06:10<07:47, 18.72s/it][0] \u001b[A[0] \n",
      "Iteration:  73%|#######2  | 64/88 [06:12<05:28, 13.70s/it][0] \u001b[A[0] \n",
      "Iteration:  74%|#######3  | 65/88 [06:14<03:54, 10.19s/it][0] \u001b[A[0] \n",
      "Iteration:  75%|#######5  | 66/88 [06:16<02:50,  7.74s/it][0] \u001b[A[0] \n",
      "Iteration:  76%|#######6  | 67/88 [06:18<02:06,  6.03s/it][0] \u001b[A[0] \n",
      "Iteration:  77%|#######7  | 68/88 [06:20<01:36,  4.83s/it][0] \u001b[A[0] \n",
      "Iteration:  78%|#######8  | 69/88 [06:22<01:17,  4.07s/it][0] \u001b[A[0] \n",
      "Iteration:  80%|#######9  | 70/88 [06:24<01:02,  3.49s/it][0] \u001b[A[0] \n",
      "Iteration:  81%|########  | 71/88 [06:27<00:52,  3.06s/it][0] \u001b[A[0] \n",
      "Iteration:  82%|########1 | 72/88 [06:29<00:44,  2.78s/it][0] \u001b[A[0] \n",
      "Iteration:  83%|########2 | 73/88 [06:31<00:38,  2.58s/it][0] \u001b[A[0] \n",
      "Iteration:  84%|########4 | 74/88 [06:33<00:34,  2.44s/it][0] \u001b[A[0] \n",
      "Iteration:  85%|########5 | 75/88 [06:35<00:30,  2.32s/it][0] \u001b[A[0] \n",
      "Iteration:  86%|########6 | 76/88 [06:37<00:26,  2.23s/it][0] \u001b[A[0] \n",
      "Iteration:  88%|########7 | 77/88 [06:39<00:23,  2.16s/it][0] \u001b[A[0] \n",
      "Iteration:  89%|########8 | 78/88 [06:41<00:21,  2.13s/it][0] \u001b[A[0] \n",
      "Iteration:  90%|########9 | 79/88 [06:43<00:18,  2.10s/it][0] \u001b[A[0] \n",
      "Iteration:  91%|######### | 80/88 [06:45<00:16,  2.08s/it][0] \u001b[A[0] \n",
      "Iteration:  92%|#########2| 81/88 [06:47<00:14,  2.07s/it][0] \u001b[A[0] \n",
      "Iteration:  93%|#########3| 82/88 [06:49<00:12,  2.05s/it][0] \u001b[A[0] \n",
      "Iteration:  94%|#########4| 83/88 [06:51<00:10,  2.05s/it][0] \u001b[A[0] 10/10/2022 13:14:04 - WARNING - module.nlp.optimization -   Training beyond specified 't_total' steps with schedule. Learning rate set. Please set 't_total' correctly.\n",
      "[0] \n",
      "Iteration:  95%|#########5| 84/88 [06:53<00:08,  2.04s/it][0] \u001b[A[0] 10/10/2022 13:14:06 - WARNING - module.nlp.optimization -   Training beyond specified 't_total' steps with schedule. Learning rate set. Please set 't_total' correctly.\n",
      "[0] \n",
      "Iteration:  97%|#########6| 85/88 [06:55<00:06,  2.03s/it][0] \u001b[A[0] 10/10/2022 13:14:08 - WARNING - module.nlp.optimization -   Training beyond specified 't_total' steps with schedule. Learning rate set. Please set 't_total' correctly.\n",
      "[0] \n",
      "Iteration:  98%|#########7| 86/88 [06:57<00:04,  2.02s/it][0] \u001b[A[0] 10/10/2022 13:14:10 - WARNING - module.nlp.optimization -   Training beyond specified 't_total' steps with schedule. Learning rate set. Please set 't_total' correctly.\n",
      "[0] \n",
      "Iteration:  99%|#########8| 87/88 [06:59<00:02,  2.03s/it][0] \u001b[A[0] 10/10/2022 13:14:12 - WARNING - module.nlp.optimization -   Training beyond specified 't_total' steps with schedule. Learning rate set. Please set 't_total' correctly.\n",
      "[0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|##########| 88/88 [07:01<00:00,  2.02s/it][0] \u001b[A[0] \r\n",
      "Iteration: 100%|##########| 88/88 [07:02<00:00,  4.80s/it]\r\n",
      "[0] \r\n",
      "Epoch: 100%|██████████| 2/2 [12:25<00:00, 381.39s/it][0] \r\n",
      "Epoch: 100%|██████████| 2/2 [12:25<00:00, 372.65s/it]\r\n",
      "[0] **************S*************\r\n",
      "[0] task_name = squad1\r\n",
      "[0] architecture = {'sample_layer_num': 11, 'sample_num_attention_heads': [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], 'sample_qkv_sizes': [704, 704, 704, 704, 704, 704, 704, 704, 704, 704, 704], 'sample_hidden_size': 768, 'sample_intermediate_sizes': [1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408]}\r\n",
      "[0] parameter size = 72096320\r\n",
      "[0] total training time = 745.3035025596619\r\n",
      "[0] best_acc = f1: 14.333607124473287; em: 10.178571428571429\r\n",
      "[0] time_per_batch_infer = 807.586 ms\r\n",
      "[0] infer_cnt = 162\r\n",
      "[0] **************E*************\r\n",
      "[0] \r\n"
     ]
    }
   ],
   "source": [
    "!cd /home/vmagent/app/aidk/DeNas && python -m intel_extension_for_pytorch.cpu.launch --distributed --nproc_per_node=1 --nnodes=1 ./trainer/train.py --domain bert --conf /home/vmagent/app/aidk/conf/denas/nlp/aidk_denas_train_bert.conf --do_lower_case --is_transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE-NAS with TLK performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./img/DENAS_performance.png\" width=\"500\"/><figure>DE-NAS Performance</figure>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/DENAS_W_TLK_performance.png\" width=\"500\"/><figure>DE-NAS Performance with TLK</figure>\n",
    "</center>\n",
    "\n",
    "* As shown in the above two figures:\n",
    "    * Models in DE-NAS can deliver lighter and training speedup.\n",
    "    * Furthermore, DE-NAS with TLK delivered higher F1 score in almost all steps within one epoch, which demonstrates that TLK helps DE-NAS models to get faster convergence and achieve better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the DE-NAS w/wo TLK Model to Hugging Face\n",
    "* Through the DE-NAS w/wo TLK model, we can optimize the model from Hugging Face to a lighter and faster DE-NAS model with the similar or higher F1 score, which can be uploaded into the Hugging Face and expected to help easily deployment into the hardware.\n",
    "* Below figure shows that the uploading process to Hugging Face Personal repo, which can be acted as the github ops (etc., submitted the PR) to Intel open repo.\n",
    "    * Step 1: create the model repo\n",
    "\n",
    "    <center>\n",
    "    <img src=\"./img/Create_HuggingFace_Model.png\" width=\"500\"/><figure>Create Model Repo in Hugging Face</figure>\n",
    "    </center>\n",
    "\n",
    "    * Step 2: upload the model files\n",
    "    <center>\n",
    "    <img src=\"./img/Upload_HuggingFace_Model.png\" width=\"800\"/><figure>Upload Model into Hugging Face Repo</figure>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id=\"5\"></p> \n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DE-NAS automatically designs a well-performed and compact BERT.\n",
    "* TLK helps DE-NAS BERT in the fine-tuning stage to further improve its performance.\n",
    "* Hugging Face is as the source to offer models to DE-NAS and TLK to do optimization, and as the repo to contain the optimized model for broader usage.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./img/Overall_Workflow.png\" width=\"600\"/><figure>Overall Workflow</figure>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
