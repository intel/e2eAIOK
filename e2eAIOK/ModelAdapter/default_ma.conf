############################## Global Setting ##########################
# basic setting
seed: 0
output_dir: "/home/vmagent/app/data/model/"
tensorboard_dir: "/home/vmagent/app/data/tensorboard"
model_save_interval: 40
log_interval_step: 10
start_epoch: 0

# Experiment setting
experiment:
    project: "Model Adapter"
    tag: "default"
    strategy: ""

# Dataset
ddp_eval_nosplit: True
train_transform: "default"
test_transform: "default"

# Source Dataset - to be added

# Model
initial_pretrain: ""
pretrain: ""  
 
# finetuner
finetuner:
    type: ""
    initial_pretrain: ""
    pretrain: ""
    pretrained_num_classes: 10
    finetuned_lr: 0.01
    frozen: False

# distiller
distiller:
    type: ""  # Vanilla as default
    teacher:
        type: ""
        initial_pretrain: ""
        pretrain: ""
        frozen: True
    save_logits: False
    use_saved_logits: False
    check_logits: False
    logits_path: ""
    logits_topk: 0
    save_logits_start_epoch: 0

# adapter
adapter:
    type: ""
    feature_size: 1
    feature_layer_name: "x"

# loss
loss_weight:
    backbone: 1.0
    distiller: 0.0
    adapter: 0.0

######################## For Distiller #####################
# kd cfg
kd:
    temperature: 4

# dkd(Decoupled Knowledge Distillation) cfg
dkd:
    alpha: 1.0
    beta: 8.0
    temperature: 4.0
    warmup: 20